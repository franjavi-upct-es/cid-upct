\section{Vectores aleatorios}
\subsection{Introducción}
Una \va multidimensional o vector aleatorio es una aplicación $\vec{x}:\Omega\longrightarrow\R$ tal que cada componente \[ w\longrightarrow \vec{x}(w)=(X_1(w),\dots,X_n(w)) \]$x_i$ es una \va unidimensional.\\
Para $n=2$, usaremos $(X,Y)$ \va bidimensional.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Llamaremos distribución \lb{conjunta} a la distribución de $(X,Y)$ y llamaremos marginales a las distribuciones individuales de $X$ e $Y$.
\begin{itemize}
	\item Función de \lb{distribución conjunta} de $(X,Y)$\[ F_{(X,Y)}(x,y)=P(X\le x,Y\le y)=P\left((X\le x)\cap(Y\le y)\right)\:\forall x,y\in\R^2\]
	\item Función de \lb{distribución marginal de $X$:}\[ F_X(x)=P(X\le x)=F_{(X,Y)}(x,+\infty) \]
	\item Función de \lb{distribución marginal de $Y$:}\[ F_Y(y)=P(Y\le y)=F_{(X,Y)}(+\infty,y) \]
\end{itemize}
\subsection{Distribución conjunta y marginales}
\begin{enumerate}[label=\color{red}\textbf{\Alph*)}, leftmargin=*]
	\item \lb{Caso discreto}
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Definición
	\end{itemize}
	Sean $X$ e $Y$ \vas discreta, se define la función puntual de probabilidad (\fpp) conjunta.\[ P_{(X,Y)}(x,y)=P(X=x,Y=y)=P\left((X=x)\cap(Y=y)\right)\:\forall x,y\in\R^2 \]
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Propiedades
	\end{itemize}
	\begin{enumerate}[label=\color{lightblue}\arabic*)]
		\item $P_{(X,Y)}(x,y)\ge0\;\forall x,y\in\R^2$
		\item $\sum_{(x,y)\in\sop(X,Y)}P_{(X,Y)}(x,y)=1$
\begin{itemize}[leftmargin=*]
	\item \Fpp conjunta: $P_{(X,Y)}(x,y)=P(X=x,Y=y)\:\forall(x,y)\in\R^2$
	\item \Fpp marginal de $X:\,p_X(x)=P(X=x)=\sum_{y\in\sop(Y)}P_{(X,Y)}(x,y)$
	\item \Fpp marginal de $Y:\;p_Y(y)=P(Y=y)=\sum_{x\in\sop(X)}P_{(X,Y)}(x,y)$
\end{itemize}
En general, para calcular la probabilidad en un recinto de $\R^2$ haremos: $P((X,Y)\in A)=\sum_{(x,y)\in A}P_{(X,Y)}(x,y),\:A\subseteq\R^2$
	\end{enumerate}
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Definición
	\end{itemize}
	Sea $g:\R^2\longrightarrow\R$ tal que $g(X,Y)$ es \va unidimensional. Entonces $E(g(X,Y))=\sum_{(x,y)\in\sop(X,Y)}g(x,y)\cdot P_{(X,Y)}(x,y)$
	
	\Ej
	
	$E(X\cdot Y)=\sum_{(x,y)\in\sop(X,Y)}x\cdot y\cdot P_{(X,Y)}(x,y)$
	\item \lb{Caso continuo}
	
	$(X,Y)$ \va bidireccional con $X$ e $Y$ continuas.
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Definición
	\end{itemize}
	Diremos que $f_{(X,Y)}:\R^2\longrightarrow\R$ es la función de densidad conjunta de $(X,Y)$ si permite obtener la distribución integrando \[ F_{(X,Y)}(x,y)=P(X\le x,Y\le y)=\int_{-\infty}^{y}\left(\int_{-\infty}^{x}f_{(X,Y)}(u,v)\du\right)\dv \]La densidad conjunta permite obtener probabilidades en regiones de $\R^2$\[ P((X,Y)\in A)=\iint_Af_{(X,Y)}(x,y)\dx\dy \]
	\begin{itemize}[leftmargin=*]
		\item Densidad marginal de $X$:\[ f_X(x)=\int_{y=-\infty}^{y=+\infty}f_{(X,Y)}(x,y)\dy=\int_{y\in\sop(Y)}f_{(X,Y)}(x,y)\dy=\int_{-\infty}^{+\infty}f_{(X,Y)}(x,y)\dy \]
		\item Densidad marginal de $Y$:\[ f_Y(y)=\int_{\R} f_{(X,Y)}(x,y)\dx \]
		\item Función de distribución marginal de $X:\,F_X(x)=P(X\le x)=\int_{u=-\infty}^{u=x}\int_{y\in\R}f_{(X,Y)}(u,y)\du\dy$
		\item Función de distribución marginal de $Y:\,F_Y(y)=P(Y\le y)=\int_{v=-\infty}^{v=y}\int_{x\in\R}f_{(X,Y)}(x,v)\dx\dv$
	\end{itemize}
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Definición
	\end{itemize}
	Sea $g:\R^2\longrightarrow\R$ tal que $g(X,Y)$ es \va unidimensional. \[ E(g(X,Y))=\int_{\R}\int_{\R} g(x,y)\cdot f_{(X,Y)}(x,y)\dx\dy \]
\end{enumerate}
\subsection{Distribuciones condicionadas e independencia}
A veces, conocer el valor que toma una de las variables aporta información adicional sobre la otra variable.\begin{center}
	$(X,Y)\longrightarrow$\lb{Dependientes}
\end{center}Otras veces no aporta nada $\longrightarrow$\lb{Independencias}.
\begin{enumerate}[label=\color{red}\textbf{\Alph*)},leftmargin=*]
	\item \lb{Caso discreto:} $(X,Y)$ con $X,Y$ discretas
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Definición
	\end{itemize}
	Llamaremos \fpp de $Y$ condicionada a $X=x$ a la función \[ P_{Y|X=x}(y)=\dfrac{P_{(X,Y)}(x,y)}{P_X(x)}\text{ con }p_X(x)>0\text{ y }\begin{cases}
		y\in\R\\
		y\in\sop(Y)
	\end{cases} \]
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \lb{Propiedades:} \[P_{Y|X=x}(y)\text{ es \fpp}\]
		\item \lb{Demostración}
	\end{itemize}
	\begin{enumerate}[label=\color{lightblue}\arabic*)]
		\item $P_{Y|X=x}(y)=\dfrac{P_{(X,Y)}(x,y)}{P_X(x)}\ge0\;\forall y$
		\item $\sum_{y\in\R}P_{Y|X=x}(y)=\dfrac{\displaystyle\sum_yp(x,y)}{p_X(x)}=\dfrac{P_X(x)}{P_X(x)}=1$\[ P_{Y|X=x}(y)=\dfrac{P(Y=y|X=x)=P\left((Y=y)\cap(X=x)\right)}{P(X=x)}=\dfrac{P_{(X,Y)}(x,y)}{P_X(x)} \]
	\end{enumerate}
	Sirve para obtener probabilidades del tipo: \[ P(Y\in A|X=x)=\sum_{\begin{subarray}{c}
			y\in A\\
			y\in\sop(Y)
	\end{subarray}}P_{Y|X=x}(y) \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Si $g:\R\longrightarrow\R$ aplicación tal que $g(Y)$ es variable aleatoria, se define la esperanza condicionada por $X=x$ como: \[ E\left(g(y)|X=x\right)=\sum_{y\in\sop(Y)}g(y)\cdot P_{Y|X=x}(y) \]
\item \lb{Caso continuo:} $(X,Y)$ con $X$ e $Y$ continuas
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Se define la función de densidad de $Y$ condicionada a $X=x$ como: \[ f_{Y|X=x}(y)=\dfrac{f_{(X,Y)}(x,y)}{f_X(x)}\text{ si }f_X(x)>0,\;\forall y\in\R \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Propiedades
\end{itemize}
$f_{Y|X=x}(y)$ es densidad.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $f_{Y|X=x}(y)\ge0\:\forall y$ por serlo numerador y denominador.
	\item $\int_{0}^{+\infty}f_{Y|X=x}(y)\dy=\dfrac{1}{f_X(x)}\int_{-\infty}^{+\infty}f(x,y)\dy=\dfrac{f_X(x)}{f_X(x)}=1$
\end{enumerate}
Sirve para obtener probabilidades del tipo: \[ P(Y\in A|X=x)=\int_{y\in A}f_{Y|X=x}(y)\dy \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Si $g:\R\longrightarrow\R$ aplicación tal que $g(Y)$ es variable aleatoria, se define la esperanza condicionada por $X=x$ como: \[ E\left(g(Y)|X=x\right)=\int_{-\infty}^{+\infty}g(y)\cdot f_{Y|X=x}(y)\dy \]
\item \lb{Independencia}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Sea $(X,Y)$ \va bidimensional discreta. Diremos que $X$ e $Y$ son independientes si se cumple alguna de las siguientes condiciones equivalentes:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $P_{(X,Y)}(x,y)=p_X(x)\cdot p_Y(y)\:\forall(x,y)\in\R^2$
	\item $F_{(X,Y)}(x,y)=F_X(x)\cdot F_Y(y)\:\forall(x,y)\in\R^2$
	\item $P_{(Y|X=x)}(y)=P_Y(y)\;\forall x\in\sop(X),\,\forall y\in\sop(Y)$
	\item $P_{(X|Y=y)}(x)=P_X(x)\;\forall x\in\sop(X),\,\forall y\in\sop(Y)$
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Sea $(X,Y)$ \va bidimensional continua. Diremos que $X$ e $Y$ son independientes si se cumple alguna de las siguientes condiciones equivalentes:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $f_{(X,Y)}(x,y)=f_X(x)\cdot p_Y(y)\;\forall(x,y)\in\R^2$
	\item $F_{(X,Y)}(x,y)=F_X(x)\cdot F_Y(y)\;\forall(x,y)\in\R^2$
	\item $f_{(Y|X=x)}(y)=f_Y(y)\;\forall x\in\sop(X),\,\forall y\in\sop(Y)$
	\item $f_{(X|Y=y)}(x)=f_X(x)\;\forall x\in\sop(X),\,\forall y\in\sop(Y)$
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Propiedades
\end{itemize}
Si $X$ e $Y$ son \vas independientes, entonces:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $P(X\in A,Y\in B)=P\left((X\in A)\cap(Y\in B)\right)=P(X\in A)\cdot P(Y\in B)$
	\item $g(X)$ y $h(Y)$ son independientes $\forall g:\R\longrightarrow\R$ y $\forall h:\R\longrightarrow\R$ tales que $g(X)$ y $h(Y)$ son \vas.
	\item $E\left(g(X)\cdot h(Y)\right)=E(g(X))\cdot E(h(Y))$
\end{enumerate}
\subsection{Covarianza}
Son medidas de asociación lineal entre $X$ e $Y$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
$\begin{array}{l}
	\sigma_{xy}=\cov(X,Y)=E\left((X-\mu_X)\cdot(Y-\mu_Y)\right)-E(X\cdot Y)\cdot E(X)\cdot E(Y)\\
	\rho_{xy}=\mathrm{Corr}(X,Y)=\dfrac{\cov(X,Y)}{\sqrt{\var(X)}\cdot\sqrt{\var(Y)}}=\dfrac{\sigma_{xy}}{\sigma_x\cdot\sigma_x}\in[-1,1]
\end{array}$
\begin{itemize}
	\item Si $\rho_{xy}\simeq0\longrightarrow$ No hay relación lineal
	\item Si $\rho_xy\simeq1\longrightarrow$ Relación lineal creciente
	\item Si $\rho_{xy}\simeq-1\longrightarrow$ Relación lineal decreciente
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Propiedades
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $\var(X+Y)=\var(X)+\var(Y)+2\cov(X,Y)$
	\item $\var(aX+bY)=a^2\var(X)+b^2\var(Y)+2ab\cov(X,Y)$
	\item Si $X$ e $Y$ son independientes, entonces: \[ \sigma_{xy}=\cov(X,Y)=0\text{ y }\rho_{xy}=\mathrm{Corr}(X,Y)=0 \]El recíproco de \lb{(3)} no es cierto
	\item $\cov(aX+bY)=ab\cov(X,Y)$
	\item $\cov(X_1+X_2,Y)=\cov(X_1,Y)+\cov(X_2,Y)$
\end{enumerate}
\end{enumerate}
\subsection{Modelos multivariantes}
\begin{enumerate}[label=\color{red}\textbf{\Alph*)}, leftmargin=*]
	\item \lb{\underline{Modelo multinomial}, $\vec{X}\sim M(n,p_1,p_2,\dots,p_k)$}
	
	Consideremos un experimento aleatorio con $k$ resultados posibles, $\Omega=\{A_1,A_2,\dots,A_k\}$. Repito el experimento $n$ veces de forma independiente. Denotemos por $p_1=P(A_i),\:i=1,\dots,k$.
	
	Llamaremos $X_i=$"Nº de veces que ocurre $A_i$ en las $n$ repeticiones"$\sim B(n,p_i)$
	
	\fcolorbox{red}{red!10}{\rc{$\cov(X_i,X_j)=-n\cdot p_i\cdot p_j$ no son independientes}}
	
	Entonces, el vector aleatorio \[ \vec{X}=(X_1,X_2,\dots,X_k)\sim M(n,p_1,p_2,\dots,p_k) \]
	\begin{itemize}
		\item \Fpp conjunta: \[ P(X_1=x_1,X_2=x_2,\dots,X_k=x_k)=\dfrac{n!}{x_1!\cdot x_2!\cdot~~\cdots~~x_k!}\cdot p_1^{x_1}\cdot p_2^{x_2}\cdot~~\cdots~~\cdot p_k^{x_k} \]$\bboxed{\begin{array}{l}
				p_1+p_2+\cdots+p_k=1\\
				P(X_1)+P(X_2)+\cdots+P(X_k)=n
		\end{array}}$
	\end{itemize}
	\item \lb{\underline{Normal multivariante} $\vec{X}\sim\vec{N}(\vec{\mu},V)$}
	
	Su función densidad es:\[ f(x_1,\dots,x_n)=\dfrac{1}{\sqrt{|V(2\pi)^n}}\exp\left\{-\dfrac{1}{2}(\vec{x}-\vec{\mu}),V^{-1}(\vec{c}-\vec{y})\right\} \] donde $\vec{x}=(x_1,\dots,x_n),\;V$ es una matriz simétrica y definida positiva, $V^{-1}$ su inversa y$|V|$ determinante de $V$.\[ E(X_1,\dots,X_n)=(\mu_1,\dots,\mu_n)=\vec{\mu} \]
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Propiedades
	\end{itemize}
	\begin{enumerate}[label=\color{lightblue}\arabic*)]
		\item Si $\vec{X}=(X_1,\dots,X_n)\sim N(\vec{\mu},v)\longrightarrow x_j\sim N(\mu_j,\sqrt{v_{jj}})$
		\item Si $\vec{X}\sim N(\vec{\mu},V)\longrightarrow A\vec{X}+B\sim N(A\vec{\mu}+B,AVA')\;\forall A\in M_{m\times n},\,B\in M_{m\times n}$
		\item Si $\vec{X}\sim N(\vec{\mu},V)$ y $\cov(X_i,X_j)=0\longrightarrow X_i$ y $X_j$ son independientes.
		\item Si $\vec{X}\sim N(\vec{\mu},V)\longrightarrow\exists A$ tal que $Z=A^{-1}(\vec{X}-\vec{\mu})\sim N(0,In)$.
	\end{enumerate}
\end{enumerate}

\subsection{Distribuciones multidimensionales $\overset{\to}{X}=(X_1,\dots,X_n)$}

En esta sección extendemos los resultados anteriores al caso de variables $n$-dimensionales $\vec{X}=(X_1,\dots,X_n)$
\begin{itemize}
	\item Función de distribución conjunta: \[ F_{\vec{X}}(x_1,\dots,x_n)=P(X_1\le x_1,X_2\le x_2,\dots,X_n\le x_n) \]
	\item \Fpp conjunta: \[ P_{\vec{X}}(x_1,\dots,x_n)=P(X_1=x_1,\dots,X_n=x_n) \]
	\item Densidad conjunta (permite obtener la función de distribución integrando). \[ F_{\vec{X}}(x_1,\dots,x_n)=\int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_n}f_{\vec{X}}(t_1,\dots,t_n)\dt_1,\dots,\dt_u \]
	\item Función distribución marginal de $X_{k_1},\dots,X_{k_s}$:\[ F_{X_{k_1},\dots,X_{k_s}}(t_1,\dots,t_s)=F_{\vec{X}}(-\infty,\dots,t_i,\dots,+\infty) \]
	\item Función puntual marginal de $X_{k_1},\dots,X_{k_s}$: \[ p_{k_1,\dots,k_s}(t_1,\dots,t_s)=\sum_{x_1,\dots,x_n}p_{\vec{X}}(x_1,\dots,t_i,\dots,x_n) \]
	\item Densidad marginal de $X_{k_1},\dots,X_{k_s}$: \[ f_{k_1,\dots,k_s}(t_1,\dots,t_s)=\int_{\R^{n-s}}f_{\vec{X}})(x_1,\dots,t_i,\dots,x_n)\dx_1,\dots,\dx_s \]
	\item Función puntual de probabilidad de $\left(X_{l_1},\dots,X_{l_s}\right)$ condicionada a $\left(X_{k_1}=Z_1,\dots,X_{l_s}=Z_s\right)$ \[ P_{\left(X_{l_1},\dots,X_{l_r}|X_{k_1}=Z_1,\dots,X_{k_s}=Z_s\right)}\left(X_{l_1},\dots,X_{l_s}\right)=\dfrac{P_{\vec{X}}(x_1,\dots,x_i,\dots,x_n)}{P_{k_1,\dots,k_s}}(Z_1,\dots,Z_s) \]
	\item Densidad de $\left(X_{l_1},\dots,X_{l_r}\right)$ condicionada a $\left(X_{k_1}=Z_2,\dots,X_{k_s}=Z_s\right)$ \[ f_{\left(X_{l_1},\dots,X_{l_r}|X_{k_1}=Z_1,\dots,X_{k_s}=Z_s\right)}\left(X_{l_1},\dots,X_{l_s}\right)=\dfrac{f_{\vec{X}}(x_1,\dots,x_i,\dots,x_n)}{f_{k_1,\dots,k_s}(Z_1,\dots,Z_s)}\]
	\item $(X_1,\dots,X_n)$ independientes: \[ \begin{rcases}
		\text{caso}\\
		\text{continuo}
	\end{rcases}f_{\vec{X}}(x_1,\dots,x_n)=f_{X_1}(x_1),\dots,f_{X_n}(x_n)\| \overbrace{P_{\vec{X}}(x_1,\dots,x_n)}^{\text{Caso discreto}}=p_{X_1}(x_1)\cdot~~\cdots~~\cdot p_{X_n}(x_n) \]
\end{itemize}
Consecuencia: Si $(X_1,\dots,X_m)$ independiente $\longrightarrow$ cualquier subconjunto es independiente. \[ \begin{array}{l}
	\mathrm{MCov}(X_1,\dots,X_n)=\begin{bmatrix}
		\overbrace{\cov(X_1,X_1)}^{\sigma_{x_1}^2} & \cov(X_1,X_2) & \cdots & \cov(X_1,X_n)\\
		\vdots & \vdots & & \vdots\\
		\cov(X_n,X_1) & \cdots & \cdots & \underbrace{\cov(X_n,X_n)}_{\sigma_{x_n}^2}
	\end{bmatrix}\\
	\\
	\mathrm{MCov}(X_1,\dots,X_n)=\begin{bmatrix}
		\rho_{X_1X_1} & \rho_{X_1X_2} & \cdots & \rho_{X_1X_n}\\
		\vdots & \vdots & & \vdots\\
		\rho_{X_nX_1} & \cdots & \cdots & \rho_{X_nX_n}
	\end{bmatrix}=\begin{bmatrix}
	1 & \rho_{12} & \cdots & \rho_{1n}\\
	 & \ddots & & \\
	 & & \ddots & 1\\
	 \rho_{n1} & \cdots & \cdots& \cdots 
	\end{bmatrix}
\end{array} \]
\subsection{Cambios de variables}
$\vec{X}=(X_1,\dots,X_n)$ variable aleatoria y sea $g:\R^n\longrightarrow\R^m$ aplicación. ¿Cuándo será $g(\vec{X})$ una variable aleatoria $m$-dimensional?

Si $g$ es continua $\longrightarrow \vec{Y}=g\left(\vec{X}\right)$ es una variable aleatoria.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \lb{Caso discreto:} $P_{\vec{Y}}P(Y_1=y_1,\dots,Y_n=y_n)=P(\vec{Y}=\vec{y})=P\left(g(\vec{X})=\vec{y}\right)=P\left(\vec{X}\in g^{-1}(\vec{y})\right)=\sum_{\vec{x}\in g^{-1}(\vec{y})}P_{\vec{X}}(\vec{x})$
	\item \lb{Caso continuo:} Si $g:\R^n\longrightarrow\R^n\quad(n=m)$ biyectiva. \[ \underset{\lb{\begin{subarray}{l}
				\text{Función densidad}\\
				\text{conjunta de $Y$}
	\end{subarray}}}{P_{\vec{Y}}(\vec{y})}=\underset{\rc{\begin{subarray}{l}
	\text{función densidad}\\
	\text{conjunta de $X$}
\end{subarray}}}{f_{\vec{X}}\left(g^{-1}(\vec{y})\right)}\cdot\underbrace{\begin{vmatrix}
\dfrac{\partial g_1^{-1}(\vec{y})}{\partial y_1} & \cdots & \dfrac{\partial g_1^{-1}(\vec{y})}{\partial y_n}\\
\vdots & & \vdots \\
\dfrac{\partial g_n^{-1}(\vec{y})}{\partial y_1} & \cdots & \dfrac{\partial g_n^{-1}(\vec{y})}{\partial y_n}
\end{vmatrix}}_{\text{determinante Jacobina}} \]
\item \lb{Distribuciones $X^2$ de Pearson, t-Student y F de Snedecor}

Veremos la construcción de estas nuevas distribuciones a partir de la Normal.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Sean $Z_1,Z_2,\dots,Z_n$ variable aleatoria $N(0,1)$ independientes. Entonces, se dice que la variable aleatoria: \begin{center}
	$\sum_{i=1}^{4}Z_i^2\sim X_n^2 $ sigue una Chi-cuadrado con $n$ grafos libertad.
\end{center}
\begin{itemize}
	\item $E(X_n^2)=n,\:\var(X_n^2)=2n$
\end{itemize}
\item \lb{Propiedad:} Si $X\sim X_{n_1}^2,\:Y\sim Y\sim X_{n_2}^2$ independientes $\longrightarrow X+4\sim X_{n_1+n_2}^2$

\begin{minipage}[l]{0.5\textwidth}
	\begin{tikzpicture}[scale=1.5]
		\begin{axis}[
			xtick=\empty,
			ytick=\empty,
			axis lines=middle,
			ymin=-0.0025,
			xmin=-1,
			xmax=15,
			legend pos=north east,
			legend cell align={left},
			]
			
			% Distribución chi-cuadrado con 4 grados de libertad
			\addplot[domain=0:15,smooth,variable=\x,lightblue] {1/8*x^(2-1)*exp(-x/2)/2^2};
			
			% Distribución chi-cuadrado con 8 grados de libertad
			\addplot[domain=0:15,smooth,variable=\x,blue] {1/48*x^(4-1)*exp(-x/2)/2^4};
		\end{axis}
	\end{tikzpicture}
\end{minipage}\qquad\begin{minipage}[l]{0.4\textwidth}
\fcolorbox{lightblue}{lightblue!10}{Densidad de $X_n^2$}\\
Sólo definitiva para valores positivos
\end{minipage}
\item \lb{Definición}

Sea $X\sim N(0,1)$ y sea $Y\sim X_n^2$ independientes. Entonces $\dfrac{X}{\sqrt{\frac{Y}{n}}}\sim tn$ t-Student con $n$ grados de libertad.
\begin{itemize}
	\item $E(t_n)=0,\var(t_n)=\dfrac{n}{n-2}$\quad(si $n\ge 3$)
\end{itemize}

\begin{tikzpicture}
	\begin{axis}[
		xtick=\empty,
		ytick=\empty,
		axis lines=middle,
		width=10cm,
		height=8cm,
		samples=100,
		]
		\addplot[domain=-5:5,smooth,variable=\x,lightblue] {1/(sqrt(2*pi*2)*(1+(\x)^2/2)^((2+1)/2))};
		\addplot[domain=-5:5,smooth,variable=\x,blue] {1/(sqrt(2*pi*5)*(1+(\x)^2/5)^((5+1)/2))};
		\addplot[domain=-5:5,smooth,variable=\x,red, dashed] {1/sqrt(2*pi)*exp(-(\x)^2/2)};
		\legend{$n=2$, $n=5$,$\text{Normal, } n=\infty$}
	\end{axis}
\end{tikzpicture}\qquad\fcolorbox{lightblue}{lightblue!10}{Densidad de $t_n$}
\item \lb{Propiedad:} La $t_n$ es simétrica respecto del origen, está definida en todo $\R$ y si $n\ge30$ se puede aproximar por $N(0,1)$. Para $n=100,\,t_{100}$ es casi idéntica a $N(0,1)$.
\item \lb{Definición}

Sea $X\sim X_n^2$ e $Y\sim X_m^2$ independiente. Entonces: \begin{center}
	$\dfrac{\frac{X}{n}}{\frac{Y}{m}}\sim F_{n,m}\quad$ F-Snedercor con $n$ y en grados de libertad.
\end{center}
\begin{itemize}
	\item $E(F_{n,m})=\dfrac{m}{m-2}$ si $m>2$.
	\item $\var(F_{n,m})=\dfrac{2m^2(m+n-2)}{m(m-2)^2(m-4)}$ si $m>4$.
	
	Está definida sólo para sólo para valores positivos
	
		\begin{tikzpicture}
		\begin{axis}[
			xtick=\empty,
			ytick=\empty,
			axis lines=middle,
			ymin=-0.0025,
			xmin=-1,
			xmax=15,
			legend pos=north east,
			legend cell align={left},
			]
			
			% Distribución chi-cuadrado con 4 grados de libertad
			\addplot[domain=0:15,smooth,variable=\x,lightblue] {1/8*x^(2-1)*exp(-x/2)/2^2};
			
			% Distribución chi-cuadrado con 16 grados de libertad
			\addplot[domain=0:15,smooth,variable=\x,red] {1/32768*x^(8-1)*exp(-x/2)/2^8};
			
			\legend{$n=4$, $n=16$}
		\end{axis}
	\end{tikzpicture}
\end{itemize}
\item \lb{Propiedad:} Por definición $F_{n,m}=\dfrac{1}{F_{m,n}}$. Además, $t_n^2=F_{1,n}$
\end{itemize}
\begin{tikzpicture}
	\node[red, draw=red, fill=red!10, line width=1.5, text width=\textwidth] {\underline{Nota:} Para el cálculo de probabilidades de estas distribuciones usaremos tablas o la calculadora estadística.};
\end{tikzpicture}

