\begin{center}
    \textbf{\large Hoja de ejercicios Tema 5: Inferencia Bayesiana} 
\end{center}
\begin{enumerate}[label=\color{red}\textbf{\arabic*)}]
    \item \lb{Consideramos el caso en que $X\sim P(\lambda)$ y $\pi(\lambda)$ sigue una distribución gamma con densidad \[
    \pi(\lambda)\propto \lambda^{\alpha-1}\exp\left( -\dfrac{\lambda}{\beta} \right) 
    \]Observamos una m.a.s de $X$, estudiar la distribución a posteriori de  $\lambda$.}
    
    \begin{enumerate}[label=Paso \arabic*:]
      \item Función de verosimilitud

        Para una muestra aleatoria $X_1,X_2,\dots,X_n$ proveniente dee una distribución de Posisson, la función de verosimilitud es: \[
        L(\lambda|\mathbf{X})=\prod_{i=1}^{n} \dfrac{\lambda^{X_i}e^{-\lambda}}{X_i!}=\lambda^{\sum_{i=1}^{n} X_i}e^{-n\lambda}\prod_{i=1}^{n} \dfrac{1}{X_i!}. 
        \] 
        Como la constante $\prod_{i=1}^{n} \dfrac{1}{X_i!} $ no depende de $\lambda$, podemos escribir: \[
        L(\lambda_1|\mathbf{X})\propto \lambda^{\sum_{i=1}^{n} X_i}e^{-n\lambda}. 
        \] 
      \item Distribución a posteriori

        La distribución a posteriori de $\lambda$ se obtiene aplicando el teorema de Bayes: \[
        \pi(\lambda|\mathbf{X})\propto L(\lambda|\mathbf{X})\cdot \pi(\lambda).
        \] 
        Sustiyendo la verosimilitud y la densidad a priori: \[
        \pi(\lambda|\mathbf{X})\propto \left[ \lambda^{\sum_{i=1}^{n} X_i}e^{-n\lambda}  \right] \cdot \left[ \lambda^{\alpha-1}e^{-\frac{\lambda}{\beta} }  \right]=\lambda^{\sum_{i=1}^{n} X_i+\alpha-1}e^{-\lambda\left( n+\frac{1}{\beta}  \right) }  .
        \] 
        Esta es la forma de la función de densidad de una distribución gamma. Específicamente: \[
        \pi(\lambda|\mathbf{X})\sim \mathrm{Gamma}\left( \alpha+\sum_{i=1}^{n} X_i,\dfrac{1}{n+\frac{1}{\beta} } \right).
        \] 
    \end{enumerate}
\item \lb{Consideramos una m.a.s. de $X\sim \mathcal{N}(\mu,\sigma^2)$, con $\sigma^2$ conocida y escogemos como distribución a priori $\pi(\mu)$, una distribución normal con densidad \[
\pi(\mu)\propto \exp\left( -\dfrac{1}{2\tau^2}(\mu-\mu_0)^2 \right) 
\]Demostrar que la distribución a posteriori de $\mu$ es Normal con media y varianza: \[
\begin{array}{c}
    E[\mu|x]=\mu_0 \dfrac{\frac{\sigma^2}{n}}{\tau^2+\frac{\sigma^2}{n}}+\overline{x} \dfrac{\tau^2}{\tau^2+\frac{\sigma^2}{n}}\\
    \mathrm{Var}[\mu|x]=\kappa^2=\dfrac{\frac{\tau^2\sigma^2}{n}}{\tau^2+\frac{\sigma^2}{n}}
\end{array}
\] } 

\begin{enumerate}[label=Paso \arabic*:]
  \item Función de verosimilitud

    La función de verosimilitud para $\mu$, dado $\mathbf{X}$, es: \[
    L(\mu|\mathbf{X})\propto \prod_{i=1}^{n} \exp\left( -\dfrac{1}{2\sigma^2}(X_i-\mu)^2 \right).
    \] 
    Simplificando, la función de verosimilitud se puede escribir como: \[
    L(\mu|\mathbf{X})\propto \exp\left( -\dfrac{1}{2\sigma^2}\sum_{i=1}^{n} (X_i-\mu)^2 \right) .
    \] 
    Expandiendo el término cuadrático: \[
    \sum_{i=1}^{n} (X_i-\mu)^2=\sum_{i_{01}}^{n} X_i^2-2\pi \sum_{i=1}^{n} X_i+n\mu^2.
    \] 
    Por lo tanto: \[
    L(\mu|\mathbf{X})\propto \exp\left( -\dfrac{1}{2\sigma^2}\left( \sum_{i=1}^{n} X_i^2-2\mu \sum_{i=1}^{n} X_i+n\mu^2 \right)  \right) .
    \] 
    Agrupando términos en función de $\mu$: \[
    L(\mu|\mathbf{X})\propto \exp\left( -\dfrac{n}{2\sigma^2}(\mu^2)-2\mu\overline{X} \right) ,
    \] donde $\overline{X}=\dfrac{1}{n}\sum_{i=1}^{n} X_i$ es la media muestral.
  \item Distribución a posteriori

    Por el teorema de Bayes: \[
    \pi(\mu|\mathbf{X})\propto L(\mu|\mathbf{X})\cdot \pi(\mu).
    \] 
    Sustituyendo $L(\mu|\mathbf{X})$ y $\pi(\mu)$: \[
    \pi(\mu|\mathbf{X})\propto \exp\left( -\dfrac{n}{2\sigma^2}(\mu^2-2\mu\overline{X}) \right) \cdot \exp\left( -\dfrac{1}{2\tau^2}(\mu-\mu_0)^2 \right) .
    \] 
    Agrupando los exponentes: \[
    \pi(\mu|\mathbf{X})\propto \exp\left( -\dfrac{1}{2}\left[ \dfrac{n}{\sigma^2}\mu^2-2 \dfrac{n}{\sigma^2}\mu\overline{X}+\dfrac{1}{\tau^2}(\mu^2-2\mu\mu_0+\mu_0^2) \right]  \right) .
    \] 
    Expandiendo los términos: \[
    \pi(\mu|\mathbf{X})\propto \exp\left( -\dfrac{1}{2}\left[ \left( \dfrac{n}{\sigma^2}+\dfrac{1}{\tau^2} \right) \mu^2-2\mu\left( \dfrac{n}{\sigma^2}\overline{X}+\dfrac{\mu_0^2}{\tau^2} \right) +\dfrac{\mu_0^2}{\tau^2}  \right]  \right) 
    \] 
    El término cuadrático de $\mu$ muestra que $\dfrac{\pi}{\mu|\mathbf{X}}$ sigue una distribución normal. Ahora identificamos los parámetros.
  \item Parámetros de la distribución a posteriori
    
    La distribución normal tiene forma: \[
      \pi(\mu|\mathbf{X})\propto \exp\left( -\dfrac{1}{2\mathrm{Var}[\mu|\mathbf{X}]}(\mu-\mathbb{E}[\mu|\mathbf{X}])^2 \right),
    \] 
    donde:
    \begin{itemize}[label=\textbullet]
      \item $\mathbb{E}[\mu|\mathbf{X}]$ es la media a posteriori.
      \item $\mathrm{Var}[\mu|\mathbf{X}]$ es la varianza a posteriori.
    \end{itemize}
    \textbf{Varianza a posteriori:}

    El coeficiente de $\mu^2$ determina la inversa de la varianza: \[
      \dfrac{1}{\mathrm{Var}[\mu|\mathbf{X}]}=\dfrac{n}{\sigma^2}+\dfrac{1}{\tau^2}\longrightarrow \mathrm{Var}[\mu|\mathbf{X}]=\dfrac{1}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}=\dfrac{\sigma^2\tau^2}{n\tau^2+\sigma^2}.
    \] 
    \textbf{Media a posteriori:} 

    El coeficiente de $-2\mu$ determina la media: \[
      \mathbb{E}[\mu|\mathbf{X}]=\dfrac{\frac{n}{\sigma^2}\overline{X}+\frac{\mu_0}{\tau^2}  }{\frac{n}{\sigma^2}+\frac{1}{\tau^2}  }.
    \] 
\end{enumerate}

\item \lb{Consideramos una m.a.s. de $X\sim \mathrm{Exp}(1 /\kappa)$. Nota: $\kappa$ es la esperanza de  $X$. Consideramos a priori  $\pi(\kappa)\propto \dfrac{1}{\kappa}$, que se considera una a priori no informativa y es la usual en este caso.}
    \begin{enumerate}[label=\color{red}\textbf{\alph*)}]
        \item \db{Demostrar que la densidad a posteriori de $\kappa|\mathbf{x}$ es proporcional $\kappa^{-n-1}\exp\left( -\sum_{i} \dfrac{x_i}{\kappa} \right) $} 

          \textbf{Función de verosimilitud}

          Dado que $X_i\sim \mathrm{Exp}\left( \dfrac{1}{\kappa} \right) $, la función de densidad es: \[
          f(x_i|\kappa)=\dfrac{1}{\kappa}\exp\left( -\dfrac{x_i}{\kappa} \right) ,\quad x_i>0.
          \] 
          La función de verosimilitud para una muestra $\mathbf{x}=\{x_1,x_2,\dots,x_{n}\} $ es: \[
          L(\kappa|\mathbf{x})=\prod_{i=1}^{n} \dfrac{1}{\kappa}\exp\left( -\dfrac{x_i}{\kappa} \right)=\kappa^{-n}\exp\left( -\dfrac{\sum_i x_i}{\kappa} \right) .
          \] 
          \textbf{Distribución a posteriori}

          La distribución a posteriori de $\kappa|\mathbf{x}$ se obtiene aplicando el teorema de Bayes: \[
          \pi(\kappa|\mathbf{x})\propto L(\kappa|\mathbf{x})\cdot \pi(\kappa).
          \] 
          Simplificando: \[
          \pi(\kappa|\mathbf{x})\propto \kappa^{-n-1}\exp\left( -\dfrac{\sum_i x_i}{\kappa} \right) .
          \] 
          Esto demuestra el resultado.
        \item \db{Demostrar que, si transformamos $\kappa$, considerando  $\lambda=\dfrac{1}{\kappa},\,\lambda|\mathbf{x}$ admite una densidad gamma con parámetros $n$ y $\dfrac{1}{\sum_i x_i}$.}

          \textbf{Transformación de variables}

          Si $\lambda=\dfrac{1}{\kappa}\longrightarrow \kappa=\dfrac{1}{\lambda}$ y: \[
          \mathrm{d}\kappa=-\dfrac{1}{\lambda^2}\mathrm{d}\lambda\longrightarrow |\mathrm{d}\kappa|=\dfrac{1}{\lambda^2}\mathrm{d}\lambda.
          \] 
          La densidad de $\kappa|\mathbf{x}$ es: \[
          \pi(\kappa|\mathbf{x})\propto \kappa^{-n-1}\exp\left( -\dfrac{\sum_i x_i}{\kappa} \right) .
          \] 
          Sustituyendo $\kappa=\dfrac{1}{\lambda}$, tenemos: \[
          \pi(\kappa|\mathbf{x})\propto \left( \dfrac{1}{\lambda} \right) ^{-n-1}\exp\left( -\lambda \sum_{i}x_i  \right) \cdot \dfrac{1}{\lambda^2}
          \] 
          Simplificando: \[
          \pi(\lambda|\mathbf{x})\propto \lambda^{n-1}\exp\left( -\lambda \sum_i x_i \right) 
          \] 
          Esta es la forma de la densidad de una distribución gamma con parámetros: $\alpha^*=n,\beta^*=\dfrac{1}{\sum_i x_i}$. Por lo tanto: \[
          \lambda|\mathbf{x}\sim \mathrm{Gamma}\left( n,\dfrac{1}{\sum_i x_i} \right) .
          \] 
          
        \item \db{En un experimento sobre duración de dispositivos, obtenemos los siguientes valores en horas: 6562, 2280, 50, 989, 1797. Obtener un intervalo de credibilidad al 90\% para la inversa de $k$. Obtener el mismo intervalo para  $\kappa$.} 
    \end{enumerate}
\end{enumerate}
