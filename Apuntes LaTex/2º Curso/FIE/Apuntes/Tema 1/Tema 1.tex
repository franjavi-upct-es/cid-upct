\section{Muestreo y distribuciones muestrales}
\subsection{Introducción}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{El contexto}]
\begin{itemize}[label=\textbullet]
    \item Tenemos una pregunta acerca de un fenómenos aleatorio.
    \item Formulamos un modelo para la varaible de interés $X$.
    \item Traducimos la pregunta de interés en términos de uno o varios parámetros del modelo.
    \item Repetimos el experimento varias veces, apuntamos los valores de  $X$.
    \item ¿Cómo usar estos valores para extraer información sobre el parámetro?
\end{itemize}
\end{tcolorbox}

\subsection{Ejemplos}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{¿Está la moneda trucada?}]
\begin{itemize}[label=\textbullet]
    \item Experimento: tirar la modena. $X=$ resultado obtenido.

         $P(X=+)=p,P(X=c)=1-p$  \[
         \text{¿} p = \dfrac{1}{2}?
         \] 
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Sondeo sobre intención de participación en unas elecciones}]
\begin{itemize}[label=\textbullet]
    \item Queremos estima la tasa de participación antes de unas elecciones generales.
    \item Formulamos un modelo:
        \begin{itemize}[label=\textrightarrow]
            \item Experimento: "escoger una persona al azar en el censo".
            \item $X$: participación, variable dicotómica ("Sí" o "No").  $p=P(X=\text{Si})$.
        \end{itemize}
    \item ¿Cuánto vale $p$?
    \item Censo: aproximadamente 37 000 000. Escogemos aproximadamente 3000 personas.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Determinación de la concentración de un producto}]
\begin{itemize}[label=\textbullet]
    \item Quiero determinar la concentración de un producto.
    \item Formulo el modelo:
        \begin{itemize}[label=\textrightarrow]
            \item Experimento: "llevar a cabo una medición".
            \item $X$: "valor proporcionado por el aparato".
            \item  $X\sim \mathcal{N}(\mu,\sigma^2)$.
        \end{itemize}
    \item ¿Qué vale $\mu$?
\end{itemize}
\end{tcolorbox}
\subsection{Surge una pregunta}
En todas estas situaciones donde nos basamos en la repetición de un experimento simple\dots
\begin{itemize}[label=\textbullet]
    \item ¿Cómo sabemos que nuestra estimación es fiable?
    \item ¿Qué confianza tenemos al extrapolar los resultados de una muestra de 3000 personas a una población de 37 millones de personas?
\end{itemize}
\subsection{Esbozo de respuesta: tasa de participación}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Para convenceros, un experimento de simulación}]
\begin{itemize}[label=\textbullet]
    \item Voy a simular el proceso de extracción de una muestra de 3000 personas en una población de 37 millones de personas.
    \item Construyo a mi antojo los distintos componentes:
        \begin{itemize}[label=\textrightarrow]
            \item \textbf{La población:} defino en mi ordenador un conjunto de 37 000 000 de ceros y unos. ($\Leftrightarrow$ el censo electoral)
                \begin{itemize}[label=\textbullet]
                    \item "1" $\Leftrightarrow$ "la persona piensa ir a votar".
                    \item "0" $\Leftrightarrow$ "la persona \textbf{no} peinsa ir a votar"
                \end{itemize}
            \item \textbf{La tasa de participación "real":} Decido que en mi población el 70\% piensa ir a votar $\to 25\,900\,000$ "1"s.
            \item \textbf{La extracción de una muestra:} construyo un pequeño programa que extrae al azar una muestra de 3000 números dentro del conjunto grande. 
        \end{itemize}
\end{itemize}
\end{tcolorbox}
\begin{lstlisting}
poblacion <- c(rep(1, 25900000), rep(0, 11100000))
set.seed(314159)
p_muestra <- mean(sample(poblacion, 3000, replace = FALSE))
p_muestra   
\end{lstlisting}
\begin{verbatim}
## [1] 0.705667    
\end{verbatim}

Queremos descartar que haya sido suerte. Vamos a repetir muchas veces (10000 veces por ejemplo), la extracción de una muestra de 3000 personas en la población.
\begin{lstlisting}
library(tidyverse)
lista_muestras <- replicate(
    10000,
    sample(poblacion, 3000, replace = FALSE),
    simplify = FALSE
)
p_muestras <- map_dbl(lista_muestras, mean)
head(p_muestras)
\end{lstlisting}
\begin{verbatim}
## [1] 0.6970000  0.7030000  0.7036667  0.7023333  0.7013333  0.7226667
\end{verbatim}

Recogemos los valores obtenidos en un histograma.

\begin{center}
\includegraphics[width=0.7\linewidth]{"Tema 1/figures/Figure 1"}
\end{center}

\subsection{Realización del experimento: conclusiones}
\begin{itemize}[label=\textbullet]
  \item La enorme mayoría de las muestras de 3000 individuos proporcionan una tasa de partición muy próxima a la de la población.
    \begin{itemize}[label=\textrightarrow]
      \item \textbf{\rc{El riesgo}} de cometer un error superior a $\pm 2$ puntos, al coger \textbf{\rc{una}} muestra de 3000 individuos es muy pequeño (y asumible\dots)
    \end{itemize}
    \item Si nos limitamos a muestras de 300 individuos, ¿qué esperáis?
\end{itemize}
\begin{center}
  \includegraphics[width=0.7\linewidth]{"Tema 1/figures/Figure 2"}
\end{center}
\subsection{En la práctica}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Usamos las distribuciones muestrales}]
\begin{itemize}[label=\textbullet]
  \item Las empresas de sondeos no se basan en simulaciones sino en cálculos teóricos.
  \item Experimento aleatorio: escoger al azar una muestra de 3000 personas dentro de una población de 37 000 000, con una tasa de participación $p$.
  \item Llamamos a  $\hat{p}$ la variables aleatoria: proporción de "1"s en la muestra escogida.
  \item ¿Cuál es la distribución de valores de $\hat{p}$? \[
  \hat{p}\sim \mathcal{N}\left( p,\dfrac{p(1-p)}{n} \right) 
  \] 
  Es lo qe llamamos la \lb{distribución muestral} de $\hat{p}$.
\end{itemize}
\end{tcolorbox}
\subsection{Uso de la distribución muestral}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{La distribución muestral de $\hat{p}$:}]
Es la distribución esperada de los valores de $\hat{p}$ respecto a todas las muestras de ese tamaño que podría extraer.
\end{tcolorbox}
\begin{center}
  \includegraphics[width=0.7\textwidth]{"Tema 1/figures/Figure 3"}
\end{center}
\subsection{Antes de extraer una muestra:}
\begin{itemize}[label=\textbullet]
  \item ¿Es suficiente el tamaño de la muestra para el riesgo asumible y la precisión requerida?
  \item Una vez extraida la muestra:
    \begin{itemize}[label=\textrightarrow]
      \item ¿Puedo dar un margen de error?
      \item ¿Puedo de decidir si $p$ poblacional es, por ejemplo, mayor que un valor dado?
    \end{itemize}
\end{itemize}
\subsection{Otro ejemplo: valores muestrales de una distribución normal}
\begin{center}
  \includegraphics[width=\textwidth]{"Tema 1/figures/Figure 4"}\\
  \includegraphics[width=\textwidth]{"Tema 1/figures/Figure 5"}\\
\end{center}
\subsection{Un resultado importante}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Ley (débil) de los grandes números}]
Sea $X$ una variable aletoria y $g(X)$ una variable aleatoria transformada de  $X$, con esperanza y momento de orden 2 finitos. Supongamos $X_1,X_2,\dots,X_n,\dots$ una sucesión de variables aleatorias (vv.aa) independientes con la misma distribución que $X$, entonces  \[
  \lim_{n \to +\infty} P\left[ \left| \dfrac{\sum_{i=1}^{n} g(X_i)}{n}-E[g(X)] \right| < \varepsilon \right] =1,\text{ para todo }\varepsilon>0.
\] 
\end{tcolorbox}
\subsection{Algunos términos}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
\begin{itemize}[label=\textbullet]
  \item Sea una variable aleatoria $X$. Consideramos $n$ variables aleatorias independientes e idénticamente distribuidas  $X_1,X_2,\dots,X_n$, que se distribuyen como $X$. La variable aleatoria multidimensional  $(X_1,X_2,\dots,X_n)$ es una \lb{muestra aleatoria simple} (m.a.s) de $X$.
  \item Cualquier cantidad calculada a partir de las observaciones de un muestra: \lb{estadístico}.
  \item Experimento aleatorio: extraer una muestra. Consideramos un estadístico como una variable aleatoria. Nos interesa conocer la distribución del estadístico: \lb{distribución muestral}. 
\end{itemize}
\end{tcolorbox}
\subsection{Ejemplos de estadísticos}
\begin{itemize}[label=\textbullet]
  \item Proporción muestral: $\hat{p}$
  \item Media muestral: $\overline{X}=\dfrac{1}{n}\sum_{i=1}^{n} X_i$.
  \item Desviación típica muestral: $S_X=\sqrt{\dfrac{1}{n+1}\sum_{i=1}^{n} (X_i-\overline{X})^2} $
\end{itemize}
\subsection{La media muestral}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Contexto}]
Estudiamos una variable $X$ cuantitativa.
\begin{itemize}[label=\textbullet]
  \item Estamos interesados en $\mu$, el centro de la distribución de $X$.
  \item Extraemos una muestra de tamaño  $n$:  \[
  x_1,x_2,\dots,x_{n}
  \] 
\item Calculamos su media $\overline{x}$ para aproximar  $\mu$.
\item ¿Cuál es la distribución muestral de $\overline{X}$?
\end{itemize}
\end{tcolorbox}
\Ej
\begin{itemize}[label=\textbullet]
  \item Quiero medir una cantidad. Hay variabilidad en las mediciones.
  \item Introduzco una variable aleatoria $X$="valor proporcionado por el aparato".
  \item  $\mu$ representa el centro de los valores.
  \item Extraigo una muestra de tamaño 5 del valor de $X$
\end{itemize}
\subsubsection{Esperanza y varianza de la media muestral}
Llamamos $\mu=E[X]$ y $\sigma^2=\mathrm{Var}(X)$.
\begin{itemize}[label=\textbullet]
  \item Tenemos \[
      \bboxed{E[\overline{X}]=\mu.} 
  \] 
  \begin{itemize}[label=\textrightarrow]
    \item Es decir que el centro de la distribución muestral de $\overline{X}$ coincide con el centro de la distribución  $X$.
  \end{itemize}
\item Tenemos $\mathrm{Var}(\overline{X})=\dfrac{\sigma^2}{n}$, es decir, la dispersión de la distribución muestral de $\overline{X}$ es  $\sqrt{n} $ veces más pequeña que la dispersión inicial de $X$.
\end{itemize}
\textbf{Ilustración: $X$ inicial,  $\overline{X}$ con  $n=3, \overline{X} $ con $n=10$.}
\begin{center}
  \includegraphics[width=\textwidth]{"Tema 1/figures/Figure 6"}
\end{center}
\subsection{Consecuencia práctica}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Aparato de medición}]
\begin{itemize}[label=\textbullet]
  \item Experimento: llevar a cabo una medición con un aparato.
  \item Variable aleatoria $X$: "valor propocionado por el aparato".
  \item  $E[X]$: centro de la distribución de los valores proporcionados por el aparato.
    \begin{itemize}[label=\textrightarrow]
      \item Lo deseable: $E[X]$=valor exacto de la cantidad que buscamos medir.
      \item En este caso, decimos: el aparato es  \lb{exacto}. 
    \end{itemize}
  \item $\sigma_X$: dispersión de la distribución de los valores proporcionados por el aparato.
    \begin{itemize}[label=\textrightarrow]
      \item Lo deseable: $\sigma_X$ pequeño.
      \item En este caso, decimos: el aparato es \lb{preciso}. 
    \end{itemize}
\end{itemize}
\end{tcolorbox}
\subsubsection{Analogía con una diana}
\begin{center}
  \includegraphics[width=\textwidth]{"Tema 1/figures/Figure 7"}
\end{center}
\subsection{Varianza muestral}
Si $(X_1,X_2,\dots,X_n)$ es una muestra aleatoria simple de $X$, definimos la  \lb{varianza muestral} $S_n^2$ como \[
  S_n^2=\dfrac{1}{n-1}\sum_{i=1}^{n} \left( X_i-\overline{X}_n \right)^2. 
\] 
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Fórmula alternativa para $S_n^2$:}]
\[
  S_n^2=\dfrac{n}{n-1}\left( \overline{X^2}_n-(\overline{X}_n)^2 \right) ,
\] donde $\overline{X^2}_n=\dfrac{1}{n}\sum_{i=1}^{n}X_i^2$.
\end{tcolorbox}
\subsubsection{Dos apuntes}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{En algunos textos en castellano:}]
  Se suele llama $S_n^2$ \lb{cuasi-varianza muestra}, reservando el término varianza muestral para la cantidad $\dfrac{1}{n}\sum_{i=1}^{n} \left( X_i-\overline{X}_n \right) ^2$. 
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{En estas fórmulas:}]
  Omitimos, si no hay confusión posible, el subíndice $n$, escribiendo  $S^2,\, \overline{X}=\sum_{i=1}^{n} X_i$ y $\overline{X^2}=\dfrac{1}{n}\sum_{i=1}^{n} X_i^2$.
\end{tcolorbox}
\subsection{Esperanza de la varianza muestral}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Proposición}]
Si $(X_1,X_2,\dots,X_n)$ es una muestra aleatoria simple de $X$ con varianza  $\sigma_X^2$, \[
  E[S_n^2]=\sigma_X^2.
\] 
\end{tcolorbox}
\subsection{Distribuciones muestrales de $\overline{X}$ y  $S^2$}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Tened en cuenta}]
\begin{itemize}[label=\textbullet]
  \item Los resultados anteriores sobre $E[\overline{X}]$ y  $\sigma_{\overline{X}}$ son válidos sea cual sea el modelo escogido para la distribución de $X$.
  \item Si queremos decir algo más preciso sobre la distribución de  $\overline{X}$ (densidad, etc\dots) necesitamos especificar la distribución de $X$.
  \item En el caso en que la variable  $X$ siga una distribución normal, el  \textbf{teorema de Fisher} analiza cómo se comportan los estadísticos anteriores y nos permiten establecer una serie de consecuencias que serán utilizadas posteriormente en los temas de intervalos de confianza y de constrastes de hipótesis. 
\end{itemize}
\end{tcolorbox}
\subsection{Distribución de $\overline{X}$ y  $S^2$ para una m.a.s. de una distribución normal}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Teorema de Fisher}]
Consideramos una muestra aleatoria simple de una variable aleatoria $X$ con distribución normal  $\mathcal{N}(\mu,\sigma^2)$, entonces se verifica:
\begin{enumerate}[label=\arabic*)]
  \item $\overline{X}_n$ y  $S_n^2$ son dos variables aleatorias independientes.
  \item $\dfrac{\overline{X}_n-\mu}{\sigma / \sqrt{n} }\sim \mathcal{N}(0,1)$
  \item $\dfrac{(n-1)S_n^2}{\sigma^2}\sim \chi_{n-1}^2$.
\end{enumerate}
\end{tcolorbox}
\subsection{Recordatorio: distribución $\chi^2$ con $p$ grados de libertad}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{La distribución $\chi^2$.}]
Para $p\in \N^{+}$, la función de densidad de la distribución $\chi^2$ es igual a \[
\dfrac{1}{\Gamma\left( \frac{p}{2}  \right) 2^{\frac{p}{2} }}\cdot x^{\frac{p}{2} -1}e^{\frac{x}{2} },\quad \text{si }x>0,
\] donde $\Gamma$ denota la función Gamma (Nota: para cualquier real $\alpha>0,\Gamma(\alpha)=\int_{0}^{+\infty} t^{\alpha-1}e^{-t}\dt$).
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Caracterización de la $\chi^2$}]
Si $ Z_1,\dots,Z_p$ son $p$ variables aleatorias independientes, con $Z_i\sim \mathcal{N}(0,1)$, entonces la variable aleatoria $X$ definida como \[
X=Z_1^2+\cdots+Z_p^2=\sum_{i=1}^{p} Z_i^2
\] tiene una distribución $\chi^2$ con $p$ grados de libertad.
\end{tcolorbox}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
  \item \lb{¿Cómo es su función de densidad?}
\end{itemize}
Depende de los grados de libertad
\begin{center}
  \includegraphics[width=\textwidth]{"Tema 1/figures/Figure 8"}
\end{center}
\subsection{Distribución t-Student}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Hemos visto, si $X$ es Normal:}]
\[
  \dfrac{\overline{X}_n-\mu}{\sigma / \sqrt{n} }\sim \mathcal{N}(0,1).
\] Si queremos centrarnos en $\mu$ es natural sustituir en ella $\sigma$ por $S_n$.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Proposición}]
Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una población $\mathcal{N}(\mu,\sigma^2)$, \[
  T=\dfrac{\overline{X}-\mu}{S / \sqrt{n} }
\] tiene por densidad 
\begin{equation}
  f_{n-1}(t)\propto \dfrac{1}{\left( \frac{1+t^2}{n-1}  \right)^{\frac{n}{2}  }},\quad-\infty<t<\infty,
\end{equation}
La distribución que admite esta densidad se llama \lb{distribución t-Student} con $n-1$ grados de libertad. Escribimos  $T\sim t_{n-1}$.
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Su densidad}]
La función de densidad de un t-Student con $k$ grados de libertad: \[
f_k(t)=\dfrac{\Gamma\left( \frac{k+1}{2}  \right) }{\Gamma\left( \frac{k}{2}  \right) }\cdot \dfrac{1}{\sqrt{k\pi} }\cdot \dfrac{1}{\left( \frac{1\tau^2}{k}  \right) ^{\frac{k+1}{2} }},\quad-\infty<t<\infty,
\] donde $\Gamma$ denota la función Gamma.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Caracterización de la t-Student como cociente}]
Si $Z$ e $Y$ son dos variables aleatorias independientes, con $Z\sim \mathcal{N}(0,1)$ e $Y\sim \chi_p^2$, el cociente \[
T=\dfrac{Z}{\sqrt{\frac{Y}{p} } }\sim t_p,
\] donde $t_p$ denota la t-Student con  $p$ grados de libertad.
\end{tcolorbox}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
  \item \lb{¿Cuál es la forma de la densidad de una t-Student?}
\end{itemize}
Tiene colas más pesadas que una normal
\begin{center}
  \includegraphics[width=0.7\textwidth]{"Tema 1/figures/Figure 9"}
\end{center}
\subsection{Distribución F de Snedecor para el cociente de varianzas}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Proposición}]
Consideremos $U_1$ y $U_2$ dos variables aleatorias independientes con distribución $\chi^2$ con $p_1$ y $p_2$ grados de libertad, respectivamente.

El cociente $F=\dfrac{\frac{U_1}{p_1} }{\frac{U_2}{p_2}}$ admite la densidad \[
  f_F(x)=\dfrac{\Gamma\left( \frac{p_1+p_2}{2}  \right) }{\Gamma(p_1)\Gamma(p_2)}\left( \dfrac{p_1}{p_2} \right) ^{p_1}\dfrac{x^{\frac{p_{1}}{2 }-1}}{\left( 1+\frac{p_1}{p_2} x \right) ^{\frac{p_1+p_2}{2} }}.
\] Esta distribución se llama F de Snedecor $p_1$ y $p_2$ grados de libertad y escribimos $F\sim F_{p_1,p_2}$.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Consecuencia}]
Consideremos $X$ e $Y$ variables aleatorias normales independientes con varianzas $\sigma_X^2$ y $\sigma_Y^2$, así como $X_1,\dots,X_{n_x}$ e $Y_1,\dots,Y_{n_Y}$ dos muestras aleatorias simples de $X$ e $Y$, respectivamente. Deducimos que \[
  \dfrac{\frac{S_X^2}{\sigma_X^2}}{\frac{S_Y^2}{\sigma_Y^2} }\sim F_{n_X-1,n_Y-1}.
\] 
\end{tcolorbox}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
  \item \lb{¿Cuál es la forma de la densidad de una F de Snedecor?}
\end{itemize}
Depende mucho de los grados de libertad

\begin{center}
  \includegraphics[width=0.85\textwidth]{"Tema 1/figures/Figure 10"}
\end{center}
\subsection{Si la distribución de $X$ no es Normal}
No podemos decir nada en general, \lb{excepto} si $n$ es grande\dots
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Teorema Central del Límite}]
  Si $n$ es \textit{"suficientemente"} grande, se puede aproximar la distribución de $\overline{X}$ por una Normal con media  $\mu$ y varianza $\dfrac{\sigma^2}{n}$: \[
    \overline{X}\sim \mathcal{N}\left( \mu,\dfrac{\sigma^2}{n} \right) \text{ aproximadamente. }
  \]  
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Formulación matemática}]
  El resultado anterior se taduce por una convergencia de la sucesión de las variables aleatorias $\left( \overline{X}_n \right)_n $ en distribución cuando $n\to \infty$.
\end{tcolorbox}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
  \item \lb{¿Cuándo considerar que $n$ es grande?}
\end{itemize}
Depende de la forma de la distribución de $X$:
 \begin{itemize}[label=\textbullet]
  \item Si $X$ casi Normal: $n$ pequeño es suficiente.
  \item Si $X$ es muy asimétrico: $n$ mucho más grande necesario.
\end{itemize}
En general, se suele considerar $n\ge 30$ suficiente\dots

\textbf{Ilustración, $X$ inicial $\sim \mathrm{Exp}(\lambda=0.5),\,\overline{X}$ con $n=3,10$ y  $n=30$}
\begin{center}
  \includegraphics[width=\textwidth]{"Tema 1/figures/Figure 11"}
\end{center}
\subsection{Distribución muestral de la proporción muestral}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Contexto}]
\begin{itemize}[label=\textbullet]
    \item Hay situaciones donde $X$ toma el valor 0 o 1, con probabilidades $1-p$ y $p$, respectivamente.
    \item Por ejemplo, el siguiente experimento: escoger una pieza en la producción. $X=1$ si es defectuosa,  $X=0$ si es correcta.
    \item Repetimos  $n$ veces el experimento. Obtenemos $$x_1=1,x_2=0,x_3=0,x_4=1,x_5=0,\dots,x_n$$ Contamos $N$ el número de "1"s.
    \item La proporción muestral es: \[
    \hat{p}=\dfrac{N}{n}.
    \] 
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Distribución exacta de $\hat{p}$}]
¿Cuál es la distribución de $N$?
\begin{itemize}[label=\textbullet]
    \item Experimento simple con situación dicotómica, repetimos $n$ veces \dots \[
    N=\mathcal{B}(n,p).
    \] 
\item POdemos usar esta distribución para hacer cálculos exactos\dots
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Distribución aproximada de $\hat{p}$}]
Tenemos que $N=X_1+X_2+\cdots+X_n$, por lo tanto \[
    \hat{p}=\dfrac{N}{n}=\dfrac{X_1+X_2+\cdots+X_n}{n}=\overline{X}.
\] 
Por el Teorema Central del Límite: \[
\hat{p}\sim \mathcal{N}\left( p, \dfrac{p(1-p)}{n} \right) \:\text{aproximadamente.}
\] Podemos usar esta distribución para hacer cálculos aproximados.
\end{tcolorbox}
\subsection{Simulación y método de Monte-Carlo}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Motivación}]
En muchas situaciones, la capacidad de simular valores de las distribuciones de interés puede resultar útil calcular o estimar cantidades relevantes para la inferencia sobre la distribución de $X_1,X_2,\dots,X_n$.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{¿Qué es ser capaz de simular valores de una distribución $f$?}]
\begin{itemize}[label=\textbullet]
    \item Se refiere a la posibilidad de producir, para cualquier tamaño $k$, conjuntos de valores $u_1,u_2,\dots,u_k$, cuyo comportamiento imita el de $k$ realizaciones aleatorias independientes de la distribución $f$.
    \item Quiere ecir que las propiedades del conjunto generado lo hacer indistinguible, si le aplicamos tests de independencia o de bondad de ajuste, de  $k$ realizaciones independientes de $f$.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Simulación y método de Monte-Carlo}]
\begin{itemize}[label=\textbullet]
    \item Como hemos visto en los primeros ejemplos, gráficos como el histograma de frecuencias se comportan como la función de densidad de la variable de la que provienen las observaciones. También se pueden utilizar gráficos como la función de distribución empírica que veremos más adelante.
    \item Como consecuencia, dado un estadístico, si podemos obtener un número grande de observaciones del mismo podemos a través de alunos gráficos obtener información sobre su distribución.
    \item Podemos generar esas observaciones a través de lo que se conoce como simulaciones, observaciones generadas mediante algún algoritmo.
    \item Esta metodología que se puede aplicar en muchas otras situaciones se conoce como el \lb{método de Monte-Carlo}. 
\end{itemize}
\end{tcolorbox}
\subsubsection{Muestreo de Monte-Carlo para aproximar esperanzas}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Ley de los grandes números}]
    Consideremos una muestra aleatoria simple $X_1,X_2,\dots,X_n$ de una distribución $X$. Para cualquier función  $g$ que cumple  $E[g^2(X)]<+\infty$, tenemos que, con probabilidad 1, \[
        \lim_{n \to \infty} \dfrac{1}{n}\sum_{i=1}^{n} g(X_i)=E[g(X)].
    \] 
\end{tcolorbox}
\subsubsection*{Ejemplos}
\begin{itemize}[label=\textbullet]
    \item $g(x)=x: \lim_{n \to \infty} \dfrac{1}{n}\sum_{i=1}^{n} X_i=E[X]$, es decir $\overline{X}_n\longrightarrow E[X]=\mu_X$.
    \item $g(x)=(x-\mu_X)^2: \lim_{n \to \infty} \dfrac{1}{n}\sum_{i=1}^{n} (X_i-\mu_X)^2=E[(X-\mu_X)^2]=\mathrm{Var}(X)$.
    \item Si combinamos las dos convergencias anteriores: $\lim_{n \to \infty} \dfrac{1}{n}(X_i-\overline{X}_n)^2=\mathrm{Var}(X)$, es decir, $S_n^2\longrightarrow \mathrm{Var}(X)$.
    \item $g(x)=1_{x\le q}: \lim_{n \to \infty} \dfrac{1}{n}\sum_{i=1}^{n} 1_{X_i\le q}=P(X\le q)$, es decir, que las frecuencias acumuladas relativas convergen hacia la probabilidad asociada.
\end{itemize}
\subsubsection*{Aplicaciones}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Ejemplo: el movimiento Browniano}]
    \begin{itemize}[label=\textbullet]
        \item Es proceso muy usado en la predicción de precios (opciones) en matemáticas financieras.
        \item Una caracterización simplificada: es la suma infinitesimal de pequeñas contribuciones normales independientes.
        \item Para cualquier $t,W_t\sim \sum_{i=1}^{\frac{t}{h} }\sqrt{h} \cdot Z_i $, donde $h$ es el paso infinitesimal y $Z_i$ son normales estándares independientes e idénticamente distribuidos 
    \end{itemize}
\end{tcolorbox}
\subsection{Movimiento Browniano}
\begin{center}
    \includegraphics[width=0.8\textwidth]{"Tema 1/figures/Figure 12"}
\end{center}
\subsection{En finanzas, el modelo de Black-Scholes}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{El movimiento Browniano Geométrico:}]
Lo propusieron Merton, Scholes y Black como modelo teórico para precios de acciones: \[
S_t=S_0\exp\left( \left( \mu-\dfrac{1}{2}\sigma^2 \right) t+\sigma W_t \right) .
\] 
\begin{itemize}[label=\textbullet]
    \item $S_0$: precio inicial de la acción.
    \item  $\mu$: el drift.
    \item $\sigma^2$: la volatilidad
\end{itemize}
\end{tcolorbox}
\begin{center}
    \includegraphics[width=0.8\textwidth]{"Tema 1/figures/Figure 13"}
\end{center}
\subsection*{¿Cuán observaremos $S(t)\ge 110$€?}
\begin{center}
    \includegraphics[width=0.8\textwidth]{"Tema 1/figures/Figure 14"}
\end{center}
\begin{itemize}[label=\textbullet]
    \item Podemos simular muchas trayectorias del Movimiento Browniano Geométrico y observa qué pasa con el tiempo en el que supera el umbral 110.
    \item Esto es Monte-Carlo.
    \item Luego podremos obtener indicadores de la distribución de este tiempo.
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
    \item \lb{Representamos las 8 primeras}
        \begin{center}
            \includegraphics[width=0.8\textwidth]{"Tema 1/figures/Figure 15"}
        \end{center}
    \item \lb{¿Cuál es la probabilidad de que alcance el umbral antes de un año?}

        \begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Por Monte-Carlo:}]
        Simulamos 1000 trayectorias hata el año y aproximamos la probabilidad que nos interesa por la porporción de trayectorias que alcanzan los 110 euros.
        \end{tcolorbox}
\end{itemize}
\subsection{Simulación y método de Monte-Carlo}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Simulación de variables aleatorias}]
    \begin{itemize}[label=\textbullet, leftmargin=*]
        \item Lo que nos planteamos ahora es qué algoritmo podemos utilizar para generar observaciones (simulaciones) de una variable aleatoria.
        \item Para un gran número de varaibles aleatorias y modelos probabilísticos, estas simulaciones ya están incorporadas en los paquetes y lenguajes de programación con enfoque matemático y estadístico, como \textbf{\texttt{R}}.
        \item Aquí describiremo uno de los métodos más básicos, basado en la inversa de la función de distribución.
        \item El \lb{método de la función inversa} es uno de los principales métodos de simulación de variables aleatorias.
        \item Está basado en la inversa de la función de distribución de una variable aleatoria.
        \item No todas las funciones de distribucion admiten inversa, como la conocemos usualmente. Para ello es necesaria que sea continua y estrictamente creciente.
        \end{itemize} 
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Inversa generalizada de la función de distribución o función cuantil}]
 Dada una función de distribución $F$, su \lb{función cuantil (inversa generalizada)}  se define como \[
 F^{-1}(p)=\inf \{x|F(x)\ge p\}\text{, para todo }p\in (0,1). 
 \] 
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Propiedades de la función cuantil}]
\begin{enumerate}[label=\arabic*)]
    \item $F(F^{-1}(p))\ge p$, para todo $p \in (0,1)$.
    \item $F^{-1}(F(x))\le x$, para todo $x$ donde  $F(x)>0$.
    \item Si $U\sim U(0,1)$ entonces $F^{-1}(U)$ tiene como distribución $F$.
    \item Dada una variable aleatoria  $X$, con media finita y función de distribución  $F$ se verifica que \[
            E[X]=\int_{0}^{1} F^{-1}(u) \du .
    \] 
\end{enumerate}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Método de la función inversa}]
\begin{itemize}[label=\textbullet]
    \item Dada una variable aleatoria $X$ con función de distribución $F$, si generamos observaciones independientes $U\sim U(0,1),\,u_1,u_2,\dots,u_n$, entonces los valores $$x_1=F^{-1}(u_1),x_2=F^{-1}(u_2),\dots,x_n=F^{-1}(u_n)$$ constituyen una observación de la muestra aleatoria simple de tamaño $n$ de la variable aleatoria $X$.
\end{itemize}
\end{tcolorbox}

\subsubsection*{Generación de valores de una distribución uniforme}
\begin{center}
    \includegraphics[width=0.7\textwidth]{"Tema 1/figures/Figure 16"}
\end{center}
\subsubsection*{Generación de valores de una distribución gamma}
\begin{center}
    \includegraphics[width=0.7\textwidth]{"Tema 1/figures/Figure 17"}
\end{center}
\subsection*{Transformación de una variable aleatoria}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Planteamiento del problema}]
En muchas ocasiones, tenemos una variable aleatoria continua $X$, de la que conocemos la función de densidad, pero nos interesa conocer cómo se comporta una transformación de $X,\,Y=\varphi(X)$.

\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Teorema}]
Sea $X$ una viarble aleatoria con función de densidad $f_X$ definida en un intervalo abierto  $(a,b)\subseteq \R$. Sea $\varphi:(a,b)\to \R$ tal que:
\begin{itemize}[label=\textbullet]
    \item Es continua.
    \item Es estrictamente creciente o decreciente.
    \item $\varphi^{-1}$ es diferenciable.
\end{itemize}
Entonces, la variable aleatoria $Y=\varphi(X)$ tiene función de densidad \[
f_Y(y)=\begin{cases}
    f_X(\varphi^{-1}(y))\left| \dfrac{\mathrm{d}}{\mathrm{d}x}(\varphi^{-1}(y)) \right| , & \text{si }y\in \varphi(a,b)\\
    0 & \text{en otro caso}

\end{cases}
\] 
\end{tcolorbox}
\subsection*{Función característica}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
Sea $X$ una variable aleatoria cualquiera. La \lb{función característica} de  $X$ se define como \[
    \phi(t)=E[e^{itX} ]=\int_{-\infty}^{\infty} e^{itx}\mathrm{d}F(x)  
\] en donde $i=\sqrt{-1} $
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black]
Nota: $\phi(t)=\int_{-\infty}^{\infty} xos(tx)\mathrm{d}F(x)+i \int_{-\infty}^{\infty} \sin(tx)\mathrm{d}F(x).  $
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Propiedades}]
\begin{itemize}[label=\textbullet]
    \item \lb{Existencia:} $|\phi(t)|\le 1$, para todo $t\in \R$.
    \item Si $X$ e  $Y$ son variables aleatorias \lb{independientes}, entonces \[
    \phi_{X+Y}(t)=\phi_X(t)\phi_Y(t),
    \]para todo $t\in \R$.
\end{itemize}
\end{tcolorbox}
\subsection*{Desigualdades}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Desigualdad de Markov}]
    Si $Z$ es una variable aleatoria no negativa con medi finita  $E[Z]$ y  $\varepsilon>0 $, entonces \[
        \varepsilon\mathrm{Pr}[Z\ge\varepsilon ]=\varepsilon\int_{[\varepsilon,\infty)}\mathrm{d}F_Z(x)\le \int_{[\varepsilon,\infty)}x\mathrm{d}F_Z(x)\le \int_{[0,\infty)}x\mathrm{d}F_Z(x)=E(Z)
    \] (donde $F_Z(x)=\mathrm{Pr}[Z\le x]$ es su función de distribución), es decir \[
    \mathrm{Pr}[Z\ge \varepsilon]\le \dfrac{E[Z]}{\varepsilon}.
    \] 
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Desigualdad de Chebyshev}]
    Si $X$ es una variable aleatoria con media finita $\mu=E[X]$ y varianza $\sigma^2=\mathrm{Var}(X)>0$, entonces tomando $Z=\dfrac{(X-\mu)^2}{\sigma^2}\ge 0$ y aplicando la desigualdad de Markov, tenemos \[
    \mathrm{Pr}\left[ \dfrac{(X-\mu)^2}{\sigma^2}\ge \varepsilon \right] \le \dfrac{1}{\varepsilon}
    \] para todo $\varepsilon>0$.

    También se puede escribir como \[
        \mathrm{Pr}[(X-\mu)^2<\varepsilon\sigma^2]\ge 1-\dfrac{1}{\varepsilon}
    \] o como \[
    \mathrm{Pr}[|X-\mu|<r]\ge 1-\dfrac{\sigma^2}{r^2},
    \] para todo $r>0$.
\end{tcolorbox}
\newpage
\input{"Tema 1/Ejercicios Tema 1/Ejercicios Tema 1.tex"}
