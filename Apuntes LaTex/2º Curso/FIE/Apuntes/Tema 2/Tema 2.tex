\section{Estimación}
\subsection{Introducción}
\begin{itemize}[label=\textbullet]
    \item Hemos modelizado un experimento con una variable aleatoria $X$.
    \item La \lb{estimación} hace referencia al proceso de conseguir información sobre la distribución de $X$ a partir de los valores de una muestra, aproximando valores asociados a la distribución mediante el valor de un estadístico en una muestra concreta.
\end{itemize}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Dos situaciones}]
\begin{itemize}[label=\textbullet]
    \item Nuestro modelo supone que la distribución de $X$ pertence a una familia paramétrica de distribuciones: tienen una determinada forma con unos parámetros variables.
        \begin{itemize}[label=\textrightarrow]
            \item Buscamos información sobre el valor de los parámetros. \lb{Estimación paramétrica}. 
        \end{itemize}
    \item No limitados la familia de distribuciones a la que pertence nuestro modelo.
        \begin{itemize}[label=\textrightarrow]
            \item Buscamos información sobre la distribución en sí (función de distribución, de densidad o función puntual de probabilidad). \lb{Estimación no paramétrica}.
        \end{itemize}
    \item A lo largo de las prácticas veremos también la estimación de parámetros que no necesitan de una familia paramétrica, como es el caso de la mediana.
\end{itemize}
\end{tcolorbox}
\subsection{Ejemplos de estimación paramétrica}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Sondeo sobre intención de participación en una elecciones}]
\begin{itemize}[label=\textbullet]
    \item Queremos estimar la tasa de participación antes de unas elecciones generales.
    \item Formulamos un modelo: experimento: "escoger una persona al azar en el censo". $X$: variable dicotómica ("Sí", o "No").  $p=P(X=\mathrm{Si})$
    \item El modelo pertenece a la familia paramétrica de Bernoulli.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Determinación de la concentración de un producto}]
\begin{itemize}[label=\textbullet]
    \item Quiero determinar la concentración.
    \item Formulo el modelo: experimento="llevar a cabo una medición". $X$: "valor proporcionado por el aparato".  $X\sim \mathcal{N}(\mu,\sigma^2)$.
    \item El modelo pertence a la familia paramétrica de las distribuciones normales
\end{itemize}
\end{tcolorbox}
\subsection{Estimación paramétrica: estimación puntual}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Ingredientes del modelo}]
\begin{itemize}[label=\textbullet]
    \item Experimento aleatoria
    \item Variable aleatoria $X$ con una distribución $f$ que pertence a una familia paramétrica $\{f_\theta,\theta \in \Theta\} $.
    \item Disponemos de una muestra de la distribución de $X$.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
Cualquier estadístico diseñado para aproximar el valor de un parámetro $\theta$ del modelo, se llama \lb{estimador puntual} del parámetro $\theta$.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Ejemplos de estimadores paramétricos}]
\begin{center}
    \begin{tabular}{cc}
        $\theta$ & \textbf{Estimador}\\ \hline
        $\mu$ & $\overline{X}$, media muestral\\ \hline
        $\sigma^2$ & $S^2$, varianza muestral\\ \hline
        $p$ & $\hat{p}$, proporción muestral
    \end{tabular}
\end{center}
\end{tcolorbox}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{A tener en cuenta:}]
\begin{itemize}[label=\textbullet]
    \item Un estimador es una variable aleatoria, su valor depende de la muestra concreta escogida.
    \item Para controlar bondad de nuestra estimación, nos basaremos en el estudio de la distribución del estimador.
\end{itemize}
\end{tcolorbox}
\subsection{Métodos de construcción de estimadores}
Para $\mu,\sigma^2$ o $p$, es fácil pensar en estimadores naturales, pero para modelos o parámetros más sofisticados, vamos a ver métodos generales.

\textbf{Veremos dos métodos en este tema}:
\begin{itemize}[label=\textbullet]
    \item El método de los momentos
    \item El método de la máxima verosimilitud
\end{itemize}
\subsection{Método de los momentos}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Contexto}]
\begin{itemize}[label=\textbullet]
    \item Experimento con una variable aleatoria $X$, suponemos $f_X \in \{x\longmapsto f_\theta(x),\theta \in \Theta \} $.
    \item El parámetro $\theta$ tiene dimensión $p$.
    \item Consideramos una muestra aleatoria simple  $X_1,\dots,X_n$ de $X$.
\end{itemize}
\end{tcolorbox}
Si $\theta$ tiene dimensión $p$, igualamos los $p$ primeros momentos de $f_\theta$ con los equivalentes muestrales.

Sea $\mu_k(\theta)$ el momento de orden $k$ de la distribución $f_\theta, \mu_k=E[X^k]$. Resolvemos: \[
\begin{array}{r}
    \mu_1(\theta)=\overline{X},\\
    \mu_2(\theta)=\overline{X^2},\\
    \vdots\\
    \mu_p(\theta)=\overline{X^p}.
\end{array}
\] 
\subsection*{Ejemplos}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Calculad los estimadores usando el método de los momentos en los dos casos:}]
\begin{itemize}[label=\textbullet]
    \item Modelo normal: $X\sim \mathcal{N}(\mu, \sigma^2)$, donde $\theta=(\mu,\sigma^2)$. \[
    \begin{array}{ll}
        X\leadsto \mathcal{N}(\mu,\sigma^2) & \mu=E[X]\\
        \theta=(\mu,\sigma^2),\quad p=2 & \sigma^2=\mathrm{Var}[X]=E[X^2]-(E[X])^2\longrightarrow E[X^2]=\sigma^2+\mu^2\\
        \mu_1(\theta)=E[X]=\overline{x} & \hat{\mu}=\overline{x}\\
        \mu_2(\theta)=E[X^2]=\overline{x^2} & \hat{\sigma}^2=\overline{x^2}-(\overline{x})^2
    \end{array}
    \] 
\item Modelo de Bernoulli: $X\sim \mathrm{Bernoulli}(p)$, donde desconocemos $p$.
     \[
    \begin{array}{l}
        X\leadsto \mathrm{Bernoulli}(p),\quad 0<p<1\\
        \theta=p\\
        \mu(\theta)=E[X]=\overline{x}=p\\
        \hat{p}=\overline{x}\lb{\text{ (proporción muestral)}} 
    \end{array}
    \] 
\end{itemize}
\end{tcolorbox}
\subsection{Método de máxima verosimilitud}
\begin{itemize}[label=\textbullet]
    \item El método más utilizado de construicción de un estimador puntual.
    \item Se basa en lo que se conoce como \lb{función de verosimilitud}. 
\end{itemize}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
\begin{itemize}[label=\textbullet]
    \item Sea $X$ una variable aleatoria, con distribución $x\longmapsto f_X(x;\theta)$ (función de densidad o función puntual de probabilidad), donde $\theta$ es de dimensión $p:\theta\in \Theta\subset \R^p$.
    \item Para un valor concreto de una muestra aleatoria simpl $(X_1,\dots,X_n)$, que denotamos por $(x_1,\dots,x_n)$, consideramos la función de $\theta$: \[
    L_n: \begin{cases}
        \Theta\subset \R^p\longrightarrow \R^+\\
        \theta\longmapsto L_n(\theta)=f_{X_1,\dots,X_n}(x_1,\dots,x_n;\theta)=\prod_{i=1}^{n} f_X(x_i;\theta). 
    \end{cases}
    \] 
\item La función $L_n$ es la  \lb{función de verosimilitud}. Nos dice lo creíblles (verosímiles) que son las observaciones para ese valor del parámetro. 
\end{itemize}
\end{tcolorbox}

\subsubsection*{Ejemplo de cálculo de la verosimilitud}
\begin{itemize}[label=\textbullet]
    \item Tiramos 10 veces una moneda (1 es cara, 0 es cruz), y obtenemos: 0,0,1,0,1,1,1,1,1,1.
    \item La verosimilitud asocia a cada $p$ el valor de \[
    \mathrm{Pr}(X_1=0,X_2=0,X_3=1,X_4=0,X_5=1,X_6=1,X_7=1,X_8=1,X_9=1,X_{10}=1),
    \] 
    por lo que \[
    L_n(p)=(1-p)(1-p)p(1-p)p^6=(1-p)^3\cdot p^7.
    \] 
    \begin{center}
        \includegraphics[width=0.7\textwidth]{"Tema 2/figures/Figure 1"}
    \end{center}
\end{itemize}
\subsection{Estimador de máxima verosimilitud}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
El \lb{estimación de máxima verosimilitud} $\hat{\theta}$  de $\theta$ es cualquier valor de $\theta$ que maximiza $\theta\longmapsto L_n(\theta)$, es decir, \[
    \hat{\theta}\underset{\theta \in \Theta}{\mathrm{argmax}}L_n(\theta).
\] 
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Nota}]
\begin{itemize}[label=\textbullet]
    \item La maximización se realiza sobre todos los valores admisibles para el parámetro $\theta$.
    \item Podría haber de un máximo.
\end{itemize}
\end{tcolorbox}
\subsection*{Estimación de la proporción}
Retomamos el ejemplo de las 10 monedas: \[
p\longmapsto L_n(p)=(1-p)(1-p)p(1-p)p^6=(1-p)^3\cdot p^7.
\] 
\begin{center}
    \includegraphics[width=0.7\textwidth]{"Tema 2/figures/Figure 2"}
\end{center}
\subsubsection*{Ejemplos}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Ejemplos de estimación por máxima verosimilitud}]
Calculad los estimadores usando el método de máxima verosimilitud en los dos casos:
\begin{itemize}[label=\textbullet]
    \item $X\sim \mathrm{Bernoulli}(p)$, donde desconocemos $p$.
    \item $X\sim \mathcal{N}(\mu,\sigma^2)$, donde desconocemos $\theta=(\mu,\sigma^2)$.
\end{itemize}
\end{tcolorbox}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
    \item \lb{En el primer caso anterior, calcular la distribución de Bernoulli, donde desconocemos $p$.}

        $X\leadsto \mathrm{Bernoulli}(p),\quad 0<p<1$ 

        $(X_1,\dots,X_n)$ m.a.s de tamaño $n$

         $L_n(\theta)=f_{X_1,\dots,X_n}(x_1,\dots,x_n,\theta)=\prod_{i=1}^{n} f_X(x_i;\theta) $ 

         $L_n(p)=\prod_{i=1}^{n} f_X(x_i;p)=\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}$

         $\log L_n(p)=\log p^{\sum_i x_i}+\log(1-p)^{n-\sum_{i=1}^{n} x_i}=\left( \sum_i x_i \right) \cdot \log p+\left( n-\sum_{i=1}^{n} x_i \right) \cdot \log(1-p)$

         $\begin{aligned}
             \frac{\partial L_n(p)}{\partial p} &=\left( \sum_{i=1}^{n} x_i \right) \cdot \dfrac{1}{p}+\left( n-\sum_{i=1}^{n} x_i \right) \cdot \left( -\dfrac{1}{1-p} \right) \\
             &= (1-p)\cdot \sum_{i=1}^{n} x_i-\left( n-\sum_{i=1}^{n} x_i \right) \cdot p\\
             &= \sum_{i=1}^{n} x_i-\cancel{p \cdot \sum_{i=1}^{n} x_i}-np+\cancel{p\cdot \sum_{i=1}^{n} x_i}=0   \\
             & \bboxed{\hat{p}=\dfrac{1}{n}\sum_{i=1}^{n} x_i=\overline{x}} 
         \end{aligned}
         $
\item \lb{En el segundo caso anterior, calculad la esperanza del estimador de máxima verosimilitud de $\sigma^2$}

         La función de densidad de una varaible aleatoria $X$ con distribución normal es: \[
             f(x|\mu, \sigma^2)=\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp \left( -\dfrac{(x-\mu)^2}{2\sigma^2} \right) 
         \] 
         Si tenemos una muestra de tamaño $n$, es decir, $X_1,X_2,\dots,X_n$ que son obervaciones independientes y distribuidas como $\mathcal{N}(\mu,\sigma^2)$, entonces la función de verosimilitud $L(\mu,\sigma^2)$ es el producto de las funciones de densidad de cada observación \[
         L(\mu,\sigma^2)=\prod_{i=1}^{n} f(x_i|\mu,\sigma^2)=\prod_{i=1}^{n} \dfrac{1}{\sqrt{2\pi\sigma^2} }  \exp\left( -\dfrac{(x_i-\mu)^2}{2\sigma^2} \right) 
         \] 
         Para simplificar los cálculos, se toma el logaritmo de la función de verosimilitud, lo que da la función de log-verosimilitud $\ell (\mu,\sigma^2)$: \[
         \begin{aligned}
             \ell(\mu,\sigma^2)=\log L(\mu,\sigma^2)&= \sum_{i=1}^{n} \log\left( \dfrac{1}{\sqrt{2\pi\sigma^2} } \right) +\sum_{i=1}^{n} \log\left( \exp\left( -\dfrac{(x_i-\mu)^2}{2\sigma^2} \right)  \right)  \\
             &= \sum_{i=1}^{n} \log\left( (2\pi\sigma^2)^{-\frac{1}{2} } \right) +\sum_{i=1}^{n} -\dfrac{(x_i-\mu)^2}{2\sigma^2}=\sum_{i=1}^{n} -\dfrac{1}{2}\log(2\pi\sigma^2)-\dfrac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2 \\
             &= -\dfrac{n}{2}\cdot \log(2\pi)-\dfrac{n}{2}\cdot \log(\sigma^2)-\dfrac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2 \\
         \end{aligned}
         \] 
         Para encontrar los estimadores de máxima verosimilitud, derivamos $\ell(\mu,\sigma^2)$ con respecto a $\mu$ y $\sigma^2$, e igualamos a cero:
         \[
         \begin{array}{l}
             \frac{\partial \ell(\mu,\sigma^2)}{\partial \mu} =\dfrac{\cancel{2} }{\cancel{2} \sigma^2}\sum_{i=1}^{n} (x_i-\mu)=0\longrightarrow \sum_{i=1}^{n} (x_i-\mu)=0\longrightarrow \bboxed{\hat{\mu}=\dfrac{1}{n}\sum_{i=1}^{n} x_i=\overline{x}}\\
             \frac{\partial \ell(\mu,\sigma^2)}{\partial \sigma^2} =-\dfrac{n}{2\sigma^2}+\dfrac{1}{2\sigma^4}\sum_{i=1}^{n} (x_i-\mu)^2=0\longrightarrow -n\sigma^2+\sum_{i=1}^{n} (x_i-\mu)^2=0\longrightarrow \bboxed{\hat{\sigma}^2=\dfrac{1}{n}\sum_{i=1}^{n} (x_i-\hat{\mu})^2} 
         \end{array}
         \] 
\end{itemize}
\subsection{Métodos para evaluar un estimador}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Recordad}]
\begin{itemize}[label=\textbullet]
    \item Un estimador es una variable aleatoria.
    \item Es valioso disponer de conocimiento sobre la distribución del estimador (su \textbf{distribución en el muestreo}) $\longrightarrow $ permite manejar el riesgo y el error que podemos cometer al aproximar $\theta$ por $\hat{\theta}$ asociado.
\end{itemize}
\end{tcolorbox}
\textbf{Consideramos dos aspectos de la distribución muestral de $\hat{\theta}$} 
\begin{itemize}[label=\textbullet]
    \item Su localización: \lb{sesgo}.
    \item Su variabilidad: \lb{error cuadrático medio}.
    \item Mencionaremos su comportamiento cuando $n\to \infty$.
\end{itemize}
\subsection{Sesgo}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
    Consideramos para un estimador $\hat{\theta}$ de un parámetro $\theta$: $E_\theta[\hat{\theta}]-\theta$.

    Esta diferencia se llama el \lb{sesgo}.
\end{tcolorbox}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Una propiedad deseable para un estimador}]
Si el sesgo de un estimador es nulo para todo valor de $\theta$, decimos que el estimador \textbf{inesgado}. 
\end{tcolorbox}
\subsection{Error cuadrático medio}
Para medir la variabilidad en el muestreo de un estimador.
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
El \lb{error cuadrático medio del estimador} $\hat{\theta}$ es la función de $\theta$ definida por \[
    \theta\longmapsto E_\theta[(\hat{\theta}-\theta)^2]
\]  
\end{tcolorbox}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Para practicar}]
Calculad el error cuadrático medio del estimador de máxima verosimilitud (e.m.v.) de $\mu$ para una muestra aleatoria simple de $X\sim \mathcal{N}(\mu,\sigma^2)$.
\end{tcolorbox}
$X\leadsto \lbb{\mathcal{N}(\mu,\sigma^2)}{\text{desconocidos}} $ 

$\hat{\mu}=\overline{x}$ \lb{(estimador basado en los movimientos)} 

$E[\overline{X}]=\mu\longrightarrow \hat{\mu}=\overline{X}$ es un estimador insesgado para $\mu$

$\begin{rcases}
    \mathrm{Var}[\overline{X}]=\dfrac{\sigma^2}{n}\\
    \mathrm{Var}[\overline{X}]=E[(\overline{X}-\mu)^2]
\end{rcases}\longrightarrow E[(\overline{X}-\mu)^2]=\dfrac{\sigma^2}{n} $ \lb{(E.C.M)}
\subsection{Balance entre sesgo y varianza}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Sesgo y varianza}]
El error cuadrático medio se puede descomponer \[
    E_\theta[(\hat{\theta}-\theta)^2]=\mathrm{Var}(\hat{\theta})+[\mathrm{sesgo}(\hat{\theta})]^2
\] 
\end{tcolorbox}
En ocasiones, se consigue un menor error cuadrático medio con un estimador sesgado y estamos dispuestos a sacrificar el sesgo, por conseguir una menor varianza.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
    \item \lb{Demostración}
        \[
        \begin{aligned}
            E_\theta[(\hat{\theta}-\theta)^2]&= E_\theta[(\hat{\theta}-E_\theta[\hat{\theta}]+E_\theta[\hat{\theta}]-\theta)^2] \\
                                             &= E_\theta[(\hat{\theta})-E_\theta[\hat{\theta}]+(E_\theta[\hat{\theta}]-\theta)^2+2(\hat{\theta}-E_\theta[\hat{\theta}])(E_\theta[\hat{\theta}]-\theta)] \\
                                             &= \mathrm{Var}[\hat{\theta}]+[\mathrm{sesgo}(\hat{\theta})]^2 \\
        \end{aligned}
        \] 

\end{itemize}
\subsection{Comportamiento asintótico de un estimador}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Cuando el tamaño muestral crece}]
        Muchos de los resultados en inferencia se obtienen en un contexto asintótico: nos interesa comprobar el comportamiento de nuestro estimador cuando crece el número de observaciones.
\end{tcolorbox}
        \begin{itemize}[label=\textbullet]
            \item ¿Converge $\hat{\theta}$ hacia el verdadero valor del parámetro?
            \item ¿Cómo se comporta el error $\hat{\theta}-\theta$?
                
        \end{itemize}

\subsection{Consistencia}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
UNsa sucesión de estimadores $(\hat{\theta}_n)_n$ es consistente si converge hacia $\theta$ en probabilidad, para todo $\theta$.

Es decir: \[
    \forall \epsilon > 0,\quad P_\theta\left[ |\hat{\theta}_n-\theta| > \epsilon\right] \xrightarrow[n\to \infty]{}0
\] 
\end{tcolorbox}

\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Nota}]
Si el error cuadrático medio de una sucesión de estimadores tiende a cero, esta sucesión es consistente.
\end{tcolorbox}
\subsection{Normalidad asintótica}
\begin{itemize}[label=\textbullet]
    \item Cuando un estimador es consistente, $(\hat{\theta}_n-\theta)$ converge hacia cero en probabilidad.
    \item Se busca entonces la velocidad de convergencia hacia cero.
\end{itemize}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Un resultado que se puede demostrar para muchos modelos, para el e.m.v:}]
\[
    \sqrt{n} (\hat{\theta}-\theta)\xrightarrow[n\to \infty]{\mathcal{L}}\mathcal{N}(0,\Lambda)
\] (la distribución de $\sqrt{n} (\hat{\theta}_n-\theta)$ se aproxima a la distribución $\mathcal{N}(0,\Lambda)$).
\end{tcolorbox}
\subsubsection*{Ejemplo: distribución gamma$(\alpha, \beta)$}
En práctica, veremos que los estimadores de los momentos para $(\alpha,\beta)$ son \[
\begin{array}{l}
    \hat{\alpha}=\dfrac{(\overline{X})^2}{\overline{X^2}-(\overline{X})^2}\\
\hat{\beta}=\dfrac{\overline{X^2}-(\overline{X})^2}{\overline{X}}
\end{array}
\] 
Simulamos 10000 muestras de tamaño 10 y 10000 muestras de tamañao 10000.
\begin{center}
    \includegraphics[width=0.7\textwidth]{"Tema 2/figures/Figure 3"}
\end{center}
\subsection{Estimación no paramétrica}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Estimación de funciones asociadas a una variable aleatoria}]
En muchas ocasiones no interesa suponer un modelo específico para la variable aleatoria, y por lo tanto, no necesitamos conocer el valor de los parámetros que los caracterizan, sino que nos interesa aproximar el valor de alguna función asociada a la variable. Estas funciones suelen ser:
\begin{itemize}[label=\textbullet]
    \item Función de distribución: $F(x)=P(X\le x)$ para todo $x\in \R$.
    \item Función puntual de probabilidad: $p(x)=P(X=x)$, para todo $x$ en el soporte de $X$ (en el caso discreto).
    \item Función de densidad:  $f(x)=F'(x)$, para todo  $x$ en el soporte de $X$ (en el caso continuo).
    \item En estas situaciones es posible considerar lo que se conoce como  \lb{estimadores no paramétricas} de las funciones anteriores en un valor concreto de $x$.
    \item En los dos pimeros casos, puesto que se trata de estimar probabilidades, podremos usar la proporción muestral para hacer aproximaciones (lo veremos a continuación).
    \item En el caso de la función de densidad recurriremos a lo que se conoce como  \lb{estimadores tipo núcleo}. 
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Estimación de la función de distribución}]
Dado que la función de distribución en un punto $x$ es una probabilidad, $F(x)=P(X\le x)$, tenemos como estimador la correspondiente proporción muestral que vendrá dada por \[
    F_n(x)=\dfrac{1}{n}\sum_{i=1}^{n} I_{(\infty,x]}(X_i),
\] que se conoce como la \lb{función de distribución empírica.} 
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Estimación de la función puntual de probabilidad}]
De la misma forma que antes el estimador de $p(x)=P(X=x)$ vendrá dado por  \[
    \hat{p}(x)=\dfrac{\text{número de veces que aparece el valor $x$ en la muestra}}{n}
\] 
\end{tcolorbox}
Primera opción: nos basamos en frecuencias relativas
\begin{itemize}[label=\textbullet]
    \item La estimación de la función de densidad no es tan sencilla.
    \item A partir de la expresión de la función de densidad como derivada de la función de distribución, y dada una muestra aleatoria simple $X_1,\dots,X_n$ de una variable aleatoria $X$, para cada $x$, estimaciones $F_X(x)$ por la frecuencia relativa de un vecindario de  $x$.
         \[
        \hat{f}_n(x)=\dfrac{1}{2nh}\sum_{i=1}^{n} I_{(x-h,x+h)}(x_i),
        \] $h$ es la mitad del ancho del vecindario, "suficientemente pequeño".
\end{itemize}
        Es una una especie de histograma con ventana móvil, que se va deslizando.
        \begin{center}
            \includegraphics[width=0.7\textwidth]{"Tema 2/figures/Figure 4"}
        \end{center}
\subsection{Estimación tipo núcleo}
Podemos escribir \[
\hat{f}_n(x)=\dfrac{1}{2nh}\sum_{i=1}^{n} I_{(x-h,x+h)}(x_{i}),
\] como \[
\begin{array}{c}
\hat{f}_n(x)=\dfrac{1}{nh}\sum_{i=1}^{n} K\left( \dfrac{x-x_i}{h} \right) ,\text{ con }K(z)=\dfrac{1}{2}I_{(-1,1)}(z).\\
\hat{f}_n(x)=\dfrac{1}{n}\sum_{i=1}^{n} K_h(x-x_i),\text{ con } K_h(z)\dfrac{1}{h}K\left( \dfrac{z}{h} \right) =\dfrac{1}{2h}I_{(-h,h)}(z)
\end{array}
\] 
\begin{itemize}[label=\textbullet]
    \item En la expresión anterior hay dos elementos principales, la función $K$, que en el ejemplo es la función de densidad de una distribución uniforma, y el parámetro $h$, que hemos dijado en 0.5 en el ejemplo, y en función de su valor tiene en cuenta valores de la muestra cercanos o alejados al valor $x$.

    \item En el primer caso hablamos sobre la  \lb{función núcleo (kernel)}  y en el segundo caso del \lb{ancho de banda (bandwidth)} .
    \item La principal característica de la función $K$ es que al ser la densidad de una distribución uniforme da el mismo peso a todos los puntos de la muestra que se encuentran en un entorno de valor $x$ donde estimamos la densidad.
    \item Podríamos pensar en otra función $K$ con la propiedad de ser simétrica y unimodal en el 0, como en el caso anterior, pero con distintos pesos para las observaciones.
\end{itemize}
\subsubsection{Podemos variar el núcleo}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Función núcleo}]
\begin{itemize}[label=\textbullet]
    \item En la literatura hay distintas propuestas de la función $K$, \lb{función núcleo}.
    \item Cualquier estimador de la densidad en la forma anterior, con una función núcleo $K$, recibe el nombre de \lb{estimador tipo núcleo}.
    \item Los casos más usados de función núcleo son los siguientes:
        \begin{itemize}[label=\textrightarrow]
            \item \lb{Núcleo normal:} $K$ es la función de densidad de una distribución normal estándar. Suele ser el núcleo que más se utiliza.
            \item \lb{Núcleo de Epanechnikov:} $K(z)=(1-z^2)I_{(-1,1)}$. Se considera que es el núcleo más eficiente.
        \end{itemize}
    \item El caso considerado en la introducción se conoce como el \lb{núcleo rectangular} 
\end{itemize}
\end{tcolorbox}
Por ejemplo que tenga más peso cuando el valor muestral esté cerca del valor $x$. Este argumento parece bastante razonable, es decir, que en la estimación tengan más influencia los valores muestrales cercanos a valor $x$ en el intervalo $x\pm h$. 

\begin{center}
\includegraphics[width=0.5\textwidth]{"Tema 2/figures/Figure 5"}
\end{center}
\begin{itemize}[label=\textbullet]
    \item $K$ uniforme.
    \item $K$ gaussiano.
    \item $K$ Kernel Epanechnikov: \[
    K(z)=\dfrac{3}{4}(1-z^2)I_{(-1,1)}(z)
    \]  
\end{itemize}
\vspace{1cm}
\begin{center}
    \includegraphics[width=\textwidth]{"Tema 2/figures/Figure 6"}
\end{center}
\subsubsection{Se pueden demostrar resultados asintóticos}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Función núcleo}]
Vamos a justificar, mediante el comportamiento asintótico, que los valores del estimador tipo núcleo dan buenas aproximaciones de la función de densidad.

Para el principal resultado se supodrán las siguientes condiciones sobre la función de densidad, la función núcleo y el ancho de banda:
\begin{itemize}[label=\textbullet]
    \item La densidad al cuadrado es integrable, admite derivada continua de segundo orden y esta al cuadrado es integrable. Seguiremos la notación $R(f'')=\int (f''(x))^2\dx $.
    \item El núcelo es una función de densidad simétrica y acotada, con momento de orden dos finito y al cuadrado es integrable. Seguiremos en este caso la notación $R(K)=\int(K(x))^2\dx $, y por $\mu_2(K)=\int x^2K(x)\dx $ el valor del momento de orden $2$ asociada a la densidad  $K$.
    \item Conforme aumentamos el tamaño de muestra el ancho de banda, $h_n$, es una secuencia de valores positivos con límite $0$ y $nh_n$ tienda a  $+\infty$.
\end{itemize}
\end{tcolorbox}
Si suponemos:
\begin{itemize}[label=\textbullet]
    \item $f_X$ es  $L^2$, es derivable dos veces y $f_X''$ es  $L^2$.
    \item $K$ es simétrica, acotada, $K$ es $L^2$ y admite momento de orden 2.
    \item Consideramos $h_n\to 0$ cuando $n\to \infty$, y además $nh_n\to \infty$.
\end{itemize}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Resultado}]
Para todo $x,$  \[
E\left[ \hat{f}_n(x)-f(x) \right] =\dfrac{1}{2}\mu_2(K)f''(x)h_n^2+o(h_n^2)\quad\text{y}\quad \mathrm{Var}\left[ \hat{f}_n(x) \right] =\dfrac{R(K)}{nh_n}f(x)+o((nh_n)^{-1}).
\] 
\begin{itemize}[label=\textbullet]
    \item Este resultado junto con las suposiciones que hemos hecho anteriormente nos lleva a concluir a que se producen buenas aproximaciones en términos del sesgo y la varianza, pues ambos tienden a 0 cuando aumentamos el tamaño de muestra.
\end{itemize}
\end{tcolorbox}
\subsection{Ancho de banda}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Si variamos el ancho de banda:}]
\begin{itemize}[label=\textbullet]
    \item Cuánto maupr es la vetana o vecindario, la curva estimada es más suavizada.
    \item Si la ventana es pequeña, la curva estimada es \textit{"wiggly"}.
    \item Puesto que el ancho de banda se puede hacer tan grande como queramos nos planteamos cómo elegir un ancho de banda que sea óptimo en algún sentido.
    \item En general el problema de la selección del ancho de banda no es sencillo.
    \item Se han propuesto elecciones óptima del ancho de banda, que minimicen la distancia entre la función de densidad $f_X$ y el estimador  $\hat{f}_X$.
\end{itemize}

\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Elección óptima del ancho de banda}]
\[
    h_n=\left( \dfrac{R(K)}{\mu_2^2(K)R(f'')n} \right)^{\frac{1}{5} }. 
\]
\end{tcolorbox}
\subsubsection{Elección óptima del ancho de banda}
En la fórmula \[
h_n=\left( \dfrac{R(K)}{\mu_2^2(K)R(f'')n} \right) ^{\frac{1}{5} },
\] aparece $R(f'')$ que es deconocida.
 \begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Veremos dos maneras de aproximar $R(f'')$:}]
 \begin{itemize}[label=\textbullet]
     \item Aproximación Gaussiana de $f_X$.
     \item Estimar $f_X$ de manera no paramétrica.
 \end{itemize}
 \end{tcolorbox}
 \subsubsubsection{Aproximación gaussiana para el cálculo de $R(f'')$}
 \begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Suponemos que $f$ es la función de densidad de una distribución normal con desviación típica $\sigma$:}]
 \[
 h_n=\left( \dfrac{8\pi^{\frac{1}{2} }R(K)}{3\mu_2^2(K)n} \right) ^{\frac{1}{5} }\sigma
 \] 
 En la práctica, si la desviación típica $\sigma$ es desconocida la aproximamos por la cuasi-desviación típica muestral $S$: \[
 \hat{h}_n=\left( \dfrac{8\pi^{\frac{1}{2} }R(K)}{3\mu_2^2(K)n} \right) ^{\frac{1}{5} }S.\quad \text{Normal scales bandwidth selector}
 \] 
 \end{tcolorbox}
 \begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Si además consideramos que el núcleo $K$ es gaussiano:}]
 \[
 \hat{h}_{n,RP}=\left( \dfrac{4}{3n} \right) ^{\frac{1}{5} }S.
 \] 
 Se llama la \textit{"rule of thumb"} (la regla del pulgar)
 \end{tcolorbox}
 La forma más complicada es dar una estimación no paramétrica de $R(f'')$. La metodología va más allá de los contenidos de esta asignatura y no veremos los detalles, solo indicar que podemos seleccionarla con  \lb{\textbf{\texttt{R}} } y lo veremos en prácticas.

\subsubsection*{Implementación en \textbf{\texttt{R}} }
\begin{lstlisting}
# mas de una distribucion normal de tamaño 100
set.seed(314159)
muestra <- rnorm(100, 0, 1)
# valor del ancho de banda segun la regla del pulgar
bw.nrd(x = muestra)
\end{lstlisting}
\begin{verbatim}
## [1] 0.4047312
\end{verbatim}
\begin{lstlisting}
# grafica del estimador de la función de densidad
tibble(x = muestra) |>
    ggplot(aes(x = x)) +
    geom_density(kernel = "gaussian", bw = "nrd", from = -4, to = 4) +
    geom_function(fun = dnorm, col = "red") +
    xlim(-5, 5) + ylim(0, 0.5)
\end{lstlisting}
\begin{center}
    \includegraphics[width=0.7\textwidth]{"Tema 2/figures/Figure 7"}
\end{center}
\subsubsubsection{Aproximación no paramétrica para el cálculo de $R(f'')$, implementación en  \textbf{\texttt{R}} }
Se usa la especificación "SJ" (Sheather \& Jones, 1991) para el bandwidth.
\begin{lstlisting}
# valor del ancho de banda con estimación no paramétrica
bw.SJ(x = muestra)
\end{lstlisting}
\begin{verbatim}
## [1] 0.442874
\end{verbatim}
\begin{lstlisting}
# grafica del estimador de la función de densidad
tibble(x = muestra) |>
    ggplot(aes(x = x)) +
    geom_density(kernel = "gaussian", bw = "SJ", from = -4, to = 4) +
    geom_function(fun = dnorm, col = "red") + 
    xlim(-5, 5) + ylim(0, 0.5)
\end{lstlisting}
\begin{center}
    \includegraphics[width=0.8\textwidth]{"Tema 2/figures/Figure 8"}
\end{center}
\subsection{Introducción al Bootstrap}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Bootstrap: una manera de evaluar la incertidumbre asociada a un estimador}]
Permite obtener información acerca de la distribución muestral de un estadístico relevante, usando remuestreo de la muestra disponible, es decir, submuestras de la muestra original.
\begin{itemize}[label=\textbullet]
    \item Permite estimar la distribución de un estimador (como la media, mediana, varianza, etc.) a partir de muestras obtenidas de los datos disponibles.
    \item Se utiliza especialmente cuando la distribución poblacional es deconocida o cuando el tamaño de la muestra es pequeño.
    \item Es muy útil para hacer inferencias estadísticas sin hacer suposiciones sobre la distribución subyaciente de los datos.
    \item Se aplica incluso cuando el modelo no permite obtener la distribución muestral.
    \item Nos permite aproximar el error estándar el estimador, lo que nos da una idea de su precisión.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{¿De dónde viene el nombre?}]
Está asociado a la expresión: \textit{"To pull oneself up by one's bootstraps"}, que hace referencia a situaciones en la que uno cuenta con sus propios medios para sair de una dificultad, sin contar con ayuda externa.
\end{tcolorbox}
\begin{itemize}[label=\textbullet]
    \item Se basa en la aproximación de la función de distribución mediante la función de distribución empírica y los métodos de simulación.
    \item Para utilizar el método de simulación necesitamos conocer la distribución $F$. Como en muchas situaciones no conocemos $F$, lo que vamos a hacer es generar muestras a partir de $F_n$.
\end{itemize}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Determinación del error muestral de un estadístico}]
\begin{itemize}[label=\textbullet]
    \item Recordamos que el error estándar asociado a un estadístico es su desviación típica muestral.
    \item Para un estimador, nos indica la variabilidad de los valores que toma respecto a todas las muestras posibles. Es, por lo tanto un indicador valioso de su precisión.
    \item Por ejemplo, para una población normal, $\sigma$ desconocida, el error estándar de $\overline{X}$ es $\dfrac{\sigma}{\sqrt{n} }$.
    \item En general, dado un parámetro $\theta$ y un estadístico $\hat{\theta}$ que aproxima (estima) el valor de $\theta$, su error muestral viene dado por \[
            E[(\hat{\theta}-\theta)^2].
    \] 
    \item La idea es simular muestras de $F_n$ para obtener en cada una de ellas al valor del estadístico.
    \item Con cada una de ellas calculamos el equivalente empírico de $E[(\hat{\theta}-\theta)^2]$, mediante el promedio de las desviaciones al cuadrado de los valores del estadístico respecto del promedio de todas las estimaciones.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{La distribución empírica asociada a una muestra}]
\begin{itemize}[label=\textbullet]
    \item Consideramos una muestra concreta $\mathbf{x}=(x_1,\dots,x_n)$, asociada a una distribución $F$ en una población.
    \item La distribución empírica $\hat{F}$ se construye únicamente a partir de esos valores observados y es la distribución que asigna una probabilidad $\dfrac{1}{n}$ a cada valor $x_i,i=1,2,\dots,n$.
    \item Es por lo tanto una distribución discreta, sus valores posibles son los de $x_1,\dots,x_n$.
    \item Es decir $\hat{F}$ asocia a cualquier evento $A$, la probabilidad \textit{empírica}: \[
    \hat{\mathrm{Prob}}(A)=\dfrac{\#\{x_i\in A\} }{n}.
    \]  
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Tened en cuenta}]
Se pueden repetir los valores en la muestra $\mathbf{x}=(x_1,\dots,x_n)$.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{La distribución empírica asociada a una muestra}]
\begin{itemize}[label=\textbullet]
    \item Es una distribución discreta, sus valores posibles son los de $x_1,\dots,x_n$.
    \item Es decir $\hat{F}$ asocia a cualquier evento $A$, la probabilidad \textit{empírica}: $\hat{\mathrm{Prob}}(A)=\dfrac{\#\{x_i\in A\} }{n}$.
    \item Podemos calcular la esperanza de cualquier función $g$:  \[
            \hat{E}[g]=\dfrac{1}{n}\sum_{i=1}^{n} g(x_i).
    \] 
\item Por ejemplo: \[
\begin{array}{c}
    \text{La esperanza de }\hat{F}=\dfrac{1}{n}\sum_{i=1}^{n} x_i=\overline{x}\\
    \text{La varianza de }\hat{F}=\dfrac{(n-1)}{n}s^2.
\end{array}
\] 
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{El principio de plug-in:}]
\begin{itemize}[label=\textbullet]
    \item Es un método simple para estimar parámetros a partir de una muestra.
    \item Estimamos un parámetro $\theta$ que se puede calcular como una función $t(F)$ de la distribución  $F$, a través de su equivalente empírico: \[
    \hat{\theta}=t(\hat{F}).
    \] 
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Nota}]
\begin{itemize}[label=\textbullet]
    \item Por ejemplo, estimamos $\mu_F$ usando la esperanza de $\hat{F}$, que es $\overline{x}$.
    \item Es la idea que usamos en el método de los momentos también.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Muestra Bootstrap}]
Extraer una muestra Bootstrap es extraer una muestra de la distribución empírica $\hat{F}$.
\begin{itemize}[label=\textbullet]
    \item Consiste en extraer al azar, de manera equiprobable y con reemplazo, $n$ elementos del vector $\mathbf{x}=(x_1,\dots,x_n)$.
    \item Denotamos la muestra extraída por $\mathbf{x}^*=(x_1^*,\dots,x_n^*)$.
    \item Al ser con reemplazo, algunos elementos de $\mathbf{x}$ pueden aparecer repetidos y otros pueden no aparecer.
\end{itemize}
\end{tcolorbox}
\subsubsection{El error estándar Bootstrap}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Recordad:}]
\begin{itemize}[label=\textbullet]
    \item El error estándar asociado a un estadístico es su desviación típica muestral
    \item Para un estimador, nos indica la variabilidad de los valores que toma respecto a todas las muestras posibles, es por lo tanto un indicador valioso de su precisión.
    \item Por ejemplo, para una población normal, $\sigma$ desconocida, el error estándar de $\overline{X}$ es $\dfrac{S}{\sqrt{n} }$.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Idea del error estándar Bootstrap:}]
\begin{itemize}[label=\textbullet]
    \item Consideramos un estimador $\hat{\theta}$ que se expresa como $T(\mathbf{x})$.
    \item Vamos a aplicar el prinicipio plug-in para aproximar el error estándar de $\hat{\theta}$, usando $\hat{F}$.
    \item El principio plug-in en este caso se describe como:
        \begin{itemize}[label=\textrightarrow]
            \item Consideramos el proceso de extraer una muestra de $\hat{F}:\mathbf{x}^*$.
            \item Consideramos el estimador $T(\mathbf{x}^*)$.
            \item Calculamos la desviación típica de la distribución muestral del estimador anterior, respecto a todas las muestras de $\hat{F}$ que podría extraer.
        \end{itemize}
    \item Consideramos una variable aleatoria $X$ y una muestra $x_1,\dots,x_n$ de $X$.
    \item Hemos observado $\mathbf{x}$, hemos calculado el estadístico $\hat{\theta}=T(\mathbf{x})$. Procederemos de la manera siguiente:
        \begin{itemize}[label=\textrightarrow]
            \item Extraemos $B$ muestras Bootstrap $\mathbf{x}^{*1},\dots,\mathbf{x}^{*n}$. Estas muestras son muestras independientes con reemplazamiento (pueden aparecer valores repetidos) de tamaño $n$ de $x_1,\dots,x_n$.
            \item Para cada muestra Bootstrap, calculamos el valor del estadístico y lo denotamos por $\hat{\theta}^{*b}=T(\mathbf{x}^{*b})$, para $b=1,\dots,B$.
        \end{itemize}
    \item Aproximamos el error estándar de $\hat{\theta}$, que denotamos por $\hat{se}$, como la desviación típica muestral de $\hat{\theta}^{*1},\dots,\hat{\theta}^{*B}$, esto es, como \[
    \hat{se}=\sqrt{\sum_{b=1}^{B} \dfrac{\left(\hat{\theta}^{*b}-\sum_{i=1}^{B}\frac{\hat{\theta}^{*b}}{B} \right) }{B-1}} 
    \] 
\end{itemize}
\end{tcolorbox}
\newpage
\input{"Tema 2/Ejercicios Tema 2/Ejercicios Tema 2.tex"}
