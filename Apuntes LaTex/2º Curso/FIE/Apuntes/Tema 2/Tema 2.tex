\section{Estimación}
\subsection{Introducción}
\begin{itemize}[label=\textbullet]
    \item Hemos modelizado un experimento con una variable aleatoria $X$.
    \item La \lb{estimación} hace referencia al proceso de conseguir información sobre la distribución de $X$ a partir de los valores de una muestra, aproximando valores asociados a la distribución mediante el valor de un estadístico en una muestra concreta.
\end{itemize}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Dos situaciones}]
\begin{itemize}[label=\textbullet]
    \item Nuestro modelo supone que la distribución de $X$ pertence a una familia paramétrica de distribuciones: tienen una determinada forma con unos parámetros variables.
        \begin{itemize}[label=\textrightarrow]
            \item Buscamos información sobre el valor de los parámetros. \lb{Estimación paramétrica}. 
        \end{itemize}
    \item No limitados la familia de distribuciones a la que pertence nuestro modelo.
        \begin{itemize}[label=\textrightarrow]
            \item Buscamos información sobre la distribución en sí (función de distribución, de densidad o función puntual de probabilidad). \lb{Estimación no paramétrica}.
        \end{itemize}
    \item A lo largo de las prácticas veremos también la estimación de parámetros que no necesitan de una familia paramétrica, como es el caso de la mediana.
\end{itemize}
\end{tcolorbox}
\subsection{Ejemplos de estimación paramétrica}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Sondeo sobre intención de participación en una elecciones}]
\begin{itemize}[label=\textbullet]
    \item Queremos estimar la tasa de participación antes de unas elecciones generales.
    \item Formulamos un modelo: experimento: "escoger una persona al azar en el censo". $X$: variable dicotómica ("Sí", o "No").  $p=P(X=\mathrm{Si})$
    \item El modelo pertenece a la familia paramétrica de Bernoulli.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Determinación de la concentración de un producto}]
\begin{itemize}[label=\textbullet]
    \item Quiero determinar la concentración.
    \item Formulo el modelo: experimento="llevar a cabo una medición". $X$: "valor proporcionado por el aparato".  $X\sim \mathcal{N}(\mu,\sigma^2)$.
    \item El modelo pertence a la familia paramétrica de las distribuciones normales
\end{itemize}
\end{tcolorbox}
\subsection{Estimación paramétrica: estimación puntual}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Ingredientes del modelo}]
\begin{itemize}[label=\textbullet]
    \item Experimento aleatoria
    \item Variable aleatoria $X$ con una distribución $f$ que pertence a una familia paramétrica $\{f_\theta,\theta \in \Theta\} $.
    \item Disponemos de una muestra de la distribución de $X$.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
Cualquier estadístico diseñado para aproximar el valor de un parámetro $\theta$ del modelo, se llama \lb{estimador puntual} del parámetro $\theta$.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Ejemplos de estimadores paramétricos}]
\begin{center}
    \begin{tabular}{cc}
        $\theta$ & \textbf{Estimador}\\ \hline
        $\mu$ & $\overline{X}$, media muestral\\ \hline
        $\sigma^2$ & $S^2$, varianza muestral\\ \hline
        $p$ & $\hat{p}$, proporción muestral
    \end{tabular}
\end{center}
\end{tcolorbox}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{A tener en cuenta:}]
\begin{itemize}[label=\textbullet]
    \item Un estimador es una variable aleatoria, su valor depende de la muestra concreta escogida.
    \item Para controlar bondad de nuestra estimación, nos basaremos en el estudio de la distribución del estimador.
\end{itemize}
\end{tcolorbox}
\subsection{Métodos de construcción de estimadores}
Para $\mu,\sigma^2$ o $p$, es fácil pensar en estimadores naturales, pero para modelos o parámetros más sofisticados, vamos a ver métodos generales.

\textbf{Veremos dos métodos en este tema}:
\begin{itemize}[label=\textbullet]
    \item El método de los momentos
    \item El método de la máxima verosimilitud
\end{itemize}
\subsection{Método de los momentos}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Contexto}]
\begin{itemize}[label=\textbullet]
    \item Experimento con una variable aleatoria $X$, suponemos $f_X \in \{x\longmapsto f_\theta(x),\theta \in \Theta \} $.
    \item El parámetro $\theta$ tiene dimensión $p$.
    \item Consideramos una muestra aleatoria simple  $X_1,\dots,X_n$ de $X$.
\end{itemize}
\end{tcolorbox}
Si $\theta$ tiene dimensión $p$, igualamos los $p$ primeros momentos de $f_\theta$ con los equivalentes muestrales.

Sea $\mu_k(\theta)$ el momento de orden $k$ de la distribución $f_\theta, \mu_k=E[X^k]$. Resolvemos: \[
\begin{array}{r}
    \mu_1(\theta)=\overline{X},\\
    \mu_2(\theta)=\overline{X^2},\\
    \vdots\\
    \mu_p(\theta)=\overline{X^p}.
\end{array}
\] 
\subsection*{Ejemplos}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Calculad los estimadores usando el método de los momentos en los dos casos:}]
\begin{itemize}[label=\textbullet]
    \item Modelo normal: $X\sim \mathcal{N}(\mu, \sigma^2)$, donde $\theta=(\mu,\sigma^2)$. \[
    \begin{array}{ll}
        X\leadsto \mathcal{N}(\mu,\sigma^2) & \mu=E[X]\\
        \theta=(\mu,\sigma^2),\quad p=2 & \sigma^2=\mathrm{Var}[X]=E[X^2]-(E[X])^2\longrightarrow E[X^2]=\sigma^2+\mu^2\\
        \mu_1(\theta)=E[X]=\overline{x} & \hat{\mu}=\overline{x}\\
        \mu_2(\theta)=E[X^2]=\overline{x^2} & \hat{\sigma}^2=\overline{x^2}-(\overline{x})^2
    \end{array}
    \] 
\item Modelo de Bernoulli: $X\sim \mathrm{Bernoulli}(p)$, donde desconocemos $p$.
     \[
    \begin{array}{l}
        X\leadsto \mathrm{Bernoulli}(p),\quad 0<p<1\\
        \theta=p\\
        \mu(\theta)=E[X]=\overline{x}=p\\
        \hat{p}=\overline{x}\lb{\text{ (proporción muestral)}} 
    \end{array}
    \] 
\end{itemize}
\end{tcolorbox}
\subsection{Método de máxima verosimilitud}
\begin{itemize}[label=\textbullet]
    \item El método más utilizado de construicción de un estimador puntual.
    \item Se basa en lo que se conoce como \lb{función de verosimilitud}. 
\end{itemize}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
\begin{itemize}[label=\textbullet]
    \item Sea $X$ una variable aleatoria, con distribución $x\longmapsto f_X(x;\theta)$ (función de densidad o función puntual de probabilidad), donde $\theta$ es de dimensión $p:\theta\in \Theta\subset \R^p$.
    \item Para un valor concreto de una muestra aleatoria simpl $(X_1,\dots,X_n)$, que denotamos por $(x_1,\dots,x_n)$, consideramos la función de $\theta$: \[
    L_n: \begin{cases}
        \Theta\subset \R^p\longrightarrow \R^+\\
        \theta\longmapsto L_n(\theta)=f_{X_1,\dots,X_n}(x_1,\dots,x_n;\theta)=\prod_{i=1}^{n} f_X(x_i;\theta). 
    \end{cases}
    \] 
\item La función $L_n$ es la  \lb{función de verosimilitud}. Nos dice lo creíblles (verosímiles) que son las observaciones para ese valor del parámetro. 
\end{itemize}
\end{tcolorbox}

\subsubsection*{Ejemplo de cálculo de la verosimilitud}
\begin{itemize}[label=\textbullet]
    \item Tiramos 10 veces una moneda (1 es cara, 0 es cruz), y obtenemos: 0,0,1,0,1,1,1,1,1,1.
    \item La verosimilitud asocia a cada $p$ el valor de \[
    \mathrm{Pr}(X_1=0,X_2=0,X_3=1,X_4=0,X_5=1,X_6=1,X_7=1,X_8=1,X_9=1,X_{10}=1),
    \] 
    por lo que \[
    L_n(p)=(1-p)(1-p)p(1-p)p^6=(1-p)^3\cdot p^7.
    \] 
    \begin{center}
        \includegraphics[width=0.7\textwidth]{"Tema 2/figures/Figure 1"}
    \end{center}
\end{itemize}
\subsection{Estimador de máxima verosimilitud}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
El \lb{estimación de máxima verosimilitud} $\hat{\theta}$  de $\theta$ es cualquier valor de $\theta$ que maximiza $\theta\longmapsto L_n(\theta)$, es decir, \[
    \hat{\theta}\underset{\theta \in \Theta}{\mathrm{argmax}}L_n(\theta).
\] 
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Nota}]
\begin{itemize}[label=\textbullet]
    \item La maximización se realiza sobre todos los valores admisibles para el parámetro $\theta$.
    \item Podría haber de un máximo.
\end{itemize}
\end{tcolorbox}
\subsection*{Estimación de la proporción}
Retomamos el ejemplo de las 10 monedas: \[
p\longmapsto L_n(p)=(1-p)(1-p)p(1-p)p^6=(1-p)^3\cdot p^7.
\] 
\begin{center}
    \includegraphics[width=0.7\textwidth]{"Tema 2/figures/Figure 2"}
\end{center}
\subsubsection*{Ejemplos}
\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=\textbf{Ejemplos de estimación por máxima verosimilitud}]
Calculad los estimadores usando el método de máxima verosimilitud en los dos casos:
\begin{itemize}[label=\textbullet]
    \item $X\sim \mathrm{Bernoulli}(p)$, donde desconocemos $p$.
    \item $X\sim \mathcal{N}(\mu,\sigma^2)$, donde desconocemos $\theta=(\mu,\sigma^2)$.
\end{itemize}
\end{tcolorbox}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
    \item \lb{En el primer caso anterior, calcular la distribución de Bernoulli, donde desconocemos $p$.}

        $X\leadsto \mathrm{Bernoulli}(p),\quad 0<p<1$ 

        $(X_1,\dots,X_n)$ m.a.s de tamaño $n$

         $L_n(\theta)=f_{X_1,\dots,X_n}(x_1,\dots,x_n,\theta)=\prod_{i=1}^{n} f_X(x_i;\theta) $ 

         $L_n(p)=\prod_{i=1}^{n} f_X(x_i;p)=\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}$

         $\log L_n(p)=\log p^{\sum_i x_i}+\log(1-p)^{n-\sum_{i=1}^{n} x_i}=\left( \sum_i x_i \right) \cdot \log p+\left( n-\sum_{i=1}^{n} x_i \right) \cdot \log(1-p)$

         $\begin{aligned}
             \frac{\partial L_n(p)}{\partial p} &=\left( \sum_{i=1}^{n} x_i \right) \cdot \dfrac{1}{p}+\left( n-\sum_{i=1}^{n} x_i \right) \cdot \left( -\dfrac{1}{1-p} \right) \\
             &= (1-p)\cdot \sum_{i=1}^{n} x_i-\left( n-\sum_{i=1}^{n} x_i \right) \cdot p\\
             &= \sum_{i=1}^{n} x_i-\cancel{p \cdot \sum_{i=1}^{n} x_i}-np+\cancel{p\cdot \sum_{i=1}^{n} x_i}=0   \\
             & \bboxed{\hat{p}=\dfrac{1}{n}\sum_{i=1}^{n} x_i=\overline{x}} 
         \end{aligned}
         $
\item \lb{En el segundo caso anterior, calculad la esperanza del estimador de máxima verosimilitud de $\sigma^2$}

         La función de densidad de una varaible aleatoria $X$ con distribución normal es: \[
             f(x|\mu, \sigma^2)=\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp \left( -\dfrac{(x-\mu)^2}{2\sigma^2} \right) 
         \] 
         Si tenemos una muestra de tamaño $n$, es decir, $X_1,X_2,\dots,X_n$ que son obervaciones independientes y distribuidas como $\mathcal{N}(\mu,\sigma^2)$, entonces la función de verosimilitud $L(\mu,\sigma^2)$ es el producto de las funciones de densidad de cada observación \[
         L(\mu,\sigma^2)=\prod_{i=1}^{n} f(x_i|\mu,\sigma^2)=\prod_{i=1}^{n} \dfrac{1}{\sqrt{2\pi\sigma^2} }  \exp\left( -\dfrac{(x_i-\mu)^2}{2\sigma^2} \right) 
         \] 
         Para simplificar los cálculos, se toma el logaritmo de la función de verosimilitud, lo que da la función de log-verosimilitud $\ell (\mu,\sigma^2)$: \[
         \begin{aligned}
             \ell(\mu,\sigma^2)=\log L(\mu,\sigma^2)&= \sum_{i=1}^{n} \log\left( \dfrac{1}{\sqrt{2\pi\sigma^2} } \right) +\sum_{i=1}^{n} \log\left( \exp\left( -\dfrac{(x_i-\mu)^2}{2\sigma^2} \right)  \right)  \\
             &= \sum_{i=1}^{n} \log\left( (2\pi\sigma^2)^{-\frac{1}{2} } \right) +\sum_{i=1}^{n} -\dfrac{(x_i-\mu)^2}{2\sigma^2}=\sum_{i=1}^{n} -\dfrac{1}{2}\log(2\pi\sigma^2)-\dfrac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2 \\
             &= -\dfrac{n}{2}\cdot \log(2\pi)-\dfrac{n}{2}\cdot \log(\sigma^2)-\dfrac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2 \\
         \end{aligned}
         \] 
         Para encontrar los estimadores de máxima verosimilitud, derivamos $\ell(\mu,\sigma^2)$ con respecto a $\mu$ y $\sigma^2$, e igualamos a cero:
         \[
         \begin{array}{l}
             \frac{\partial \ell(\mu,\sigma^2)}{\partial \mu} =\dfrac{\cancel{2} }{\cancel{2} \sigma^2}\sum_{i=1}^{n} (x_i-\mu)=0\longrightarrow \sum_{i=1}^{n} (x_i-\mu)=0\longrightarrow \bboxed{\hat{\mu}=\dfrac{1}{n}\sum_{i=1}^{n} x_i=\overline{x}}\\
             \frac{\partial \ell(\mu,\sigma^2)}{\partial \sigma^2} =-\dfrac{n}{2\sigma^2}+\dfrac{1}{2\sigma^4}\sum_{i=1}^{n} (x_i-\mu)^2=0\longrightarrow -n\sigma^2+\sum_{i=1}^{n} (x_i-\mu)^2=0\longrightarrow \bboxed{\hat{\sigma}^2=\dfrac{1}{n}\sum_{i=1}^{n} (x_i-\hat{\mu})^2} 
         \end{array}
         \] 
\end{itemize}
\subsection{Métodos para evaluar un estimador}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Recordad}]
\begin{itemize}[label=\textbullet]
    \item Un estimador es una variable aleatoria.
    \item Es valioso disponer de conocimiento sobre la distribución del estimador (su \textbf{distribución en el muestreo}) $\longrightarrow $ permite manejar el riesgo y el error que podemos cometer al aproximar $\theta$ por $\hat{\theta}$ asociado.
\end{itemize}
\end{tcolorbox}
\textbf{Consideramos dos aspectos de la distribución muestral de $\hat{\theta}$} 
\begin{itemize}[label=\textbullet]
    \item Su localización: \lb{sesgo}.
    \item Su variabilidad: \lb{error cuadrático medio}.
    \item Mencionaremos su comportamiento cuando $n\to \infty$.
\end{itemize}
\subsection{Sesgo}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
    Consideramos para un estimador $\hat{\theta}$ de un parámetro $\theta$: $E_\theta[\hat{\theta}]-\theta$.

    Esta diferencia se llama el \lb{sesgo}.
\end{tcolorbox}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Una propiedad deseable para un estimador}]
Si el sesgo de un estimador es nulo para todo valor de $\theta$, decimos que el estimador \textbf{inesgado}. 
\end{tcolorbox}
\subsection{Error cuadrático medio}
Para medir la variabilidad en el muestreo de un estimador.
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=\textbf{Definición}]
El \lb{error cuadrático medio del estimador} $\hat{\theta}$ es la función de $\theta$ definida por \[
    \theta\longmapsto E_\theta[(\hat{\theta}-\theta)^2]
\]  
\end{tcolorbox}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Para practicar}]
Calculad el error cuadrático medio del estimador de máxima verosimilitud (e.m.v.) de $\mu$ para una muestra aleatoria simple de $X\sim \mathcal{N}(\mu,\sigma^2)$.
\end{tcolorbox}
$X\leadsto \lbb{\mathcal{N}(\mu,\sigma^2)}{\text{desconocidos}} $ 

$\hat{\mu}=\overline{x}$ \lb{(estimador basado en los movimientos)} 

$E[\overline{X}]=\mu\longrightarrow \hat{\mu}=\overline{X}$ es un estimador insesgado para $\mu$

$\begin{rcases}
    \mathrm{Var}[\overline{X}]=\dfrac{\sigma^2}{n}\\
    \mathrm{Var}[\overline{X}]=E[(\overline{X}-\mu)^2]
\end{rcases}\longrightarrow E[(\overline{X}-\mu)^2]=\dfrac{\sigma^2}{n} $ \lb{(E.C.M)}
\subsection{Balance entre sesgo y varianza}
\begin{tcolorbox}[colback=olive!5!white, colframe=olive!75!black, title=\textbf{Sesgo y varianza}]
El error cuadrático medio se puede descomponer \[
    E_\theta[(\hat{\theta}-\theta)^2]=\mathrm{Var}(\hat{\theta})+[\mathrm{sesgo}(\hat{\theta})]^2
\] 
\end{tcolorbox}
En ocasiones, se consigue un menor error cuadrático medio con un estimador sesgado y estamos dispuestos a sacrificar el sesgo, por conseguir una menor varianza.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
    \item \lb{Demostración}
        \[
        \begin{aligned}
            E_\theta[(\hat{\theta}-\theta)^2]&= E_\theta[(\hat{\theta}-E_\theta[\hat{\theta}]+E_\theta[\hat{\theta}]-\theta)^2] \\
                                             &= E_\theta[(\hat{\theta})-E_\theta[\hat{\theta}]+(E_\theta[\hat{\theta}]-\theta)^2+2(\hat{\theta}-E_\theta[\hat{\theta}])(E_\theta[\hat{\theta}]-\theta)] \\
                                             &= \mathrm{Var}[\hat{\theta}]+[\mathrm{sesgo}(\hat{\theta})]^2 \\
        \end{aligned}
        \] 
\end{itemize}
