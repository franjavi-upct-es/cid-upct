# Estimación

## Introducción

-   Hemos modelizado un experimento con una variable aleatoria $X$.
-   La \textcolor[HTML]{007aff}{estimación} hace referencia al proceso de conseguir información sobre la distribución de $X$ a partir de los valores de una muestra, aproximando valores asociados a la distribución mediante el valor de un estadístico en un muestra concreta.

::: callout-important
# Dos situaciones

-   Nuestro modelo supone que la distribución de $X$ pertenece a una familia paramétrica de distribuciones: tienen una determinada forma con unos parámetros variables.
    -   Buscamos información sobre el valor de los parámetro. \textcolor[HTML]{007aff}{Estimación paramétrica}.
-   No limitamos la familia de distribuciones a la que pertenece nuestro modelo.
    -   Buscamos información sobre la distribución en sí (función de distribución, de densidad o función de probabilidad). \textcolor[HTML]{007aff}{Estimación no paramétrica}.
-   A lo largo de las prácticas veremos también la estimación de parámetros que no necesitan de una familia paramétrica, como es el caso de la mediana.
:::

## Ejemplos de estimación paramétrica

::: callout-note
# Sondeo sobre intención de participación en unas elecciones

-   Queremos estimar la tasa de participación antes de unas elecciones generales.
-   Formulamos un modelo: experimento: "escoger una persona al azar en el censo". $X$: variable dicotómica ("Sí" o "No"). $p=P(X=\text{Si})$.
-   El modelo pertenece a la familia paramétrica de Bernoulli.
:::

::: callout-note
# Determinación de la concrentación de un producto

-   Quiero determinar la concentración.
-   Formulo el modelo: experimento$=$"llevar a cabo una medición". $X$: "valor proporcionado por el aparato". $X\sim \mathcal{N}(\mu,\sigma^2)$.
-   El modelo pertenece a la familia paramétrica de las distribuciones normales.
:::

## Estimación paramétrica: estimación puntual

::: callout-note
# Ingredientes del modelo

-   Experimento aleatorio
-   Variable aleatoria $X$ con una distribución $f$ que pertenece a una familia paramétrica $\{f_\theta,\theta\in \Theta\}.$
-   Disponemos de una muestra de la distribución de $X$.
:::

\begin{definition}
    Cualquier estadístico diseñado para aproximar el valor de un parámetro $\theta$ del modelo, se llama \textcolor[HTML]{007aff}{estimador puntual} del parámetro $\theta$.
\end{definition}

::: callout-note
# Ejemplos de estimadores paramétricos

\begin{center}
    \begin{tabular}{cc}
        $\theta$ & \textbf{Estimador} \\ \hline
        $\mu$ & $\bar{X}$, media muestral\\ \hline
        $\sigma^2$ & $S^2$, varianza muestral\\ \hline
        $p$ &  $\hat{p}$, proporción muestral\\ 
    \end{tabular}
\end{center}
:::

::: callout-tip
# A tener en cuenta:

-   Un estimador es un variable aleatoria, su valor depende de la muestra concreta escogida.
-   Para controlar bondad de nuestra estimación, nos basaremos en el estudio de la distribución del estimador.
:::

## Métodos de contrucción de estimadores

Para $\mu,\sigma^2$ o $p$, es fácil pensar en estimadores naturales, pero para modelos o parámetros más sofisticados, vamos a ver métodos generales.

**Veremos dos métodos en este tema:**

-   El método de los momentos.
-   El método de la máxima verosimilitud.

## Método de los momentos

::: callout-note
# Contexto
-   Experimento con una variable aleatoria $X$, suponemos $f_X\in \{x\mapsto f_{\theta}(x),\theta\in \Theta\}$.
-   El parámetro $\theta$ tiene dimensión $p$.
-   Consideramos una muestra aleatoria simple $X_1,X_2,\dots,X_n$ de $X$.
:::
Si $\theta$ tiene dimensión $p$, igualamos los  $p$ primeros momentos de  $f_\theta$ con los equivalentes muestrales.

Sea $\mu_k(\theta)$ el momento de orden $k$ de la distribución  $f_{\theta,\mu_k}=E[X^k]$. Resolvemos:

$$
\begin{array}{r}
    \mu_1(\theta)=\overline{X},\\
    \mu_2(\theta)=\overline{X^2},\\
    \vdots\\
    \mu_p(\theta)=\overline{X^p}.
\end{array}
$$

#### Ejemplos #### {.unnumbered .unlisted}

:::{.callout-important}
# Calculad los estimadores usando el método de los momentos en los dos casos:
- Modelo normal: $X\sim \mathcal{N}(\mu,\sigma^2)$, donde $\theta=(\mu,\sigma^2)$.


$$
\begin{array}{ll}
X\leadsto \mathcal{N}(\mu,\sigma^2) & \mu=E[X]\\
\theta=(\mu,\sigma^2),\quad p=2 & \sigma^2=\mathrm{Var}[X]=E[X^2]-(E[X])^2\rightarrow E[X^2]=\sigma^2+\mu^2\\
\mu_1(\theta)=E[X]=\overline{x} & \hat{\mu}=\overline{x}\\
\mu_2(\theta)=E[X^2]=\overline{x^2} & \hat{\sigma}^2=\overline{x^2}-(\overline{x})^2
\end{array}
$$

- Modelo de Bernoulli: $X\sim \text{Bernoulli}(p)$, donde desconocemos $p$.
 
$$
\begin{array}{l}
X\leadsto \text{Bernoulli}(p),\quad 0<p<1\\
\theta=p\\
\mu(\theta)=E[X]=\bar{x}=p\\
\hat{p}=\bar{x} \text{ \textcolor[HTML]{007aff}{(proporción muestral)}  }
\end{array}
$$
:::

## Método de máxima verosimilitud ##
- El método más utilziado de construcción de un estimador puntual.
- Se basa en lo que se conoce como \textcolor[HTML]{007aff}{función de verosimilitud}.

\begin{definition}
    \begin{itemize}
    \item Sea $X$ una variable aleatoria, con distribución  $x\mapsto f_X(x;\theta)$ (función de densidad o función puntual de probabilidad), donde $\theta$ es de dimensión $p:\theta\in \Theta\subset\mathbb{R}^p$.
    \item Para un valor concreto de una muestra aleatoria simple $(X_1,\dots,X_n)$, que denotamos por $(x_1,\dots,x_n)$, consideramos la función de $\theta$: $$L_n:\begin{cases}\Theta\subset\mathbb{R}^p\rightarrow \mathbb{R}^+\\ \theta\mapsto L_n(\theta)=f_{X_1,\dots,X_n}(x_1,\dots,x_{n};\theta)=\prod_{i=1}^{n} f_X(x_i;\theta). \end{cases}$$ 
    \item La función $L_n$ es la  \textcolor[HTML]{007aff}{función de verosimilitud}. Nos dice lo creíbles (verosímiles) que son las observaciones para ese valor del parámetro. 
    \end{itemize}
\end{definition}

#### Ejemplo de cálculo de la verosimilitud #### {.unnumbered .unlisted}
- Tiramos 10 veces una moneda (1 es cara, 0 es cruz), y obtenemos: 0,0,1,0,1,1,1,1,1,1.
- La verosimilitud asocia a cada $p$ el valor de  $$\mathrm{Pr}(X_1=0,X_2=0,X_3=1,X_4=0,X_5=1,X_6=1,X_7=1,X_8=1,X_9=1,X_{10}=1),$$ por lo que $$L_n(p)=(1-p)(1-p)p(1-p)p^{6}=(1-p)^3\cdot p^7.$$ 

```{r}
library(tidyverse)
p <- seq(0, 1, by = 0.001)
n <- 10
n1 <- 7
l <- (1 - p)^(n - n1) * p^n1
tibble(p = p, l = l) |>
  ggplot(aes(p, l)) +
  geom_line() +
  labs(y = "Verosimilitud")
```

## Estimador de máxima verosimilitud ##
\begin{definition}
    El \textcolor[HTML]{007aff}{estimador de máxima verosimilitud} $\hat{\theta}$ de $\theta$ es cualquier valor de $\theta$ que maximiza $\theta\mapsto L_n(\theta)$, es decir, $$\hat{\theta}\mathrm{argmax}_{\theta\in \Theta}L_n(\theta).$$
\end{definition}

:::{.callout-important}
# Nota
- La maximización se realiza sobre todos los valores admisibles para el parámetro $\theta$.
- Podría haber de un máximo.
:::

### Estimación de la proporción ### {.unnumbered .unlisted}
Retomamos el ejemplo de las 10 monedas:

$$
p\mapsto L_n(p)=(1-p)(1-p)p(1-p)p^{6}=(1-p)^3\cdot p^7.
$$

```{r}
#| fig-cap: "Verosimilitud correspondiente al ejemplo de 10 tiradas de una moneda."
tibble(p = p, l = l) |>
  ggplot(aes(p, l)) +
  geom_line() +
  geom_vline(xintercept = n1 / n, col="red") +
  labs(y = "Verosimilitud")
```

#### Ejemplos #### {.unnumbered .unlisted}
:::{.callout-important}
# Ejemplos de estimación por máxima verosimilitud
Calculad los estimadores usando el método de máxima verosimilitud en los dos casos:

- $X\sim \mathrm{Bernoulli}(p)$, donde desconocemos $p$.
- $X\sim \mathcal{N}(\mu,\sigma^2)$, donde desconocemos $\theta=(\mu,\sigma^2)$.
:::

\quad \textcolor{red}{\textbullet}\quad \textcolor[HTML]{007aff}{En el primer caso anterior, calcular la distribución de Bernoulli, donde desconocemos $p$.}

$X\leadsto \mathrm{Bernoulli}(p),\quad 0<p<1$

$(X_1,\dots,X_N)$ m.a.s. de tamaño $n$

$L_n(\theta)=f_{X_1,\dots,X_n}(x_1,\dots,x_n;\theta)=\prod_{i=1}^{n} f_X(x_i;\theta)$

$L_n(p)=\prod_{i=1}^{n}f_X(x_i;p)=\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}$ 

$\ell(p)=\log L_n(p)=\log p^{\sum_i x_i}+\log(1-p)^{n-\sum_{i=1}^{n} x_i}=\left( \sum_i x_i \right)\cdot \log p+\left( n-\sum_{i=1}^{n} x_i \right)\cdot \log(1-p)$ 

$\begin{aligned}\frac{\partial \ell(p)}{\partial p} &=\left( \sum_{i=1}^{n} x_i \right)\cdot \dfrac{1}{p}+\left( n-\sum_{i=1}^{n} x_i \right)\cdot \left( -\dfrac{1}{1-p} \right)=(1-p)\cdot \sum_{i=1}^{n} x_i-\left( n-\sum_{i=1}^{n} x_i \right) \cdot p\\ &= \sum_{i=1}^{n} x_i-\cancel{p\cdot \sum_{i=1}^{n} x_i} -np+\cancel{p\cdot \sum_{i=1}^{n} x_{i}} =0\implies\bboxed{\hat{p}=\dfrac{1}{n}\sum_{i=1}^{n} x_i=\bar{x}} \\\end{aligned}$

\quad \textcolor{red}{\textbullet}\quad \textcolor[HTML]{007aff}{En el segundo caso anterior, calculad la esperanza del estimador de máxima verosimilitud de $\sigma^2$.}

La función de denisdad de una variable aleatoria $X$ con distribución normal es:
 
$$
f(x|\mu,\sigma^2)=\dfrac{1}{\sqrt{2\pi \sigma^2}}\exp\left( -\dfrac{(x-\mu)^2}{2\sigma^2} \right) 
$$

Si tenemos una muestra de tamaño $n$, es decir,  $X_1,X_2,\dots,X_n$ que son observaciones independientes y distribuidas como $\mathcal{N}(\mu,\sigma^2)$, entonces la función de verosimilitud $L(\mu,\sigma^2)$ es el producto de las funciones de densidad de cada observación 
$$
L(\mu,\sigma^2)=\prod_{i=1}^{n} f(x_i|\mu,\sigma^2)=\prod_{i=1}^{n} \dfrac{1}{\sqrt{2\pi \sigma^2}}\exp\left( -\dfrac{(x_i-\mu)^2}{2\sigma^2} \right)
$$
Para simplificar los cálculos, se toma el logaritmo de la función de verosimilitud, lo que da la función de log-verosimilitud $\ell(\mu,\sigma^2)$:
$$
\begin{aligned}
    \ell(\mu,\sigma^2)=\log L(\mu,\sigma^2)&= \sum_{i=1}^{n} \log\left( \dfrac{1}{\sqrt{2\pi \sigma^2} } \right) +\sum_{i=1}^{n} \log\left( \exp \left( -\dfrac{(x_i-\mu)^2}{2\sigma^2} \right)  \right)  \\
    &= \sum_{i=1}^{n} \log\left( (2\pi \sigma^2)^{-\frac{1}{2} } \right) +\sum_{i=1}^{n} -\dfrac{(x_i-\mu)^2}{2\sigma^2}=\sum_{i=1}^{n}-\dfrac{1}{2}\log(2\pi \sigma^2)-\dfrac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2 \\
    &= -\dfrac{n}{2}\cdot \log(2\pi)-\dfrac{n}{2}\cdot \log(\sigma^2)-\dfrac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2 \\
\end{aligned}
$$
Para encontrar los estimadores de máxima verosimilitud, derivamos $\ell(\mu,\sigma^2)$ con respecto a $\mu$ y $\sigma^2$, e igualamos a cero:

$$
\begin{array}{l}
    \frac{\partial \ell(\mu,\sigma^2)}{\partial \mu} =\dfrac{\cancel{2} }{\cancel{2} \sigma^2}\sum_{i=1}^{n} (x_i-\mu)=0\rightarrow \sum_{i=1}^{n} (x_i-\mu)=0\rightarrow \bboxed{\hat{\mu}=\dfrac{1}{n}\sum_{i=1}^{n} x_i=\bar{x}}\\
    \frac{\partial \ell(\mu,\sigma^2)}{\partial \sigma^2}=-\dfrac{n}{2\sigma^2}+\dfrac{1}{2\sigma^4}\sum_{i=1}^{n} (x_i-\mu)^2=0\rightarrow -n \sigma^2+\sum_{i=1}^{n} (x_i-\mu)^2=0\rightarrow \bboxed{\hat{\sigma}^2=\dfrac{1}{n}\sum_{i=1}^{n} (x_i-\hat{\mu})^2} 
\end{array}
$$

## Métodos para evaluar un estimador ##

:::{.callout-tip}
# Recordar
- Un estimador es una variable aleatoria.
- Es valioso disponer de conocimiento sobre la distribución del estimador (su \textcolor[HTML]{007aff}{distribución en el muestreo}) $\implies$ permite manejar el riesgo y el error que podemos cometer al aproxima $\theta$ por $\hat{\theta}$ asociado.
:::

**Consideramos dos aspectos de la distribución muestral de $\hat{\theta}$**

- Su localización: \textcolor[HTML]{007aff}{sesgo}.
- Su variabilidad: \textcolor[HTML]{007aff}{error cuadrático medio}.
- Mencionaremos su comportamiento cuando $n\to \infty$.

## Sesgo ##
\begin{definition}
Consideramos para un estimador $\hat{\theta}$ de un parámetro $\theta:\,E_\theta[\hat{\theta}]-\theta$.

Esta diferencia se llama el \textcolor[HTML]{007aff}{sesgo}. 
\end{definition}

:::{.callout-tip}
# Una propiedad deseable para un estimador
Si el sesgo de un estimador es nulo para todo valor de $\theta$, decimos que el estimador es **insesgado**.
:::

## Error cuadrático medio ##
Para medir la variabilidad en el muestreo de un estimador.
\begin{definition}
    El \textcolor[HTML]{007aff}{error cuadrático medio del estimador} $\hat{\theta}$ es la función de $\theta$ definida por
    
    $$
    \theta\mapsto E_\theta[(\hat{\theta}-\theta)^2]
    $$
    
\end{definition}

:::{.callout-tip}
# Para practicar
Calculad el error cuadrático medio del estimador de máxima verosimilitud (e.m.v.) de $\mu$ para una muestra aleatoria simple de $X\sim \mathcal{N}(\mu,\sigma^2)$.
:::

$X\leadsto \underbrace{\mathcal{N}(\mu,\sigma^2)}_{\text{desconocidos}}$

$\hat{\mu}=\bar{x}$ \textcolor[HTML]{007aff}{(estimador basado en los movimientos)} 

$E[\bar{X}]=\mu\rightarrow \hat{\mu}=\bar{X}$ es un estimador insesgado para $\mu$

$\begin{rcases}\mathrm{Var}[\bar{X}]=\dfrac{\sigma^2}{n}\\ \mathrm{Var}[\bar{X}]=E[(\bar{X}-\mu)^2]\end{rcases}\rightarrow E[(\bar{X}-\mu)^2]=\dfrac{\sigma^2}{n}$ \textcolor[HTML]{007aff}{(E.C.M.)} 

## Balance entre sesgo y varianza ##

:::{.callout-tip}
# Sesgo y varianza

El error cuadrático medio se puede descomponer

$$
E_\theta[(\hat{\theta}-\theta)^2]=\mathrm{Var}(\hat{\theta})+[\text{sesgo}(\hat{\theta})]^2
$$
:::

En ocasiones, se consigue un menor error cuadrático medio con un estimador sesgado y estamos dispuestos a sacrificar el sesgo, por conseguir una menor varianza.

\begin{demostration}
   
   $$
   \begin{aligned}
       E_\theta[(\hat{\theta}-\theta)^2]&= E_\theta[(\hat{\theta}-E_\theta[\hat{\theta}] + E_\theta[\hat{\theta}-\theta])^2] \\
       &= E_\theta[(\hat{\theta})-E_\theta[\hat{\theta}] + (E_\theta[\hat{\theta}]-\theta)^2+2(\hat{\theta}-E_\theta[\hat{\theta}])(E_\theta[\hat{\theta}]-\theta)] \\
       &= \mathrm{Var}[\hat{\theta}]+[\text{sesgo}(\hat{\theta})]^2 \\
   \end{aligned}
   $$ 
\end{demostration}

## Comportamiento asintótico de un estimador ##

:::{.callout-note}
# Cuando el tamaño muestral crece #
Muchos de los resultados en inferencia se obtienen en un contexto asintótico: nos interesa comprobar el comportamiento de nuestro estimador cuando crece el número de observaciones.
:::

- ¿Converge $\hat{\theta}$ hacia el verdadero valor del parámetro?
- ¿Cómo se comporta el error $\hat{\theta}-\theta$?

## Consistencia ##

\begin{definition}
    Una sucesión de estimadores $(\hat{\theta}_n)_n$ es consistente si converge hacia $\theta$ en probabilidad, para todo $\theta$. 

    Es decir: 
    
    $$
    \forall \varepsilon>0,\quad P_\theta\left[ |\hat{\theta}_n-\theta|>\varepsilon \right] \xrightarrow[n\to \infty]{}0
    $$
\end{definition}

:::{.callout-note}
Si el error cuadrático medio de una sucesión de estimadores tiende a cero, esta sucesión es consistente.
:::

## Normalidad asintótica ##

- Cuando un estimador es consistente, $(\hat{\theta}_n-\theta)$ converge hacia cero en probabilidad.
- Se busca entonces la velocidad de convergencia hacia cero.

:::{.callout-note}
# Un resultado que se puede demostrar para muchos modelos, para el e.m.v: #

$$
\sqrt{n} \left( \hat{\theta}-\theta \right) \xrightarrow[n\to \infty]{\mathcal{L}}\mathcal{N}(0,\Alpha)
$$

(la distribución de $\sqrt{n}(\hat{\theta}_n-\theta)$ se aproxima a la distribución $\mathcal{N}(0,\Alpha)$).
:::

### Ejemplo: distribución $\text{gamma}(\alpha,\beta)$ ### {.unnumbered .unlisted}


En práctica, veremos que los estimadores de los momentos para $(\alpha,\beta)$ son

$$
\begin{array}{c}
    \hat{\alpha}=\dfrac{(\hat{X})^2}{\overline{X^2}-(\bar{X})^2}\\
    \hat{\beta}=\dfrac{\overline{X^2}-(\bar{X})^2}{\bar{X}}
\end{array}
$$

Simulamos 10000 muestras de tamaño 10 y 10000 muestras de tamaño 10000.

```{r}
# Estimador por momentos de alpha (Pareto tipo I con xm = 1)
# E[X] = alpha / (alpha - 1)  =>  alpha_hat = xbar / (xbar - 1)

library(ggplot2)
library(dplyr)
library(tidyr)

set.seed(123)

rpareto1 <- function(n, alpha, xm = 1) {
  # Inversa: X = xm / U^(1/alpha), U ~ Unif(0,1)
  u <- runif(n)
  xm / (u^(1 / alpha))
}

alpha_true <- 3
xm <- 1

n_sims <- 20000
ns <- c(10, 10000)

sim_df <- tidyr::crossing(n = ns, sim = seq_len(n_sims)) |>
  dplyr::mutate(
    xbar = purrr::map_dbl(n, \(nn) mean(rpareto1(nn, alpha = alpha_true, xm = xm))),
    alpha_hat = xbar / (xbar - xm),
    z = sqrt(n) * (alpha_hat - alpha_true),
    n_lab = paste("Número de observaciones:", n)
  )

ggplot(sim_df, aes(x = z)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 30, fill = "lightblue", alpha = 0.5, color = NA) +
  geom_density(color = "red", linewidth = 0.7) +
  facet_wrap(~ n_lab, nrow = 1) +
  labs(
    title = expression("Estimador momentos de " * alpha),
    x = expression(sqrt(n) * (hat(theta) - theta)),
    y = "Densidad"
  ) +
  coord_cartesian(xlim = c(-10, 120)) +
  theme_gray(base_size = 14)
```

## Estimación no paramétrica ##

:::{.callout-note}
# Estimación de funciones asociadas a una variable aleatoria

En muchas ocasiones no interesa suponer un modelo específico para la variable aleatoria, y por lo tanto, no necesitamos conocer el valor de los parámetros que los caracterizan, sino que nos interesa aproximar el valor de alguna función asociada a la variable. Estas funciones suelen ser:

- Función de distribución: $F(x)=P(X\le x)$ para todo $x\in \mathbb{R}$.
- Función puntual de probabilidad: $p(x)=P(X=x)$, para todo  $x$ en el soporte de  $X$ (en el caso discreto).
- Función de densidad:  $f(x)=F'(x)$, para todo  $x$ en el soporte de  $X$ (en el caso continuo).
- En estas situaciones es posible considerar lo que se conoce como  \textcolor[HTML]{007aff}{estimadores no paramétricas} de las funciones anteriores en un valor concreto de $x$.
- En los dos primeros casos, puesto que se trata de estimar probabilidades, podremos usar la proporción muestral para hacer aproximaciones (lo veremos a continuación).
- En el caso de la función de densidad recurriremos a lo que se conoce como  \textcolor[HTML]{007aff}{estimadores tipo núcleo}. 
:::

:::{.callout-note}
# Estimación de la función de distribución #
Dado que la función de distribución en un punto $x$ es una probabilidad,  $F(x)=P(X\le x)$, tenemos como estimador la correspondiente proporción muestral que vendrá dada por 
$$
F_n(x)=\dfrac{1}{n}\sum_{i=1}^{n} I_{(\infty,x]}(X_i),
$$

que se conoce como la \textcolor[HTML]{007aff}{función de distribución empírica}. 
:::

:::{.callout-note}
# Estimación de la función puntual de probabilidad #
De la misma forma que antes el estimador $p(x)=P(X=x)$ vendrá dado por
 
$$
\hat{p}(x)=\dfrac{\text{número de veces que aparece el valor $x$ en la muestra}}{n}
$$
:::

Primera opción: nos basamos en frecuencias relativas

- La estimación de la función de densidad no es tan sencilla.
- A partir de la expresión de la función de densidad como derivada de la función de distribución, y dada una muestra aleatoria simple $X_1,\dots,X_n$ de una variable aleatoria $X$, para cada  $x$, estimaciones  $F_X(x)$ por la frecuencia relativa de un vencidario de  $x$: $$\hat{f}_n(x)=\dfrac{1}{2nh}\sum_{i=1}^{n} I_{(x-h,x+h)}(x_i),$$ $h$ es la mitad del ancho del vecindario, "suficientemente pequeño".

Es una especie de histograma con ventana móvil, que se va deslizando.

```{r}
set.seed(314159)
h <- 0.5
bw <- 2 * h / sqrt(12)
## h <- bw * sqrt(12) / 2 
values <- tibble(x = rnorm(100, 5, 1), y = 0)
estimacion <- density(
    x = values$x,
    kernel = "rectangular",
    bw = bw
    )
points <- tibble(x = estimacion$x, y = estimacion$y)
values |>
    ggplot(aes(x = x)) +
    geom_point(aes(y = y)) +
    geom_density(alpha = 0.5, col = "red", fill = "lightblue", kernel = "rectangular", bw = bw) +
    labs(
        title = "100 valores observados, ventana móvil con h = 0.5",
        y = "Frecuencia relativa"
    ) +
    geom_area(
        data = points |>
            filter(
                between(x, 5 - h , 5 + h)  
            ),
        aes(x = x, y = y),
        alpha = 0.1,
        fill = "red"
    )
```

## Estimación tipo núcleo ##

Podemos escribir $$\hat{f}_n(x)=\dfrac{1}{2nh}\sum_{i=1}^{n} I_{(x-h,x+h)}(x_i),$$ como $$\begin{array}{c}\hat{f}_n(x)=\dfrac{1}{nh}\sum_{i=1}^{n} K\left( \dfrac{x-x_i}{h} \right) ,\text{ con }K(z)=\dfrac{1}{2}I_{(-1,1)}(z).\\ \hat{f}_n(x)=\dfrac{1}{n}\sum_{i=1}^{n} K_h(x-x-i),\text{ con }K_h(z)\dfrac{1}{h}K\left( \dfrac{z}{h} \right) =\dfrac{1}{2h}I_{(-h,h)}(z)\end{array}$$

- En la expresión anterior hay dos elementos principales, la función $K$, que en el ejemplo es la función de densidad de una distribución uniforme, y el parámetro  $h$, que he dejado en  $0.5$ en el ejemplo, y en función de su valor tiene en cuenta valores de la muestra cercanos o alejados al valor $x$.
- En el primer caso hablamos sobre la  \textcolor[HTML]{007aff}{función núcleo (kernel)} y en el segundo caso del \textcolor[HTML]{007aff}{ancho de banda (bandwith)}.
- La principal característica de la función $K$ es que al ser la densidad de una distribución uniforme da el mismo peso a todos los puntos de la muestra que se encuentran en un entorno de valor  $x$ donde estimamos la densidad.
- Podríamos pensar en otra función  $K$ con la propiedad de ser simétrica y unimodal en el 0, como en el caso anterior,  pero con distintos pesos para las observaciones.

## Podemos variar el núcleo ##

:::{.callout-note}
# Función núcleo

- En la literatura hay distintas propuestas de la función $K$,  \textcolor[HTML]{007aff}{función núcleo}.
- Cualquier estimador de la densidad en la forma anterior, con una función núcleo $K$, recibe el nombre de  \textcolor[HTML]{007aff}{estimador tipo núcleo}.
- Los casos más usados de función núcleo son los siguientes:
  - \textcolor[HTML]{007aff}{Núcleo normal:} $K$ es la función de densidad de una distribución normal estándar. Suele ser el núcleo que más se utiliza.
  - \textcolor[HTML]{007aff}{Núcleo de Epanechnikov:} $K(z)=(1-z^2)I_{(-1,1)}$. Se considera que es el núcleo más eficiente.
- El caso considerado en la introducción se conoce como el \textcolor[HTML]{007aff}{núcleo rectangular}. 
:::

Por ejemplo que tenga más peso cuando el valor muestral esté cerca del valor $x$. Este argumento parece bastante razonable, es decir, que en la estimación tengan más influencia los valores muestrales cercanos al valor  $x$ en el intervalo  $x\pm h$.

```{r}
K_Epa <- function(z, h = 1) 3 / (4 * h) * (1 - (z / h)^2) * (abs(z) < h)
mu2_K_Epa <- integrate(function(z) z^2 * K_Epa(z), lower = -1, upper = 1)$value
sigma <- sqrt(mu2_K_Epa) / (2/sqrt(12))
ggplot() +
    geom_function(fun = \(x) dunif(x, -1, 1)) +
    geom_function(fun = \(x) dnorm(x, sd=2/sqrt(12)))+
    geom_function(fun = \(x) K_Epa(sigma * x) * sigma ) +
    xlim(-2.5, 2.5) + ylim(0, 1)
```

- $K$ uniforme.
- $K$ gaussiano.
- $K$ Kernel Epanechnikov: $$K(z)=\dfrac{3}{4}(1-z^2)I_{(-1,1)}(z)$$ 

```{r}
colors <- c(
    "rectangular" = "red",
    "gaussiano" = "blue",
    "epanechnikov" = "green")
values |>
    ggplot(aes(x = x)) +
    geom_point(aes(y = y)) +
    geom_density(aes(color = "rectangular"), alpha = 0.5, kernel = "rectangular", bw = bw) +
    geom_density(aes(color = "gaussiano"), alpha = 0.5, kernel = "gaussian", bw = bw) +
    geom_density(aes(color = "epanechnikov"), alpha = 0.5, kernel = "epanechnikov", bw = bw) +
    labs(
        title = "100 valores observados, estimación núcleo",
        y = "Densidad"
    ) +
    scale_color_manual(
        name = "Núcleo",
        breaks = c("rectangular", "gaussiano", "epanechnikov"),
        values = colors
    )
```

### Se pueden demostrar resultados asintóticos ###
:::{.callout-note}
# Función núcleo

Vamos a justificar, mediante el comportamiento asintótico, que los valores del estimador tipo núcleo dan buenas aproximaciones de la función de densidad.

Para el principal resultado se supondrán las siguientes condiciones sobre la función de densidad, la función núcleo y el ancho de banda:

- La densidad al cuadrado es integrable, admite derivada continua de segundo orden y esta al cuadrado es integrable. Seguiremos la notación $R(f'')=\int (f''(x))^2\:\mathrm{d}x$.
- El núcleo es una función de densidad simétrica y acotada, con momento de orden dos finito y al cuadrado es integrable. Seguiremos en este caso la notación $R(K)=\int (K(x))^2\:\mathrm{d}x$, y por $\mu_2(K)=\int x^2K(x)\:\mathrm{d}x$ el valor del momento de orden 2 asociado a la densidad $K$.
- Conforme aumentamos el tamaño de muestra el ancho de banda,  $h_n$, es una secuencia de valores positivos con límite 0 y  $nh_n$ tiende a  $+\infty$.
:::
