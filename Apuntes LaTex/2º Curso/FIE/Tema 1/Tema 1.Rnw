\section{Muestreo y distribuciones muestrales}
\subsection{Introducción}
\begin{qnote}[El contexto]
    \begin{itemize}[label=\textbullet]
        \item Tenemos una pregunta acerca de un fenómeno aleatorio.
        \item Formulamos un modelo para la variable de interés $X$.
        \item Traducimos la pregunta de interés en términos de uno o varios parámetros del modelo.
        \item Repetimos el experimento varias veces, apuntamos los valores de  $X$.
        \item ¿Cómo usar estos valores para extraer información sobre el parámetro?
    \end{itemize}
\end{qnote}
\subsection{Ejemplos}
\begin{qnote}[¿Está la moneda trucada?]
    \begin{itemize}[label=\textbullet]
        \item Experimento: tirar una moneda. $X=$resultado obtenido:  \[
            \begin{array}{c}
                P(X=+)=p,P(X=c)=1-p\\
                \text{¿}p=\dfrac{1}{2}?
            \end{array}
        \] 
    \end{itemize}
\end{qnote}
\begin{qnote}[Sondeo sobre intención de participación en una elecciones]
   \begin{itemize}[label=\textbullet]
       \item Queremos estimar la tasa de participación antes de unas elecciones generales.
       \item Formulamos un modelo:
           \begin{itemize}[label=\textrightarrow]
               \item Experimento: "escoger una persona al azar en el censo".
               \item $X$: participación, variable dicotómica ("Sí" o "No").  $p=P(X=\text{"Sí"})$.
           \end{itemize}
       \item ¿Cuánto vale $p$?
       \item Censo: aprox. 37 000 000. Escogemos aprox. 3000 personas.
   \end{itemize} 
\end{qnote}
\begin{qnote}[Determinación de la concentración de un producto]
   \begin{itemize}[label=\textbullet]
       \item Quiero determinar la concentración de un producto.
       \item Formulo el modelo:
           \begin{itemize}[label=\textrightarrow]
               \item Experimento: "llevar a cabo una medicación".
               \item $X$: "valor proporcionado por el aparato".
               \item $X\sim \mathcal{N}(\mu,\sigma^2)$.
           \end{itemize}
       \item ¿Qué vale $\mu$?
   \end{itemize} 
\end{qnote}
\subsection{Surge una pregunta}
En todas estas situaciones donde nos basamos en la repetición de un experimento simple...
\begin{itemize}[label=\textbullet]
    \item ¿Cómo sabemos que nuestra situación es fiable?
    \item ¿Qué confianza tenemos al extrapolar los resultados de una muestra de 3000 personas a una población de 37 millones de personas?
\end{itemize}
\subsection{Esbozo de respuesta: tasa de participación}
\begin{qnote}[Para convenceros, un experimento de simulación]
    \begin{itemize}[label=\textbullet]
        \item Voy a simular el proceso de extracción de una muestra de 3000 personas en una población de 37 millones de personas.
        \item Construyo a mi antojo los distintos componentes:
            \begin{itemize}[label=\textrightarrow]
                \item \textbf{La población:} defino en mi ordenador un conjunto de 37 000 000 de ceros y unos ($\Leftrightarrow$ el censo electoral)
                    \begin{itemize}[label=\textbullet]
                        \item "1" $\Leftrightarrow$ "la persona piensa ir a votar". 
                        \item "0" $\Leftrightarrow$ "la persona no piensa ir a votar".
                    \end{itemize}
                \item \textbf{La tasa de participación "real":} Decido que en mi población el 70\% piensa en ir a votar $\to $ 25 900 000 "1"s.
                \item \textbf{La extracción de una muestra:} construyo un pequeño programa que extrae al azar una muestra de 3000 números dentro del conjunto grande. 
            \end{itemize}
    \end{itemize}
\end{qnote}

<<>>=
poblacion <- c(rep(1, 25900000), rep(0, 11100000))
set.seed(314159)
p_muestra <- mean(sample(poblacion, 3000, replace = FALSE))
p_muestra
@

Queremos descartar que haya sido suerte. Vamos a repetir muchas veces (10000 veces por ejemplo), la extracción de una muestra de 3000 personas en la población.

<<echo=TRUE>>=
library(tidyverse)
lista_muestras <- replicate(
    10000,
    sample(poblacion, 3000, replace = FALSE),
    simplify = FALSE
)
p_muestras <- map_dbl(lista_muestras, mean)
head(p_muestras)
@

<<echo=TRUE>>=
library(tidyverse)
p_muestras <- replicate(
    10000,
    sample(poblacion, 3000, replace = FALSE),
    simplify = FALSE
) |>
    map_dbl(mean)
head(p_muestras)
@

Recogemos los valores obtenidos en un histograma.

<<>>=
tibble(p = p_muestras) |> 
    ggplot(aes(x = p)) +
    geom_histogram(
        aes(y = after_stat(density)),
        alpha = 0.8,
        fill = "lightblue"
    ) +
    labs(title = expression(paste("Distribución de ", hat(p), ", 10000 muestras"))) +
  theme_bw()
@

\subsection{Realización del experimento: conclusiones}
\begin{itemize}[label=\textbullet]
    \item La enorme mayoría de las muestras de 300 individuos proporcionan una tasa de participación muy próxima a la de la población.
        \begin{itemize}[label=\textrightarrow]
            \item \textbf{\rc{El riesgo}}  de cometer un error superior a $\pm$2 puntos, al coger una \textbf{\rc{una}} muestra de 3000 individuos es muy pequeño (y asumible...)
        \end{itemize}
    \item Si nos limitamos a muestras de 300 individuos, ¿qué esperáis?
\end{itemize}

<<>>=
phat_df <- map_dfr(
    c(3000, 300),
    \(n) tibble(
           p = map_dbl(
                 replicate(
                   100000,
                   sample( poblacion, n, replace = FALSE),
                   simplify = FALSE
                 ),
               mean
           )
         ) |> transform(n_muestra = factor(n))
)
phat_df|> 
    ggplot(aes(x = p, fill = n_muestra)) +
    geom_density(alpha = 0.5) +
    geom_histogram(aes(y = after_stat(density)), alpha=0.5) +
    scale_fill_discrete(name = "Tamaño muestral") +
  theme_bw()
@

\subsection{En la práctica}
\begin{qnote}[Usamos las distribuciones muestrales]
    \begin{itemize}[label=\textbullet]
        \item Las empresas de sondeos no se basan en simulaciones sino en cálculos teóricos.
        \item Experimento aleatorio: escoger al azar una muestra de 3000 personas dentro de una población de 37 000 000, con una tasa de participación $p$.
        \item Llamamos a  $\hat{p}$ la variable aleatoria: proporción de "1"s en la muestra escogida.
        \item ¿Cuál es la distribución de valores de $\hat{p}$? \[
        \hat{p}\sim \mathcal{N}\left( p, \dfrac{p(1-p)}{n} \right) 
        \] 
        Es lo que llamamos la \lb{distribución muestral} de $\hat{p}$. 
    \end{itemize}
\end{qnote}

\subsection{Uso de la distribución muestral}

\begin{qnote}[La distribución muestra de $\hat{p}$:]
    Es la distribución esperada de los valores de $\hat{p}$ respecto a todas las muestras de ese tamaño que podría extraer
\end{qnote}

<<>>=
p <- 0.7
n <- 3000
muphat <- p
sigmaphat <- sqrt( (p * (1 - p) / n))
x <- seq(qnorm(0.01, muphat, sigmaphat), qnorm(0.99, muphat, sigmaphat), length = 200)
y <- dnorm(x, muphat, sigmaphat)
tibble(x = x, y = y) |>
    ggplot(aes(x = x, y = y)) +
    geom_line() +
  theme_bw()
@

\subsection{Antes de extraer una muestra:}
\begin{itemize}[label=\textbullet]
    \item ¿Es suficiente el tamaño para el riesgo asumible y la precisión requerida?
    \item Una vez la muestra:
        \begin{itemize}[label=\textrightarrow]
            \item ¿Puedo dar un margen de error?
            \item ¿Puedo decidir si $p$ poblacional es, por ejemplo, mayor que un valor dado?
        \end{itemize}
\end{itemize}

\subsection{Otro ejemplo: valores muestrales de una distribución normal}
<<>>=
par(mfrow=c(1,2))
curve(dnorm(x, 0, 1), from = -3, to = 3,      
      xlab = "Distribución normal", ylab = " ", ylim = c(-0.1, 0.5))
abline(0, 0)
xsim <- rnorm(100, 0, 1)
y = 0
for(i in 1:100){
  points(xsim[i], y, col="blue")
  Sys.sleep(0.15)
}

hist(xsim, xlim = c(-3, 3), breaks = "Sturges", main = "", xlab = "Valores muestrales", ylab = "", col = "lightgreen")
@

<<>>=
par(mfrow=c(1,2))

curve(dgamma(x, 2, 3), from = 0, to = 3, xlab = "Distribución gamma", ylab = " ", ylim = c(-0.1, 1.5))
abline(0, 0)
xsim <- rgamma(100, 2, 3)
y = 0
for(i in 1:100){
  points(xsim[i], y, col = "blue")
  Sys.sleep(0.15)
}

hist(xsim, xlim = c(0, 3), breaks = "Sturges", main = "", xlab = "Valores muestrales", ylab = "", col = "lightblue")
@

\subsection{Un resultado importante}
\begin{qnote}[Ley (débil) de los grandes números]
    Sea $X$ una variable aleatoria y  $q(X)$ una variable aleatoria transformada de  $X$, con esperanza y momento de orden 2 finitos. Supongamos  $X_1,X_2,\dots,X_n,\dots$ una sucesión de variables aleatorias (vv.aa) independientes con la misma distribución que $X$, entonces  \[
        \lim_{n \to \infty} P\left[ \left| \dfrac{\sum_{i=1}^{n} g(X_i)}{n}-E[g(X)] \right| <\varepsilon \right] = 1, \text{ para todo }\varepsilon>0.
    \] 
\end{qnote}
\subsection{Algunos términos}
\begin{definition}
    \begin{itemize}[label=\textbullet]
        \item Sea una variable aleatoria $X$. Consideramos  $n$ variables aleatorias independientes e idénticamente distribuidas  $X_1,X_2,\dots,X_n$, que se distribuyen como $X$. La variable aleatoria multidimensional  $(X_1,X_2,\dots,X_n)$ es una \lb{muestra aleatoria simple} (m.a.s.) de $X$.
        \item Cualquier cantidad calculada a partir de las observaciones de una muestra: \lb{estadístico}.
        \item Experimento aleatorio: extraer una muestra. Consideramos un estadístico como una variable aleatoria. Nos interesa conocer la distribución del estadístico: \lb{distribución muestral}. 
    \end{itemize}
\end{definition}
\subsection{Ejemplos de estadísticos}
\begin{itemize}[label=\textbullet]
    \item Proporción muestral: $\hat{p}$.
    \item Media muestral: $\bar{X}=\dfrac{1}{n}\sum_{i=1}^{n} X_i$.
    \item Desviación típica muestral: $S_X=\sqrt{\dfrac{1}{n+1}\sum_{i=1}^{n} (X_i-\bar{X})^2} $
\end{itemize}
\subsection{La media muestral}
\begin{qnote}[Contexto]
    Estudiamos una variable $X$ cuantitativa.
     \begin{itemize}[label=\textbullet]
        \item Estamos interesados en $\mu$, el centro de la distribución de $X$.
        \item Extraemos una muestra de tamaño  $n$:  \[
                x_1,x_2,\dots,x_n.
            \] 
        \item Calculamos su media $\bar{x}$ para aproximar $\mu$.
        \item ¿Cuál es la distribución muestral de $\bar{X}$?
    \end{itemize}
\end{qnote}
\lb{\underline{Ejemplo:} } 
\begin{itemize}[label=\textbullet]
    \item Quiero medir una cantidad. Hay variabilidad en las mediciones.
    \item Introduzco una variable aleatoria $X=$"valor proporcionado por el aparato".
    \item  $\mu$ representa el centro de los valores.
    \item Extraigo una muestra de tamaño 5 del valor de $X$.
\end{itemize}
\subsubsection{Esperanza y varianza de la media muestral}
Llamamos $\mu=E[X]$ y $\sigma^2=\mathrm{Var}(X)$.
\begin{itemize}[label=\textbullet]
    \item Tenemos \[
            \bboxed{E[\bar{X}]=\mu.} 
    \] 
    \begin{itemize}[label=\textrightarrow]
        \item Es decir que el centro de la distribución muestral de $\bar{X}$ coincide con el centro de la distribución $X$.
    \end{itemize}
    \item Tenemos $\mathrm{Var}(\bar{X})=\dfrac{\sigma^2}{n}$, es decir, la dispersión de la distribución muestral de $\bar{X}$ es $\sqrt{n} $ veces más pequeña que la dispersión inicial de $X$.
\end{itemize}
\textbf{Iluestración: $X$ inicial,  $\bar{X}$ con $n=3,\,\bar{X}$ con $n=10$.} 

<<>>=
mu <- 5 
sigma <- 2
muxbar <- mu
sigmaxbar <- function(n, sigma){
    sigma / sqrt(n)
}
construct_xy <- function(mu, sigma, label){
    x <- seq(0, 10, by = 0.01)
    y <- dnorm(x, mu, sigma)
    tibble(x = x, y = y, label = label)
}
map_dfr(
    c(1, 3, 10),
    \(n) construct_xy(mu, sigmaxbar(n, sigma), paste("n = ", n))
) |>
    transform(label = factor(label, levels = unique(label))) |>
    ggplot(aes(x, y, col = label)) +
    geom_line() + xlim(0, 10) + ylim(0, 0.65) +
    scale_color_discrete(name = "Tamaño muestral") +
  theme_bw()
@

\subsection{Consecuencia práctica}
\begin{qnote}[Aparato de medición]
    \begin{itemize}[label=\textbullet]
        \item Experimento: llevar a cabo una medición con un aparato.
        \item Variable aleatoria $X$: "valor proporcionado por el aparato".
        \item  $E[X]$: centro de la distribución de los valores proporcionados por el aparato.
             \begin{itemize}[label=\textrightarrow]
                 \item Lo deseable: $E[X]=$valor exacto de la cantidad que buscamos medir.
                 \item En este caso, decimos: el aparato es  \lb{exacto}. 
            \end{itemize}
        \item $\sigma_X$: dispersión de la distribución de los valores proporcionados por el aparato.
             \begin{itemize}[label=\textrightarrow]
                \item Lo deseable: $\sigma_X$ pequeño.
                \item En este caso, decimos: el aparato es  \lb{preciso}. 
            \end{itemize}
    \end{itemize}
\end{qnote}
\subsubsection{Analogía con una diana}

<<>>=
library(ggplot2)
library(dplyr)
library(ggforce)
library(gridExtra)

# Función para crear una diana
crear_diana <- function(puntos, titulo) {
  # Crear círculos concéntricos
  circulos <- data.frame(
    x0 = 0, y0 = 0, r = seq(1, 5)
  )
  
  ggplot() +
    geom_circle(data = circulos, aes(x0 = x0, y0 = y0, r = r), color = "black", size = 0.4) +
    geom_point(data = puntos, aes(x, y), size = 2) +
    coord_fixed() +
    theme_void() +
    ggtitle(titulo) +
    theme(plot.title = element_text(hjust = 0.5, size = 10))
}

# 1️⃣ Preciso pero no exacto (agrupados pero lejos del centro)
p1 <- data.frame(x = rnorm(8, 3, 0.2), y = rnorm(8, 3, 0.2))
g1 <- crear_diana(p1, "Preciso pero no exacto")

# 2️⃣ Exacto pero no preciso (dispersos pero centrados)
p2 <- data.frame(x = rnorm(8, 0, 1.5), y = rnorm(8, 0, 1.5))
g2 <- crear_diana(p2, "Exacto pero no preciso")

# 3️⃣ Preciso y exacto (agrupados y centrados)
p3 <- data.frame(x = rnorm(8, 0, 0.2), y = rnorm(8, 0, 0.2))
g3 <- crear_diana(p3, "Preciso y exacto")

# 4️⃣ Ni exacto ni preciso (dispersos y lejos del centro)
p4 <- data.frame(x = rnorm(8, 3, 1), y = rnorm(8, 3, 1))
g4 <- crear_diana(p4, "Ni exacto ni preciso")

# Mostrar los cuatro gráficos juntos
grid.arrange(g1, g2, g3, g4, ncol = 4)

@
\subsection{Varianza muestral}

Si $(X_1,X_2,\dots,X_n)$ es una muestra aleatoria simple de $X$, definimos la  \lb{varianza muestral} $S_n^2$ como \[
S_n^2=\dfrac{1}{n-1}\sum_{i=1}^{n} (X_i-\bar{X})^2.
\]  
\begin{qnote}[Fórmula alternativa apra $S_n^2$:]
    \[
    S_n^2=\dfrac{n}{n-1}\left( \overline{X^2}_n-(\bar{X}_n)^2 \right),
    \] donde $\overline{X^2}_n=\dfrac{1}{n}\sum_{i=1}^{n} X_i^2$.
\end{qnote}

\subsubsection{Dos apuntes}
\begin{qnote}[En algunos textos en castellano]
    Se suele llamar $S_n^2$ \lb{cuasi-varianza muestral}, reservando el término varianza muestral para la cantidad $\dfrac{1}{n}\sum_{i=1}^{n} (X_i-\bar{X}_n)^2$ 
\end{qnote}

\begin{qnote}[En estas fórmulas:]
    Omitimos, si no hay confusión posible, el subíndice $n$, escribiendo  $S^2,\,\bar{X}=\sum_{i=1}^{n} X_i$ y $\overline{X^2}=\dfrac{1}{n}\sum_{i=1}^{n} X_i^2$.
\end{qnote}

\subsection{Esperanza de la varianza muestral}
\begin{proposition}
    Si $(X_1,X_2,\dots,X_n)$ es una muestra aleatoria simple de $X$ con varianza  $\sigma_X^2$, \[
        E[S_n^2]=\sigma_X^2.
    \] 
\end{proposition}

\subsection{Distribuciones muestrales de $\bar{X}$ y $S^2$}

\begin{qtip}[Tened en cuenta]
    \begin{itemize}[label=\textbullet]
        \item Los resultados anteriores sobre $E[\bar{X}]$ y $\sigma_{\bar{X}}$ son válidos sea cual seal el modelo escogido para la distribución de $X$.
        \item Si queremos decir algo más preciso sobre la distribución de  $\bar{X}$ (densidad, etc...) necesitamos especificar la distribución de $X$.
        \item En el caso en que la variable  $X$ siga una distribución normal, el  \textbf{teorema de Fisher} analiza cómo se comportan los estadísticos anteriores y nos permiten establecer una serie de consecuencias que serán utilizadas posteriormente en los temas de intervalos de confianza y de contrastes de hipótesis. 
    \end{itemize}
\end{qtip}

\subsection{Distribución de $\bar{X}$ y $S^2$ para una m.a.s. de una distribuación normal}

\begin{qnote}[Teorema de Fisher]
    Consideramos una muestra aleatoria simple de una variable aleatoria $X$ con distribución normal $\mathcal{N}(\mu,\sigma^2)$, entonces se verifica:
    \begin{enumerate}[label=\arabic*)]
      \item $\bar{X}_n$ y $S_n^2$ son dos variables aleatorias independientes.
      \item $\dfrac{\bar{X}_n-\mu}{\sigma / \sqrt{n}}\sim\mathcal{N}(0,1).$
      \item $\dfrac{(n-1)S_n^2}{\sigma^2}\sim \chi_{n-1}^2.$
    \end{enumerate}
\end{qnote}
\subsection{Recordatorio: distribución $\chi^2$ con $p$ grados de libertad}

\begin{qnote}[La distribución $\chi^2$.]
    Para $p\in \N^+$, la función de densidad de la distribución $\xi^2$ es igual a \[
    \dfrac{1}{\Gamma\left( \tfrac{p}{2}  \right) 2^{\tfrac{p}{2} }}\cdot x^{\tfrac{p}{2} -1}e^{\tfrac{x}{2} },\text{ si }x>0,
    \] donde $\Gamma$ denota la función Gamma (Nota: para cualquier real $\alpha>0,\,\Gamma(\alpha)=\int_{0}^{+\infty} t^{\alpha-1}e^{-t}\dt$).
\end{qnote}

\begin{qnote}[Caracterización de la $\chi^2$]
    Si $Z_1,\dots,Z_p$ son $p$ variables aleatorias independientes, con  $Z_i\sim \mathcal{N}(0,1)$, entonces la variable aleatoria $X$ definida como  \[
    X=Z_1^2+\cdots+Z_p^2=\sum_{i=1}^{p} Z_i^2
    \] tiene una distribución $\chi^2$ con $p$ grados de libertad.
\end{qnote}

\begin{qtip}[¿Cómo es su función de densidad?]
    Depende de los grados de libertad
\end{qtip}

<<>>=
ggplot() +
    geom_function(
        aes(colour = "grados libertad: 1"),
        fun = dchisq, args=list(df = 1)
    ) +
    geom_function(
        aes(colour = "grados libertad: 5"),
        fun = dchisq, args=list(df = 5)
    ) +  
    geom_function(
        aes(colour = "grados libertad: 10"),
        fun = dchisq, args=list(df = 10)
    ) +  
    geom_function(
        aes(colour = "grados libertad: 20"),
        fun = dchisq, args=list(df = 20)
    ) + xlim(0, 25) + scale_color_discrete(guide = guide_legend(title =  NULL)) + theme_bw()
@

\subsection{Distribución t-Student}

\begin{qnote}[Hemos visto, si $X$ es Normal:]
    \[
        \dfrac{\bar{X}_n-\mu}{\sigma / \sqrt{n} }\sim \mathcal{N}(0,1).
    \] 
    Si queremos centrarnos en $\mu$ es natural sustituir en ella $\sigma$ por $S_n$.
\end{qnote}

\begin{proposition}
    Sea $(X_1,\dots,X_n)$ una muestra aleatoria simple de una población $\mathcal{N}(\mu,\sigma^2)$, \[
    T=\dfrac{\bar{X}-\mu}{S / \sqrt{n} }
    \] tiene por densidad
    \begin{equation}
        f_{n-1}(t)\propto \dfrac{1}{\left( \dfrac{1+t^2}{n-1} \right)^{n / 2} },\quad-\infty<t<\infty,
    \end{equation}
    La distribución que admite esta densidad se llama \lb{distribución t-Student} con $n-1$ grados de libertad. Escribimos  $T\sim t_{n-1}$. 
\end{proposition}

\begin{qwarning}[Su densidad]
    La función de densidad de una t de Student con $k$ grados de libertad:  \[
    f_k(t)=\dfrac{\Gamma\left( \tfrac{k+1}{2}  \right) }{\Gamma\left( \tfrac{k}{2}  \right) }\dfrac{1}{\sqrt{k\pi} }\dfrac{1}{(1+t^2 / k)^{\frac{k+1}{2}}},\quad-\infty<t<\infty,
    \] donde $\Gamma$ denota la función Gamma.
\end{qwarning}
\begin{qnote}[Caracterización de la t de Student como cociente]
    Si $Z$ e  $Y$ son dos variables aleatorias independientes, con  $Z\sim \mathcal{N}(0,1)$ e $Y\sim \chi_p^2$, el cociente \[
    T=\dfrac{Z}{\sqrt{Y / p} }\sim t_p,
    \] donde $t_p$ denota la t de Student con  $p$ grados de libertad.
\end{qnote}
\subsubsection{¿Cuál es la forma de la densidad de una t-Student?}
\begin{qtip}[Tiene colas más pesadas que una normal]
<<>>=
# Cargar librería
library(ggplot2)
library(dplyr)
library(tidyr)

# Secuencia de valores t
t_vals <- seq(-6, 6, length.out = 500)

# Grados de libertad
g_vals <- c(1, 3, 10, 30, 150)

# Calcular las densidades para cada g
df_t <- expand.grid(t = t_vals, g = g_vals) %>%
  mutate(f_t = dt(t, df = g))

# Graficar con ggplot2
ggplot(df_t, aes(x = t, y = f_t, color = factor(g))) +
  geom_line(size = 0.8) +
  labs(
    x = "t",
    y = expression(f[g](t)),
    color = "Grados de libertad (g)"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "right",
    legend.title = element_text(face = "bold")
  )

@
   \begin{center}
       Densidad de la distribución t de Student para varios valores de los grados de libertad
   \end{center} 
\end{qtip}
\subsection{Función F de Snedecor para el cociente de varianzas}
\begin{proposition}
    Consideremos $U_1$ y $U_2$ dos variables aleatorias independientes distribución $\chi^2$ con $p_1$ y $p_2$ grados de libertad, respectivamente.

    El cociente $F=\dfrac{U_1 / p_1}{U_2 / p_2}$ admite la densidad \[
    f_F(x)=\dfrac{\Gamma\left( \frac{p_1+p_2}{2} \right) }{\Gamma(p_1)\Gamma(p_2)}\left( \dfrac{p_1}{p_2} \right) ^{p_1}\dfrac{x^{p_1 / 2-1}}{\left( 1+\frac{p_1}{p_2} x \right)^{\frac{p_1+p_2}{2} } }.
    \] 
    Esta distribución se llama F de Snedecor 
\end{proposition}
