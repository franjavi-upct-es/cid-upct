\setcounter {tocdepth}{4}
\color {black}
\contentsline {section}{\numberline {1}Introducción al Machine Learning}{1}{section.1}%
\contentsline {subsection}{\numberline {1.1}Tareas básicas del Machine Learning}{1}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Generalización: subajuste y sobreajuste}{2}{subsection.1.2}%
\contentsline {subsubsection}{\numberline {1.2.1}Planteamiento del problema}{2}{subsubsection.1.2.1}%
\contentsline {subsubsection}{\numberline {1.2.2}Planteamiento de la solución}{3}{subsubsection.1.2.2}%
\contentsline {subsubsection}{\numberline {1.2.3}Peligros}{3}{subsubsection.1.2.3}%
\contentsline {subsubsection}{\numberline {1.2.4}Subajuste y sobreajuste}{3}{subsubsection.1.2.4}%
\contentsline {subsubsection}{\numberline {1.2.5}Complejidad del modelo vs número de datos}{4}{subsubsection.1.2.5}%
\contentsline {subsubsection}{\numberline {1.2.6}Conclusión}{4}{subsubsection.1.2.6}%
\contentsline {subsubsection}{\numberline {1.2.7}Descomposición sesgo-varianza. Coste cuadrático}{4}{subsubsection.1.2.7}%
\contentsline {subsubsection}{\numberline {1.2.8}Algunas técnicas de generalización}{8}{subsubsection.1.2.8}%
\contentsline {subsubsection}{\numberline {1.2.9}Evitar el sobreajuste: "early stopping"}{9}{subsubsection.1.2.9}%
\contentsline {subsubsection}{\numberline {1.2.10}Evitar el sobreajuste: "Weight Decay"}{10}{subsubsection.1.2.10}%
\contentsline {subsection}{\numberline {1.3}Evaluación de prestaciones}{11}{subsection.1.3}%
\contentsline {subsubsection}{\numberline {1.3.1}Regresión}{12}{subsubsection.1.3.1}%
\contentsline {subsubsection}{\numberline {1.3.2}Clasificación}{13}{subsubsection.1.3.2}%
\contentsline {subsubsubsection}{\numberline {1.3.2.1)}Interpretación de la Precisión y la Sensibilidad}{14}{subsubsubsection.1.3.2.1}%
\contentsline {subsubsubsection}{\numberline {1.3.2.2)}Puntuación Kappa}{16}{subsubsubsection.1.3.2.2}%
\contentsline {subsubsection}{\numberline {1.3.3}Hold-out}{16}{subsubsection.1.3.3}%
\contentsline {subsubsubsection}{\numberline {1.3.3.1)}Validación Cruzada}{18}{subsubsubsection.1.3.3.1}%
\contentsline {section}{\numberline {2}Aprendizaje Supervisado}{22}{section.2}%
\contentsline {subsection}{\numberline {2.1}Árboles de Decisión}{22}{subsection.2.1}%
\contentsline {subsubsubsection}{\numberline {2.1.0.1)}Arquitectura}{22}{subsubsubsection.2.1.0.1}%
\contentsline {subsubsubsection}{\numberline {2.1.0.2)}Ventajas y desventajas}{23}{subsubsubsection.2.1.0.2}%
\contentsline {subsubsubsection}{\numberline {2.1.0.3)}Clasificación vs Regresión}{24}{subsubsubsection.2.1.0.3}%
\contentsline {subsubsection}{\numberline {2.1.1}Construcción de árboles de decisión}{24}{subsubsection.2.1.1}%
\contentsline {subsubsubsection}{\numberline {2.1.1.1)}Particiones}{25}{subsubsubsection.2.1.1.1}%
\contentsline {subsubsubsection}{\numberline {2.1.1.2)}Particiones posibles}{25}{subsubsubsection.2.1.1.2}%
\contentsline {subsubsection}{\numberline {2.1.2}ID3: Algoritmo básico de aprendizaje}{26}{subsubsection.2.1.2}%
\contentsline {subsubsubsection}{\numberline {2.1.2.1)}Entropía}{26}{subsubsubsection.2.1.2.1}%
\contentsline {subsubsubsection}{\numberline {2.1.2.2)}Ganancia de Información}{26}{subsubsubsection.2.1.2.2}%
\contentsline {subsubsubsection}{\numberline {2.1.2.3)}Error global}{29}{subsubsubsection.2.1.2.3}%
\contentsline {subsubsubsection}{\numberline {2.1.2.4)}Algoritmo}{29}{subsubsubsection.2.1.2.4}%
\contentsline {subsubsection}{\numberline {2.1.3}Sobre-ajuste}{30}{subsubsection.2.1.3}%
\contentsline {subsubsubsection}{\numberline {2.1.3.1)}Espacio de hipótesis y sobre-ajuste}{30}{subsubsubsection.2.1.3.1}%
\contentsline {subsubsubsection}{\numberline {2.1.3.2)}Proceso de poda}{30}{subsubsubsection.2.1.3.2}%
\contentsline {subsubsubsection}{\numberline {2.1.3.3)}Otras medidas}{31}{subsubsubsection.2.1.3.3}%
\contentsline {subsubsection}{\numberline {2.1.4}Algoritmo CART y Otros}{31}{subsubsection.2.1.4}%
\contentsline {subsubsubsection}{\numberline {2.1.4.1)}CART}{31}{subsubsubsection.2.1.4.1}%
\contentsline {subsubsubsection}{\numberline {2.1.4.2)}C4.5, C5.0}{31}{subsubsubsection.2.1.4.2}%
\contentsline {subsubsection}{\numberline {2.1.5}Random Forests}{32}{subsubsection.2.1.5}%
\contentsline {subsubsubsection}{\numberline {2.1.5.1)}Algoritmo}{32}{subsubsubsection.2.1.5.1}%
\contentsline {subsubsubsection}{\numberline {2.1.5.2)}OBB e importancia de las variables}{32}{subsubsubsection.2.1.5.2}%
\contentsline {subsubsection}{\numberline {2.1.6}Conclusiones}{32}{subsubsection.2.1.6}%
\contentsline {subsubsection}{\numberline {2.1.7}Bosques Aleatorios (\textit {"Random Forest"})}{33}{subsubsection.2.1.7}%
\contentsline {subsubsubsection}{\numberline {2.1.7.1)}Motivación}{33}{subsubsubsection.2.1.7.1}%
\contentsline {subsubsubsection}{\numberline {2.1.7.2)}Solución}{33}{subsubsubsection.2.1.7.2}%
\contentsline {subsubsubsection}{\numberline {2.1.7.3)}Número de árboles}{34}{subsubsubsection.2.1.7.3}%
\contentsline {subsubsubsection}{\numberline {2.1.7.4)}Generación conjuntos de entrenamiento}{35}{subsubsubsection.2.1.7.4}%
\contentsline {subsubsubsection}{\numberline {2.1.7.5)}Entrenamiento de los árboles}{35}{subsubsubsection.2.1.7.5}%
\contentsline {subsubsubsection}{\numberline {2.1.7.6)}Modo operación}{36}{subsubsubsection.2.1.7.6}%
\contentsline {subsubsubsection}{\numberline {2.1.7.7)}Bagging y Conjunto de Validación}{37}{subsubsubsection.2.1.7.7}%
\contentsline {subsubsubsection}{\numberline {2.1.7.8)}Selección del número de árboles}{37}{subsubsubsection.2.1.7.8}%
\contentsline {subsubsubsection}{\numberline {2.1.7.9)}Seleccionar del número de características}{37}{subsubsubsection.2.1.7.9}%
\contentsline {subsubsubsection}{\numberline {2.1.7.10)}Selección del criterio de parada}{38}{subsubsubsection.2.1.7.10}%
\contentsline {subsubsubsection}{\numberline {2.1.7.11)}Relevancia de las características}{38}{subsubsubsection.2.1.7.11}%
\contentsline {subsection}{\numberline {2.2}Perceptrón monocapa y multicapa (MLP)}{38}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Teoría de la decisión}{38}{subsubsection.2.2.1}%
\contentsline {subsubsubsection}{\numberline {2.2.1.1)}Decisión analítica}{39}{subsubsubsection.2.2.1.1}%
\contentsline {subsubsubsection}{\numberline {2.2.1.2)}Modelos discriminatorios}{39}{subsubsubsection.2.2.1.2}%
\contentsline {subsubsubsection}{\numberline {2.2.1.3)}Funciones discriminantes}{39}{subsubsubsection.2.2.1.3}%
\contentsline {subsubsection}{\numberline {2.2.2}Fundamento biológico}{40}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Perceptrón monocapa}{41}{subsubsection.2.2.3}%
\contentsline {subsubsubsection}{\numberline {2.2.3.1)}(Widrow): filtro transversal (adaptativo) + umbral duro}{41}{subsubsubsection.2.2.3.1}%
\contentsline {subsubsubsection}{\numberline {2.2.3.2)}(Rosenblatt): dados $N$ pares entrada-salida: $\{\mathbf {x}_k,\mathbf {d}_k\}_1^N$}{41}{subsubsubsection.2.2.3.2}%
\contentsline {subsubsection}{\numberline {2.2.4}Limitaciones del Perceptron Monocapa}{42}{subsubsection.2.2.4}%
\contentsline {subsubsection}{\numberline {2.2.5}El algoritmo LMS}{42}{subsubsection.2.2.5}%
\contentsline {subsubsection}{\numberline {2.2.6}Características del LMS}{43}{subsubsection.2.2.6}%
\contentsline {subsubsection}{\numberline {2.2.7}Error cuadrático (SSE) para clasificación}{43}{subsubsection.2.2.7}%
\contentsline {subsubsubsection}{\numberline {2.2.7.1)}Problema con los "OUTLIERS"}{43}{subsubsubsection.2.2.7.1}%
\contentsline {subsubsection}{\numberline {2.2.8}Activación blanda}{43}{subsubsection.2.2.8}%
\contentsline {subsubsection}{\numberline {2.2.9}LMS con activación blanda}{44}{subsubsection.2.2.9}%
\contentsline {subsubsection}{\numberline {2.2.10}Arquitectura del Perceptron Multicapa (MLP)}{44}{subsubsection.2.2.10}%
\contentsline {subsubsubsection}{\numberline {2.2.10.1)}Redes Multicapa}{45}{subsubsubsection.2.2.10.1}%
\contentsline {subsubsection}{\numberline {2.2.11}Propiedades de las MLP}{45}{subsubsection.2.2.11}%
\contentsline {subsubsection}{\numberline {2.2.12}Capacidades del MLP}{46}{subsubsection.2.2.12}%
\contentsline {subsubsection}{\numberline {2.2.13}Notación utilizada MLP}{47}{subsubsection.2.2.13}%
\contentsline {subsubsection}{\numberline {2.2.14}Algoritmo de retropropagación (BackPropagation (BP))}{47}{subsubsection.2.2.14}%
\contentsline {subsubsection}{\numberline {2.2.15}Modos de entrenamiento}{48}{subsubsection.2.2.15}%
\contentsline {subsubsection}{\numberline {2.2.16}Sobre las muestras}{48}{subsubsection.2.2.16}%
\contentsline {subsubsection}{\numberline {2.2.17}Sobre el dimensionado}{49}{subsubsection.2.2.17}%
\contentsline {subsubsection}{\numberline {2.2.18}Sobre el algoritmo de entrenamiento}{49}{subsubsection.2.2.18}%
\contentsline {subsection}{\numberline {2.3}Redes de funciones de base radial}{52}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Aproximadores universales}{52}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Funciones radiles}{53}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Propuestas de RBF}{53}{subsubsection.2.3.3}%
\contentsline {subsubsubsection}{\numberline {2.3.3.1)}Primera propuesta: \textbf {Interpolación exacta}}{53}{subsubsubsection.2.3.3.1}%
\contentsline {subsubsubsection}{\numberline {2.3.3.2)}Segunda propuesta: \textbf {Interpolación no exacta}}{54}{subsubsubsection.2.3.3.2}%
\contentsline {subsubsection}{\numberline {2.3.4}Entrenamiento de las RBF}{54}{subsubsection.2.3.4}%
\contentsline {subsubsubsection}{\numberline {2.3.4.1)}Método 1: Centroides fijados aleatoriamente}{54}{subsubsubsection.2.3.4.1}%
\contentsline {subsubsubsection}{\numberline {2.3.4.2)}Cálculo de los pesos de la capa de salida}{54}{subsubsubsection.2.3.4.2}%
\contentsline {subsubsubsection}{\numberline {2.3.4.3)}Método 2: Centroides fijados según cuantifiación vectorial}{55}{subsubsubsection.2.3.4.3}%
\contentsline {subsubsubsection}{\numberline {2.3.4.4)}Método 3: Entrenamiento completo por gradiente supervisado}{55}{subsubsubsection.2.3.4.4}%
\contentsfinish 
