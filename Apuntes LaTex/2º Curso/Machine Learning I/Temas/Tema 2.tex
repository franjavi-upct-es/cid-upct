\section{Aprendizaje Supervisado}
\subsection{Árboles de Decisión}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Los árboles de decisión son máquinas de aprendizaje supervisado que sirven para clasificar o aproximar.

Supongamos el siguiente problema

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\rowcolor{lightblue!20}
		\hline
		Paciente & Presión Arterial & Urea en sangre & Gota & Hipotiroidismo & Administrar Tratamiento \\
		\hline
		1 & Alta & Alta & Sí & No & No \\
		\hline
		2 & Alta & Alta & Sí & Sí & No \\
		\hline
		3 & Normal & Alta & Sí & No & Sí \\
		\hline
		4 & Baja & Normal & Sí & No & Sí \\
		\hline
		5 & Baja & Baja & No & No & Sí \\
		\hline
		6 & Baja & Baja & No & Sí & No \\
		\hline
		7 & Normal & Baja & No & Sí & Sí \\
		\hline
		8 & Alta & Normal & Sí & No & No \\
		\hline
		9 & Alta & Baja & No & No & Sí \\
		\hline
		10 & Baja & Normal & No & No & Sí \\
		\hline
		11 & Alta & Normal & No & Sí & Sí \\
		\hline
		12 & Normal & Normal & Sí & Sí & Sí \\
		\hline
		13 & Normal & Alta & No & No & Sí \\
		\hline
		14 & Baja & Normal & Si & Sí & No \\
		\hline
	\end{tabular}
\end{center}
\begin{itemize}
	\item Planteamiento del problema: ¿Cuál es la \textbf{mejor secuencia de preguntas} para saber la clase a la que pertenece un objeto descrito por sus atributos?
	\item Evidentemente, la "mejor respuesta" es aquella que con el \textbf{menor número de preguntas}, devuelve una respuesta suficientemente buena.
	\item ¿Qué es mejor preguntar primero si tiene gota o cómo tiene la presión arterial?
\end{itemize}
\subsubsubsection{Arquitectura}
Un árbol de decisión es una estructura jerárquica que consta de un nodo raíz, ramas, nodos internos y nodos hoja.
\begin{itemize}
	\item Comienzo con un \textbf{nodo raíz} sin ramas entrantes. Las ramas salientes del nodo raíz alimentan los nodos internos.
	\item Los \textbf{nodos internos} evalúan características disponibles para formar subconjuntos homogéneos, indicados por nodos hoja o nodos terminales.
	\item Los \textbf{nodos hoja} representan todos los resultados posibles dentro del conjunto de datos.
\end{itemize}
\begin{center}
	\includegraphics{"Temas/Tema 1/Screenshot002"}
\end{center}
\subsubsubsection{Ventajas y desventajas}
\begin{itemize}[label=\color{lightblue}\textbullet]
	\item Pros
	\begin{itemize}
		\item Fáciles de entender e interpretar.
		\item Sirven también para establecer reglas
		\item No lineales
		\item Menos pre-procesado de los datos: son robustos ante presencia de datos erróneos (outlier), valores faltantes o tipo de datos.
		\item Es un método no paramétrico (por ejemplo, no hay suposición acerca del espacio de distribución y la estructura del clasificador).
	\end{itemize}
	\item Contras
	\begin{itemize}
		\item \textbf{Sobreajuste:} Los árboles más pequeños son más fáciles de interpretar, pero los más grandes pueden resultar en sobreajuste.
		\item Perdida de información al categorizar variables continuas.
		\item \textbf{Precisión:} Otros métodos (por ejemplo, SVM) a menudo tienen tasas de error 30\% más bajas que los árboles básico (ID.3 y CART).
		\item \textbf{Inestabilidad:} un pequeño cambio en los datos puede modificar ampliamente la estructura del árbol (distintos conjuntos, distintos árboles). Varianza elevada.
	\end{itemize}
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición alternativa: \textbf{recursividad}
\end{itemize}
Un árbol de decisión es una estructura recursiva formada por nodos, en el que existe:
\begin{itemize}
	\item Un nodo raíz
	\item El nodo raíz tiene uno o más subnodos.
	\item Cada uno de los subnodos puede ser, a su vez, raíz de un árbol
\end{itemize}
Esta característica recursiva hace que muchos de los algoritmos para crearlos se comporten también de manera recursiva.
\begin{center}
	\includegraphics{"Temas/Tema 1/Screenshot003"}
\end{center}
\subsubsubsection{Clasificación vs Regresión}
\begin{itemize}[label=\color{lightblue}\textbullet]
	\item Clasificación
	\begin{itemize}
		\item La variable dependiente es categórica.
		\item Los valores de los nodos hoja son la \textbf{moda} de las observaciones de la región
	\end{itemize}
	\begin{center}
		\includegraphics{"Temas/Tema 1/Screenshot004"}
	\end{center}
	\item Regresión
	\begin{itemize}
		\item La variable dependiente es continua.
		\item Los valores de los nodos hoja son la \textbf{media} de las observaciones de la región.
	\end{itemize}
	\begin{center}
		\includegraphics{"Temas/Tema 1/Screenshot005"}
	\end{center}
	
\end{itemize}
\subsubsection{Construcción de árboles de decisión}
\subsubsubsection{Particiones}
Cada nodo define una \textbf{partición} del conjunto de entrenamiento en función de los datos que representa.\\
Las particiones producen subconjuntos que son \textbf{exhaustivos} y \textbf{excluyentes}.\\
Cuestiones clave:
\begin{itemize}
	\item \textbf{Tipos de particiones:} cuantos más, más posibilidad de encontrar patrones y, por tanto, los árboles más precisos y expresivos.
	\item \textbf{Número de particiones:} A más particiones mayor complejidad. Equilibrio entre complejidad y precisión.
	\item Selección del \textbf{mejor atributo} en cada paso.
	\item Selección del \textbf{mejor valor} de umbral de los valores.
\end{itemize}
\subsubsubsection{Particiones posibles}
Los algoritmos más populares sólo proponen un tipo de partición para valores nominales y otro para valores numéricos:
\begin{itemize}
	\item \textbf{Particiones nominales:} En el caso que tengamos un atributo $x_i$ que tenga como posibles valores $\{v_1,v_2,\dots,v_n\}$ sólo es posible la partición \[ (x_1=v_1,x_2=v_2,\cdots,x_n=v_n) \]que da lugar a árboles con nodos con más de dos nodos hijos.
	\begin{center}
		\includegraphics{"Temas/Tema 1/Screenshot006"}
	\end{center}
	En el caso de árboles binarios se tienen que evaluar $n$ particiones (una por cada posible valor), definidas por $(x_i=v_i,x_i\neq v_i)$.
	\item \textbf{Particiones numéricas:} Si el atributo $x_i$ es numérico y continuo, se intenta definir particiones que separe las instancias en intervalos de la forma \begin{center}
		$(x_i\le a,x_i>a)$\qquad\begin{minipage}{0.3\textwidth}
			\includegraphics{"Temas/Tema 1/Screenshot007"}
		\end{minipage}
	\end{center}
	eligiendo diferentes valores de $a$ tenemos diferentes particiones. La expresividad resultante se conoce como \textit{expresividad cuadrangular} y que no relacionan atributos (sólo un atributo cada vez).
	\begin{center}
		\includegraphics{"Temas/Tema 1/Screenshot008"}
	\end{center}
	
\end{itemize}
\subsubsection{ID3: Algoritmo básico de aprendizaje}

\begin{quote}
	El algoritmo básico de aprendizaje es el \textbf{ID3 (Iterative Dichotomiser 3)}, J. Ross Quinlan, investigador australiano que propuso el método en 1983
\end{quote}
El método ID3 trata de encontrar una partición que asegure la \textbf{máxima capacidad predictiva y la máxima homogeneidad} de las clases\\
Medida de homogeneidad: la \textbf{entropía}\\
Repetición de \textbf{"cortes en dos"} hasta que se cumpla una determinada condición
\subsubsubsection{Entropía}
Para determinar el mejor atributo, el ID3 utiliza la \textbf{entropía}.

\begin{tikzpicture}
	\node[draw=lightblue, fill=lightblue!10, line width=1.5, text width=\linewidth] {Sea $S$ un conjunto de entrenamiento. Sea $p_\oplus$ la proporción de instancias positivas en $S$ y $p_\ominus$ la proporción de instancias negativas en $S$. La \textbf{entropía de $S$} es: \[ H(S)=p_\oplus\log_2\dfrac{1}{p_\oplus}+p_\ominus\log_2\dfrac{1}{p_\ominus}=-p_\oplus\log_2p_\oplus-p_\ominus\log_2p_\ominus \]};
\end{tikzpicture}

(Relación de la entropía con los conceptos de desorden, equiprobabilidad y homogeneidad).

La entropía nos mide la homogeneidad de los datos (relación inversa).
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot001"}
\end{center}
\begin{minipage}{0.45\textwidth}
	Para el caso binario:
	\begin{itemize}
		\item Entropía igual a 1 $\to$ mínima homogeneidad (equiprobabilidad: $p_\ominus=p_\oplus$).
		\item Entropía igual a 0 $\to$ máxima homogeneidad (todas las instancias de una clase)
	\end{itemize}
\end{minipage}\qquad\begin{minipage}{0.45\textwidth}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot002"}
\end{center}
\end{minipage}
A la hora de construir un árbol es preferible crear nodos con nodos hoja homogéneos, es decir, de \textbf{baja entropía}.
\subsubsubsection{Ganancia de Información}
Sea un conjunto de datos $\mathcal{X}$ con entropía $H(\mathcal{X})$.

Si elegimos un atributo $A$ para crear un nodo del árbol, la entropía esperada es: \[ H(\mathcal{X},A)=\sum_{v\in\mathrm{valores}(A)}\dfrac{|\mathcal{X}_v|}{\mathcal{X}}H(\mathcal{X}_v) \]siendo $\mathcal{X}_v$ el subconjunto de $\mathcal{X}$ con todas las instancias con $A=v$.

Por lo tanto, la reducción esperada de la entropía, o lo que es lo mismo la \textbf{Ganancia de Información}, al elegir el atributo $A$ como nodo de decisión del árbol es \[ \bboxed{\mathrm{Ganacia}(\mathcal{X},A))H(\mathcal{X})-H(\mathcal{X},A)} \]
Por tanto, se elige el atributo que produzca hojas homogéneas, es decir, la \textbf{máxima} ganancia de información.

\Ej

Atributos nominales (no numéricos)

\begin{center}
	\begin{tabular}{|cccccc|}
		\hline
		\rowcolor[HTML]{A5FFC4} 
		\hline
		Día & Cielo & Temperatura & Humedad & Viento & \cellcolor[HTML]{FF8787}Jugar \\ \hline
		\rowcolor[HTML]{34CDF9} 
		\hline
		{\color[HTML]{333333} D1} & {\color[HTML]{333333} Soleado} & {\color[HTML]{333333} Calor} & {\color[HTML]{333333} Alta} & {\color[HTML]{333333} Flojo} & \cellcolor[HTML]{FF8787}No \\
		\rowcolor[HTML]{007AFF} 
		D2 & Soleado & Calor & Alta & Fuerte & \cellcolor[HTML]{FC5D5D}No \\
		\rowcolor[HTML]{34CDF9} 
		D3 & Nublado & Calor & Alta & Flojo & \cellcolor[HTML]{FF8787}Si \\
		\rowcolor[HTML]{007AFF} 
		D4 & Lluvia & Templado & Alta & Flojo & \cellcolor[HTML]{FC5D5D}Si \\
		\rowcolor[HTML]{34CDF9} 
		D5 & Lluvia & Frío & Normal & Flojo & \cellcolor[HTML]{FF8787}Si \\
		\rowcolor[HTML]{007AFF} 
		D6 & Lluvia & Ario & Normal & Fuerte & \cellcolor[HTML]{FC5D5D}No \\
		\rowcolor[HTML]{34CDF9} 
		D7 & Nublado & Ario & Normal & Fuerte & \cellcolor[HTML]{FF8787}Si \\
		\rowcolor[HTML]{007AFF} 
		D8 & Soleado & Templado & Alta & Flojo & \cellcolor[HTML]{FC5D5D}No \\
		\rowcolor[HTML]{34CDF9} 
		D9 & Soleado & Ario & Normal & Flojo & \cellcolor[HTML]{FF8787}Si \\
		\rowcolor[HTML]{007AFF} 
		D10 & Lluvia & Templado & Normal & Flojo & \cellcolor[HTML]{FC5D5D}Si \\
		\rowcolor[HTML]{34CDF9} 
		D11 & Soleado & Templado & Normal & Fuerte & \cellcolor[HTML]{FF8787}Si \\
		\rowcolor[HTML]{007AFF} 
		D12 & Nublado & Templado & Alta & Fuerte & \cellcolor[HTML]{FC5D5D}Si \\
		\rowcolor[HTML]{34CDF9} 
		D13 & Nublado & Calor & Normal & Flojo & \cellcolor[HTML]{FF8787}Si \\
		\rowcolor[HTML]{007AFF} 
		D14 & Lluvia & Templado & Alta & Fuerte & \cellcolor[HTML]{FC5D5D}No \\ \hline
	\end{tabular}
\end{center}
En el ejemplo del tenis tenemos 9 objetos clasificados como $\oplus$ y 5 como $\ominus$, con lo que \[ H([9\oplus,5\ominus])=-0.642\cdot\log_20.642-0.58\cdot\log_20.358=0.94 \]
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot003"}
\end{center}
Por lo tanto, el atributo que ofrece una mayor ganancia de información es el atributo \textbf{Cielo}.

Utilizando \textbf{Cielo} como nodo raíz el árbol inicial quedaría:
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot004"}
\end{center}
\begin{minipage}{0.45 \textwidth}
	Ahora habría que repetir el proceso con los nodos correspondientes a los valores \textbf{soleado} y \textbf{lluvia} (el nodo \textbf{nublado} sólo contiene una clase).
\end{minipage}\qquad\begin{minipage}{0.45\textwidth}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot005"}
\end{center}
\end{minipage}

\begin{itemize}
	\item Para el nodo Cielo = Soleado:
	\begin{itemize}
		\item $\mathcal{X}_{\mathrm{soleado}}=\{D_1,D_2,D_8,D_9,D_11\}$ con $H(\mathcal{X}_{\mathrm{soelado}})=0.971$
		\item $\mathrm{Ganancia}(\mathcal{X}_{\mathrm{soleado}},\mathrm{Temperatura})=0.971-\dfrac{2}{5}\cdot0-\dfrac{2}{5}\cdot1-\dfrac{1}{5}\cdot0=0.570$
		\item $\mathrm{Ganancia}(\mathcal{X}_{\mathrm{soelado}},\mathrm{Humedad})=0.971-\dfrac{3}{5}\cdot0-\dfrac{2}{5}\cdot0=0.971$
		\item $\mathrm{Ganancia}(\mathcal{X}_{\mathrm{soleado}},\mathrm{Viento})=0.971-\dfrac{2}{5}\cdot1-\dfrac{3}{5}\cdot0.918=0.019$
	\end{itemize}
	\item Para el nodo Cielo = Lluvia:
	\begin{itemize}
		\item $\mathcal{X}_{\mathrm{lluvia}}=\{D_4,D_5,D_6,D_{10},D_{14}\}$ con $H(\mathcal{X}_{\mathrm{lluvia}})=0.971$
		\item $\mathrm{Ganancia}(\mathcal{X}_{\mathrm{lluvia}},\mathrm{Temperatura})=0.971-\dfrac{3}{5}\cdot0.918-\dfrac{2}{5}\cdot1-\dfrac{1}{5}\cdot0=0.820$
		\item $\mathrm{Ganancia}(\mathcal{X}_{\mathrm{lluvia}},\mathrm{Humedad})=0.971-\dfrac{2}{5}\cdot1-\dfrac{3}{5}\cdot0.918=0.820$
		\item $\mathrm{Ganancia}(\mathcal{X}_{\mathrm{lluvia}},\mathrm{Viento})=0.971-\dfrac{3}{5}\cdot0-\dfrac{2}{5}\cdot0=0.971$
	\end{itemize}
\end{itemize}

Por lo tanto, el árbol resultante sería
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot006"}
\end{center}
Todos los nodos hoja tienen una entropía nula (solo instancias de una clase).
\subsubsubsection{Error global}
Es la probabilidad de error, es decir, suma ponderada de los errores de todas las hojas del árbol. \[ E=\sum_{i=1}^{n_h}w_ie_i \]donde
\begin{itemize}
	\item $n_h$ es el numero de hojas del árbol.
	\item $w_i$ es el peso o probabilidad de la hoja $i$, es decir, la probabilidad de que una instancia sea clasificada por la partición representada por la rama que acaba en la hoja $i$.
	\item $e_i$ es el error correspondiente a la rama que acaba en la hoja $i$ (número de instancia erróneas que caen en la hoja $i$ entre el número de instancias que caen en la hoja $i$).
\end{itemize}
\subsubsubsection{Algoritmo}
El algoritmo básico de aprendizaje es el ID3 (Iterative Dichotomiser 3).

\begin{algorithm}
	\caption{árbol $\gets$ aprenderArbol(\textit{datos})}
	\begin{algorithmic}[1]
		\IF{todos los ejemplos en datos tienen la misma etiqueta}
		\RETURN un nodo hoja con dicha etiqueta
		\ELSE
		\STATE Sea $A$ el atributo que clasifica mejor a los objetos en datos
		\FORALL{posible valor $v$ de $A$}
		\STATE $data(v) \leftarrow$ todos los objetos con $A = v$
		\STATE Añadir nueva rama $\leftarrow$ aprenderArbol($data(v)$)
		\ENDFOR
		\RETURN árbol
		\ENDIF
	\end{algorithmic}
\end{algorithm}

\pagebreak

\textbf{ID3(Instancias, Etiquetas, Atributos)}

\begin{algorithmic}
	\REQUIRE \texttt{Instancias:} el conjunto de datos
	\REQUIRE \texttt{Etiquetas:} el conjunto de posibles clases.
	\REQUIRE \texttt{Atributos:} el conjunto de atributos en el conjunto \texttt{Instancias}.
	\IF{todas las instancias son positivas}
	\RETURN el nodo raíz con etiqueta $+$.
	\ELSIF{todas las instncias son negativas}
	\RETURN el nodo raíz con la etiqueta $-$.
	\ELSIF{\texttt{Atributos}=$\varnothing$}
	\RETURN el nodo raíz con el valor de \texttt{Etiquetas} más probable en \texttt{Instancias}.
	\ENDIF
	\STATE Sea $A$ el atributo que clasifica mejor las instancias en \textit{datos}.
	\STATE Crear un árbol con un nodo etiquetado con $A$
	\FORALL{posible valor $v_i$ del atributo $A$}
	\STATE añadir un arco bajo la raíz con la comprobación $A=v_i$.
	\STATE sea \texttt{Instancias} $v_i$ el subconjunto de \texttt{Instancias} con $A=v_i$.
	\IF{\texttt{Instancias}$_{v_i}=\varnothing$}
	\STATE añadir un nodo hoja al arco añadido con el valor de \texttt{Etiquetas} más probable en \texttt{Ejemplos}.
	\ELSE 
	\STATE añadir al nuevo árbol el subárbol generado por \textbf{ID3} (\texttt{Instancias}$_{v_i}$,\texttt{Etiquetas}, \texttt{Atributos}$-\{A\}$).
	\ENDIF
	\ENDFOR
	\RETURN nodo raíz
\end{algorithmic}
\subsubsection{Sobre-ajuste}
\subsubsubsection{Espacio de hipótesis y sobre-ajuste}
El \textbf{espacio de hipótesis} $H$ (no confundir con la entropía) en árboles de decisión
abarca todas las posibles combinaciones de atributos y valores que pueden formar
árboles de decisión, y nuestra tarea es encontrar la hipótesis más adecuada para
clasificar correctamente las instancias de entrada.

En general, se prefieren hipótesis cortas para evitar el sobre-ajuste (\textbf{overfitting}).

\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Sobre-ajuste
\end{itemize}
Dado un espacio de hipótesis $H$, se dice que una hipótesis particular $h\in H$ sobreajusta los datos de entrenamiento si existe un hipótesis alternativa $h'\in H$, tal que $h$ presenta un error menor que $h'$ sobre los ejemplos de entrenamiento, pero $h'$ presenta un error menor que $h$ sobre el conjunto total de observaciones.
\subsubsubsection{Proceso de poda}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}¿Cómo podemos evitar el sobre-aprendizaje o sobre-ajuste?
\end{itemize}
\textbf{Poda:} Eliminar condiciones de las ramas del árbol encontrar más pequeños. Existe dos tipos de poda:\begin{itemize}
	\item \textbf{Prepoda:} El proceso se realiza durante la construcción del árbol, estableciendo un criterio de parada.
	\begin{itemize}
		\item El número de instancias por nodo.
		\item El error esperado.
		\item MDL (Minimun Description Lenght).
	\end{itemize}
	\item \textbf{Postpoda:} El proceso se realiza después de la construcción del árbol
	\begin{itemize}
		\item Consiste en ir eliminando nodos de abajo a arriba mientras se vaya cumpliendo un criterio determinado.
	\end{itemize}
\end{itemize}
Se pueden combinar ambas aproximaciones.
\subsubsubsection{Otras medidas}
\textbf{Medidas alternativas:} En algunos casos se suele utilizar otras medidas como:
\begin{itemize}
	\item El \textit{Ratio} de la ganancia de información, para evitar el hecho de que se favorece la selección de los atributos con más valores.
	\item MSE para regresión
	\item Índice Gini empleado por el algoritmo CART
	\item DKM, basados en AUC, MDL
\end{itemize}
\subsubsection{Algoritmo CART y Otros}
\subsubsubsection{CART}
\textbf{Cart} (\textbf{C}lasificación \textbf{A}nd \textbf{R}egression \textbf{T}rees). Similar al ID3 pero:
\begin{itemize}
	\item Permite que la variable que define la clase sea continua y no construye un conjunto de reglas.
	\item Utiliza el índice de Gini en vez de la ganancia de información para seleccionar el mejor atributo (la mejor partición).
	\item Utiliza también el esquema de partición recursiva utilizando una estrategia voraz.
	\item También permite resolver problemas de regresión.
\end{itemize}
Se elige la partición que produce el menor valor de la función de coste.
\begin{itemize}
	\item Para regresión: RSME.
	\item Para clasificación: GINI \[ \mathrm{Gini}(p)=\sum_{i=1}^{n}p_i(1-p_i) \] con $p_i$ las proposiciones de instancias de la clase $i$ en la partición.
\end{itemize}
Prepoda: Utiliza como criterio de parada el número mínimo de instancias asignadas al nodo.

Postpoda: Utiliza un criterio que controla la importancia relativa del error frente la complejidad (tamaño del árbol).
\subsubsubsection{C4.5, C5.0}
\textbf{C4.5:} Permite, a diferencia que ID3, que las características puedan ser continuas, definiendo de forma dinámica un atributo discreto particionando los atributos continuos en un conjunto discreto de intervalos.
\begin{itemize}
	\item C4.5 transforma los árboles obtenidos en un conjunto de reglas del tipo \textit{if-then}. La precisión de cada regla es evaluada de forma independiente para determinar el orden en el que deben ser aplicadas.
	\item Un proceso de poda elimina antecedentes de las reglas si con esto se mejora la precisión de la misma.
	\item C5.0, utiliza menos memoria y obtiene un conjunto de reglas menor y más preciso.
	\item J48 es la implementación en código abierto de C4.5
\end{itemize}
\subsubsection{Random Forests}
Random Forest es una técnica que construye un gran número de árboles de decisión no correlacionados.

Se basa en la técnica de Agregación de Bootstrap (Bagging), técnica de agregación de clasificadores o regresores que:
\begin{itemize}
	\item Aumenta la precisión y estabilidad reduciendo los efectos del ruido.
	\item Reduce la varianza en las predicciones.
	\item Ayuda a evitar el sobre ajuste.
\end{itemize}
La predicción del modelo se elige analizando las predicciones de cada uno de los árboles considerados en el modelo.
\subsubsubsection{Algoritmo}
\begin{algorithm}
	\caption{RF $\gets$ RandomForest(\textit{datos})}
	\begin{algorithmic}[1]
		\FOR{$i\gets1\to$ \textit{n_arboles}}
		\STATE Extraer una muestra de tamaño \textit{size(data)} de \textit{datos} por bootstrapping
		\STATE Construir un árbol $T_i$ repitiendo recursivamente para cada nodo hoja
		\STATE \begin{enumerate}[leftmargin=1.5cm]
			\item Seleccionar aleatoriamente $m$ atributos.
			\item Seleccionar la mejor partición de las inducidas por los $m$ atributos.
			\item Dividir el nodo en dos nodos hijos.
			\item Si el tamaño de nodo alcanza $n_{\min}$ no continuar dividiendo.
		\end{enumerate}
		\ENDFOR
		\RETURN El conjunto de árboles $\{T_i\}_1^{\text{\textit{n_arboles}}}$.
	\end{algorithmic}
\end{algorithm}

Si tenemos $p$ atributos las recomendaciones para el valor de $m$ son:
\begin{itemize}
\item Clasificación: $\lfloor\sqrt{m}\rfloor$

\item Regresión: $\lfloor p/3\rfloor$
\item Siendo el valor mínimo 1.
\end{itemize}
Una vez se han construido los árboles para hacer predicciones:
\begin{itemize}
	\item Clasificación: La clase más votada por los árboles.
	\item Regresión: La media de todas las predicciones
\end{itemize}
\subsubsubsection{OBB e importancia de las variables}
\textbf{OBB (out of the bag) error estime:} Para cada muestra se predice el error utilizando sólo los árboles en los que no ha sido utilizada (no ha sido elegida en bootstrapping)
\begin{itemize}
	\item Los valores son parecidos a los que se obtienen mediante una validación cruzada de $N$-pliegues.
\end{itemize}
\textbf{Importancia de los atributos:}
\begin{itemize}
	\item En cada partición del árbol se registra la mejora del criterio de división.
	\item Este valor se considera la importancia del atributo.
	\item Para cada variable se agregan los valores generados en cada árbol.
\end{itemize}
\subsubsection{Conclusiones}
La utilización de árboles de decisión es muy popular en muchas disciplinas.

Experimentalmente han demostrado una buena capacidad de clasificación.

La clave reside en la función a optimizar a la hora de elegir el atributo para disgregar el árbol.

Los ensambles nos permiten mejorar los resultados combinando información.
\subsubsection{Bosques Aleatorios (\textit{"Random Forest"})}
\subsubsubsection{Motivación}
Surgen para resolver el \textbf{"overfitting"} producido por los árboles de decisión.

El gran problema de los árboles de decisión: sesgo bajo y varianza alta.

Cambios imperceptibles en la distribución de los datos modifican totalmente la partición generada por un Árbol de Decisión (varianza alta).

\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot007"}
	
	¿Cómo reducir la varianza?
\end{center}
\subsubsubsection{Solución}

\textbf{Muchos árboles} de decisión

\textbf{Aleatoriedad doble}
\begin{itemize}
	\item Conjuntos de entrenamiento 
\item Características 
\end{itemize}
\textbf{Agregación} de resultados 
\begin{itemize}
	\item Árboles malos, poco impacto en la predicción
\end{itemize}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot008"}
	
	\includegraphics{"Temas/Tema 2/screenshot009"}
\end{center}
\subsubsubsection{Número de árboles}

\begin{minipage}{0.5\textwidth}
	Lo primero es fijar el \textbf{número de árboles}
	\begin{itemize}
		\item Normalmente es elevado, cientos.
	\item En este ejemplo, \textbf{5}
	\end{itemize}
	
	\hspace{2cm}
	
	Ruido en los datos afectará a algunos árboles
\end{minipage}\quad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot010"}
\end{center}
\end{minipage}
\subsubsubsection{Generación conjuntos de entrenamiento}
\begin{minipage}{0.5\textwidth}
\begin{center}
Una vez fijado el \textbf{número de árboles}, se construyen los conjuntos de entrenamiento.
	
	\begin{tikzpicture}
		\draw[double -latex=4pt colored by lightblue and lightblue] (0,0) -- (0,-1);
	\end{tikzpicture}
	
	{\Large \lb{\textbf{BOOSTRAPPING}}}
	
	Muestreo con reemplazo
\end{center}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Obtención de muestras \textbf{“Bootstrap”}
\end{itemize}

Supongamos un conjunto de datos $X$ (población) de tamaño $N$.
Se construyen muestras bootstrap de la población a partir de los datos:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item Elegir el tamaño de la muestra $N'\le N$.
	\item For $i=1:N'$
	\begin{enumerate}[label=\color{lightblue}\arabic*)]
		\item Elegir aleatoriamente una observación de $X$ con \textit{reemplazo}
		\item Añadirlo a la muestra
	\end{enumerate}
\end{enumerate}
En machine learning, $N'=N$.
\end{minipage}\qquad\begin{minipage}{0.45\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot011"}
\end{center}
\end{minipage}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot012"}
\end{center}

\subsubsubsection{Entrenamiento de los árboles}
Se introduce una \lb{segunda aleatoriedad} en la selección de las características que se emplean para realizar las particiones.
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot013"}
\end{center}

\begin{minipage}{0.5\textwidth}
	De la misma forma, se van entrenando (y construyendo) los demás árboles
\end{minipage}\quad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot014"}
\end{center}

\end{minipage}
\subsubsubsection{Modo operación}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot015"}
\end{center}
\subsubsubsection{Bagging y Conjunto de Validación}

\textbf{Bagging:} es la la combinación del bootstrapping y la agregación, que es precisamente el nombre genérico con que se conoce a los algoritmos como los Bosques Aleatorios y otros similares.

\lb{\textbf{Conjunto de validación o "out-of-bag":}} se emplea para selección de hiperparámetros
\begin{center}
	\begin{tabular}{rl}
		$\dfrac{1}{N}$ & $\to$ probabilidad de seleccionar una instancia\\
		$1-\dfrac{1}{N}$ & $\to$ probabilidad de que no se seleccione una instancia\\
		$\left(1-\dfrac{1}{N}\right)^N\approx e^{-1}=0.368$ & $\to$ probabilidad que una instancia no se seleccione tras $N$ veces
	\end{tabular}
	
	\begin{tabular}{ll}
		Por tanto, aproximadamente: & conjunto de \textbf{validación} ("out-of-bag") $\to 36.8\%$\\
		&conjunto de \textbf{entrenamiento} $\to63.2\%$
	\end{tabular}
\end{center}
\subsubsubsection{Selección del número de árboles}
\begin{minipage}{0.4\textwidth}
Se fija el número de características y el criterio de parada, y se entrenan múltiples bosques cambiando progresivamente el número de árboles que lo conforman.

Se selecciona el que menor error de validación proporciona.

Usualmente, en aplicaciones prácticas, se usan entre 100 y 200 árboles
\end{minipage}\qquad\begin{minipage}{0.55\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot016"}
\end{center}

\end{minipage}
\subsubsubsection{Seleccionar del número de características}
\begin{minipage}{0.4\textwidth}
	Con el número de árboles ya
	determinado, fijamos el
	criterio de parada y repetimos
	el procedimiento cambiando
	el número de características, y
	escogemos aquel que genere
	el menor error.
\end{minipage}\qquad\begin{minipage}{0.55\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot017"}
\end{center}

\end{minipage}
\subsubsubsection{Selección del criterio de parada}
\begin{minipage}{0.4\textwidth}
Finalmente, con el número de árboles
y de características fijo, variamos el
criterio de parada y escogemos el que
arroje el menor error. El criterio de
parada puede ser, por ejemplo, el
mínimo número de datos de una hoja
(algún método de prepoda).
\end{minipage}\qquad\begin{minipage}{0.55\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot018"}
\end{center}

\end{minipage}
\subsubsubsection{Relevancia de las características}
\begin{minipage}{0.5\textwidth}
	Los valores de los índice Gini (para el caso de la clasificación)
o el error cuadrático medio (para el caso de la regresión)
aportan una medida de la bondad de la partición hecha en
cada nodo. Con estos valores, se puede establecer un \textbf{orden de importancia} de las características. 
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot019"}
\end{center}
\end{minipage}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot020"}
\end{center}
\subsection{Perceptrón monocapa y multicapa (MLP)}
\subsubsection{Teoría de la decisión}
Tres escenarios para resolver un problema de decisión
\begin{itemize}
	\item \textbf{Decisión analítica.} Se fundamentan en la teoría bayesiana para inferir las probabilidades a posteriori de las las clases
	\item \textbf{Modelos discriminatorios.} Infieren directamente las probabilidades a posteriori.
	\item \textbf{Funciones discriminantes.} Encuentran una función $f(x)$ que mapea directamente una entrada $x$ a una determinada clase.
\end{itemize}
\subsubsubsection{Decisión analítica}
Se fundamentan en la inferencia de $p(\mathrm{x}|C_k)$ y $p(C_k)$ (o, equivalente $p(\mathbf{x},C_k)$).

\begin{minipage}{0.5\textwidth}
	Procedimiento:
	\begin{enumerate}[label=\color{lightblue}\arabic*)]
		\item A partir de los datos, se infieren verosimilitudes y probabilidades a priori: $p(\mathbf{x}|C_k)$ y $p(C_k)$.
		\item Se aplica el Teorema de Bayes para calcular las probabilidades a posteriori \[ p(C_k|\mathrm{x})=\dfrac{p(\mathrm{x}|C_k)p(C_k)}{p(\mathrm{x})}=\dfrac{p(\mathrm{x}|C_k)p(C_k)}{\displaystyle\sum_kp(\mathrm{x}|C_k)p(C_k)} \]
		\item Se emplea la teoría de la decisión para determinar la clase de una entrada $x$
	\end{enumerate}
	Los métodos que explícita o implícitamente modelan las distribuciones de las entradas así como las salidas, $p(\mathrm{x},C_k)$, se llaman \textbf{modelos generativos} pues mediante \textit{muestreo} se pueden generar datos sintéticos.
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot021"}
\end{center}
\end{minipage}
\subsubsubsection{Modelos discriminatorios}
Producen directamente una estima de las probabilidades a posteriori de las clases para una entrada dada.
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item El procedimiento ahora consiste en, con los datos disponibles, diseñar y entrenar un modelo que minimice una determinada función de error entre las entradas y sus salidas deseadas
	\item Las salidas del modelos representan, mediante algún código, las probabilidades a posteriori de las clases para la entrada dada.
	\item El ejemplo más claro de modelos discriminativos son las \textbf{redes neuronales progresivas} como, por ejemplo, los MLPs entrenados para minimizar la entropía cruzada con unidades de salida tipo softmax
\end{enumerate}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot022"}
\end{center}
\subsubsubsection{Funciones discriminantes}
Encuentran una función $f(x)$, llamada \textit{función discriminante}, que directamente mapea cada entrada $x$ a la etiqueta de una clase.

\begin{minipage}{0.5\textwidth}
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Ejemplo: problema de dos clases
	\end{itemize}
	\[ \begin{array}{ll}
		f(x)=1, & \text{si }y(x)=\mathrm{w}^\intercal\mathrm{x}+w_0>0\\
		f(x)=0, & \text{si }y(x)=\mathrm{w}^\intercal\mathrm{x}+w_0<0
	\end{array} \]
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot023"}
\end{center}

\end{minipage}
\subsubsection{Fundamento biológico}
Membrana permeable para ciertas sustancias iónicas. Se crea una diferencia de potencial.

\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot024"}
\end{center}
Tras la activación sináptica, se propagan diferencias de potencial.entre el soma y el entorno.
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot025"}
\end{center}
\subsubsection{Perceptrón monocapa}
\subsubsubsection{(Widrow): filtro transversal (adaptativo) + umbral duro}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot026"}
\end{center}
\begin{minipage}{0.5\textwidth}
	División según hiperplano: \[ z=\mathbf{w^\intercal x}+b=0 \]donde la \lb{Función discriminante} (lineal) es \[ z=\mathbf{w^\intercal x}+b \]
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot027"}
\end{center}
\end{minipage}
\subsubsubsection{(Rosenblatt): dados $N$ pares entrada-salida: $\{\mathbf{x}_k,\mathbf{d}_k\}_1^N$}
Muestra a muestra: \[ \mathbf{w}(k+1)=\mathrm{w}(k)+\dfrac{\alpha}{2}(d_k-o_k)x_k,\quad(\alpha>0) \]
Bloque: \[ \mathrm{w}(m+1)=\mathrm{w}(m)=\dfrac{\alpha}{2}\sum_{k=1}^{N}(d_k-o_k(m))\mathrm{x}_k,\quad(\alpha>0) \]
Es aprendizaje:
\begin{itemize}
	\item \textbf{Supervisado:} pares de entrenamiento dados.
	\item \textbf{No lineal:} se emplea la salida $(o)$ en el error.
	\item \textbf{Hebbiano:} refuerza las intervenciones correctas ($\pm\alpha\mathrm{x}_k$ en $\mathbf{w}$ según el signo del error).
\end{itemize}
Solo converge si hay separabilidad lineal

Ejemplo gráficos: $D=2,b=0$ y una muestra $\{\mathbf{x}_k,\lbb{d_k=-1}{\text{salida deseada}}\}$
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot028.drawio"}
\end{center}

Las rectas narajas y amarillan se sacan sumando a $\mathrm{w}(k+1)$ y $\mathrm{w}(\alpha)$ respectivamente el $-\alpha x$.

Es decir, aplicando la fórmula de muestra a muestra.
\subsubsection{Limitaciones del Perceptron Monocapa}
Minskey y Papert: es un "solo" \textbf{discriminante lineal}, capaz de resolver problemas de juguete (por ejemplo OREX).
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot029"}
\end{center}
Si se dispone en capas: gradiente imposible por el umbral duro.

La ya comentada falta de convergencia.
\subsubsection{El algoritmo LMS}
(Widrow y Hopf): proponen emplear $z$ en el error en lugar de $o$ y minimizar el coste cuadrático (Least Mean Square) por gradiente: \[ \min_{\mathrm{x}}C(\mathrm{w})=\min_{\mathrm{w}}\dfrac{1}{2}\sum_{k=1}^{N}(d_k-z_k)^2 \]
\begin{itemize}
	\item Muestra a muestra: \[ \mathrm{w}(k+1)=\mathrm{w}(k)+\alpha(d_k-z_k)\mathrm{x}_k,\quad(\alpha>0) \] (más rápido, pero más ruidoso).
	\item Bloque: \[ \mathrm{w}(m+1)=\mathrm{w}(m)+\alpha\sum_{k=1}^{N}(d_k-z_k(m))\mathrm{x}_k,\quad(\alpha>0) \]
\end{itemize}

Para entradas independientes de valores independientes de media cero y autocorrelación $R_{\mathrm{xx}}$, converge a la solución MMSE (solución de Wiener-Hopf) \[ \mathrm{x}_{opt}=R_{xx}^{-1}\mathtt{E}\{\dx\}\text{ si }\alpha<\dfrac{2}{\lambda_{\max}} \]
\subsubsection{Características del LMS}
El LMS es muy robusto: converge en muchos casos

Aprendizaje:
\begin{itemize}
	\item \textbf{Supervisado}
	\item \textbf{Lineal:} se emplea la salida $(\mathbf{z})$ en el error
	\item Por prestaciones: según coste cuadrático
\end{itemize}
Produce resultados razonables (los mejores mediante una frontera lineal) aunque el problema no se separable linealmente.

\textbf{NLMS ó Delta-LMS:} $\alpha\longrightarrow\dfrac{\alpha}{\|\mathbf{x}\|^2}$, independiza de la energía de las muestras.
\subsubsection{Error cuadrático (SSE) para clasificación}
\subsubsubsection{Problema con los "OUTLIERS"}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot030"}
\end{center}
La figura muestra el resultado obtenido por el LMS (en morado) para dos situaciones distintas.
\subsubsection{Activación blanda}
El \textbf{umbral duro} de Widrow, se cambia por una \textbf{aproximación derivable}

Resulta adecuada la forma sigmoidal o sigmoide:

\begin{minipage}{0.5\textwidth}
	\begin{itemize}
		\item \textbf{Función logística:} \[ o=f(z)=\dfrac{1}{1+e^{-gz}} \]
		\item \textbf{Tangente hiperbólica} ($\tanh$): \[ o=f(z)=\dfrac{1-e^{-gz}}{1+e^{-gz}}=\tanh\left(\dfrac{gz}{2}\right) \]
	\end{itemize}
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics[scale=0.8]{"Temas/Tema 2/screenshot031"}
\end{center}
\end{minipage}

$g$: saturación (determina la pendiente de la sigmoide en $z=0$).

En principio, asumible por los pesos.
\subsubsection{LMS con activación blanda}
Ahora: $z\to o$: \[ \min_{\mathrm{w}}C(\mathrm{w})=\min_{\mathrm{w}}\dfrac{1}{2}\sum_{k=1}^{N}(d_k-o_k)^2 \]
\begin{itemize}
	\item Muestra a muestra: \[ \mathrm{w}(k+1)=\mathrm{w}(k)+\alpha(d_k-o_k)f'(z_k)\mathrm{x}_k,\quad(\alpha>0) \]
	\item Bloque: \[ \mathrm{w}(m+1)=\mathrm{w}(m)+\alpha\sum_{k=1}^{N}(d_k-o_k(m))f'(z_k(m))\mathrm{x}_k,\quad(\alpha>0) \]
\end{itemize}
Aparece un factor $f'(z_k)$ que se puede calcular a partir de la propia salida según:

\begin{minipage}{0.5\textwidth}
	\begin{itemize}
		\item Para función logística: $f'(z)=go(1-o)$
	
	\[ \begin{aligned}
		\text{Demo: }f'(z)&=\dfrac{ge^{-gz}}{(1+e^{-gz})^2}=g\dfrac{1}{(1+e^{-gz})}\dfrac{e^{-gz}}{(1+e^{-gz})}\\
		&=g\dfrac{1}{(1+e^{-gz})}\left(1-\dfrac{1}{(1+e^{-gz})}\right)\\
		&=go(1-o)
	\end{aligned} \]
	\item Para $\tanh:\:f'(z)=\dfrac{1}{2}g(1-o^2)$
	\end{itemize}
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot031"}
\end{center}
\end{minipage}
\subsubsection{Arquitectura del Perceptron Multicapa (MLP)}
\subsubsubsection{Redes Multicapa}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot032"}
\end{center}
\subsubsection{Propiedades de las MLP}
Son:
\begin{itemize}
	\item potentes
	\item versátiles
	\item distribuidas: robustas
	\item paralelas: rápidas (entrenadas)
\end{itemize}
pero:
\begin{itemize}
	\item de entrenamiento difícil y lento
	\item de difícil análisis
\end{itemize}
\subsubsection{Capacidades del MLP}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot033"}
\end{center}
Para MLPs con dos capas ($L = 2$), se han probado los siguientes teoremas:

Cybenko: basta con una capa oculta de unidades sigmoidales (en número indefinido) para \begin{center}
	$R^{N_0}\longrightarrow(1,-1)^{N_L}$\quad(Clasificación)
\end{center} donde $N_0$ y $N_L$ son la dimensión de la entrada y la salida, respectivamente.

Kolmogorov (adaptado por Hetch-Nielsen): basta con una capa oculta de $2N_0 + 1$ unidades de activaciones adecuadas para \begin{center}
	$(1,-1)^{N_0}\longrightarrow R^{N_L}\quad$(Regresión)
\end{center}
\subsubsection{Notación utilizada MLP}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot034"}
\end{center}
\subsubsection{Algoritmo de retropropagación (BackPropagation (BP))}
Básicamente consiste en dos fases:
\begin{itemize}
	\item \textbf{Hacia adelante:} primero se aplica una instancia a las entradas de la red y su efecto se propaga a través de la misma, capa a capa, hasta la salida.
	\item \textbf{Hacia atrás (BP):} Después, se re-calculan los pesos mediante el gradiente del error. En concreto, se calcula el valor de la función de error, que compara la respuesta actual de la red y la respuesta deseada, y este error se propaga hacia atrás.
\end{itemize}
El objetivo es minimizar \[ \min_{\mathrm{w}}C(\mathrm{w})=\min_{\mathrm{w}}\dfrac{1}{2}\sum_{m=1}^{N}(\mathbf{d}_m-\mathbf{o}_m)^2 \] donde $\mathbf{w}$ representa todos los pesos de la red y $m$ la muestra o instancia.

Algoritmo de gradiente \[ w_{ji}^{(l)}(k+1)=e_{ji}^{(l)}(k)-\eta\dfrac{\partial C}{\partial w_{ji}^{(l)}}(k) \]
Para cada iteración $k$, se tienen las ecuaciones siguientes $\dfrac{\partial C}{\partial w_{ji}^{(l)}}=\dfrac{\partial C}{\partial o_j^{(l)}}\dfrac{\partial o_j^{(l)}}{\partial w_{ji}^{(l)}}=\dfrac{\partial C}{\partial o_j^{(l)}}f_j^{\prime(l)}o_i^{(l-1)}=\Delta_j^{(l)}o_i^{(l-1)}$ ya que \linebreak $o_j^{(l)}=f\left(\sum_{k=1}^{N_{l-1}}e_{jk}^{(l)}o_k^{(l-1)}\right)$
\begin{equation}
	\begin{split}
	\Delta_j^{(l)}&=\left(\sum_{n_{l+1}}^{N_{l+1}}\dfrac{\partial C}{\partial o_{n_{l+1}}^{(l+1)}}\dfrac{o_{n_{l+1}}^{(l+1)}}{\partial o_j^{(l)}}\right)f_j^{\prime(l)}=\left(\sum_{n_{l+1}}^{N_{l+1}}\dfrac{\partial C}{\partial o_{n_{l+1}}^{(l+1)}}f_{n_{l+1}}^{\prime(l+1)}w_{n_{l+1}j}^{(l+1)}\right)f_j^{\prime(l)}\\
	&=\left(\sum_{n_{l+1}}^{N_{l+1}}\Delta_{n_{l+1}}^{(l+1)}w_{n_{l+1}j}^{(l+1)}\right)f_j^{\prime(l)}
\end{split}
\end{equation}
BackPropagation (también Regla Delta Generalizada)) \[ w_{ji}^{(l)}(k+1)=w_{ji}^{(l)}-\eta\Delta_j^{(l)}(k)o_i^{(l-1)}(k) \]
Se procede: \[ l=L\longrightarrow L-1,L-2,\dots,1\quad\text{(¡retroprogramación!)} \] insertando gradiente y considerando que
\begin{itemize}
	\item Para $l=L$ se aplica \[ \Delta_j^L=\dfrac{\partial C}{\partial o_j^{(L)}}f_j^{\prime (L)} \]
	\item Y para $l<L$, se aplica (1)
\end{itemize}
Recuérdese que, para sigmoides, las derivadas dependen solo de los salidas: $f'(z)=go(1-o)$ para función logística y $f'(z)=\dfrac{1}{2}g(1-o^2)$ para $\tanh$.
\subsubsection{Modos de entrenamiento}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}¿Cada cuanto se actualizan los pesos?
\end{itemize}
\begin{itemize}
	\item \lb{Batch:} depués de cada época del conjunto de entrenamiento.
	\item \lb{Mini-Batch:} despuñes de una muestra (conjunto de patrones) del conjunto de entrenamiento.
	\item \lb{Online:} depués de cada ejemplo.
\end{itemize}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 1/Screenshot030"}
\end{center}
\subsubsection{Sobre las muestras}
Conjunto de muestras de entrenamiento \lb{representativo}

Conviene \lb{preprocesar} las muestras para eliminar información que de seguro se sabe irrelevante

Conviene \lb{normalizar} (características con media nula y varianza unidad)

Pueden ser útiles \lb{códigos sencillos} (en general, las NNs muestran sensibilidad al formato de presentación). Ejemplos: 1:N, termómetro, etc.

Para el mini-batch y online, conviene \lb{Aleatorizar} el orden de presentación de las muestras y \lb{ciclar} las series de entrenamiento.

\begin{minipage}{0.5\textwidth}
\lb{Decorrelar las entradas.} Un método para decorrelar las variables de entrada: \textbf{Análisis de Componentes Principales (PCA)}
\begin{itemize}
	\item Permite reducir la dimensionalidad de los datos (eliminando componentes con menores autovalores).
	\item Permite convertir una superficie de error elíptica en una circular, en la que el gradiente apunta directamente al mínimo (dividiendo los componentes principales por las raíces cuadradas de sus respectivos autovalores)
\end{itemize}
\end{minipage}\qquad\begin{minipage}{0.45\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot036"}
\end{center}

\end{minipage}

Si no se aplican bien estos principios pueden aparecer \lb{problemas de convergencia}.

\subsubsection{Sobre el dimensionado}

No hay reglas fijas, sino empíricas.

En un principio, un número aconsejable de capas ocultas es:
\begin{itemize}
	\item 1 o 2 para clasificación
	\item 2 para correspondencia
\end{itemize}
(No contradice los teoremas de Cybenko y Kolmogorov: 2 pueden dar lugar a una arquitectura más eficiente).

Hoy día, el \textbf{Deep Learning} permite el empleo de muchas más capas ocultas.
\subsubsection{Sobre el algoritmo de entrenamiento}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}\textbf{Inicialización} de parámetros y variables
\end{itemize}
Para entradas normalizadas:
\begin{itemize}
	\item $\eta$ entre $0.001$ y $1$.
	\item Pesos iniciales aleatorios con distribución $U[-0.5,0.5]$
\end{itemize}

\begin{minipage}{0.5\textwidth}
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Problemas de convergencia: \textbf{Mínimos locales}
	\end{itemize}
	Para evitar ser atrapados por ellos:
	\begin{itemize}
		\item Método simple: \textbf{varias inicializaciones} de los pesos, quedándonos con el mejor resultado.
		\item \textbf{Métodos naturales:}
		\begin{itemize}
			\item \textbf{Neo-Darwinianos:} Genéticos, Evolutivos, etc.
			\item \textbf{Temple simulado}
			\item Perforación de \textbf{túneles}
		\end{itemize}
	\end{itemize}
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot037"}
\end{center}
\end{minipage}

\begin{minipage}{0.4\textwidth}
	\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
		\item \color{lightblue}Problemas de convergencia: \textbf{Minimos locales}
	\end{itemize}
	\textbf{Aprendizaje incremental} mediante selección de muestras. Garantizan o facilitan llegar al mínimo local (con más carga computacional).
\end{minipage}\qquad\begin{minipage}{0.6\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot038"}
\end{center}
\end{minipage}
En la convergencia al mínimo, lo que se busca es:
\begin{itemize}
	\item Movernos más rápido en direcciones con gradientes pequeños pero consistentes (que apuntan al mínimo).
	\item Movernos más despacio en direcciones con gradientes grandes pero inconsistentes.
\end{itemize}
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot039"}
\end{center}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Mejora de la \textbf{convergencia}
\end{itemize}
\textbf{Método del Momento} (modificación del gradiente): \[ w_{ji}^{(l)}(k+1)=w_{ji}^{(l)}(k)-\eta\dfrac{\partial C}{\partial w_{ji}^{(l)}}(k)+\mu\left(w_{ji}^{(l)}(k)-w_{ji}^{(l)}(k-1)\right) \]
\begin{itemize}
	\item Se amortiguan osciliaciones en direcciones de alta curvatura combinando gradientes de signo contrario.
	\item Se aumneta la velocidad en direcciones con un gradiente pequeño pero consistente.
\end{itemize}
El momento añade \lb{inercia} al movimiento; útil en llanuras

\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot040"}
\end{center}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot041"}
\end{center}
\textbf{Modificación de la tasa de aprendizaje:}
\begin{itemize}
	\item \textbf{Descenso de la tasa de aprendizaje} (Learning Rate Decay): Se comienza con una tasa de aprendizaje (parámetro de aprendizaje o \textbf{escalón}) alta y luego se disminuye gradualmente durante el entrenamiento. Esto permite un rápido avance al principio y una convergencia más suave hacia el mínimo global a medida que avanzas.
	\item \textbf{Tasa de aprendizaje adaptativa:} Utiliza algoritmos como Adam, RMSprop o Adagrad que ajsutan automáticamente la tasa de aprendizaje en función del historial de los errores. Estos algoritmos pueden converger más rápido en muchos casos. Idea básica:
	\begin{itemize}
		\item Si la convergencia al mínimo va bien (diminuye el error), se aumenta el escalón.
		\item Si la convergencia al mínimo va mal (aumenta el error), se vuelve atrás y se disminuye el escalón.
	\end{itemize}
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Soluciones generales ante la \textbf{lentitud}
\end{itemize}
\begin{itemize}
	\item Modificación o cambio de la \textbf{función de coste}.
	\item Gestión del \textbf{parámetro de aprendizaje}.
	\item Técnicas de \textbf{regularización}.
	\item Utilización de otros \textbf{algoritmos de búsqueda}.
	\item Métodos de \textbf{selección de muestras}.
\end{itemize}
\subsection{Redes de funciones de base radial}
Son redes progresivas, no lineales, multicapa y aproximadores universales.
\begin{center}
	\includegraphics{"Temas/Tema 2/screenshot042"}
\end{center}
\subsubsection{Aproximadores universales}
La salida de una RBF viene dada por: \[ y(\mathbf{x})=\sum_{i=0}^{m}w_i\varphi(\|\mathbf{x-c}_i\|) \] donde $m$ es el número de centroides, $w_0$ es el sego y $\varphi(\|\mathbf{x-c}_0\|)=1$.

Se busca que las salidas igual a las deseadas, es decir, $d=y(\mathbf{x})$. Así, \[ \begin{pmatrix}
	d_1\\
	d_2\\
	\vdots\\
	d_N
\end{pmatrix}=\begin{pmatrix}
1 &\varphi(\mathbf{x}_1,\mathbf{c}_1) & \varphi(\mathbf{x}_1,c_2) & \cdots & \varphi(\mathbf{x}_1,\mathbf{c}_m)\\
1 &\varphi(\mathbf{x}_2,\mathbf{c}_1) & \varphi(\mathbf{x}_2,c_2) & \cdots & \varphi(\mathbf{x}_2,\mathbf{c}_m)\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & \varphi(\mathbf{x}_N,\mathbf{c}_1) & \varphi(\mathbf{x}_N,c_2) & \cdots & \varphi(\mathbf{x}_N,\mathbf{c}_m)\\
\end{pmatrix}\begin{pmatrix}
w_0\\
w_1\\
w_2\\
\vdots\\
w_m
\end{pmatrix}\longrightarrow\mathbf{d=\Phi w} \]

En general, las RFB se definen como \textbf{funciones de una norma o distancia}, normalmente euclidiana. Es decir, \[ \varphi(\mathbf{x})=\varphi(r)=\varphi(\|\mathbf{x-c}\|) \] donde $\mathbf{c}$ es el centro de la RBF.
\subsubsection{Funciones radiles}
\begin{itemize}
	\item Gausiana regular (la más usada): \[\varphi(\mathbf{x,\,c}_i)=\exp\left(-\dfrac{1}{2\sigma_i^2}\|\mathbf{x-c}_i\|^2\right)  \]
	\item Gausiana: \[ \varphi(\mathbf{x,\,c}_i)=\exp\left(-\dfrac{1}{2}(\mathbf{x-c}_i)^\intercal\Sigma_i^{-1}(\mathbf{x-c}_i)\right) \]No muy usados por compensar el aumento de prestaciones con el computacional.
	\item Multicuadrática inversa \[ \varphi(\mathbf{x,\,c}_i)=\dfrac{c}{\sqrt{\|\mathbf{x-c}_i\|^2+c^2}},\quad c>0 \] donde $c$ es una constante (distinta que los centroides $\mathbf{c}_i$).
\end{itemize}
\begin{center} 
	\includegraphics{"Temas/Tema 2/screenshot044"}
\end{center}
\subsubsection{Propuestas de RBF}
\subsubsubsection{Primera propuesta: \textbf{Interpolación exacta}}
En la interpolación exacta, \textbf{cada muestra es un centroide}, es decir \[ \mathbf{c}_i=\mathbf{x}_i\longrightarrow y(\mathbf{x})=\sum_{i=0}^{N}w_i\varphi(\|\mathbf{x-x}_i\|) \] Entrenamiento: directo de forma analítica para conseguir $y(\mathbf{x}_i)=d_i$\[ \mathbf{d=\Phi w\longrightarrow w=\Phi^{-1}d} \]Caso particular: Para un problema de clasificación binaria, basta multiplicar las gausianas por la salida deseada ($+1,-1$) de la muestra correspondiente.

Normalmente, la \rc{interpolación perfecta es mala solución}. Ejemplo con diferentes anchuras
\begin{center} 
	\includegraphics[scale=0.6]{"Temas/Tema 2/screenshot043"}
\end{center}
\subsubsubsection{Segunda propuesta: \textbf{Interpolación no exacta}}
\[ y(\mathbf{x})=\sum_{i=0}^{m}w_i\varphi(\|\mathbf{x-c}_i\|) \] donde $m<N$ es un hiper-parámetro del modelo.
\subsubsection{Entrenamiento de las RBF}
\subsubsubsection{Método 1: Centroides fijados aleatoriamente}
Primero, se inicializan los centroides de las funciones de base radial de manera aleatoria.

Luego, se ajustan los parámetros de las funciones de base radial (como la varianza) y los pesos de la capa de salida utilizando algún algoritmo de entrenamiento, como el método de la pseudoinversa o el de mínimos cuadrados por gradiente.
\begin{enumerate}[label=\arabic*)]
	\item Se inicializan los centroides aleatoriamente (según la distribución de los datos).
	\item Se fija la varianza: \[ \boxed{\sigma=\dfrac{d_{\max}^2}{2m}},\qquad\text{donde }d_{\max}=\max_{j,i}\|\mathbf{x}_j-\mathbf{c}_i\|,\quad j=1,\dots,N \quad i=1,\dots,m \]
	\item Se calculan los pesos de la capa de salida para que se cumpla: $\mathbf{d=\Phi w}$ \[ \begin{pmatrix}
		d_1\\
		d_2\\
		\vdots\\
		d_N
	\end{pmatrix}=\begin{pmatrix}
		1 &\varphi(\mathbf{x}_1,\mathbf{c}_1) & \varphi(\mathbf{x}_1,c_2) & \cdots & \varphi(\mathbf{x}_1,\mathbf{c}_m)\\
		1 &\varphi(\mathbf{x}_2,\mathbf{c}_1) & \varphi(\mathbf{x}_2,c_2) & \cdots & \varphi(\mathbf{x}_2,\mathbf{c}_m)\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		1 & \varphi(\mathbf{x}_N,\mathbf{c}_1) & \varphi(\mathbf{x}_N,c_2) & \cdots & \varphi(\mathbf{x}_N,\mathbf{c}_m)\\
	\end{pmatrix}\begin{pmatrix}
		w_0\\
		w_1\\
		w_2\\
		\vdots\\
		w_m
	\end{pmatrix}\longrightarrow\mathbf{d=\Phi w} \]a
	\textbf{Solución:} Normalmente, la solución exacta no existe (sistema sobredeterminado). Sí existe solución óptima en términos de error cuadrático.
\end{enumerate}
\subsubsubsection{Cálculo de los pesos de la capa de salida}
\begin{minipage}{0.65\textwidth}
Mediante la \textbf{psudoinversa}. La solución óptima analítica que minimiza \[ E(\mathbf{w})=\dfrac{1}{2}\sum_{j=1}^{N}(y_i-d_j)^2=\dfrac{1}{2}\|\mathbf{y-d}\|^2 \]viene dada por $\mathbf{w^*=\Phi^+d}$, donde $\mathbf{Phi^+}$ es la pseudoinversa de Moore-Penrose dada por $\Phi^+=(\Phi^\intercal\Phi)^{-1}\Phi^\intercal$.
\end{minipage}\qquad\begin{minipage}{0.3\textwidth}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Tema 2/screenshot045"}
\end{center}
\end{minipage}

La salida producida por la solución óptima, $\mathbf{y=\Phi w^*}$, es la proyección de $\mathbf{d}$ sobre el subespacio $S$ generado por las columnas de $\mathbf{\Phi}$, minimizándose la distancia euclidea entre $\mathbf{d}$ e $\mathbf{y}$.

Mediante \textbf{gradiente}. Se minimiza el SSE mediante gradiente, por ejemplo, con el algoritmo LMS (ya visto).
\subsubsubsection{Método 2: Centroides fijados según cuantifiación vectorial}
En este entrenamiento, se inicializan los centroides de RBF mediante el empleo de algún método de \textbf{cuantificación vectorial (VQ)} y se fijan las varianza como se ha visto anteriormente.

Luego, se ajustan los pesos de la capa de salida utilizando algún algoritmo de entrenamiento, como el método de la pseudoinversa o el de mínimos cuadrados

Algunos ejemplos de método VQ son:
\begin{itemize}
\item Algoritmo FSCL ("Frequency Sensitive Competitive Learning")
\item Mapas autoorganizados
\item Clustering. Ej.: Algoritmo de K-medias.
\end{itemize}
\subsubsubsection{Método 3: Entrenamiento completo por gradiente supervisado}
Todas las variables de la red se entrenan por gradiente para minimizar la función de error. \[ \boxed{E=\dfrac{1}{2}\sum_{j=1}^{N}e_j^2}\quad\mathrm{donde}\quad e_j=d_j-\sum_{i=0}^{m}w_i\varphi(\|\mathbf{x}_j-\mathbf{c}_i\|) \]

\[ \begin{array}{rcl}
	\mathbf{c}_i(k+1)=\mathbf{c}_i(k)-\eta_1\dfrac{\partial E(k)}{\partial \mathbf{c}_i(k)} & ~~~~ &\dfrac{\partial E(k)}{\partial \mathbf{c}_i(k)}=2w_i(k)\sum_{j=1}^Ne_j(k)\varphi(\|\mathbf{x}_j-\mathbf{c}_i(k)) \\
	w_{i}(k+1)=w_{i}(k)-\eta_{2}\dfrac{ \partial E(k) }{ \partial w_{i}(k) }  & &\dfrac{ \partial E(k) }{ \partial w_{i}(k) }=\sum_{j=1}^{N}e_{j}(k)\varphi(\|\mathbf{x}_{j}-\mathbf{c}_{i}(k) \|)  \\
	\Sigma_{i}^{-1}(k+1)=\Sigma_{i}^{-1}(k)-\eta_{3}\dfrac{ \partial E(k) }{ \partial \Sigma_{i}^{-1}(k) }  & &\dfrac{ \partial E(k) }{ \partial \Sigma_{i}^{-1}(k) } =-w_{i}\sum_{j=1}^{N}e_{j}(k)\varphi'(\|\mathbf{x}_{j}-\mathbf{c}_{i}(k)\|)\mathbf{Q}_{ji}(k) \\
	& & \mathbf{Q}_{ji}(k)=(\mathbf{x}_{j}-\mathbf{c}_{i}(k))(\mathbf{x}_{j}-\mathbf{c}_{i}(k))^{\intercal}
\end{array}  \]