\section{Análisis Cluster}
\subsection{Introducción}
\subsubsection{Objetivo}
\lb{Objetivo:} cómo agrupar observaciones estableciendo grupos (o clusters) con las más similares.

\lb{Aprendizaje supervisado:} en la muestra \lb{se indica a qué grupo pertenece cada observación}.
\begin{itemize}
\item Regresión Logística.
\item Análisis Discriminante.
\end{itemize}
\lb{Aprendizaje no supervisado} (o automático): en este caso \lb{no disponemos de una muestra inicial donde se indique a qué grupo pertenece cada observación}. De hecho, en algunas ocasiones podemos decidir cuántos grupos queremos establecer.
\begin{itemize}
\item Análisis Cluster
\end{itemize}
\subsubsection{Contexto}
Dispondremos de \lb{una muestra} (o población) de $n$ individuos (objetos) en los que hemos medido $k$ variables numéricas $(X_1,\dots,X_k)$.

Sin embargo, en este caso, \lb{no dispondremos de una variable} $Y$ que nos diga a qué grupo (población) pertence cada observación.

Incluso, en algunos casos, \lb{no sabremos ni siquiera el número de grupos}.

De hecho, lo que haremos será determinar los valores de $Y$ que nos asigne los grupos que minimicen una \lb{función costo} adecuada.

Para ello tendremos que utilizar una función \lb{distancia} que nos mida cómo de similares son dos observaciones (individuos).

La \lb{elección de esta distancia} es muy importante y la solución final dependerá de la distancia elegida.

\subsection{Distancias entre individuos}
\subsubsection{La distancia Euclídea}
La distancia más popular es la \lb{distancia Euclídea}, definida como \[ d_E(\mathbf{x,c})=\sqrt{(\mathbf{x-c})'(\mathbf{x-c})}=\sqrt{\sum_{j=1}^{k}(x_j-c_j)^2} \]para todo $\mathbf{x,c}\in\R^k$ (vectores columna).

En nuestro contexto, habitualmente $\mathbf{x}=(x_1,\dots,x_k)'$ representará un \lb{individuo} y $\mathbf{c}=(c_1,\dots,c_k)'$ el \lb{centroide} de un grupo.

En \code{R} se puede computar como 
\begin{lstlisting}
dE <- function(x, y) sqrt(sum((x - y)*(x - y)))
\end{lstlisting}
Por ejemplo, para $x=(0,0)'$ e $y=(1,1)'$
\begin{lstlisting}
x <- c(0,0)
y <- c(1,1)
dE(x,y)
\end{lstlisting}
\begin{verbatim}
## [1] 1.414214
\end{verbatim}
\subsubsection{La distancia de Mahalanobis}
Otra opción es la \lb{distancia de Mahalanobis} que usa le métrica de los datos, definida como \[ d_M(\mathbf{x,c})=\sqrt{(\mathbf{x-c})'V^{-1}(\mathbf{x-c})}, \]donde $V=\cov(X_1,\dots,X_m)$.

El principal problema es que \lb{si hay grupos}, esta matriz puede ser \lb{distinta en cada grupo}.

Incluso, aunque supongamos que todos los grupos tienen la misma matriz de covarianzas, estos tendrán medias distintas y, como desconocemos los grupos, no podemos estimar estimar $V$ (como hacíamos en el Ánalisis Discriminante).

Una solución es suponer inicialmente que todos los individuos están en un mismo grupo (población) y calcular (estimar) la media y la covarianzas en ella.

En \code{R} se puede calcular la función \code{mahalanobis(x, y, V)}, que proporciona el cuadro de esta distancia, para \[ V=\begin{pmatrix}
1 & \tfrac{1}{2}\\
\tfrac{1}{2} & 1
\end{pmatrix}, \]$x=(0,0)'$ e $y=(1,1)'$.
\begin{lstlisting}
V <- matrix(c(1, 1/2,
              1/2, 2), nrow = 2, ncol = 2, byrow = TRUE)
x <- c(0,0)
y <- c(1,1)
mahalanobis(x, y, V)
\end{lstlisting}
\begin{verbatim}
## [1] 1.333333
\end{verbatim}
O bien, como 
\begin{lstlisting}
dM <- function(x, y, V) sqrt(sum(t(x - y ) %*% solve(V) %*% (x-y)))
dM(x, y, V)
\end{lstlisting}
\begin{verbatim}
## [1] 1.154701
\end{verbatim}
\begin{lstlisting}
## Si hacemos el cuadrado
dM(x, y, V)^2
\end{lstlisting}
\begin{verbatim}
## [1] 1.333333
\end{verbatim}
Obviamente, si $V=I$ (matriz identidad), se obtiene la \lb{distancia Euclídea} que, por lo tanto, representará a \lb{\vas independientes con varianza uno}.

En otros casos, la distancia de Mahalanobis tendrá en cuenta las varianzas de las variables y sus covarianzas (correlaciones o dependencia).

Las circunferencias (\lb{elipsoides}) obtenidas con $d_V(\mathbf{x},\mu,V)=\mathrm{cte}.$ coincidirán con los \lb{conjuntos de nivel de la distribución normal multivariante} $\mathcal{N}_k(\mu,V)$ cuya función de densidad es \[ f(x)=\dfrac{1}{\sqrt{(2\pi)^k|V|}}\exp\left(-\dfrac{1}{2}(\mathbf{x}-\mu)'V^{-1}(\mathbf{x}-\mu)\right). \]
\begin{itemize}
\item De esta forma, bajo este modelo y si conocemos $V$, el individuo con medidas $\mathbf{x}$ se asignará al grupo en donde sea más verosímil, es decir, donde$f_i(x)$ sea máxima, siendo $f_i$, la densidad $\mathcal{N}_k(\mu_i, V)$ (tal y como hacíamos en Análisis Discriminante).
\end{itemize}
Ahora el problema es que no sabemos cómo estimar $\mu_i$ y $V$ y tampoco sabemos si hay una matriz de covarianzas $V$ común.
\subsubsection{Otras distancias interesantes}
La \lb{distancia absoluta} (Manhattan, de ciudad o geométrica del taxista) \[ d_A(\mathbf{x,c})=\sum_{j=1}^{k}\left|x_j-c_j\right| \](que usa las cuadrículas como caminos).

La \lb{distancia} $L_s$ \[ d_s(\mathbf{x,c})=\left(\sum_{j=1}^{k}(x_j-c_j)^s\right) ^{\frac{1}{s}}\]para $s>0$.

La \lb{distancia de Pearson} \[ d_P(\mathbf{x,c})=\sqrt{\sum_{j=1}^{k}\left(\dfrac{x_j-c_j}{\sigma_j}\right)^2} \]donde $\sigma_i$ es la desviación típica de $X_i,\:i=1,\dots,k$.
\begin{itemize}
\item Este último caso es equivalente a estandarizar los datos usando $Z_i=\dfrac{X_i}{\sigma_i}$ o $Z_i=\dfrac{X_i-\mu_i}{\sigma_i}$ lo que nos asegura que las variables tendrán magnitudes similares aunque se usen unidades diferentes en ellas (esto no ocurre en la distancia Euclídea).
\item El principal problema es que desconocemos $\sigma_i$ y $\mu_i$ que tendrán que ser estimados usando todos los datos (sin grupos).
\item Obviamente, es equivalente a usar la distancia Euclídea con los datos estandárizados.
\item La distancia no dependerá de las unidades usadas en cada variable (es invariante por cambio de escala).
\end{itemize}
\subsection{Distancia de individuos a grupos y distancias entre grupos}
\subsubsection{Distancias de individuos a grupos}
Además de definir las distancias entre individuos, también tendremos que definir \lb{distancias de individuos a grupos} o \lb{distancias entre grupos}, lo que nos llevará a definir diversas funciones \lb{coste} que determinarán diferentes soluciones finales.
\begin{itemize}
\item Estas vendrán determinadas por el problema que queremos resolver.
\end{itemize}
Por ejemplo, si queremos calcular la \lb{distancia de un individuo} \textbf{x} \lb{a un grupo} $\{\mathbf{z}_i:i\in G\}$ formado por $m=|G|$ individuos podemos definir las distancias siguientes: \[ \begin{array}{c}
d_1(\mathbf{x},G)\coloneq d(\mathbf{x,C}),\quad \mathbf{C}=\dfrac{1}{|G|}\sum_{i\in G}\mathbf{z}_i\\
d_2(\mathbf{x},G)\coloneq\min_{i\in G}d(\mathbf{x,z}_i),\\
d_3(\mathbf{x},G)\coloneq\max_{i\in G}d(\mathbf{x,z}_i),\\
d_4(\mathbf{x},G)\coloneq\sum_{i\in G}d(\mathbf{x,z}_i),\\
d_5(\mathbf{x},G)\coloneq\sum_{i\in G}d^2(\mathbf{x,z}_i),
\end{array} \]donde $d$ es una distancia entre individuos.

Otra opción interesante es calcular (o estimar) una función de densidad para los individuos de un mismo grupo y calcular las distancias como \[ d(\mathbf{x},G_j)=1-\dfrac{f_j(\mathbf{x})}{f_1(\mathbf{x})+\cdots+f_m(\mathbf{x})}. \]
Análogamente, para las \lb{distancias entre grupos} se pueden usar: \[ \begin{array}{c}
\begin{aligned}
D_1(G-1,G_2)&=d(C_1,C_2),\quad C_j=\dfrac{1}{|G_j|}\sum_{i\in G_j}\mathbf{z}_i,\quad j=1,2\\
&D_2(G_1,G_2)=\min_{i\in G_1,j\in G_2}d(\mathbf{z}_i,\mathbf{z}_j),\\
&D_3(G_1,G_2)=\max_{i\in G_1,j\in G_2}d(\mathbf{z}_i,\mathbf{z}_j),
\end{aligned}\\
D_4(G_1,G_2)=\dfrac{1}{|G_1|\cdot|G_2|}\sum_{i\in G_1,j\in G_2}d(\mathbf{z}_i,\mathbf{z}_j),\\
D_5(G_1,G_2)=\dfrac{1}{|G_1|\cdot|G_2|}\sum_{i\in G_1,j\in G_2}d^2(\mathbf{z}_i,\mathbf{z}_j).\\
\end{array} \]
En $d_1$ o en $D_1$ podemos utilizar otros \lb{centroides} $C_1$ y $C_2$ distintos de la medida de cada grupo.

Estas \lb{distancias entre grupos} nos permitirán representar sus distancias y, posteriormente establecer a partir de qué nivel uniremos los grupos formando los gráficos denominados \lb{dendogramas}.

Finalmente debemos definir una \lb{función costo} que trataremos de minimizar para obtener la solución óptima de ese problema.
\subsubsection{Función costo}
Supongamos que asignamos los $n$ individuos a un grupo mediante una variable $Y$ que nos indicará con $y_i=j$ que el individuo $i$ se asigna al grupo $j$.

Podemos definir la \lb{función costo} \[ J(y)=\sum_j\sum_{i:y_i=j}d(\mathbf{x}_i,G_j), \]donde $\sum_{j:y_i=j}1=1$ para todo $i$ (cada elemento se asigna a un único grupo).

También \lb{se pueden usar distancias al cuadrado} \[ J^*(y)=\sum_j\sum_{i:y_i=j}d^2(\mathbf{x}_i,G_j). \]
En estos métodos, tenemos que \lb{fijar un número máximo de grupos} ya que si no, la solución óptima será tener $n$ grupos (uno para cada elemento).

\lb{Otra opción} podría ser \lb{maximizar la sima total de las distancias entre grupos} para la clasificación $y$: \[ D(y)=\sum_{i<j}D(G_i,G_j). \]
Todas estas opciones nos llevarán a problemas diferentes que tendrán que resolverse (cuando sea posible) usando sus técnicas específicas (la mayoría de Investigación Operativa).
\subsection{Métodos cluster}
\subsubsection{Clasificación}
Estos métodos se pueden dividir en dos grandes grupos:
\begin{itemize}
\item Los \lb{métodos jerárquicos:} Parten de la idea de juntar las unidades (individuos o grupos) más similares (cercanas).
\item Los \lb{métodos no jerárquicos:} Establecen un determinado número de grupos y se irá asignado cada individuo al grupo más cercano.
\end{itemize}
Solamente veremos un método de cada tipo.
\subsection{Método no jerárquico de las K-medias}
El método de las K-medias (\lb{K-means}) es sin duda el método no jerárquico \lb{más popular}.

Habitualmente usa la distancia Euclídea con los datos sin estandarizar (cuando tienen escalas similares) o estandarizados (distancia de Pearson, cuando tienen escalas diferentes) pero se puede aplicar a otras distancias.

En este caso tenemos que \lb{fijar un número de grupos predeterminado} $K$ con $1<K\le\dfrac{n}{2}$.

Posteriormente podremos aumentar o disminuir $K$ según la solución obtenida.

\begin{itemize}
\item $K$ es el número de grupos.
\item $k$ es el número de variables.
\item $n$ es el número de observaciones.
\item Estos números pueden ser diferentes.
\end{itemize}
\subsubsection{Algoritmo del método de las K-medias}
\begin{enumerate}[label=\textbullet \hspace{5pt}\lb{Paso \arabic*:}, leftmargin=2cm, start=0]
\item Determinar $K$ centroides $C_1^0,\dots,C_K^0\in\R^k$ al azar.
\item En la iteración $m$, formar el grupo $G_j^m$ con las observaciones que están más cercanas al centroide $C_j^{m-1}$ para $j=1,\dots,K$.
\item Calcular el centroide $C_j^m$ del grupo $G_k^m$ definido como el punto que minimiza \[ \sum_{i\in G_j^m}d(\mathbf{x}_i,C_j^m), \]o considerando las distancias al cuadrado \[ \sum_{i\in G_j^m}d^2(\mathbf{x}_i,C_j^m), \]para $j=1,\dots,K$.
\item Repetir pasos 1 y 2 hasta que no se produzcan cambios en los grupos del paso  o hasta que se haya iterado un número determinado de veces.
\end{enumerate}
Si usamos la distancia Euclídea y el error cuadrático, los \lb{centroides del paso 2} serán las \lb{medias aritméticas de los datos de cada grupo} ya que si queremos minimizar \[ \min_P\sum_{j\in G}d^2(\mathbf{O}_j,P) \] para un grupo $G$, tenemos que 
$$
\begin{aligned}
\sum_{j \in G}d^{2}(\mathbf{O}_{j},P)&=\sum_{j\in G}(\mathbf{O}_{j}-P)'(\mathbf{O}_{j}-P)\\
&=\sum_{j\in G}(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G}+\overline{\mathbf{O}}_{G}-P)'(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G}+\overline{\mathbf{O}}_{G}-P)\\
&=|G|(\overline{\mathbf{O}}_{G}-P)'(\overline{\mathbf{O}}_{G}-P)+\sum_{j\in G}(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G})'(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G})+2\sum_{j\in G}(\overline{\mathbf{O}}_{G}-P)'(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G})\\
\sum_{j\in G}d^{2}(\mathbf{O}_{j},P)&=|G|(\overline{\mathbf{O}}_{G}-P)'(\overline{\mathbf{O}}_{G}-P)+\sum_{j\in G}(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G})'(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G})+2(\overline{\mathbf{O}}_{G}-P)'\sum_{j\in G}(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G})\\
&=|G|(\overline{\mathbf{O}}_{G}-P)'(\overline{\mathbf{O}}_{G}-P)+\sum_{j\in G}(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G})'(\mathbf{O}_{j}-\overline{\mathbf{O}}_{G}),
\end{aligned}
$$
donde $\overline{\mathbf{O}}_G=\dfrac{1}{|G|}\sum_{j\in G}\mathbf{O}_j$ es la media del grupo G.

En la expresión final, el segundo sumando es constante (no depende de $P$) y el mínimo del primer sumando sumando se alcanza con $P=\overline{\mathbf{O}}_G$ (ya que es la distancia entre esos dos puntos).

De esta forma el paso 2 es inmediato y en el paso 1 simplemente calculamos las distancias a estos nuevos $K$ centroides (medias) asignando cada individuo al grupo del centroide más cercano (distancia $d_1$).

El nombre \code{k-means} proviene de esta propiedad.

Además, si usamos como función de coste las dadas previamente y hay cambios en los grupos, esta función es estrictamente decrecientes en el paso 1 ya que hay al menos un \code{objeto} cuya distancia al grupo (centroide) ha disminuido.

Al recalcular los centroides en el paso 2 las sumas de estas distancias en los grupos disminuirán aún más o se quedarán iguales a las del paso 1.

Como las \lb{opciones del paso 1} son \lb{finitas}, este algoritmo conducirá hasta una solución óptima local en un número finito de pasos, que puede ser muy grande.
\begin{itemize}
\item Para \lb{evitar este problema} podemos aplicar el algoritmo varias veces con centroides iniciales diferentes y comparar las soluciones óptimas finales de cada algoritmo.
\end{itemize}
Veremos que con unos pocos pasos podemos obtener soluciones muy buenas.
\subsubsection{Un ejemplo sencillo}
Mostramos con un ejemplo sencillo el funcionamiento del algoritmo.

Para ello usaremos los datos analizados previamente con regresión logística pero ahora supondremos que no conocemos los grupos de esa muestra.

Supongamos que tenemos dos variables predictoras $X_1$ y $X_2$ (k = 2) y los datos siguientes:

\begin{center}
\begin{tabular}{c|ccc}
Individuo $i$ & $X_1$ & $X_2$ & $Y$ \\
\hline
1 & 1 & 2 &  \\
2 & 2 & 1 &  \\
3 & 3 & 1 &  \\
4 & 2 & 2 &  \\
5 & 5 & 1 &  \\
6 & 5 & 3 &  \\
7 & 3 & 2 &  \\
8 & 4 & 3 &  \\
9 & 4 & 4 &  \\
10 & 5 & 4 &  \\
\hline
\end{tabular}
\end{center}
Individuos sin agrupamiento inicial.
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = 20, 
     xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, 1:length(X1), cex = 0.9, col = "red")
\end{lstlisting}
\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot001"}
\end{center}
Observamos que \lb{tienen unidades similares} (por lo que podremos usar la distancia Euclídea) y que parecen formar dos grupos diferentes.

El \lb{objetivo} es determinar la variable $Y$ que nos asigne cada individuo a un grupo.

En tabla anterior, hemos dejado en blanco la columna de la variable $Y$ para señalar que en este caso no tenemos una muestra de entrenamiento y, por lo tanto, no podremos saber cuál es la solución óptima (que mejor clasifique a los individuos).
\begin{itemize}
\item \lb{Análisis no supervisado} (o automático).
\end{itemize}

Para aplicar el algoritmo con $K=2$ medias (grupos) elegimos dos centroides al azar (dentro de la zona donde están los individuos).

Para ello usamos la instrucción \code{runif(2,0,6)} (fijando previamente la semilla con \code{set.seed}).

Primer centroide en el paso 0:
\begin{lstlisting}
set.seed(123124)
C1_0 = runif(2, 0, 6)
C1_0
\end{lstlisting}
\begin{verbatim}
## [1] 3.101323 1.401716
\end{verbatim}
Segundo centroide en el paso 0:
\begin{lstlisting}
set.seed(123121)
C2_0 = runif(2, 0, 6)
C2_0
\end{lstlisting}
\begin{verbatim}
## [1] 2.2950230 0.3726236
\end{verbatim}
Otra opción sería usar dos de esos puntos al azar.

Con los centroides obtenidos, $C_1^0=(3.101323, 1.401716)$ y $C_2^0=(2.2950230,0.3726236)$, obtenemos las distancias y agrupaciones siguientes:
\begin{lstlisting}
library("dplyr")
df = data.frame(X1, X2) %>%
  mutate(d_C1 = sqrt((X1 - C1_0[1])^2 + (X2 - C1_0[2])^2),
         d_C2 = sqrt((X1 - C2_0[1])^2 + (X2 - C2_0[2])^2),
         Y = ifelse(d_C1 <  d_C2, 1, 2))
\end{lstlisting}
\begin{center}
\begin{tabular}{c|cccc|c}
Individuo $i$ & $X_1$ & $X_2$ & $d(\mathbf{X},C_1^0)$ & $d(\mathbf{X},C_2^0)$ & $Y		$ \\ 
\hline 
1 & 1 & 2 & 2.1848343 & 2.0797689 & 2 \\ 
2 & 2 & 1 & 1.1723001 & 0.6932819 & 2 \\ 
3 & 3 & 1 & 0.4142971 & 0.9437127 & 1 \\ 
4 & 2 & 2 & 1.2533377 & 1.6539022 & 1 \\ 
5 & 5 & 1 & 1.9407090 & 2.7767790 & 1 \\ 
6 & 5 & 3 & 2.4818314 & 3.7709425 & 1 \\ 
7 & 3 & 2 & 0.6068031 & 1.7735125 & 1 \\ 
8 & 4 & 3 & 1.8336119 & 3.1321005 & 1 \\ 
9 & 4 & 4 & 2.7493091 & 4.0080926 & 1 \\ 
10 & 5 & 4 & 3.2180825 & 4.529044 & 1 \\ 
\hline 
\end{tabular} 
\end{center}
Cada individuo se asigna al grupo más cercano (midiendo su distancia a cada centroide).

Individuos agrupados en el primer paso del algoritmo \lb{K-means} con los centroides iniciales (negro).
\begin{lstlisting}
Y <- df$Y
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), 
     xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, 1:length(X1), cex = 0.9, col = "red")
text(C1_0[1], C1_0[2], "C1", cex = 0.9, col = "black")
text(C2_0[1], C2_0[2], "C2", cex = 0.9, col = "black")
legend('topleft', legend = c('Y = 1','Y = 2'), 
       pch = 1:2, cex = 1.2)
\end{lstlisting}
\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot002"}
\end{center}
En el siguiente paso, calculamos los nuevos centroides, que son las medias de los individuos de cada grupo.
\begin{lstlisting}
centroides = df %>% 
  group_by(Y) %>%
  summarise(C_x1 = mean(X1),
            C_x2 = mean(X2)) %>%
  ungroup
C1_1 = c(centroides$C_x1[1], centroides$C_x2[1])
C1_1
\end{lstlisting}
\begin{verbatim}
## [1] 3.875 2.500
\end{verbatim}
\begin{lstlisting}
C2_1=  c(centroides$C_x1[2], centroides$C_x2[2])
C2_1
\end{lstlisting}
\begin{verbatim}
## [1] 1.5 1.5
\end{verbatim}
Estos centroides son: 

$\begin{array}{l}
C_1^1=(3.875,2.5)\\
C_2^1=(1.5,1.5).
\end{array}$

Individuos agrupados en el primer y segundo paso del algoritmo K-means con los centroides iniciales (negro) y los nuevos (rojo).
\begin{lstlisting}
df = df %>%
  mutate(d_C1 = sqrt((X1 - C1_1[1])^2 + (X2 - C1_1[2])^2),
         d_C2 = sqrt((X1 - C2_1[1])^2 + (X2 - C2_1[2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
Y <- df$Y
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), 
     xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, 1:length(X1), cex = 0.9, col = "red")
text(C1_0[1], C1_0[2], "C1", cex = 0.9, col = "black")
text(C2_0[1], C2_0[2], "C2", cex = 0.9, col = "black")
text(C1_1[1], C1_1[2], "C1", cex = 0.9, col = "red")
text(C2_1[1], C2_1[2], "C2", cex = 0.9, col = "red")
legend('topleft', legend = c('Y = 1','Y = 2'), pch = 1:2, cex = 1.2)
\end{lstlisting}
\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot003"}
\end{center}
Repetimos los cálculos con los nuevos centroides. El reparto del paso uno conduce a los mismos grupos en la iteración anterior y el algoritmo se detiene, obteniendo los centroides finales: $C_1^2=(4.6,3.0)$ y $C_2^2=(2.2,1.6)$.
\begin{lstlisting}
df = df %>%
  mutate(d_C1 = sqrt((X1 - C1_1[1])^2 + (X2 - C1_1[1])^2),
         d_C2 = sqrt((X1 - C2_1[1])^2 + (X2 - C2_1[2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))

centroides = df %>% 
  group_by(Y) %>%
  summarise(C_x1 = mean(X1),
            C_x2 = mean(X2))
C1_2 = c(centroides$C_x1[1], centroides$C_x2[1])
C2_2 =  c(centroides$C_x1[2], centroides$C_x2[2])
df = df %>%
  mutate(d_C1 = sqrt((X1 - C1_2[1])^2 + (X2 - C1_2[2])^2),
         d_C2 = sqrt((X1 - C2_2[1])^2 + (X2 - C2_2[2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
Y <- df$Y
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), 
     xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, 1:length(X1), cex = 0.9, col = "red")
text(C1_1[1], C1_1[2], "C1", cex = 0.9, col = "black")
text(C2_1[1], C2_1[2], "C2", cex = 0.9, col = "black")
text(C1_2[1], C1_2[2], "C1", cex = 0.9, col = "red")
text(C2_2[1], C2_2[2], "C2", cex = 0.9, col = "red")
legend('topleft', legend = c('Y = 1','Y = 2'), pch = 1:2, cex = 1)
\end{lstlisting}

\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot004"}
\end{center}
Cuando los grupos no varían en dos iteraciones seguidas los centroides coinciden y el algoritmo se detiene.

\lb{Problema:} este algoritmo puede depender de los valores iniciales.

Presentamos la agrupación de los individuos con otros centroides iniciales.

Centroides iniciales: $C_1^0=(2.482099,2.270985)$ y $C_2^0=(3.851084,5.344700)$.

Centroides finales: $C_1=(2.6667,1.5)$ y $C_2=(4.5,3.5)$.

\begin{lstlisting}
K = 2
## Paso 0
C = matrix(NA, ncol = 2, nrow = K)

C[1, ] = c(2.482099,2.270985)
C[2, ] = c(3.851084, 5.344700)

clusters = matrix(NA, ncol = 2, nrow = nrow(df))
clusters[, 1] = 1
clusters[, 2] = 2

i = 2
while(sum(clusters[, 1] == clusters[, 2]) != nrow(df)) {
## Paso 1
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
clusters[, 1] = df$Y
## Paso 2
centroides = df %>% 
  group_by(Y) %>%
  summarise(C_x1 = mean(X1),
            C_x2 = mean(X2))
C[1, ] = c(centroides$C_x1[1], centroides$C_x2[1])
C[2, ] =  c(centroides$C_x1[2], centroides$C_x2[2])
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
clusters[, 2] = df$Y
i = i + 1 
}

Y <- df$Y

plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), 
     xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, 1:length(X1), cex = 0.9, col = "red")
text(C[1, 1], C[1, 2], "C1", cex = 0.9, col = "red")
text(C[2, 1], C[2, 2], "C2", cex = 0.9, col = "red")
legend('topleft', legend = c('Y = 1','Y = 2'), pch = 1:2, cex = 1)
\end{lstlisting}
\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot005"}
\end{center}
Centroides iniciales: $C_1^0=(2,3)$ y $C_2^0=(5,3)$.

Centroides finales: $C_1=(2.2, 1.6)$ y $C_2=(4.6,3.0)$

\begin{lstlisting}
library("dplyr")
K = 2
## Paso 0
C = matrix(NA, ncol = 2, nrow = K)

C[1, ] = c(2, 3)
C[2, ] = c(5, 3)

clusters = matrix(NA, ncol = 2, nrow = nrow(df))
clusters[, 1] = 1
clusters[, 2] = 2

i = 2
while(sum(clusters[, 1] == clusters[, 2]) != nrow(df)) {
## Paso 1
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
clusters[, 1] = df$Y
## Paso 2
centroides = df %>% 
  group_by(Y) %>%
  summarise(C_x1 = mean(X1),
            C_x2 = mean(X2))
C[1, ] = c(centroides$C_x1[1], centroides$C_x2[1])
C[2, ] =  c(centroides$C_x1[2], centroides$C_x2[2])
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
clusters[, 2] = df$Y
i = i + 1 
}

Y <- df$Y

plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), 
     xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, 1:length(X1), cex = 0.9, col = "red")
text(C[1, 1], C[1, 2], "C1", cex = 0.9, col = "red")
text(C[2, 1], C[2, 2], "C2", cex = 0.9, col = "red")
legend('topleft', legend = c('Y = 1','Y = 2'), pch = 1:2, cex = 0.9)
\end{lstlisting}
\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot006"}
\end{center}
\subsubsection{¿Cómo comparar distintas soluciones?}
Para comparar las soluciones podemos utilizar diversas medidas.

Por ejemplo podíamos usar la función de costo $J$ (con distancias Euclídeas).
\begin{lstlisting}
## función costo J(y)
J <- function(df, C) {
  J= df %>%
    group_by(Y) %>%
    summarise(d = ifelse(Y ==1, 
                         sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
                         sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2))) %>%
    ungroup
  return(sum(J$d))
}
\end{lstlisting}
Para la solución $C_1=(2.66667,1.5)$ y $C_2=(4.5,3.5)$ (\textbf{\textit{sol}1}):
\begin{lstlisting}
C = matrix(c(2.666667, 1.5,
             4.5, 3.5), nrow = 2, ncol =2, byrow = TRUE)
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
J(df, C)
\end{lstlisting}
\begin{verbatim}
## [1] 3.823299
\end{verbatim}
$J(y_{sol1})=9.823299$.

Para la solución $C_1=(2.2,1.6)$ y $C_2=(4.6,3.0)$ (\textbf{\textit{sol}2})
\begin{lstlisting}
C = matrix(c(2.2, 1.6,
             4.6, 3.0), nrow = 2, ncol =2, byrow = TRUE)
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
J(df, C)
\end{lstlisting}
\begin{verbatim}
## [1] 9.521839
\end{verbatim}
$J(y_{sol2})=9.521839$

Y para $J^*$ (con distancias Euclídeas al cuadrado), 
\begin{lstlisting}
## función costo J^*(y)
J2 <- function(df, C) {
  J= df %>%
    group_by(Y) %>%
    summarise(d = ifelse(Y ==1, 
                         (X1 - C[1, 1])^2 + (X2 - C[1, 2])^2,
                         (X1 - C[2, 1])^2 + (X2 - C[2, 2])^2)) %>%
    ungroup
  return(sum(J$d))
}
\end{lstlisting}

Para la solución $C_1=(2.66667,1.5)$ y $C_2=(4.5000000,3.5)$ (\textbf{\textit{sol}1}):

\begin{lstlisting}
C = matrix(c(2.666667, 1.5,
             4.500000, 3.5), nrow = 2, ncol =2, byrow = TRUE)
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
J2(df, C)
\end{lstlisting}
\begin{verbatim}
## [1] 12.83333
\end{verbatim}
$J^*(y_{sol1})=12.83333$.

Para la solución $C_1=(2.2,1.6)$ y $C_2=(4.6,3.0)$ (\textbf{\textit{sol}2}):
\begin{lstlisting}
C = matrix(c(2.2, 1.6,
             4.6, 3.0), nrow = 2, ncol =2, byrow = TRUE)
df = df %>%
  mutate(d_C1 = sqrt((X1 - C[1, 1])^2 + (X2 - C[1, 2])^2),
         d_C2 = sqrt((X1 - C[2, 1])^2 + (X2 - C[2, 2])^2),
         Y = ifelse(d_C1 < d_C2,
                    1,
                    2))
J2(df, C)
\end{lstlisting}
\begin{verbatim}
## [1] 11.2
\end{verbatim}
$J^*(y_{sol2})=11.2$

En ambos casos la solución segunda parece dar mejores resultados: \[ \begin{array}{c}
J(y_{sol1})=9.823299>J(y_{sol2})=9.521839,\\
J^*(y_{sol1})=12.83333>J^*(y_{sol2})=11.2.
\end{array} \]
\subsubsection{\textbf{\texttt{K-means}} se puede ejecutar de forma automática en \textbf{\texttt{R}}}

El algoritmo \lb{K-means} se puede ejecutar de forma automática en \code{R} con el comando \code{Kmeans}.

Por defecto, usa el algoritmo de Hartigan and Wong (1979).

Para ejecutarlo en este ejemplo:
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
d <- data.frame(X1, X2)
CA <- kmeans(d, 2)
CA$centers
\end{lstlisting}
\begin{verbatim}
##    X1  X2
## 1 4.6 3.0
## 2 2.2 1.6
\end{verbatim}
Los grupos coinciden con los obtenidos en la segunda solución anterior (óptima).

También se pueden guardar
\begin{lstlisting}
CA1 <- CA$centers[1,]
CA2 <- CA$centers[2,]
\end{lstlisting}
Los grupos se obtienen con 
\begin{lstlisting}
CA$cluster
\end{lstlisting}
\begin{verbatim}
## [1] 2 2 2 1 1 2 1 1 1
\end{verbatim}
La solución coincide con la representada en la gráfica.

Las sumas de las distancias al cuadrado en los grupos se obtienen con
\begin{lstlisting}
CA$withinss
\end{lstlisting}
\begin{verbatim}
## [1] 7.2 4.0
\end{verbatim}
obteniendo $7.2+4.0=11.2$ (como antes)

El comando 
\begin{lstlisting}
CA$totss
\end{lstlisting}
\begin{verbatim}
## [1] 30.5
\end{verbatim}
proporciona la suma de las distancias al cuadrado sin grupos (o con un único grupo) obteniéndose $30.35$.

Si se calcula directamente,
\begin{lstlisting}
centroides = df %>% 
  summarise(C_x1 = mean(X1),
            C_x2 = mean(X2))
C[1, ] = c(centroides$C_x1[1], centroides$C_x2[1])
df = df %>%
  mutate(d = (X1 - C[1, 1])^2 + (X2 - C[1, 2])^2)
sum(df$d)
\end{lstlisting}
\begin{verbatim}
## [1] 30.5
\end{verbatim}
Por lo que al agruparlos se ha producido una disminución del \[ 1-\dfrac{11.2}{30.5}=0.6327869 \] por uno, es decir, con dos grupos la \lb{variabilidad} se reduce un $63.28\%$.

Individuos agrupados con \code{Kmeans} en 2 grupos:
\begin{lstlisting}
K <- 2
CA <- kmeans(d, K)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(CA$cluster),
     xlim = c(0, 6), ylim = c(0, 6), cex = 0.9)
legend('topleft', legend = c('Y=1', 'Y=2'), pch = 1:K, cex = 1)
text(CA$centers[1, 1] + 0.15, CA$centers[1, 2], 'C1', cex = 0.9, col = 'red')
text(CA$centers[2, 1] - 0.20, CA$centers[2, 2], 'C2', cex = 0.9, col = 'red')
text(X1 + 0.15, X2, 1:length(X1), cex = 0.9, col = 'red')
\end{lstlisting}
\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot007"}
\end{center}
Individuos agrupados con \code{Kmeans} en 3 grupos (hay un grupo que tiene un único dato, por lo tanto, será su centroide):
\begin{lstlisting}
K <- 3
CA <- kmeans(d, K)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(CA$cluster),
     xlim = c(0, 6), ylim = c(0, 6), cex = 0.9)
legend('topleft', legend = c('Y=1', 'Y=2', 'Y=3'), pch = 1:K, cex = 1)
text(CA$centers[1, 1] + 0.15, CA$centers[1, 2], 'C1', cex = 0.9, col = 'red')
text(CA$centers[2, 1] - 0.20, CA$centers[2, 2], 'C2', cex = 0.9, col = 'red')
text(CA$centers[3, 1] - 0.35, CA$centers[3, 2], 'C3', cex = 0.9, col = 'red')
text(X1 + 0.15, X2, 1:length(X1), cex = 0.9, col = 'red')
\end{lstlisting}
\begin{center}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 6/screenshot008"}
\end{center}
Al ejecutar este comando pueden aparecer otras soluciones porque las \lb{soluciones dependen de los valores iniciales}.

La función \code{kmeans} permite ejecutar el algoritmo con diferentes valores de partida (argumento \code{nstart}) y proporcionar la mejor de esas soluciones.
\begin{lstlisting}
CA <- kmeans(d, 3, nstart = 10)
\end{lstlisting}
Con tres grupos la variabilidad se reduce en un 80.3 \%.
\begin{lstlisting}
CA = kmeans(d, 3, nstart = 10)
1- (sum(CA$withinss)/CA$totss)
\end{lstlisting}
\begin{verbatim}
## [1] 0.9032787
\end{verbatim}
\subsection{Método jerárquico}
\subsubsection{Índice de similaridad}
En este caso no fijamos de antemano un número de grupos.

Lo que haremos es, dada una instancia, definir un \lb{índice de similaridad entre dos observaciones} con \[ I(\mathbf{x}^{(i)},\mathbf{x}^{(j)})=1-\dfrac{d(\mathbf{x}^{(i)},\mathbf{x}^{(j)})}{\displaystyle\max_{r,s}d(\mathbf{x}^{(r)},\mathbf{x}^{(s)})}\in[0,1]. \]
Se puede dar una definición análoga para grupos.

La idea es establecer clasificaciones calculando los índices de similitud (o distancias) que se van obteniendo.

Finalmente, dependiendo del índice de similitud elegido, obtendremos un número determinado de grupos (uniendo los que tienen similitud menor que ese índice).

\subsubsection{Algoritmos}
Consideraremos dos algoritmos.

En el primero \lb{partiremos de} $n$ \lb{grupos formados por un individuo cada uno}.
\begin{itemize}
\item En el primer paso uniremos las dos observaciones más cercanas (distancia mínima) que serán las que tengan un índice de similitud mayor.
\item Recalculamos las distancias para estos grupos y unimos los dos grupos más cercanos, continuamos así hasta conseguir un único grupo.
\end{itemize}
En el segundo, procederemos de forma inversa, \lb{partiremos de un único grupo} formado por todas las observaciones.
\begin{itemize}
\item En un primer paso separaremos en dos de forma que las distancias entre estos dos grupos sea máxima (o las distancias a esos dos grupos de sus individuos sea mínima).
\item En el siguiente paso formaremos un tercer grupo tomando individuos de los grupos $1$ y $2$ con un criterio similar.
\item Procederemos así hasta conseguir $n$ grupos.
\end{itemize}
Claramente, este método es más lento que el anterior.
\subsubsection{Un ejemplo sencillo}
Para ver un ejemplo analizaremos los datos del ejemplo anterior usando el primer método.

En primer lugar calculamos las distancias Euclídeas entre todos los individuos:

\begin{lstlisting}
n <- length(X1)
D <- matrix(NA, n, n)
for (i in 1:n) {
  for (j in 1:n) {
    D[i, j] <- dE(d[i, ], d[j, ])
    } 
}
\end{lstlisting}
\begin{center}
\begin{tabular}{c|cccccccccc}
$D$ & $O_{1}$ & $O_2$ & $O_3$ & $O_4$ & $O_5$ & $O_6$ & $O_7$ & $O_8$ & $O_9$ & $O_{10}$ \\
$O_1$ & 0 & 1.41 & 2.24 & \textbf{1} & 4.12 & 4.12 & 2 & 3.16 & 3.61 & \textbf{4.47} \\
$O_2$ & 1.41 & 0 & 1 & 1 & 3 & 3.61 & 1.41 & 2.83 & 3.61 & 4.24 \\
$O_3$ & 2.24 & 1 & 0 & 1.41 & 2 & 2.83 & 1 & 2.24 & 3.16 & 3.61 \\
$O_4$ & 1 & 1 & 4.41 & 0 & 3.16 & 3.16 & 1 & 2.24 & 2.83 & 3.61 \\
$O_5$ & 4.12 & 3 & 2 & 3.16 & 0 & 2 & 2.24 & 2.24 & 3.16 & 3 \\
$O_6$ & 4.12 & 3.61 & 2.83 & 3.16 & 2 & 0 & 2.24 & 1 & 1.41 & 1 \\
$O_7$ & 2 & 1.41 & 1 & 1 & 2.24 & 2.24 & 0 & 1.41 & 2.24 & 2.83 \\
$O_8$ & 3.16 & 2.83 & 2.24 & 2.24 & 1 & 1 & 1.41 & 0 & 1 & 1.41 \\
$O_9$ & 3.61 & 3.61 & 3.16 & 2.83 & 1.41 & 1.41 & 2.24 & 1 & 0 & 1 \\
$O_{10}$ & \textbf{4.47} & 4.24 & 3.61 & 3.61 & 1 & 1 & 1.41 & 1.41 & 1 & 0 \\
\hline
\end{tabular}
\end{center}