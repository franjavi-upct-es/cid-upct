\section{Análisis de componentes principales}
\subsection{Introducción}
\begin{enumerate}[label=\arabic*)]
	\item Objetivo
\end{enumerate}
Simplificar la representación de datos multidimensionales al transformarlos en un nuevo conjunto de variables llamadas \lb{componentes principales}.
\begin{itemize}
	\item \lb{Reducción de dimensionalidad:} a veces dispondremos de muchas variables y simplemente se querrá disminuir el número de variables perdiendo la menor información posible (\lb{compresión de datos}).
	\begin{itemize}
		\item Útil en aplicaciones de almacenamiento y transmisión de información.
	\end{itemize}
	\item \lb{Visualización de datos:} al proyectar los datos en un espacio de menor dimensión, es más fácil representar gráficamente la estructura subyacente de los datos, lo que puede ayudar a identificar patrones, agrupaciones o relaciones (\lb{extracción de características}).
	\item \lb{Eliminación de multicolinealidad:} en análisis de regresión y otros contextos, la multicolinealidad (alta correlación entre variables independientes) puede ser problemática.
	\begin{itemize}
		\item En estos casos puede ayudar a reducir la multicolinealidad el \lb{transformar las variables originales} en un conjunto de variables no correlacionadas (las \lb{componentes principales}).
	\end{itemize}
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Planteamiento desde el punto de vista teórico
\end{itemize}
La idea es \lb{resumir la información} de un \vea (v.a.) $k$-dimensional $\mathbf{X}=(X_1,\dots,X_k)'$ (recordemos que $A'$ denota la traspuesta de $A$, es decir, $\mathbf{X}$ es un vector columna) en unas \lb{pocas variables} que proporcionen la información más relevante.

Se puede dar una aproximación geométrica mediante el concepto de \lb{elipsoide de concentración}.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Si $\mathbf{X}$ es un vector aleatorio de dimensión $k$, media $\mu$ y su matriz de covarianzas $V=(\sigma_{i,j})$ definida positiva, se define el \lb{elipsoide de concentración} de $\mathbf{X}$ como \[ E_k=\{\mathbf{x}\in\R^k:(\mathbf{x}-\mu)'V^{-1}(\mathbf{x}-\mu)\le k+2\}. \]
En la definición del elipsoide interviene la \lb{distancia de Mahalanobis basada en la matriz $V$} entre $\mathbf{x}$ y la media $\mu$ dada por \[ d_V(\mathbf{x},\mu)=\sqrt{(\mathbf{x}-\mu)'V^{-1}(\mathbf{x}-\mu)}. \]
Esta distancia al cuadrado se puede calcular en \code{R} con \code{mahalanobis(x, mu, V)}.

Además, si $X$ es \lb{normal}, el elipsoide se puede definir a partir de las \lb{curvas de nivel de la función de densidad} ($f(x)=cte$.) ya que \[ f(x)=\dfrac{1}{\sqrt{|V|(2\pi)^k}}\exp\left(-\dfrac{1}{2}(\mathbf{x}-\mu)'V^{-1}(\mathbf{x}-\mu)\right). \]
Una parte de los individuos (puntos estarán dentro de este elipsoide).

Si queremos \lb{distinguirlos con una única variable}, parece claro que lo mejor sería proyectarlos sobre el eje mayor del elipsoide.

Por ejemplo, para una \lb{normal bivariante} \[ \mathcal{N}_2\left(\mu=\binom{0}{0},V=\begin{pmatrix}
	1 & \tfrac{1}{2}\\
	\tfrac{1}{2} & 1
\end{pmatrix}\right) \] se tiene que \[ (x_1,x_2)\begin{pmatrix}
1 & \tfrac{1}{2}\\
\tfrac{1}{2} & 1
\end{pmatrix}^{-1}\binom{x_1}{x_2}=\dfrac{4}{3}x_1^2-\dfrac{4}{3}x_1x_2+\dfrac{4}{3}x_2^2 \] por lo que el \lb{elipsoide de concentración} sería \[ \dfrac{4}{3}x_1^2-\dfrac{4}{3}x_1x_2+\dfrac{4}{3}x_2^2\le4 \]

\begin{itemize}
	\item \lb{Elipsoide de concentración} para la normal bivariante con medias 0, varianzas 1 y correlación $\dfrac{1}{2}:$
	
	\begin{lstlisting}
hc <- function(x1, x2) (4/3)*x1^ 2- (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length =1000)
x2 <- seq(-3, 3, length =1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = 4)
title(main = "level = 4")
	\end{lstlisting}
	\begin{center}
		\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 4/screenshot001"}
	\end{center}
	\item \lb{Elipsoides obtenidos con otros niveles} (circunferencias de Mahalanobis):
	\begin{lstlisting}
hc <- function(x1, x2) (4/3)*x1^ 2- (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length =1000)
x2 <- seq(-3, 3, length =1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = c(1:6))
title(main = "level = 1, ..., 6")
	\end{lstlisting}
	\begin{center}
		\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 4/screenshot002"}
	\end{center}
\end{itemize}

\begin{minipage}{0.45\textwidth}
	Si queremos \lb{reducir las dos variables a solo una}, la mejor proyección, es decir la que mejor separa los puntos (\lb{varianza máxima}), es la proporcionada por el \lb{eje principal del elipsoide} (o curvas de nivel de la normal).
	
	En este ejemplo viene dado por la recta: \[ x_2=x_1. \]
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{lstlisting}
hc <- function(x1, x2) (4/3)*x1^ 2- (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length =1000)
x2 <- seq(-3, 3, length =1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = c(1:6))
abline(a = 0, b = 1, col = "red", lty = 2)
abline(a = 0, b = -1, col = "blue", lty = 2)
title(main = "level = 1, ..., 6")
\end{lstlisting}
\end{minipage}
\begin{flushright}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 4/screenshot003"}
\end{flushright}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Objetivo
\end{itemize}
\lb{Transformar} un conjunto de $k$ variables interrelacionadas entre sí en un nuevo conjunto con un número menor de variables:
\begin{itemize}
	\item las \lb{componentes principales}
\end{itemize}
De manera que estas nuevas variables:
\begin{itemize}
	\item sean \lb{ortogonales entre sí}.
	\item capturen la \lb{mayor variabilidad} de las variables.
	\item \lb{expliquen la mayor parte de la variabilidad} de las variables originales.
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Planteamiento teórico
\end{itemize}
Supongamos que $\mathbf{X}=(X_1,\dots,X_k)'$ es un \vea $k$-dimensional con vector de medias $\mu$ y matriz de covarianzas $V$ \lb{semidefinida positiva}.

Entonces la \lb{primera componente principal} será la \va unidimensional \[ Y_1=a_1X_1+\cdots+a_kX_k \] con $a_1^2+\cdots+a_k^2=1$ cuya \lb{varianza es máxima}.
\begin{itemize}
	\item Si no se normaliza la combinación lineal, la variable $Y_1$ puede tener varianza tan grande como queramos.
	\item Geométricamente, hacemos un cambio de variable (primer eje) para que la dispersión sea máxima y la normalización equivale a mantener la escala original (proyectar).
\end{itemize}
El problema puede expresarse de la forma siguiente: \[ \begin{rcases}
	\max&\var(a'\mathbf{X})\\
	\text{s.a.} & \mathbf{a'a}=1
\end{rcases} \] donde $\mathbf{a}=(a_1,\dots,a_k)'\in\R^k$.

Una vez calculada una primera componente principal $Y_1$, la \lb{segunda componente principal} $Y_2$ debe verificar $\cov(Y_1,Y_2)=0$ (no debe contener información ya incluida en $Y_1$) y debe tener la \lb{varianza máxima}, es decir, \[ \begin{rcases}
	\max & \var(\mathbf{a'X})\\
	\text{s.a.} & \mathbf{a'a}=1\\
	&\cov(Y_1,\mathbf{a'X})=0
\end{rcases} \]
Así, sucesivamente, por inducción, se definen las \lb{siguientes componentes principales} $(Y_j)$ como la (una) solución de 
\[ \begin{rcases}
	\max & \var(\mathbf{a'X})\\
	\text{s.a.} & \mathbf{a'a}=1\\
	&\cov(Y_1,\mathbf{a'X})=0,\quad i=1,\dots,j-1
\end{rcases} \]
\begin{itemize}[label=\color{lightblue}\textbullet]
	\item La solución general viene dada en el teorema siguiente que prueba la \lb{existencia de las (unas) componentes principales} y muestra \lb{cómo calcularlas}.
	\item Además, se demuestra que las componentes principales \lb{no son únicas} (puede haber más soluciones).
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Teorema de existencia
\end{itemize}
Si $\mathbf{X}$ es un \vea $k$-dimensional con matriz de covarianzas $V$ \lb{definida positiva}, las (unas) \lb{componentes principales} se obtienen como \[ \mathbf{Y}=(Y_1,\dots,Y_k)'=T'\mathbf{X}=\begin{pmatrix}
	t_{1,1} & \cdots & t_{k,1} \\
	\cdots & \cdots & \cdots \\
	t_{1,k} & \cdots & t_{k,k}
\end{pmatrix}\begin{pmatrix}
X_1\\
\cdots\\
X_k
\end{pmatrix}, \] donde $T$ es una \lb{matriz ortogonal} $(T'T=TT'=I)$ tal que \[ T'VT=D=\mathrm{diag}(\lambda_1,\dots,\lambda_k) \]con $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_k>0$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
Como $V$ es una matriz \lb{simétrica} y \lb{definida positiva}, existe una matriz $T=(t_{i,j})$ \lb{ortogonal} $(T'T=TT'=I)$ tal que \[ T'VT=D=\mathrm{diag}(\lambda_1,\dots,\lambda_k) \] con los valores propios verificando $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_k>0$.

De esta forma, si \[ \mathbf{Y}=(Y_1,\dots,Y_k)'=T'\mathbf{X}=\begin{pmatrix}
	t_{1,1} & \cdots & t_{k,1} \\
	\cdots & \cdots & \cdots \\
	t_{1,k} & \cdots & t_{k,k}
\end{pmatrix}\begin{pmatrix}
	X_1\\
	\cdots\\
	X_k
\end{pmatrix} \] entonces $Y_1,\dots,Y_k$ verifican que \[ \cov(\mathbf{Y})=\cov(T'\mathbf{X})=E[T'(\mathbf{X-\mu})(\mathbf{X}-\mu)'T]=T'VT=D, \]lo que indica que $\cov(Y_i,Y_j)=0$ para $i\neq j$ y $\var(Y_j)=\lambda_j$.

Para comprobar que $Y_1$ es una primera componente principal, supongamos que $\mathbf{a'X}$ es una combinación lineal con $\mathbf{a'a}=1$.

Las columnas de la matriz $T$ corresponden a los vectores propios $\mathbf{t}_i$ asociados a los autobalores $\lambda_i,T=(\mathbf{t}_1|\cdots|\mathbf{t}_k)$, y como los vectores propios son una vase, existirán $c_1,\dots,c_k$ números reales tales que \[ \mathbf{a}=c_1\mathbf{t}_1+\cdots+c_k\mathbf{t}_k=\sum_{i=1}^{k}c_i\mathbf{t}_i. \]
Con lo que 
\begin{align*}
	\var(\mathbf{a'X})&=E[\mathbf{a'(X-\mu)(X-\mu)'a}]=\mathbf{a}'\cov(\mathbf{X})\mathbf{a}=\mathbf{a'}V\mathbf{a}\\
	&=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)V\left(\sum_{i=1}^{k}c_i\mathbf{t}_i\right)=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)\left(\sum_{j=1}^{k}c_jV\mathbf{t}_j\right)\\
	&=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)\left(\sum_{j=1}^{k}c_j\lambda_j\mathbf{t}_j\right)=\sum_{i,j}c_ic_j\lambda_j\mathbf{t}_i'\mathbf{t}_j=\sum_{i=1}^{k}c_i^2\lambda_i
\end{align*}
Y, como \[ \mathbf{a'a}=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)\left(\sum_{j=1}^{k}c_j\mathbf{t}_j\right)=\sum_{i,j}c_ic_j\mathbf{t}_i'\mathbf{t}_j=\sum_{i=1}^{k}c_i^2=\mathbf{c'c}=1, \] con $\mathbf{c}=(c_1,\dots,c_k)'$, la varianza será máxima si $x_1^2=1,c_2=0,\dots,c_k=0$ ya que \[ \var(\pm\mathbf{t}_1'\mathbf{X})=\lambda_1=\mathbf{c'c}\lambda_1=\sum_{i=1}^{k}c_i^2\lambda_1\ge\sum_{i=1}^{k}c_i^2\lambda_i=\var(\mathbf{a'X}), \]para todo $\mathbf{a}$ tal que $\mathbf{a'a}=1$, es decir, $Y_1=\pm \mathbf{t}_1'\mathbf{X}$ es una primera componente principal (puede haber otras soluciones si $\lambda_1=\lambda_2$).

Por inducción, supongamos que $Y_1=\mathbf{t}_1'\mathbf{X},\dots,Y_{j-1}\mathbf{X}$ son las primeras ($j-1$) componentes principales.

Y veamos que $Y_j=\mathbf{t}_j'\mathbf{X}$ es la (una) solución de \[ \begin{rcases}
	\max & \var(\mathbf{a'X})\\
	\text{s.a.} & \mathbf{a'a}=1\\
	&\cov(Y_1,\mathbf{a'X})=0,\quad i=1,\dots,j-1
\end{rcases} \]
Como se debe verificar \begin{align*}
	\cov(\mathbf{a'X},Y_i) & = \cov(\mathbf{a'X},\mathbf{t}_i'\mathbf{X})=E[\mathbf{a'(X-\mu)(X-\mu)'t}_i]=\mathbf{a}'\cov(\mathbf{X})\mathbf{t}_i\\
	&=\mathbf{a'}V\mathbf{t}_i=\mathbf{a'\lambda}_i\mathbf{t}_i=\lambda_i\mathbf{a't}_i=\lambda_i\left(\sum_sc_s\mathbf{t}_s'\right)\mathbf{t}_i=\lambda_ic_i=0
\end{align*} para $i=1,\dots,j-1,\:\lambda_i>0$, se tiene $c_1=\cdots=c_{j-1}=0$.

Entonces, la varianza será máxima si $c_j=1$ y $c_i=0$ para $i>j$, ya que \[ \var(\pm\mathbf{t}_j'\mathbf{X})=\lambda_j=\mathbf{c'c}\lambda_j=\sum_{i=j}^{k}c_i^2\lambda_j\ge\sum_{i=j}^{k}c_i^2\lambda_i=\var(\mathbf{a'X}), \] para todo $\mathbf{a}$ tal que $\mathbf{a'a}=1$ y $\cov(\mathbf{a'X}, Y_i)=0,\:i=1,\dots,j-1$, es decir, $Y_j=\pm\mathbf{t}_j'\mathbf{X}$ es una componente principal $j$-ésima (no necesariamente la única).

\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Corolario
\end{itemize}
Si $\lambda_1>\lambda_2>\cdots>\lambda_k$, entonces las componentes principales son únicas salvo digno.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Observación
\end{itemize}
Nótese que la componente principal $j$-ésima se obtiene multiplicando la fila $j$-ésima de $T'$ (la columna $j$-ésima de $T$) por $\mathbf{X}$, es decir, \[ Y_j=\mathbf{t}_j'\mathbf{X} \]donde $\mathbf{t}_j'=(t_{1,j},\dots,t_{k,j})$ es un vector propio unitario correspondiente al $j$-ésimo valor propio (vectores columna de $T$).

Además, $\var(Y_j)=\lambda_j$, y \[ \mathrm{traza}(V)=\sum_{j=1}^{k}\sigma_{j,j}=\sum_{j=1}^{k}\var(X_j)=\sum_{j=1}^{k}\var(Y_j)=\sum_{j=1}^{k}\lambda_j \](las matrices semejantes tienen las trazas iguales), es decir, la \lb{variabilidad} (información) de las variables originales es igual a la suma de las variabilidades de las componentes principales.

La \lb{cantidad de información} (\%) contenida en cada componente será \[ I_j=100\dfrac{\lambda_j}{\displaystyle\sum_{i=1}^{k}\lambda_i}\%. \]
Por esto, la \lb{traza} se usar como una medida unidimensional de la dispersión de una variable $k$-dimensional.

La otra medida es el \lb{determinante} de $V$ para el que también se verifica: \[ |V|=\lambda_1\cdots\lambda_k=|\cov(\mathbf{Y})| \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Observación
\end{itemize}
Otros autores llaman componentes principales a \[ \mathbf{Y}=T'(\mathbf{X-\mu}) \] con lo que, además, se consigue que sean centradas ($E[Y_j]=0$),
\begin{itemize}
	\item \lb{compronentes principales centradas}
\end{itemize}
También se pueden definir las \lb{componentes principales estandarizadas} \[ Z_j=\mathbf{t}_j'(\mathbf{X-\mu})\lambda_j^{-\frac{1}{2}} \] $(\mathbf{Z}=D^{-\frac{1}{2}}T'(\mathbf{X-\mu}))$ que además de ser centradas tendrán varianza 1.

Cuando hay \lb{valores propios iguales a cero} ($V$ es \lb{semidefinida positiva}) no suelen considerarse sus correspondientes componentes principales (degeneradas) y se puede conservar toda la información en las componentes principales de valores propios distintos de cero.

En este caso hay \lb{variables} que pueden obtenerse como \lb{combinación lineal de las restantes} (aunque no siempre pueden eliminarse del análisis).

Geométricamente, las \lb{componentes principales} se corresponden con los \lb{ejes principales del elipsoide de concentración}.

Como $\mathbf{Y}=T'\mathbf{X}$, podemos interpretar las componentes en función de los pesos que tengan en ellas las variables originales.

Si ponemos $\mathbf{X}$ en función de $\mathbf{Y}$ como $\mathbf{X}=T\mathbf{Y}$, entonces las variables originales se pueden interpretar en función de las componentes principales e incluso, podemos representar aproximadamente, las variables originales usando las dos (tres) primeras componentes.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Caso de normalidad
\end{itemize}
Si la población $\mathbf{X}$ es \lb{normal}, entonces las \lb{componentes principales} son \lb{normales} e \lb{independientes entre sí}, ya que en estas poblaciones equivalen los conceptos de independencia e incorrelación (independencia lineal) y $\mathbf{Z}$ será una normal estándar multivariante ($\mathcal{N}_k(0,I)$).
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $\mathbf{Y}$ son las \lb{componentes principales} obtenidas a partir de $\mathbf{X}$, entonces $\mathbf{X}$ es \lb{normal multivariante} si, y sólo si $Y_1,\dots,Y_k$ son \lb{independientes} y \lb{normales univariantes} para todo $j=1,\dots,k$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración.
\end{itemize}
La demostración es inmediata.

Esta propiedad puede ser utilizada para estudiar la normalidad multivariante a partir de un test de normalidad univariante sobre las componentes principales.

Incluso si la normal multivariante no es de rango completo ($V$ no es definida positiva), puede utilizarse con con las $m$ primeras componentes con valores propios distintos de cero (las otras serán degeneradas) coincidiendo $m$ con el rango de $V$.

\Ej

Para el \vea normal de media $\mu=(0,0)$ y matriz de covarianzas $V=\begin{pmatrix}
	1 & \tfrac{1}{2}\\
	\tfrac{1}{2} & 1
\end{pmatrix}$
\begin{lstlisting}
library("mvtnorm")
f <- function(x1, x2) dmvnorm(data.frame(x1, x2), mu, V)
V <- matrix(c(1, 1/2,
1/2, 1), nrow = 2, ncol = 2, byrow = TRUE)
mu <- c(0, 0)
x1 <- seq(-3, 3, length = 50)
x2 <- seq(-3, 3, length = 50)
z <- outer(x1, x2, f)
persp(x1, x2, z, xlab = 'x1', ylab = 'x2', zlab = 'f(x1, x2)', col = 'orange', main = "Función de densidad")
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.6\linewidth]{"Temas/Imágenes/Tema 4/screenshot004"}
\end{center}
\begin{lstlisting}
#Se fija la semilla para la generación aleatoria
set.seed(123)
d <- rmvnorm(50, mu, V)
plot(d, xlab = "X1", ylab = "X2", pch = 20, xlim = c(-3, 3), ylim = c(-3, 3), main = "Elipsoide de concentración")
hc <- function(x1, x2) (4/3)*x1^ 2 - (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length = 1000)
x2 <- seq(-3, 3, length = 1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = 4, add = T, col = 'red')
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.6\linewidth]{"Temas/Imágenes/Tema 4/screenshot005"}
\end{center}
Sus componentes principales se calcularán diagonalizando $V$ mediante \[ \left|V-\lambda I\right|=\begin{vmatrix}
	1-\lambda & 0.5\\
	0.5 & 1-\lambda
\end{vmatrix}=1-2\lambda+\lambda^2-\dfrac{1}{4}=0 \] que tiene soluciones \[ \lambda=\dfrac{2\pm\sqrt{4-4(1-\frac{1}{4})}}{2}=1\pm0.5, \]$\lambda_1=1.5$ y $\lambda_2=0.5$.

Y la \lb{primera componente} se obtendrá resolviendo $V\mathbf{v}=\lambda\mathbf{v}$ \[ \begin{pmatrix}
	1 & 0.5\\
	0.5 & 1
\end{pmatrix}\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}=1.5\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}\xRightarrow{\qquad}\begin{pmatrix}
-0.5x_1+0.5x_2\\
0.5x_1-0.5x_2
\end{pmatrix}=\begin{pmatrix}
0\\
0
\end{pmatrix} \]lo que da $x_1=x_2$, es decir, sus vectores propios son de la forma $\mathbf{v}=\alpha(1,1)'$.

Como usamos vectores normalizados (de norma 1), una primera componente valdrá \[ Y_1=\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)\dbinom{X_1}{X_2}=\dfrac{X_1+X_2}{\sqrt{2}} \]y su varianza es $\lambda_1=1.5$.

Análogamente, la segunda valdrá $Y_2=\dfrac{X_1-X_2}{\sqrt{2}}$ (ya que tiene que ser perpendicular a la primera) y tendrá varianza $\lambda_2=0.5$.

Es decir, tenemos \[ \begin{array}{l}
	Y_1=\dfrac{X_1+X_2}{\sqrt{2}}\\
	Y_2=\dfrac{X_1-X_2}{\sqrt{2}}\\
\end{array} \]por lo que \[ \mathbf{Y}=T'\mathbf{X}=\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
\end{pmatrix}\begin{pmatrix}
X_1\\
X_2
\end{pmatrix}. \]
\lb{Varianza total explicada} por cada componente:
\begin{itemize}
	\item La primera componente explicará un 75\% de la varianza total: \[ I_1=100\dfrac{\lambda_1}{\lambda_1+\lambda_2}\%=75\%. \]
	\item La segunda un 25\% de la varianza total: \[ I_2=100\dfrac{\lambda_2}{\lambda_1+\lambda_2}\%=25\%. \]
\end{itemize}
Como las varianzas iniciales son iguales, ambas tienen igual peso en las componentes con distinto signo en el caso de la segunda de ellas.

Aunque las varianzas iniciales sean todas iguales (1) las componentes principales tienen varianzas (en general) distintas.

Si $X_1$ fuese el peso de una persona y $X_2$ su altura (estandarizadas).
\begin{itemize}
	\item La primera componente se podría interpretar como lo \lb{grande} que es dicha persona.
	\item Mientras que la segunda estará relacionada con su \lb{constitución} ($Y_2$ grande significaría mucho peso y poca altura, es decir, complexión fuerte).
\end{itemize}
Despejando, se tiene \[ \begin{array}{l}
	X_1=\dfrac{Y_1+Y_2}{\sqrt{2}}\\
	X_2=\dfrac{Y_1-Y_2}{\sqrt{2}}\\
\end{array} \] lo que nos permite representar las variables $X_1,\:X_2$ en función de las componentes $Y_1,\:Y_2$.
\begin{itemize}
	\item $Y_1$ aumenta si lo hacen $X_1$ y $X_2$.
	\item $Y_2$ aumenta si aumenta $X_1$ y disminuye $X_2$.
	\item Estas relaciones servirán para interpretar (dar significado) a las componentes principales.
\end{itemize}
\subsection{¿Cómo realizamos estos cálculo en \textbf{\texttt{R}}?}
En primer lugar definimos e introducimos $V$
\begin{lstlisting}
V <- matrix(c(1, 1/2, 1/2, 1), nrow = 2, ncol = 2, byrow = TRUE)
V
\end{lstlisting}
\begin{verbatim}
##      [,1] [,2]
## [1,]  1.0  0.5
## [2,]  0.5  1.0
\end{verbatim}
Calculamos los valores y vectores propios:
\begin{lstlisting}
eigen(V)$values; eigen(V)$vectors
\end{lstlisting}
\begin{verbatim}
## [1] 1.5 0.5
##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068
\end{verbatim}
Podemos guardar la matriz $T$ de vectores propios:
\begin{lstlisting}
T <- eigen(V)$vectors
\end{lstlisting}
Los vectores normalizados aparecen en las columnas de $T$. Podemos comprobar que $T$ es una matriz ortogonal:
\begin{lstlisting}
t(T) %*% T
\end{lstlisting}
\begin{verbatim}
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
\end{verbatim}
donde \code{t(A)} es la traspuesta de \code{A} y \code{A \%*\% B} es el producto de las matrices \code{A} y \code{B} en \code{R}.

Podemos comprobar que $T$ diagonaliza a $V$:
\begin{lstlisting}
t(T) %*% V %*% T
\end{lstlisting}
\begin{verbatim}
##      [,1] [,2]
## [1,]  1.5  0.0
## [2,]  0.0  0.5
\end{verbatim}
lo que nos dará la matriz diagonal con los valores 1.5 y 0.5 en la diagonal.

Como $\mathbf{Y}=T'\mathbf{X}$, las componentes principales serán \[ \begin{array}{l}
	Y_1=0.7071068X_1+0.7071068X_2=\dfrac{X_1+X_2}{\sqrt{2}}\\
	Y_2=-0.7071068X_1+0.7071068X_2=-\dfrac{X_1+X_2}{\sqrt{2}}\\
\end{array} \]
Para calcular las informaciones contenidas en cada una (en tanto por 100) haremos:
\begin{lstlisting}
100*eigen(V)$values/sum(eigen(V)$values)
\end{lstlisting}
\begin{verbatim}
## [1] 75 25
\end{verbatim}
obteniendo el 75\% y el 25\%
\subsection{Desigualdades}
Si $Z$ es una variable aleatoria no negativa con media finita $E[Z]$ y $\epsilon>0$, entonces \[ \epsilon Pr[Z\ge\epsilon]=\epsilon\int_{[\epsilon,\infty)}\mathrm{d}F_Z(x)\le\int_{[\epsilon,\infty)}x\mathrm{d}F_Z(x)\le\int_{[0,\infty)}x\mathrm{d}F_Z(x)=E(Z) \] (donde $F_Z(x)=Pr[Z\le x]$ es su función de distribución), es decir \[ Pr[Z\ge \epsilon]\le\dfrac{E[Z]}{\epsilon}. \]
Si $X$ es una variable aleatoria con media finita $\mu=E[X]$ y varianza $\sigma^2=\var(X)>0$, entonces tomando $Z=\dfrac{(X-\mu)^2}{\sigma^2}\ge0$ y aplicando la desigualdad de Markov, tenemos \[ Pr\left[\dfrac{(X-\mu)^2}{\sigma^2}\ge\epsilon\right]\le\dfrac{1}{\epsilon} \]para todo $\epsilon>0$.
\subsubsection{Desigualdad de Chebyshev}
También se puede escribir como \[ Pr[(X-\mu)^2<\epsilon\sigma^2]\ge1-\dfrac{1}{\epsilon}, \] o como \[ Pr[|X-\mu|<r]\ge1-\dfrac{\sigma^2}{r^2}, \] para todo $r>0$.
\subsubsection{Desigualdad de Chebyshev multivariante}
Sea $\mathbf{X}=(X_1,\dots,X_k)'$ un \vea con vector de medias finito $\mu=E(\mathbf{X})$ y matriz de covarianzas definida positiva $V$, entonces \[ Pr[(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)\ge\epsilon]\le\dfrac{k}{\epsilon} \]para todo $\epsilon>0$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Consecuencias
\end{itemize}
La desigualdad también se puede escribir como \[ Pr[(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)<\epsilon]\ge1-\dfrac{k}{\epsilon}, \] para todo $\epsilon>0$.

En particular, para el elipsoide de concentración \[ E_k=\{x\in\R^k:(\mathbf{X}-\mu)V^{-1}(\mathbf{X}-\mu)\le k+2\}, \] obtenemos \[ Pr[\mathbf{X}\in E_k]\ge1-\dfrac{k}{k+2}=\dfrac{2}{k+2}. \]
Para obtener regiones con más datos podemos tomar $\epsilon=ck$, resultando \[ Pr[(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)<ck]\ge1-\dfrac{k}{\epsilon}=1-\dfrac{1}{c}=\dfrac{c-1}{c}. \]
La \va no negativa $Z=(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)$ se puede escribir como \[ (\mathbf{X}-\mu)'TD^{-1}T'(\mathbf{X}-\mu)=\left[D^{-\frac{1}{2}}T'(\mathbf{X}-\mu)\right]'[D^{-\frac{1}{2}}T'(\mathbf{X}-\mu)]=\mathbf{Z'Z}, \] donde $\mathbf{Z}=D^{-\frac{1}{2}}T'(\mathbf{X}-\mu)\:(\mathbf{Z}=(Z_1,\dots,Z_k)')$.

Si $X$ es \lb{normal}, entonces $Z_1,\dots,Z_k$ son normales estándar independientes y \[ Z=\sum_{i=1}^{k}Z_i^2 \]sigue una \lb{distribución chi-cuadrado} con $k$ grados de libertad (ya que es la suma de $k$ normales $\mathcal{N}(0,1)$ independientes).
\subsection{Propiedades}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $\mathbf{Y}$ son las componentes principales obtenidas a partir de $\mathbf{X}$, entonces \[ \begin{array}{l}
	\cov(\mathbf{X,Y})=TD\\
	\corr(\mathbf{X,Y})=\mathrm{diag}(V)^{-\frac{1}{2}}TD^{\frac{1}{2}}
\end{array} \] donde $\mathrm{diag}(V)=\mathrm{diag}(\sigma_1^2,\dots,\sigma_k^2)$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
En primer lugar señalaremos que \[ \cov(\mathbf{X,Y})=\cov(\mathbf{X},T'\mathbf{X})=VT \] y, como $T'VT=D$ y $T$ es ortogonal, entonces $VT=TD$ y $\cov(\mathbf{X,Y})=TD$.

Por otro lado se tiene que como \[ \corr(X_i,Y_j)=\dfrac{\cov(X_i,Y_j)}{\sigma_i\lambda_j^{\frac{1}{2}}}, \]entonces \[ \corr(\mathbf{X,Y})=\mathrm{diag}(V)^{-\frac{1}{2}}\cov(\mathbf{X,Y})D^{-\frac{1}{2}} \]y\[ \corr(\mathbf{X,Y})=\mathrm{diag}(V)^{-\frac{1}{2}}TDD^{-\frac{1}{2}}=\mathrm{diag}(V)^{-\frac{1}{2}}TD^{\frac{1}{2}} \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Corolario
\end{itemize}
En las condiciones de la proposición anterior se tiene: \[ \begin{array}{l}
	\cov(X_i,Y_j)=t_{i,j}\lambda_j\\
	\corr(X_i,Y_j)=\dfrac{t_{i,j}}{\sigma_i}\lambda_j^{\frac{1}{2}}
\end{array} \] para todo $i,j$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Se denomina \lb{matriz de saturaciones} a \[ A=\corr(\mathbf{X,Y}). \]