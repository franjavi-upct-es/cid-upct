\section{Análisis de componentes principales}
\subsection{Introducción}
\begin{enumerate}[label=\arabic*)]
	\item Objetivo
\end{enumerate}
Simplificar la representación de datos multidimensionales al transformarlos en un nuevo conjunto de variables llamadas \lb{componentes principales}.
\begin{itemize}
	\item \lb{Reducción de dimensionalidad:} a veces dispondremos de muchas variables y simplemente se querrá disminuir el número de variables perdiendo la menor información posible (\lb{compresión de datos}).
	\begin{itemize}
		\item Útil en aplicaciones de almacenamiento y transmisión de información.
	\end{itemize}
	\item \lb{Visualización de datos:} al proyectar los datos en un espacio de menor dimensión, es más fácil representar gráficamente la estructura subyacente de los datos, lo que puede ayudar a identificar patrones, agrupaciones o relaciones (\lb{extracción de características}).
	\item \lb{Eliminación de multicolinealidad:} en análisis de regresión y otros contextos, la multicolinealidad (alta correlación entre variables independientes) puede ser problemática.
	\begin{itemize}
		\item En estos casos puede ayudar a reducir la multicolinealidad el \lb{transformar las variables originales} en un conjunto de variables no correlacionadas (las \lb{componentes principales}).
	\end{itemize}
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Planteamiento desde el punto de vista teórico
\end{itemize}
La idea es \lb{resumir la información} de un \vea (v.a.) $k$-dimensional $\mathbf{X}=(X_1,\dots,X_k)'$ (recordemos que $A'$ denota la traspuesta de $A$, es decir, $\mathbf{X}$ es un vector columna) en unas \lb{pocas variables} que proporcionen la información más relevante.

Se puede dar una aproximación geométrica mediante el concepto de \lb{elipsoide de concentración}.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Si $\mathbf{X}$ es un vector aleatorio de dimensión $k$, media $\mu$ y su matriz de covarianzas $V=(\sigma_{i,j})$ definida positiva, se define el \lb{elipsoide de concentración} de $\mathbf{X}$ como \[ E_k=\{\mathbf{x}\in\R^k:(\mathbf{x}-\mu)'V^{-1}(\mathbf{x}-\mu)\le k+2\}. \]
En la definición del elipsoide interviene la \lb{distancia de Mahalanobis basada en la matriz $V$} entre $\mathbf{x}$ y la media $\mu$ dada por \[ d_V(\mathbf{x},\mu)=\sqrt{(\mathbf{x}-\mu)'V^{-1}(\mathbf{x}-\mu)}. \]
Esta distancia al cuadrado se puede calcular en \code{R} con \code{mahalanobis(x, mu, V)}.

Además, si $X$ es \lb{normal}, el elipsoide se puede definir a partir de las \lb{curvas de nivel de la función de densidad} ($f(x)=cte$.) ya que \[ f(x)=\dfrac{1}{\sqrt{|V|(2\pi)^k}}\exp\left(-\dfrac{1}{2}(\mathbf{x}-\mu)'V^{-1}(\mathbf{x}-\mu)\right). \]
Una parte de los individuos (puntos estarán dentro de este elipsoide).

Si queremos \lb{distinguirlos con una única variable}, parece claro que lo mejor sería proyectarlos sobre el eje mayor del elipsoide.

Por ejemplo, para una \lb{normal bivariante} \[ \mathcal{N}_2\left(\mu=\binom{0}{0},V=\begin{pmatrix}
	1 & \tfrac{1}{2}\\
	\tfrac{1}{2} & 1
\end{pmatrix}\right) \] se tiene que \[ (x_1,x_2)\begin{pmatrix}
1 & \tfrac{1}{2}\\
\tfrac{1}{2} & 1
\end{pmatrix}^{-1}\binom{x_1}{x_2}=\dfrac{4}{3}x_1^2-\dfrac{4}{3}x_1x_2+\dfrac{4}{3}x_2^2 \] por lo que el \lb{elipsoide de concentración} sería \[ \dfrac{4}{3}x_1^2-\dfrac{4}{3}x_1x_2+\dfrac{4}{3}x_2^2\le4 \]

\begin{itemize}
	\item \lb{Elipsoide de concentración} para la normal bivariante con medias 0, varianzas 1 y correlación $\dfrac{1}{2}:$
	
	\begin{lstlisting}
hc <- function(x1, x2) (4/3)*x1^ 2- (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length =1000)
x2 <- seq(-3, 3, length =1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = 4)
title(main = "level = 4")
	\end{lstlisting}
	\begin{center}
		\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 4/screenshot001"}
	\end{center}
	\item \lb{Elipsoides obtenidos con otros niveles} (circunferencias de Mahalanobis):
	\begin{lstlisting}
hc <- function(x1, x2) (4/3)*x1^ 2- (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length =1000)
x2 <- seq(-3, 3, length =1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = c(1:6))
title(main = "level = 1, ..., 6")
	\end{lstlisting}
	\begin{center}
		\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 4/screenshot002"}
	\end{center}
\end{itemize}

\begin{minipage}{0.45\textwidth}
	Si queremos \lb{reducir las dos variables a solo una}, la mejor proyección, es decir la que mejor separa los puntos (\lb{varianza máxima}), es la proporcionada por el \lb{eje principal del elipsoide} (o curvas de nivel de la normal).
	
	En este ejemplo viene dado por la recta: \[ x_2=x_1. \]
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{lstlisting}
hc <- function(x1, x2) (4/3)*x1^ 2- (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length =1000)
x2 <- seq(-3, 3, length =1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = c(1:6))
abline(a = 0, b = 1, col = "red", lty = 2)
abline(a = 0, b = -1, col = "blue", lty = 2)
title(main = "level = 1, ..., 6")
\end{lstlisting}
\end{minipage}
\begin{flushright}
\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 4/screenshot003"}
\end{flushright}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Objetivo
\end{itemize}
\lb{Transformar} un conjunto de $k$ variables interrelacionadas entre sí en un nuevo conjunto con un número menor de variables:
\begin{itemize}
	\item las \lb{componentes principales}
\end{itemize}
De manera que estas nuevas variables:
\begin{itemize}
	\item sean \lb{ortogonales entre sí}.
	\item capturen la \lb{mayor variabilidad} de las variables.
	\item \lb{expliquen la mayor parte de la variabilidad} de las variables originales.
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Planteamiento teórico
\end{itemize}
Supongamos que $\mathbf{X}=(X_1,\dots,X_k)'$ es un \vea $k$-dimensional con vector de medias $\mu$ y matriz de covarianzas $V$ \lb{semidefinida positiva}.

Entonces la \lb{primera componente principal} será la \va unidimensional \[ Y_1=a_1X_1+\cdots+a_kX_k \] con $a_1^2+\cdots+a_k^2=1$ cuya \lb{varianza es máxima}.
\begin{itemize}
	\item Si no se normaliza la combinación lineal, la variable $Y_1$ puede tener varianza tan grande como queramos.
	\item Geométricamente, hacemos un cambio de variable (primer eje) para que la dispersión sea máxima y la normalización equivale a mantener la escala original (proyectar).
\end{itemize}
El problema puede expresarse de la forma siguiente: \[ \begin{rcases}
	\max&\var(a'\mathbf{X})\\
	\text{s.a.} & \mathbf{a'a}=1
\end{rcases} \] donde $\mathbf{a}=(a_1,\dots,a_k)'\in\R^k$.

Una vez calculada una primera componente principal $Y_1$, la \lb{segunda componente principal} $Y_2$ debe verificar $\cov(Y_1,Y_2)=0$ (no debe contener información ya incluida en $Y_1$) y debe tener la \lb{varianza máxima}, es decir, \[ \begin{rcases}
	\max & \var(\mathbf{a'X})\\
	\text{s.a.} & \mathbf{a'a}=1\\
	&\cov(Y_1,\mathbf{a'X})=0
\end{rcases} \]
Así, sucesivamente, por inducción, se definen las \lb{siguientes componentes principales} $(Y_j)$ como la (una) solución de 
\[ \begin{rcases}
	\max & \var(\mathbf{a'X})\\
	\text{s.a.} & \mathbf{a'a}=1\\
	&\cov(Y_1,\mathbf{a'X})=0,\quad i=1,\dots,j-1
\end{rcases} \]
\begin{itemize}[label=\color{lightblue}\textbullet]
	\item La solución general viene dada en el teorema siguiente que prueba la \lb{existencia de las (unas) componentes principales} y muestra \lb{cómo calcularlas}.
	\item Además, se demuestra que las componentes principales \lb{no son únicas} (puede haber más soluciones).
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Teorema de existencia
\end{itemize}
Si $\mathbf{X}$ es un \vea $k$-dimensional con matriz de covarianzas $V$ \lb{definida positiva}, las (unas) \lb{componentes principales} se obtienen como \[ \mathbf{Y}=(Y_1,\dots,Y_k)'=T'\mathbf{X}=\begin{pmatrix}
	t_{1,1} & \cdots & t_{k,1} \\
	\cdots & \cdots & \cdots \\
	t_{1,k} & \cdots & t_{k,k}
\end{pmatrix}\begin{pmatrix}
X_1\\
\cdots\\
X_k
\end{pmatrix}, \] donde $T$ es una \lb{matriz ortogonal} $(T'T=TT'=I)$ tal que \[ T'VT=D=\mathrm{diag}(\lambda_1,\dots,\lambda_k) \]con $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_k>0$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
Como $V$ es una matriz \lb{simétrica} y \lb{definida positiva}, existe una matriz $T=(t_{i,j})$ \lb{ortogonal} $(T'T=TT'=I)$ tal que \[ T'VT=D=\mathrm{diag}(\lambda_1,\dots,\lambda_k) \] con los valores propios verificando $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_k>0$.

De esta forma, si \[ \mathbf{Y}=(Y_1,\dots,Y_k)'=T'\mathbf{X}=\begin{pmatrix}
	t_{1,1} & \cdots & t_{k,1} \\
	\cdots & \cdots & \cdots \\
	t_{1,k} & \cdots & t_{k,k}
\end{pmatrix}\begin{pmatrix}
	X_1\\
	\cdots\\
	X_k
\end{pmatrix} \] entonces $Y_1,\dots,Y_k$ verifican que \[ \cov(\mathbf{Y})=\cov(T'\mathbf{X})=E[T'(\mathbf{X-\mu})(\mathbf{X}-\mu)'T]=T'VT=D, \]lo que indica que $\cov(Y_i,Y_j)=0$ para $i\neq j$ y $\var(Y_j)=\lambda_j$.

Para comprobar que $Y_1$ es una primera componente principal, supongamos que $\mathbf{a'X}$ es una combinación lineal con $\mathbf{a'a}=1$.

Las columnas de la matriz $T$ corresponden a los vectores propios $\mathbf{t}_i$ asociados a los autovalores $\lambda_i,T=(\mathbf{t}_1|\cdots|\mathbf{t}_k)$, y como los vectores propios son una base, existirán $c_1,\dots,c_k$ números reales tales que \[ \mathbf{a}=c_1\mathbf{t}_1+\cdots+c_k\mathbf{t}_k=\sum_{i=1}^{k}c_i\mathbf{t}_i. \]
Con lo que 
\begin{align*}
	\var(\mathbf{a'X})&=E[\mathbf{a'(X-\mu)(X-\mu)'a}]=\mathbf{a}'\cov(\mathbf{X})\mathbf{a}=\mathbf{a'}V\mathbf{a}\\
	&=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)V\left(\sum_{i=1}^{k}c_i\mathbf{t}_i\right)=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)\left(\sum_{j=1}^{k}c_jV\mathbf{t}_j\right)\\
	&=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)\left(\sum_{j=1}^{k}c_j\lambda_j\mathbf{t}_j\right)=\sum_{i,j}c_ic_j\lambda_j\mathbf{t}_i'\mathbf{t}_j=\sum_{i=1}^{k}c_i^2\lambda_i
\end{align*}
Y, como \[ \mathbf{a'a}=\left(\sum_{i=1}^{k}c_i\mathbf{t}_i'\right)\left(\sum_{j=1}^{k}c_j\mathbf{t}_j\right)=\sum_{i,j}c_ic_j\mathbf{t}_i'\mathbf{t}_j=\sum_{i=1}^{k}c_i^2=\mathbf{c'c}=1, \] con $\mathbf{c}=(c_1,\dots,c_k)'$, la varianza será máxima si $x_1^2=1,c_2=0,\dots,c_k=0$ ya que \[ \var(\pm\mathbf{t}_1'\mathbf{X})=\lambda_1=\mathbf{c'c}\lambda_1=\sum_{i=1}^{k}c_i^2\lambda_1\ge\sum_{i=1}^{k}c_i^2\lambda_i=\var(\mathbf{a'X}), \]para todo $\mathbf{a}$ tal que $\mathbf{a'a}=1$, es decir, $Y_1=\pm \mathbf{t}_1'\mathbf{X}$ es una primera componente principal (puede haber otras soluciones si $\lambda_1=\lambda_2$).

Por inducción, supongamos que $Y_1=\mathbf{t}_1'\mathbf{X},\dots,Y_{j-1}\mathbf{X}$ son las primeras ($j-1$) componentes principales.

Y veamos que $Y_j=\mathbf{t}_j'\mathbf{X}$ es la (una) solución de \[ \begin{rcases}
	\max & \var(\mathbf{a'X})\\
	\text{s.a.} & \mathbf{a'a}=1\\
	&\cov(Y_1,\mathbf{a'X})=0,\quad i=1,\dots,j-1
\end{rcases} \]
Como se debe verificar \begin{align*}
	\cov(\mathbf{a'X},Y_i) & = \cov(\mathbf{a'X},\mathbf{t}_i'\mathbf{X})=E[\mathbf{a'(X-\mu)(X-\mu)'t}_i]=\mathbf{a}'\cov(\mathbf{X})\mathbf{t}_i\\
	&=\mathbf{a'}V\mathbf{t}_i=\mathbf{a'\lambda}_i\mathbf{t}_i=\lambda_i\mathbf{a't}_i=\lambda_i\left(\sum_sc_s\mathbf{t}_s'\right)\mathbf{t}_i=\lambda_ic_i=0
\end{align*} para $i=1,\dots,j-1,\:\lambda_i>0$, se tiene $c_1=\cdots=c_{j-1}=0$.

Entonces, la varianza será máxima si $c_j=1$ y $c_i=0$ para $i>j$, ya que \[ \var(\pm\mathbf{t}_j'\mathbf{X})=\lambda_j=\mathbf{c'c}\lambda_j=\sum_{i=j}^{k}c_i^2\lambda_j\ge\sum_{i=j}^{k}c_i^2\lambda_i=\var(\mathbf{a'X}), \] para todo $\mathbf{a}$ tal que $\mathbf{a'a}=1$ y $\cov(\mathbf{a'X}, Y_i)=0,\:i=1,\dots,j-1$, es decir, $Y_j=\pm\mathbf{t}_j'\mathbf{X}$ es una componente principal $j$-ésima (no necesariamente la única).

\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Corolario
\end{itemize}
Si $\lambda_1>\lambda_2>\cdots>\lambda_k$, entonces las componentes principales son únicas salvo digno.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Observación
\end{itemize}
Nótese que la componente principal $j$-ésima se obtiene multiplicando la fila $j$-ésima de $T'$ (la columna $j$-ésima de $T$) por $\mathbf{X}$, es decir, \[ Y_j=\mathbf{t}_j'\mathbf{X} \]donde $\mathbf{t}_j'=(t_{1,j},\dots,t_{k,j})$ es un vector propio unitario correspondiente al $j$-ésimo valor propio (vectores columna de $T$).

Además, $\var(Y_j)=\lambda_j$, y \[ \mathrm{traza}(V)=\sum_{j=1}^{k}\sigma_{j,j}=\sum_{j=1}^{k}\var(X_j)=\sum_{j=1}^{k}\var(Y_j)=\sum_{j=1}^{k}\lambda_j \](las matrices semejantes tienen las trazas iguales), es decir, la \lb{variabilidad} (información) de las variables originales es igual a la suma de las variabilidades de las componentes principales.

La \lb{cantidad de información} (\%) contenida en cada componente será \[ I_j=100\dfrac{\lambda_j}{\displaystyle\sum_{i=1}^{k}\lambda_i}\%. \]
Por esto, la \lb{traza} se usar como una medida unidimensional de la dispersión de una variable $k$-dimensional.

La otra medida es el \lb{determinante} de $V$ para el que también se verifica: \[ |V|=\lambda_1\cdots\lambda_k=|\cov(\mathbf{Y})| \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Observación
\end{itemize}
Otros autores llaman componentes principales a \[ \mathbf{Y}=T'(\mathbf{X-\mu}) \] con lo que, además, se consigue que sean centradas ($E[Y_j]=0$),
\begin{itemize}
	\item \lb{compronentes principales centradas}
\end{itemize}
También se pueden definir las \lb{componentes principales estandarizadas} \[ Z_j=\mathbf{t}_j'(\mathbf{X-\mu})\lambda_j^{-\frac{1}{2}} \] $(\mathbf{Z}=D^{-\frac{1}{2}}T'(\mathbf{X-\mu}))$ que además de ser centradas tendrán varianza 1.

Cuando hay \lb{valores propios iguales a cero} ($V$ es \lb{semidefinida positiva}) no suelen considerarse sus correspondientes componentes principales (degeneradas) y se puede conservar toda la información en las componentes principales de valores propios distintos de cero.

En este caso hay \lb{variables} que pueden obtenerse como \lb{combinación lineal de las restantes} (aunque no siempre pueden eliminarse del análisis).

Geométricamente, las \lb{componentes principales} se corresponden con los \lb{ejes principales del elipsoide de concentración}.

Como $\mathbf{Y}=T'\mathbf{X}$, podemos interpretar las componentes en función de los pesos que tengan en ellas las variables originales.

Si ponemos $\mathbf{X}$ en función de $\mathbf{Y}$ como $\mathbf{X}=T\mathbf{Y}$, entonces las variables originales se pueden interpretar en función de las componentes principales e incluso, podemos representar aproximadamente, las variables originales usando las dos (tres) primeras componentes.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Caso de normalidad
\end{itemize}
Si la población $\mathbf{X}$ es \lb{normal}, entonces las \lb{componentes principales} son \lb{normales} e \lb{independientes entre sí}, ya que en estas poblaciones equivalen los conceptos de independencia e incorrelación (independencia lineal) y $\mathbf{Z}$ será una normal estándar multivariante ($\mathcal{N}_k(0,I)$).
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $\mathbf{Y}$ son las \lb{componentes principales} obtenidas a partir de $\mathbf{X}$, entonces $\mathbf{X}$ es \lb{normal multivariante} si, y sólo si $Y_1,\dots,Y_k$ son \lb{independientes} y \lb{normales univariantes} para todo $j=1,\dots,k$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración.
\end{itemize}
La demostración es inmediata.

Esta propiedad puede ser utilizada para estudiar la normalidad multivariante a partir de un test de normalidad univariante sobre las componentes principales.

Incluso si la normal multivariante no es de rango completo ($V$ no es definida positiva), puede utilizarse con con las $m$ primeras componentes con valores propios distintos de cero (las otras serán degeneradas) coincidiendo $m$ con el rango de $V$.

\Ej

Para el \vea normal de media $\mu=(0,0)$ y matriz de covarianzas $V=\begin{pmatrix}
	1 & \tfrac{1}{2}\\
	\tfrac{1}{2} & 1
\end{pmatrix}$

\hspace{1cm}

\begin{lstlisting}
library("mvtnorm")
f <- function(x1, x2) dmvnorm(data.frame(x1, x2), mu, V)
V <- matrix(c(1, 1/2,
1/2, 1), nrow = 2, ncol = 2, byrow = TRUE)
mu <- c(0, 0)
x1 <- seq(-3, 3, length = 50)
x2 <- seq(-3, 3, length = 50)
z <- outer(x1, x2, f)
persp(x1, x2, z, xlab = 'x1', ylab = 'x2', zlab = 'f(x1, x2)', col = 'orange', main = "Función de densidad")
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.6\linewidth]{"Temas/Imágenes/Tema 4/screenshot004"}
\end{center}

\pagebreak

\begin{lstlisting}
#Se fija la semilla para la generación aleatoria
set.seed(123)
d <- rmvnorm(50, mu, V)
plot(d, xlab = "X1", ylab = "X2", pch = 20, xlim = c(-3, 3), ylim = c(-3, 3), main = "Elipsoide de concentración")
hc <- function(x1, x2) (4/3)*x1^ 2 - (4/3)*x1*x2 + (4/3)*x2^ 2
x1 <- seq(-3, 3, length = 1000)
x2 <- seq(-3, 3, length = 1000)
z <- outer(x1, x2, hc)
contour(x1, x2, z, levels = 4, add = T, col = 'red')
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.6\linewidth]{"Temas/Imágenes/Tema 4/screenshot005"}
\end{center}
Sus componentes principales se calcularán diagonalizando $V$ mediante \[ \left|V-\lambda I\right|=\begin{vmatrix}
	1-\lambda & 0.5\\
	0.5 & 1-\lambda
\end{vmatrix}=1-2\lambda+\lambda^2-\dfrac{1}{4}=0 \] que tiene soluciones \[ \lambda=\dfrac{2\pm\sqrt{4-4(1-\frac{1}{4})}}{2}=1\pm0.5, \]$\lambda_1=1.5$ y $\lambda_2=0.5$.

Y la \lb{primera componente} se obtendrá resolviendo $V\mathbf{v}=\lambda\mathbf{v}$ \[ \begin{pmatrix}
	1 & 0.5\\
	0.5 & 1
\end{pmatrix}\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}=1.5\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}\xRightarrow{\qquad}\begin{pmatrix}
-0.5x_1+0.5x_2\\
0.5x_1-0.5x_2
\end{pmatrix}=\begin{pmatrix}
0\\
0
\end{pmatrix} \]lo que da $x_1=x_2$, es decir, sus vectores propios son de la forma $\mathbf{v}=\alpha(1,1)'$.

Como usamos vectores normalizados (de norma 1), una primera componente valdrá \[ Y_1=\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)\dbinom{X_1}{X_2}=\dfrac{X_1+X_2}{\sqrt{2}} \]y su varianza es $\lambda_1=1.5$.

Análogamente, la segunda valdrá $Y_2=\dfrac{X_1-X_2}{\sqrt{2}}$ (ya que tiene que ser perpendicular a la primera) y tendrá varianza $\lambda_2=0.5$.

Es decir, tenemos \[ \begin{array}{l}
	Y_1=\dfrac{X_1+X_2}{\sqrt{2}}\\
	Y_2=\dfrac{X_1-X_2}{\sqrt{2}}\\
\end{array} \]por lo que \[ \mathbf{Y}=T'\mathbf{X}=\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
\end{pmatrix}\begin{pmatrix}
X_1\\
X_2
\end{pmatrix}. \]
\lb{Varianza total explicada} por cada componente:
\begin{itemize}
	\item La primera componente explicará un 75\% de la varianza total: \[ I_1=100\dfrac{\lambda_1}{\lambda_1+\lambda_2}\%=75\%. \]
	\item La segunda un 25\% de la varianza total: \[ I_2=100\dfrac{\lambda_2}{\lambda_1+\lambda_2}\%=25\%. \]
\end{itemize}
Como las varianzas iniciales son iguales, ambas tienen igual peso en las componentes con distinto signo en el caso de la segunda de ellas.

Aunque las varianzas iniciales sean todas iguales (1) las componentes principales tienen varianzas (en general) distintas.

Si $X_1$ fuese el peso de una persona y $X_2$ su altura (estandarizadas).
\begin{itemize}
	\item La primera componente se podría interpretar como lo \lb{grande} que es dicha persona.
	\item Mientras que la segunda estará relacionada con su \lb{constitución} ($Y_2$ grande significaría mucho peso y poca altura, es decir, complexión fuerte).
\end{itemize}
Despejando, se tiene \[ \begin{array}{l}
	X_1=\dfrac{Y_1+Y_2}{\sqrt{2}}\\
	X_2=\dfrac{Y_1-Y_2}{\sqrt{2}}\\
\end{array} \] lo que nos permite representar las variables $X_1,\:X_2$ en función de las componentes $Y_1,\:Y_2$.
\begin{itemize}
	\item $Y_1$ aumenta si lo hacen $X_1$ y $X_2$.
	\item $Y_2$ aumenta si aumenta $X_1$ y disminuye $X_2$.
	\item Estas relaciones servirán para interpretar (dar significado) a las componentes principales.
\end{itemize}
\subsection{¿Cómo realizamos estos cálculo en \textbf{\texttt{R}}?}
En primer lugar definimos e introducimos $V$
\begin{lstlisting}
V <- matrix(c(1, 1/2, 1/2, 1), nrow = 2, ncol = 2, byrow = TRUE)
V
\end{lstlisting}
\begin{verbatim}
##      [,1] [,2]
## [1,]  1.0  0.5
## [2,]  0.5  1.0
\end{verbatim}
Calculamos los valores y vectores propios:
\begin{lstlisting}
eigen(V)$values; eigen(V)$vectors
\end{lstlisting}
\begin{verbatim}
## [1] 1.5 0.5
##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068
\end{verbatim}
Podemos guardar la matriz $T$ de vectores propios:
\begin{lstlisting}
T <- eigen(V)$vectors
\end{lstlisting}
Los vectores normalizados aparecen en las columnas de $T$. Podemos comprobar que $T$ es una matriz ortogonal:
\begin{lstlisting}
t(T) %*% T
\end{lstlisting}
\begin{verbatim}
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
\end{verbatim}
donde \code{t(A)} es la traspuesta de \code{A} y \code{A \%*\% B} es el producto de las matrices \code{A} y \code{B} en \code{R}.

Podemos comprobar que $T$ diagonaliza a $V$:
\begin{lstlisting}
t(T) %*% V %*% T
\end{lstlisting}
\begin{verbatim}
##      [,1] [,2]
## [1,]  1.5  0.0
## [2,]  0.0  0.5
\end{verbatim}
lo que nos dará la matriz diagonal con los valores 1.5 y 0.5 en la diagonal.

Como $\mathbf{Y}=T'\mathbf{X}$, las componentes principales serán \[ \begin{array}{l}
	Y_1=0.7071068X_1+0.7071068X_2=\dfrac{X_1+X_2}{\sqrt{2}}\\
	Y_2=-0.7071068X_1+0.7071068X_2=-\dfrac{X_1+X_2}{\sqrt{2}}\\
\end{array} \]
Para calcular las informaciones contenidas en cada una (en tanto por 100) haremos:
\begin{lstlisting}
100*eigen(V)$values/sum(eigen(V)$values)
\end{lstlisting}
\begin{verbatim}
## [1] 75 25
\end{verbatim}
obteniendo el 75\% y el 25\%
\subsection{Desigualdades}
Si $Z$ es una variable aleatoria no negativa con media finita $E[Z]$ y $\epsilon>0$, entonces \[ \epsilon Pr[Z\ge\epsilon]=\epsilon\int_{[\epsilon,\infty)}\mathrm{d}F_Z(x)\le\int_{[\epsilon,\infty)}x\mathrm{d}F_Z(x)\le\int_{[0,\infty)}x\mathrm{d}F_Z(x)=E(Z) \] (donde $F_Z(x)=Pr[Z\le x]$ es su función de distribución), es decir \[ Pr[Z\ge \epsilon]\le\dfrac{E[Z]}{\epsilon}. \]
Si $X$ es una variable aleatoria con media finita $\mu=E[X]$ y varianza $\sigma^2=\var(X)>0$, entonces tomando $Z=\dfrac{(X-\mu)^2}{\sigma^2}\ge0$ y aplicando la desigualdad de Markov, tenemos \[ Pr\left[\dfrac{(X-\mu)^2}{\sigma^2}\ge\epsilon\right]\le\dfrac{1}{\epsilon} \]para todo $\epsilon>0$.
\subsubsection{Desigualdad de Chebyshev}
También se puede escribir como \[ Pr[(X-\mu)^2<\epsilon\sigma^2]\ge1-\dfrac{1}{\epsilon}, \] o como \[ Pr[|X-\mu|<r]\ge1-\dfrac{\sigma^2}{r^2}, \] para todo $r>0$.
\subsubsection{Desigualdad de Chebyshev multivariante}
Sea $\mathbf{X}=(X_1,\dots,X_k)'$ un \vea con vector de medias finito $\mu=E(\mathbf{X})$ y matriz de covarianzas definida positiva $V$, entonces \[ Pr[(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)\ge\epsilon]\le\dfrac{k}{\epsilon} \]para todo $\epsilon>0$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Consecuencias
\end{itemize}
La desigualdad también se puede escribir como \[ Pr[(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)<\epsilon]\ge1-\dfrac{k}{\epsilon}, \] para todo $\epsilon>0$.

En particular, para el elipsoide de concentración \[ E_k=\{x\in\R^k:(\mathbf{X}-\mu)V^{-1}(\mathbf{X}-\mu)\le k+2\}, \] obtenemos \[ Pr[\mathbf{X}\in E_k]\ge1-\dfrac{k}{k+2}=\dfrac{2}{k+2}. \]
Para obtener regiones con más datos podemos tomar $\epsilon=ck$, resultando \[ Pr[(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)<ck]\ge1-\dfrac{k}{\epsilon}=1-\dfrac{1}{c}=\dfrac{c-1}{c}. \]
La \va no negativa $Z=(\mathbf{X}-\mu)'V^{-1}(\mathbf{X}-\mu)$ se puede escribir como \[ (\mathbf{X}-\mu)'TD^{-1}T'(\mathbf{X}-\mu)=\left[D^{-\frac{1}{2}}T'(\mathbf{X}-\mu)\right]'[D^{-\frac{1}{2}}T'(\mathbf{X}-\mu)]=\mathbf{Z'Z}, \] donde $\mathbf{Z}=D^{-\frac{1}{2}}T'(\mathbf{X}-\mu)\:(\mathbf{Z}=(Z_1,\dots,Z_k)')$.

Si $X$ es \lb{normal}, entonces $Z_1,\dots,Z_k$ son normales estándar independientes y \[ Z=\sum_{i=1}^{k}Z_i^2 \]sigue una \lb{distribución chi-cuadrado} con $k$ grados de libertad (ya que es la suma de $k$ normales $\mathcal{N}(0,1)$ independientes).
\subsection{Propiedades}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $\mathbf{Y}$ son las componentes principales obtenidas a partir de $\mathbf{X}$, entonces \[ \begin{array}{l}
	\cov(\mathbf{X,Y})=TD\\
	\corr(\mathbf{X,Y})=\mathrm{diag}(V)^{-\frac{1}{2}}TD^{\frac{1}{2}}
\end{array} \] donde $\mathrm{diag}(V)=\mathrm{diag}(\sigma_1^2,\dots,\sigma_k^2)$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
En primer lugar señalaremos que \[ \cov(\mathbf{X,Y})=\cov(\mathbf{X},T'\mathbf{X})=VT \] y, como $T'VT=D$ y $T$ es ortogonal, entonces $VT=TD$ y $\cov(\mathbf{X,Y})=TD$.

Por otro lado se tiene que como \[ \corr(X_i,Y_j)=\dfrac{\cov(X_i,Y_j)}{\sigma_i\lambda_j^{\frac{1}{2}}}, \]entonces \[ \corr(\mathbf{X,Y})=\mathrm{diag}(V)^{-\frac{1}{2}}\cov(\mathbf{X,Y})D^{-\frac{1}{2}} \]y\[ \corr(\mathbf{X,Y})=\mathrm{diag}(V)^{-\frac{1}{2}}TDD^{-\frac{1}{2}}=\mathrm{diag}(V)^{-\frac{1}{2}}TD^{\frac{1}{2}} \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Corolario
\end{itemize}
En las condiciones de la proposición anterior se tiene: \[ \begin{array}{l}
	\cov(X_i,Y_j)=t_{i,j}\lambda_j\\
	\corr(X_i,Y_j)=\dfrac{t_{i,j}}{\sigma_i}\lambda_j^{\frac{1}{2}}
\end{array} \] para todo $i,j$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Se denomina \lb{matriz de saturaciones} a \[ A=\corr(\mathbf{X,Y}). \]
\Ej

Para el \vea normal de media $\mu=(0,0)$ y matriz de covarianzas $V=\begin{pmatrix}
	1 & \tfrac{1}{2}\\
	\tfrac{1}{2} & 1
\end{pmatrix}$, se obtiene \[ T=(\mathbf{t}_1|\mathbf{t}_2)=\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
\end{pmatrix},\quad D=\begin{pmatrix}
\lambda_1 & 0\\
0 & \lambda_2
\end{pmatrix}=\begin{pmatrix}
1.5 & 0\\
0 & 0.5
\end{pmatrix} \]por lo que la matriz de saturaciones valdrá: \[ A=\mathrm{diag}(V)^{-\frac{1}{2}}TD^{\frac{1}{2}}=\dfrac{1}{2}\begin{pmatrix}
\sqrt{3} & 1\\
\sqrt{3} & -1\\
\end{pmatrix}=\begin{pmatrix}
0.86603 & 0.5\\
0.86603 & -0.5\\
\end{pmatrix} \]

Nótese que:
\begin{itemize}
	\item La primera componente explica un 75\% (0.866\$\^\,2 \$ 100) de las variables $X_1$ y $X_2$.
	\item Mientras que la segunda solo un 25\%.
\end{itemize}
Las saturaciones y sus caudrados suelen representarse en tablas de la forma siguiente:
\[ \begin{array}{c|c|c}
	a_{i,j}=\corr(X_i,Y_j) & Y_1 & Y_2\\ \hline
	X_1 & 0.866 & 0.5\\ \hline
	X_2 & 0.866 & -0.5\\ \hline
\end{array}\qquad\begin{array}{c|c|c|c}
a_{i,j} & Y_1 & Y_2 & \text{Total}\\ \hline
X_1 & 0.75 & 0.25 & 1\\ \hline
X_2 & 0.75 & 0.25 & 1\\ \hline
\end{array} \] lo que nos puede ayudar a \lb{interceptar} las componentes principales.

Las saturaciones también se pueden representar gráficamente.

Aunque en este ejemplo, las saturaciones con las distintas variables coincidan, esto no siempre es así, y tendremos variables mejor explicadas por las componentes elegidas que otras.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $A$ es la matriz de saturaciones, entonces \[ AA'=\corr(\mathbf{X}). \]
También es interesante calcular las correlaciones múltiples entre cada variable original con el grupo de las $p$ primeras componentes principales elegidas $(p\le k)$.
\begin{itemize}
	\item Para medir el máximo que podemos explicar de cada variable original a partir de combinaciones lineales de esas componentes principales.
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $\mathbf{Y}$ son las componentes principales obtenidas a partir de $\mathbf{X}$, entonces \[ \corr^2(X_i,(Y_1,\dots,Y_p))=\sum_{j=1}^{p}\corr^2(X_i,Y_j)=\dfrac{1}{\sigma_{i,i}}\sum_{j=1}^{p}t_{i,j}^2\lambda_j=\sum_{j=1}^{p}a_{i,j}^2. \]
La demostración es inmediata ya que las componentes son incorreladas entre sí.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
A estas correlaciones se las suele denominar \lb{comunalidades} \[ c_i=\corr^2(X_i,(Y_1,\dots,Y_p)) \]y se suelen representar en la tabla de las saturaciones al cuadrado (como totales de las filas).

Además, el máximo de la correlación se obtiene con la combinación lineal $\alpha_i'(Y_1,\dots,Y_p)'$ con \[ \alpha_i=\lambda V_{2,2}^{-1}v_{1,2}=\lambda\begin{pmatrix}
	\lambda_1^{-1} & \cdots & 0\\
	\cdots & \cdots & \cdots\\
	0 & \cdots & \lambda_p^{-1}
\end{pmatrix}\begin{pmatrix}
t_{i,1}\lambda_1\\
\cdots\\
t_{i,p}\lambda_p
\end{pmatrix}=\lambda\begin{pmatrix}
t_{i,1}\\
\cdots\\
t_{i,p}
\end{pmatrix}. \]
Es decir, si tenemos que obtener $\mathbf{X}$ en función de las $p$ primeras componentes principales, lo haremos a partir de la relación $\mathbf{X}=T\mathbf{Y}$ eliminando el resto de las componentes.

Lógicamente, si $p=k$, se obtiene $\alpha_i'(Y_1,\dots,Y_p)=\lambda X_i$ y \[ \corr^2(X_i,(Y_1,\dots,Y_k))=\sum_{j=1}^{k}\corr^2(X_i,Y_j)=\dfrac{1}{\sigma_{i,i}}\sum_{j=1}^{k}t_{i,j}^2\lambda_j=1. \]
Recíprocamente, la información contenida en la componente principal $j$-ésima vale: \[ \lambda_j=\lambda_j\sum_{i=1}^{k}t_{i,j}^2=\sum_{i=1}^{k}\sigma_{i,i}\dfrac{1}{\sigma_{i,i}}t_{i,j}^2\lambda_j=\sum_{i=1}^{k}\sigma_{i,i}\corr^2(X_i,Y_j), \] ya que $\sum_{i=1}^{k}t_{i,j}^2=1$ es el módulo al cuadrado del vector propio $\mathbf{t}_j$ (columnas de $T$).

Y la información (variación) total contenida en las $p$ primeras componentes principales vale: \[ \sum_{j=1}^{p}\lambda_j=\sum_{j=1}^{p}\sum_{i=1}^{k}\sigma_{i,i}\corr^2(X_i,Y_j)=\sum_{i=1}^{k}\sigma_{i,i}\corr^2(X_i,(Y_1,\dots,Y_p))=\sum_{i=1}^{k}c_i\sigma_i^2. \]
Si todas las varianzas son 1, la información total $\sum_{j=1}^{p}\lambda_j$ será la suma de las comunalidades, es decir, la suma de la información que se tiene de cada variable original.
\begin{itemize}
	\item Si $p=k$, entonces $c_i=1$ y se tiene \[ \sum_{j=1}^{p}\lambda_j=\sum_{j=1}^{p}\sigma_i^2. \]
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Seguimos con el ejemplo \dots
\end{itemize}
En el ejemplo anterior obtuvimos que: \[ \begin{array}{c|c|c|c}
	a_{i,j} & Y_1 & Y_2 & \text{Total}\\ \hline
	X_1 & 0.75 & 0.25 & 1\\ \hline
	X_2 & 0.75 & 0.25 & 1\\ \hline
	\text{Total} & 1.5 & 0.25 & 2
\end{array} \]donde:
\begin{itemize}
	\item Si $p=1$, se tiene que $\lambda_1=\dfrac{3}{2}=0.75+0.75$.
	\item Si $p=2$, se tiene que $\lambda_1+\lambda_2=\dfrac{3}{2}+\dfrac{1}{2}=2=1+1=\sigma_1^2+\sigma_2^2$.
\end{itemize}
\subsection{Cálculo a partir de la matriz de correlaciones}
Cuando se estudian variables en las que se usan unidades diferentes o queremos que éstas no sean significativas (todas las variables sean iguales a priori), las componentes principales suelen calcularse a partir de la \lb{matriz de correlaciones} \[ \Pi=(\rho_{i,j}) \]con $\rho_{i,j}=\dfrac{\sigma_{i,j}}{\sigma_i\sigma_j}$.

Esto equivale a considerar desde el principio las variables estandarizadas \[ Z_i=\dfrac{X_i-\mu_i}{\sigma_i} \](se igualan las varianzas a 1).

Usando el teorema principal, se obtienen las componentes \[ \begin{array}{c}
	\mathbf{\tilde{Y}}=\tilde{T}'\mathbf{Z}=\tilde{T}'\mathrm{diag}(V)^{-\frac{1}{2}}(\mathbf{X-\mu})\\
	\tilde{Y}_j=\mathbf{\tilde{t}}_j'\mathbf{Z}=\sum_{i=1}^{k}\tilde{t}_{i,j}Z_i=\sum_{i=1}^{k}\tilde{t}_{i,j}\dfrac{X_i-\mu_i}{\sigma_i}
\end{array} \]donde $\tilde{T}$ es la matriz ortogonal que diagonaliza $\Pi=\corr(\mathbf{X})=\cov(\mathbf{Z}),$ \[ \tilde{T}'\Pi\tilde{T}=\mathrm{diag}(\tilde{\lambda}_1,\dots,\tilde{\lambda}_k)=\tilde{D}, \]$\Pi\mathbf{\tilde{t}}_j=\lambda_j\mathbf{\tilde{t}}_j$ y $\mathbf{Z}=(Z_1,\dots,Z_k)'$.

De esta forma, se obtiene que \[ \cov(\mathbf{\tilde{Y}})=\cov(\tilde{T}'\mathbf{Z})=\tilde{T}'\Pi\tilde{T}=\tilde{D}. \]
Es decir, las componentes principales obtenidas a partir de la matriz de correlaciones serán las variables incorreladas con varianza máxima que se pueden obtener a partir de combinaciones lineales de las variables estandarizadas \[ \mathbf{Z}=\mathrm{diag}(V)^{-\frac{1}{2}}(\mathbf{X-\mu}) \]
Sin embargo, los resultados que se obtienen son (en general) diferentes de los que se obtienen a partir de $V$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $\mathbf{\tilde{Y}}$ son las componentes principales obtenidas a partir de la matriz de correlaciones de $\mathbf{X}$ entonces \[ \corr(\mathbf{X,\tilde{Y}})=\tilde{T}\tilde{D}^{-\frac{1}{2}}. \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
En efecto, si $\mathbf{\tilde{Y}}=\tilde{T}'\mathbf{Z}=\tilde{T}'\mathrm{diag}(V)^{-\frac{1}{2}}(\mathbf{X-\mu})$, entonces \[ \cov(\mathbf{Z,\tilde{Y}})=\cov(\mathbf{Z},\tilde{T}'\mathbf{Z})=\Pi\tilde{T}=\tilde{T}\tilde{D}. \]$\corr(\mathbf{X,\tilde{Y}})=\corr(\mathbf{Z,\tilde{Y}})=\cov(\mathbf{Z,\tilde{Y}})\tilde{D}^{-\frac{1}{2}}=\tilde{T}\tilde{D}\tilde{D}^{-\frac{1}{2}}=\tilde{T}\tilde{D}^{\frac{1}{2}}.$
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Observación
\end{itemize}
Nótese que las correlaciones con la componente $\tilde{Y}_j$ son proporcionales al vector propio $\mathbf{\tilde{t}}_j$ (columnas de $T$) con constante de proporcionalidad $\lambda_j^{\frac{1}{2}}\:(\corr(X_i,Y_j)=\tilde{t}_{i,j}\lambda_j^{\frac{1}{2}})$ y que \[ \sum_{i=1}^{k}\corr^2(X_i,\tilde{Y}_j)=\sum_{i=1}^{k}\tilde{t}_{i,j}\tilde{\lambda}_j=\tilde{\lambda}_j. \]
De forma similar, se define la \lb{matriz de saturaciones} $\tilde{A}=\corr(\mathbf{X,\tilde{Y}})$, que verifica \[ \tilde{A}\tilde{A}'=\tilde{T}\tilde{D}^{\frac{1}{2}}\tilde{D}^{\frac{1}{2}}\tilde{T}'=\cov(\mathbf{Z})=\corr(\mathbf{X}) \]y \[ \tilde{A}'\tilde{A}=\tilde{D}^{\frac{1}{2}}\tilde{T}'\tilde{T}\tilde{D}^{\frac{1}{2}}=\tilde{D}. \]
Es decir, la matriz de saturación es una matriz que factoriza $\Pi$ junto a su traspuesta de forma que las multiplicamos al revés nos da una matriz diagonal.

Si estudiamos $k$ variables (numéricas) en una determinada población usando una muestra de $n$ individuos, tendremos una tabla de datos de la forma siguiente: \[ \begin{array}{c|ccc}
	\text{Datos} & X_1 & \cdots& X_k\\ \hline
	\mathbf{O}_1' & X_{1,1} & \cdots & X_{1,k}\\
	\cdots& \cdots&\cdots & \cdots\\
	\mathbf{O}_n' & X_{n,1} & \cdots & X_{n,k}\\ \hline
\end{array} \]
$\mathbf{O}_1,\dots,\mathbf{O}_n$: \mas (formada por $n$ vectores aleatorios columna independientes e idénticamente distribuidos) del vector aleatorio $k$-dimensional $\mathbf{X}=(X_1,\dots,X_k)'$. 
\begin{itemize}
	\item En muchas ocasiones, podremos suponer normal.
	\item Sin embargo, otras veces prescindiremos de estas hipótesis y únicamente analizaremos una tabla de datos, tratando de condensar la información contenida en la misma y de analizar (de forma descriptiva) las relaciones entre las variables y los individuos.
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}En la práctica
\end{itemize}
Así, en la práctica, tendremos que la \lb{matriz de covarianzas} $V$ es desconocida, por lo que tendremos que estimarla.

Y, una vez estimada, procederemos al cálculo de las componentes principales.

De esta forma, las componentes principales (y los valores de la matriz $T$) dependerán de los valores muestrales y, por lo tanto serán \veas (con individuos distintos, obtendremos componentes distintas).

Lo mismo les ocurrirá a los valores propios (serán estimaciones de los verdaderos valores propios).
\subsubsection{Estimación de la matriz de covarianzas}
Para estimar $V$ podemos utilizar la matriz de cuasi-covarianzas muestrales $S$ calculada como \[ \begin{array}{l}
	\mathbf{O}_l=(X_{l,1},\dots,X_{l,k})'\\
	\overline{X}_j=\dfrac{1}{n}\sum_{l=1}^{n}X_{l,j}\\
	\overline{\mathbf{O}}=(\overline{X}_1,\dots,\overline{X}_k)'=\dfrac{1}{N}\sum_{l=1}^{n}\mathbf{O}_l\\
	S=\dfrac{1}{n-1}\sum_{l=1}^{n}(\mathbf{O}_l-\overline{\mathbf{O}})(\mathbf{O}_l-\overline{\mathbf{O}})'=(S_{i,j})\\
	S_{i,j}=\dfrac{1}{n-1}\sum_{l=1}^{n}(X_{l,i}-\overline{X}_i)(X_{l,j}-\overline{X}_j).
\end{array} \]
También podemos usar la matriz de covarianzas muestrales \[ \hat{V}=\dfrac{n-1}{n}S. \]
Ambas tendrán los mismo vectores propios, y si $n$ es grande, casi los mismo valores propios.

\subsubsection{Cálculo a partir de una muestra}
Como no conocemos $V$, la aproximaremos mediante $S$ o $\hat{V}$, las diagonalizaremos (calcularemos los ejes de sus elipsoides) y podremos calcular las componentes principales definidas como sigue.

\subsubsection{Definiciones}
Llamaremos \lb{componentes principales muestrales} a las variables $\mathbf{\hat{Y}}=\hat{T}'\mathbf{X}$, donde $\hat{T}$ es la matriz ortogonal que diagonaliza $S(\hat{V})$ y llamaremos \lb{valores propios muestrales} $\hat{\lambda}_j$ a los valores propios de $S(\hat{V})$.
\begin{itemize}
	\item Los valores de $\hat{T}$ serán las \lb{cargas} (\lb{loadings}) o \lb{coeficientes muestrales}.
	\item Llamaremos \lb{puntuaciones muestrales} (\lb{scores}) a los valores que obtendríamos para cada individuo en las componentes muestrales \[ P_{l,j}=Y_j(O_j)=\mathbf{\hat{t}}_j'O_l. \]
\end{itemize}
\subsubsection{Cálculo a partir de la matriz de correlaciones}
Si optamos por calcular las componentes principales a partir de la \lb{matriz de correlaciones}, como también es desconocida, en su lugar se usará la matriz de correlaciones (de Pearson) muestrales \[ \begin{array}{c}
	R=\mathrm{diag}(S)^{-\frac{1}{2}}S\mathrm{diag}(S)^{-\frac{1}{2}}=(R_{i,j})\\
	R_{i,j}=S_{i,j}(S_{i,i}S_{j,j})^{-\frac{1}{2}}=\hat{V}_{i,j}(\hat{V}_{i,i},\hat{V}_{j,j})^{-\frac{1}{2}}.
\end{array} \]
Esto equivaldrá a estandarizar las variables iniciales restándoles sus medias muestrales y dividiéndolas por sus cuasivarianzas (es decir, hacer que todas tengan la misma variabilidad).
\begin{itemize}
	\item En este caso, las puntuaciones se calcularán como: \[ P_{l,j}=Y_j(O_l)=\mathbf{\hat{t}}_j'O_l^* \] donde $\mathbf{\hat{t}}_j$ es el vector propio $j$-ésimo de $R$.

\end{itemize}
Los datos estandarizados se obtienen (estiman) como \[ O_l^*=\left(\dfrac{X_{l,1}-\overline{X}_1}{S_1},\dots,\dfrac{X_{l,k}-\overline{X}_k}{S_k}\right) \] siendo $S_i=\sqrt{S_{i,i}}$ la cuasidesviación típica de la variable $X_i$.

La cuasidesviación típica $S_j$ puede ser reemplazada por la desviación típica muestral $\hat{V}_j=\sqrt{\hat{V}_{j,j}}$.
\subsubsection{Caso de muestras grandes}
Si $n$ es grande, $\hat{V}$ y $S$ son prácticamente iguales.

Si $\mathbf{X}$ es normal,
\begin{itemize}
	\item $\hat{V}$ es máximo verosímil
	\item $S$ es insesgado para $V$
	\item $(n-1)S$ tiene una distribución (en el muestreo) \lb{Wischart} \code{WK(n - 1, V)}.
\end{itemize}
A partir de este resultado, se puede obtener la distribución exacta de los estimadores de los valores propios, pero ésta es bastante complicada.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Si $\hat{\theta}$ es máximo verosímil para $\theta$, entonces $g(\hat{\theta})$ es máximo verosímil para $g(\theta)$.
\subsubsection{Consecuencia}Si usamos $\hat{V}$ y todos sus valores propios son distintos, se obtendrán estimadores máximo verosímiles para $t_{i,j}$ y $\lambda_j$.
\subsubsection{Caso de normalidad}
Si $\mathbf{X}$ es normal, puede probarse que asintóticamente,
\begin{itemize}
	\item la distribución conjunta de los estimadores de los valores propios es normal multivariante,.
	\item la distribución conjunta de los estimadores de los valores $t_{i,j}$ también lo es.
	\item además, ambas son independientes entre sí.
\end{itemize}
\subsubsection{Cálculo de las componentes principales maximizando la varianza muestral}
El cálculo de las componentes principales muestrales se puede enfocar de otra forma.

Se busca la variable $\mathbf{a'X}$ (combinación lineal de las originales) con $\mathbf{a'a}=1$ que aplicada a los individuos de la muestra nos de una variable con varianza (o cuasivarianza) muestral máxima.

La puntuación o contador (\lb{scores}) del individuo $j$ en esta nueva variable será $\mathbf{a'O}_j$, su media muestral será \[ \dfrac{1}{n}\sum_{j=1}^{n}\mathbf{a'O}_j=\mathbf{a'}\dfrac{1}{n}\sum_{j=1}^{n}\mathbf{O}_j=\mathbf{a'\overline{O}} \] y su cuasivarianza será \[ \dfrac{1}{n-1}\sum_{j=1}^{n}(\mathbf{a'O}_j-\mathbf{a'\overline{O}})^2=\dfrac{1}{n-1}\sum_{j=1}^{n}\mathbf{a'}(\mathbf{O}_j-\overline{\mathbf{O}})(\mathbf{O}_j-\overline{\mathbf{O}})'\mathbf{a}=\mathbf{a'}S\mathbf{a}\qquad(\ast) \]cuyo máximo se alcanza si $\mathbf{a}$ es un vector propio del mayor de los valores propios de $S$.

De forma análoga, se procederá para el cálculo de las restantes componentes principales muestrales.

Sí, por inducción, suponemos que los primeros $i-1$ vectores propios $\mathbf{\hat{t}}_j$ de $S$ nos dan las variables incorreladas con mayor varianza y buscamos maximizar la varianza muestral de $\mathbf{a'O}_j$ (es decir $\mathbf{a}'S\mathbf{a}$) para $\mathbf{a'a}=1$ haciendo que la covarianza muestral \[ \dfrac{1}{n-1}\sum_{j=1}^{n}(\mathbf{a'O}_j-\mathbf{a'\overline{O}})(\mathbf{\hat{t}}_j\mathbf{O}_j-\mathbf{\hat{t}}_j'\overline{\mathbf{O}})=\mathbf{a}'S\mathbf{\hat{t}}_j \]sea cero para $j=1,\dots,i-1$.

Escribiendo $\mathbf{a}$ en función de la base de vectores propios y procediendo como en el teorema principal se obtiene que el óptimo es \[ \mathbf{a}=\mathbf{\hat{t}}_i. \]

De esta forma, podemos representar a los individuos mediante sus puntuaciones en las dos o tres primeras componentes manteniendo de ellos la mayor información (variabilidad o dispersión) posible (aunque $\mathbf{O}_1,\dots,\mathbf{O}_n$ no sea una \mas).

\subsubsection{Interpretaicón geométrica: cálculo minimizando las distancias cuadráticas}
Geométricamente, el espacio formado por las $m$ primeras componentes y que pasa por el punto $\mathbf{O}$ será el espacio de dimensión $m$ que \lb{minimiza la suma de las distancias al cuadrado de los individuos a dicho espacio} (regresión perpendicular).

De esta forma, el ACP será como realizar una regresión mínimo cuadrática usando las \lb{distancias mínimas} (regresión ortogonal) en lugar de las distancias verticales de la regresión clásica (para predecir $Y$ en función de $\mathbf{X}$).