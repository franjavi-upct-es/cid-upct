\section{Regresión logística y multinomial}
\subsection{Modelo de regresión logística}
\subsubsection{Contexto}
Deseamos predecir una variable binaria $Y$ que solo toma los valores 0 y 1.
\begin{itemize}
	\item Además, estos valores numéricos solo indicarán la pertenencia o no a un determinado grupo.
\end{itemize}
\bu{Ejemplos:}
\begin{itemize}
	\item Determinar si un paciente tiene o no una determinada enfermedad en función de diferentes variables (edad, presión arterial, nivel de colesterol, etc.).
	\begin{itemize}
		\item En este caso el valor 1 suele indica que sí la tiene y 0 que no.
	\end{itemize}
	\item Predecir si un estudiante aprueba o no un examen en función de las horas de estudio.
	\item Predecir si un mensaje de correo electrónico es spam o no en función de las palabras clave.
	\item Predecir si un cliente comprará o no un determinado producto en función de la edad y el salario.
	\item Predecir si un paciente tiene diabetes o no en función de variables como el nivel de glucosa, la presión arterial y el índice de masa corporal (IMC).
\end{itemize}
\subsubsection{Objetivo}
\lb{Objetivo:} predecir la variable respuesta $Y$ a partir de $k$ variables numéricas $X_1,\dots,X_k$ utilizando una única función \[ h_\theta(\mathbf{x})=g(\mathbf{\theta'x})=g(\theta_0+\theta_1x_1+\cdots+\theta_kx_k), \]donde $\mathbf{\theta}=(\theta_0,\dots,\theta,\theta_k)'\in\R^{k+1}$ contiene los parámetros del modelo y $\mathbf{X}=(X_0,\dots,X_k)'$ las variables que podemos medir en nuestro individuos para predecir $Y$.

Para mejorar la notación hemos incluido una variable artificial $X_0$ siempre que vale 1.
\subsubsection{¿Cómo elegir la función $g$?}
La función $g$ debe transformar esos valores numéricos (lineales) en números entre 0 y 1 que nos indicarán la \lb{probabilidad} de que el individuo pertenezca al grupo ($Y=1$): \[ g:\R\to[0,1] \]y $h_\theta(\mathbf{x})\approx Pr(Y=1|\mathbf{X=x})$, donde $\mathbf{X}=(X_0,\dots,X_k)'$.

\lb{Regla de decisión:} \[ \begin{array}{l}
	h_\theta(\mathbf{x})\ge0.5\to\hat{y}=1\\
	h_\theta(\mathbf{x})<0.5\to\hat{y}=0
\end{array} \]donde $\hat{y}$ representa el valor que predecimos para $Y$ cuando $\mathbf{X=x}$.

Existen diversas opciones para determinar $g$, la más popular es la \lb{función logística} (o \lb{sigmoide}) \[ g(z)=\dfrac{1}{1+\exp(-z)}=\dfrac{\exp(z)}{1+\exp(z)}. \]

\subsection{Función logística}
\begin{minipage}{0.5\textwidth}
	\begin{lstlisting}
par(mfrow = c(1, 1))
g <- function(x) 1/(1+exp(-x))
x <- seq(-4, 4, length = 100)
y <- g(x)
plot(x, y, xlab = 'z', ylab = 'g(z)', col = 'blue', type = "l", ylim = c(-0.1, 1.1), yaxp = c(0, 1, 5))
abline(h = 0, lty = 2)
abline(h = 1, lty = 2)
abline(v = 0, lty = 2, col = "red")
\end{lstlisting}
\end{minipage}\qquad\begin{minipage}{0.45\textwidth}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Propiedades
\end{itemize}
\begin{itemize}[label=$-$]
	\item Es continua 
	\item Estrictamente creciente.
	\item Recorrido de 0 a 1.
	\item Transformará el valor $\mathbf{\theta'x}\in\R$ en un valor $h_\theta(\mathbf{x})\in[0,1]$.
	\item $g(0)=0.5$
	\item \lb{Regla de decisión:} \[ \begin{array}{l}
		\mathbf{\theta'x}\ge0\to \hat{y}=1\\
		\mathbf{\theta'x}<0\to\hat{y}=0
	\end{array} \]
	\item La función $I_\theta(\mathbf{x})=\mathbf{\theta'x}$ define un \lb{índice de separación entre las categorías de $Y$}
\end{itemize}
\end{minipage}

\includegraphics[width=0.45\linewidth]{"Temas/Imágenes/Tema 3/screenshot001"}
\subsection{¿Cómo determinar una función costo que penalice las decisiones erróneas?}
\subsubsection{Función costo}
\begin{minipage}{0.45\textwidth}
	Para $z=h_\theta(\mathbf{x})$, definimos la función costo\[ c(z,y)=\begin{cases}
		-\log(z) & \text{si }y=1\\
		-\log(1-z) & \text{si }y=0\\
	\end{cases} \]
	O, equivalentemente, \[ c(z,y)=-y\log(z)-(1-y)\log(1-z), \] donde $y\in\{0,1\}$ y los logaritmos son neperianos.
\end{minipage}\quad\begin{minipage}{0.55\textwidth}
\begin{lstlisting}
par(mfrow = c(1, 1))
x <- seq(0, 1, length = 100)
plot(x, -log(x), xlab = 'z', ylab = 'c(z, y)', col = 'blue', type = "l",  ylim = c(-0.1, 4), yaxp = c(0, 4, 5))
text(0.1, 3.5, "y = 1", col = "blue", adj = c(0, 0))
lines(x, -log(1-x), xlab = 'z', ylab = 'c(z, y)', col = 'red', type = "l",  ylim = c(-0.1, 4), yaxp = c(0, 4, 5))
text(0.8, 3.5, "y = 0", col = "red", adj = c(0, 0))
\end{lstlisting}
\end{minipage}
\begin{flushright}
	\includegraphics[width=0.55\linewidth]{"Temas/Imágenes/Tema 3/screenshot002"}
\end{flushright}
\subsubsection{Criterio}
Minimizar el valor esperado de la función costo \[ \min_\theta J(\theta)=E[c(h_\theta(\mathbf{X}), Y)] \]
Para determinar los valores óptimos de los parámetros:
\begin{itemize}
	\item Dispondremos de una muestra (\lb{training sample}) y de individuos en los que se conozcan tanto los valores de $\mathbf{x}$ como los valores de $y$ (\lb{aprendizaje supervisado}).
	\item Calcularemos los costos en los valores muestrales.
	\item Determinaremos los valores de los parámetros que minimizan estos costos.
\end{itemize}
\subsubsection{Otra formulación del problema}
Si $p=Pr(Y=1)$ este modelo es equivalente a suponer que existe una relación lineal entre las variables $X_1,\dots,X_k$ y la función \code{log-odd de p}. \[ \log\dfrac{p}{1-p}=\mathbf{\theta'X}. \]
Puesto que esto es equivalente a suponer que \[ p=Pr(Y=1)=\dfrac{\exp(\mathbf{\theta'X})}{1+\exp(\mathbf{\theta'X})}=g(\mathbf{\theta'X}) \]
\subsection{Inferencia y predicción}
\subsubsection{Función costo empírica}
Datos muestrales: $(\mathbf{x}^{(1)},y^{(1)}),\dots,(\mathbf{x}^{(n)},y^{(n)})$.

Función costo: \[ J(\theta)\coloneq\dfrac{1}{n}\sum_{i=1}^{n}c(h_\theta(\mathbf{x}^{(i)}), y^{(i)}). \]
Desarrollando la función $c$ obtenemos \[ \begin{aligned}
	J(\theta)&=-\dfrac{1}{n}\sum_{i=1}^{n}\left[y^{(i)}\log(h_\theta(\mathbf{x}^{(i)}))+(1-y^{(i)})\log(1-h_\theta(\mathbf{x}^{(i)}))\right]\\
	&=-\dfrac{1}{n}\sum_{i=1}^{n}\left[y^{(i)}\log(g(\mathbf{\theta'x}^{(i)}))+(1-y^{(i)})\log(1-g(\mathbf{\theta'x}^{(i)}))\right]
\end{aligned} \]
\subsubsection{Función costo empírica en forma matricial}
Denotando
\begin{itemize}
	\item $M=(x_j^{(i)})$ a la matriz de datos.
	\item $\mathbf{y}=(y^{(i)})$ al vector columna con los valores de $Y$
	\item $h\coloneq g(M\mathbf{\theta})$ al vector columna con los ajustes en cada individuo, entonces \[ J(\theta)\coloneq-\dfrac{1}{n}\left[\mathbf{y}'\log(h)+(1_n-\mathbf{y})'\log(1_n-h)\right] \] donde $1_n$ representa un vector columna de dimensión $n$.
\end{itemize}
\subsubsection{Objetivo}
Ajsutar el parámetro $\theta$ para que $J$ tome el menor valor posible.
\begin{itemize}
	\item \lb{Solución:} Algoritmos iterativos de búsqueda como, por ejemplo, el algoritmo del gradiente descendente.
	\begin{itemize}
		\item Práctica complementaria de regresión logística.
	\end{itemize}
	\item Existen varias librerías de \code{R} que permiten obtener estimaciones de los parámetros del modelo logístico.
	\begin{itemize}
		\item Práctica de regresión logística.
	\end{itemize}
\end{itemize}
\subsection{Un ejemplo sencillo}
\subsubsection{Datos muestrales}
Como en técnicas anteriores usaremos un ejemplo sencillo para comprobar cómo funciona nuestro modelo.

Supongamos que tenemos dos variables predictoras $X_1$ y $X_2$ ($k = 2$) y los datos siguientes:
\begin{center}
\begin{tabular}{c|ccc}
	Individuo & $X_1$ & $X_2$ & $Y$ \\
	\hline
	1 & 1 & 2 & 0 \\
	2 & 2 & 1 & 0 \\
	3 & 3 & 1 & 0 \\
	4 & 2 & 2 & 0 \\
	5 & 5 & 1 & 1 \\
	6 & 5 & 3 & 1 \\
	7 & 3 & 2 & 0 \\
	8 & 4 & 3 & 1 \\
	9 & 4 & 4 & 1 \\
	10 & 5 & 4 & 1 \\
	\hline
\end{tabular}
\end{center}
Lo primero que tenemos que hacer (si es posible) es dibujar estos puntos añadiendo una etiqueta para distinguir los de cada grupo.

\begin{minipage}{0.45\textwidth}
	\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = 20, xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, Y, cex = 1)
	\end{lstlisting}
	\begin{center}
		\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot003"}
	\end{center}
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), xlim = c(0,6), ylim = c(0,6), cex = 1.2)
legend('topleft', legend = c('Y = 0','Y = 1'), pch = 0:1, cex = 1)
\end{lstlisting}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot004"}
\end{center}
\end{minipage}

En ambas gráficas podemos observar que los dos grupos se pueden separar muy bien con rectas.

Por lo tanto nuestro modelo será \[ h_\theta(\mathbf{x})=g(\theta_1+\theta_1x_1+\theta_2x_2). \]
Otra forma de analizar los grupos es calcular medidas descriptivas en cada uno de ellos.
\begin{itemize}
	\item Por ejemplo podemos calcular las medias en cada grupo:
	\pagebreak
	\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
tmp1 = tapply(X1, Y, mean)
tmp2 = tapply(X2, Y, mean)
tmp = rbind(tmp1, tmp2)
colnames(tmp) = paste0("Y = ", colnames(tmp))
rownames(tmp) = paste0("Media de ", c("X1", "X2"))
tmp
	\end{lstlisting}
	\begin{verbatim}
##             Y = 0 Y = 1
## Media de X1   2.2   4.6
## Media de X2   1.6   3.0
	\end{verbatim}
\end{itemize}
Estas diferencias también se pueden ver representado $x^{(i)}$ frente a $Y$.

\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X1, Y, ylim = c(-0.5, 1.5), cex = 1.2, pch = 20, col = "blue")
\end{lstlisting}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot005"}
\end{center}

\end{minipage}\qquad\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X2, Y, ylim = c(-0.5, 1.5), cex = 1.2, pch = 20, col = "magenta")
\end{lstlisting}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot006"}
\end{center}

\end{minipage}

Podemos observar cómo la primera variable separa mejor a los grupos que la segunda (en los valores muestrales).

De forma similar se pueden representar histogramas o diagramas caja-bigote para comparar las variables en cada grupo.

Podemos calcular la función $J(\theta)$ en \code{R} con los datos anteriores.
\pagebreak
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
n <- length(Y)
k <- 2
X0 = rep(1, n)
M <- matrix(c(X0, X1, X2), nrow = n, ncol = k + 1, byrow = FALSE)
g <- function(z){
	g = exp(z)/(1 + exp(z))
	return(g)
}
J <- function(theta){
	J = - sum(Y * log(g(M %*% theta)) + (1 - Y) * log(1 - g(M %*% theta)))/n
	return(J)
}
\end{lstlisting}
Podemos aplicar un \lb{método iterativo} para la obtención del óptimo.
\begin{itemize}
	\item Por ejemplo, el \lb{método del gradiente descendiente} (se detallará su aplicación en la práctica complementaria de regresión logística).
\end{itemize}
\begin{lstlisting}
z <- c(-3.5, 1, 0)
alpha <- 1/3
m <- 1000
J1 <- 1:m
for (i in 1:m) {
	h <- g(M %*% z)
	z <- z - (alpha/n) * t(M) %*% (h-Y)
	J1[i] <- J(z)
}
\end{lstlisting}
Partiendo del punto inicial $\theta^{(0)}=(-3.5,\,1,\,0)$ (recta vertical de separación $x_1=3.5$) y después de \lb{1000 iteraciones} obtendremos $\hat{\theta}_0=-10.7505,\,\hat{\theta}_1=2.4594$ y $\hat{\theta}_3=1.0287$, con valor de $J(\hat{\theta})=0.0597$.

En este caso,\[ I_{\hat{\theta}}(x_1,_2)=-10.7505+2.4591x_1+1.0287x_2 \]
La recta que marca la frontera de esta solución será\[ -10.7505+2.4594x_1+1.0284x_2=0 \]
Esto es, $x_2=10,4506-2.3908x_1$

Si queremos \lb{predecir} el grupo para un nuevo individuo con valores $x_1=5$ y $x_2=2$
\begin{itemize}
	\item Evaluamos la función $I_{\hat{\theta}}(x_1,_2)=-10.7505+2.4594x_1+1.0287x_2$
	\item $I_{\hat{\theta}}(5,2)=3.6037>0$, por lo que el individuo se clasifica en el grupo $y=1$.
\end{itemize}
Representamos los valores muestrales incluyendo la recta que marca la frontera y el punto $(5,2)$.

\pagebreak

\begin{lstlisting}
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), xlim = c(0,6), ylim = c(0,6), cex = 1.2)
legend('topleft', legend = c('Y = 0','Y = 1'), pch = 0:1, cex = 1)
abline(v = 3.5, col = "lightgray", lty = 2)
abline(-z[1]/z[3],-z[2]/z[3],col='red')
text(5, 2, 'x', col = "magenta", cex = 1.2)
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.4\linewidth]{"Temas/Imágenes/Tema 3/screenshot007"}
\end{center}
Para medir cómo de fiable es esta clasificación podemos:
\begin{itemize}
	\item Observar cómo se distribuyen los puntos en esta gráfica (cuando sea posible).
	\item Calcular las \lb{probabilidades a posteriori}.
\end{itemize}
$Pr(Y=1|X_1=5,X_2=2)\approx g(I_{\hat{\theta}}(5,2))=0.9735$

$Pr(Y=0|X_1=5,X_2=2)\approx 1-g(I_{\hat{\theta}}(5,2))=0.0265$

Observando la gráfica con los valores muestrales parece razonable que el nuevo individuo con valores $\mathbf{x}=(5,2)$ sea clasificado en el grupo $y=1$.

Ahora queremos predecir el grupo para otro nuevo individuo con valores $x_1=3.5$ y $x_2=2$. En este caso: $I_{\hat{\theta}}(x_1,x_2)=-0.0853<0$, por lo que el individuo se clasifica en el grupo $y=0$.

Representamos gráficamente:
\begin{lstlisting}
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), xlim = c(0,6), ylim = c(0,6), cex = 1.2)
legend('topleft', legend = c('Y = 0','Y = 1'), pch = 0:1, cex = 1)
abline(-z[1]/z[3],-z[2]/z[3],col='red')
text(3.5, 2, 'x', col = "blue", cex = 1.2)
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.4\linewidth]{"Temas/Imágenes/Tema 3/screenshot008"}
\end{center}
Observamos dónde se encuentra el nuevo individuo en la gráfica.

Calculamos las \lb{probabilidades a posteriori}:
\begin{itemize}
	\item $Pr(Y=1|X_1=3.5,X_2=2)\approx g(I_{\hat{\theta}}(3.5,2))=0.4787$
	\item $Pr(Y=0|X_1=3.5,X_2=2)\approx1- g(I_{\hat{\theta}}(3.5,2))=0.5213$
\end{itemize}
Recordemos que en realidad no estamos seguros de que esos valores sean realmente esas probabilidades.

De esta forma intuimos que esta clasificación no es muy fiable ya que ese punto está \lb{muy cerca de la frontera}.
\subsection{Regresión logística multinomial}
\subsubsection{Contexto}
Generalización del modelo de regresión logística binaria.\\
La variable dependiente tiene más de dos categorías, sin/con un orden implícito.
\begin{itemize}
\item Primer caso: considera variables de respuesta nominal,
\begin{itemize}
	\item Por ejemplo, el país de procedencia, el color de un automóvil, etc.
\end{itemize}
\item Segundo caso: trata variables de respuesta ordinal,
\begin{itemize}
	\item Por ejemplo, el nivel educativo, la fase de una enfermedad, etc.
\end{itemize}
\end{itemize}
\subsubsection{Objetivo}
Estimar la probabilidad de que un individuo presente cada una de estas categorías en función de los valores que se observen de las variables explicativas.
\subsection{Modelo teórico}
\subsubsection{Formulación}
La variable respuesta $Y$ puede presentar $g$ \lb{características}.
\begin{itemize}
	\item $Y$ toma los valores $1,2,\dots,g$, que indican la pertenencia a cada grupo definido por cada categoría, con probabilidades $p_1,p_2,\cdots,p_g$, respectivamente, tales que \[ \sum_{j=1}^{g}p_j=1. \]
	Consideramos como referencia una de las categorías, por ejemplo, la última, $g$.
\end{itemize}
Establecemos un modelo \code{logit} para cada categoría con respecto a esta: \[ \log\dfrac{p_j}{p_g}=\log\dfrac{Pr[Y=j]}{Pr[Y=g]}=\mathbf{\theta_j'X},\quad j=1,\dots,g-1, \] donde $\mathbf{\theta}_j=(\theta_{j,0},\dots,\theta_{hk})'\in\R^{k+1}$ contiene los parámetros del modelo para cada categoría $j$ y $\mathbf{X}=(X_0,\dots,X_k)'$ las variables que podemos medir en nuestros individuos para predecir $Y$.

Al cociente $\dfrac{p_j}{p_g}$ se le denomina \code{odds} de la categoría $j$ respecto de la categoría $g$.

Se ha considerado un término constante en el modelo incluyendo la variable artificial $X_0$ que siempre vale 1.

Cada uno de los coeficientes se interpreta como el efecto de cada variable explicativa sobre el logaritmo de los \code{odds} de la categoría $j$ respecto de la categoría de referencia $g$.

Cuando $g=2$, el modelo se reduce a una única ecuación equivalente a la propuesta en la regresión logística.
\subsubsection{Observaciones}
Si comparamos las probabilidades para dos categorías diferentes, $i$ y $j$, utilizando el modelo anterior obtenemos que: \[ \begin{aligned}
	\log\dfrac{p_i}{p_j}&=\log\dfrac{\frac{p_i}{p_g}}{\frac{p_j}{p_g}}=\log\dfrac{p_i}{p_g}-\log\dfrac{p_j}{p_g}=\mathbf{\theta_i'X-\theta_j'X}\\
	&=(\mathbf{\theta}_i-\mathbf{\theta}_j)'\mathbf{X}=(\theta_{i0}-\theta_{j0})+(\theta_{i1}-\theta_{j1})X_1+\cdots+(\theta_{ik}-\theta_{jk})X_k.
\end{aligned} \]
De esta forma, se obtiene una ecuación \code{logit} de la categoría $i$ con respecto a la categoría $j$, donde $\theta_0=\theta_{i0}-\theta_{j0},\:\theta_1=\theta_{i1}-\theta_{j1},\dots,\theta_k=\theta_{ik}-\theta_{jk}.$

\subsubsubsection{Un ejemplo ficticio}
Supongamos que deseamos estudiar cómo influye el sexo del neonato en la aparición de determinados problemas durante el parto.

Se contemplan únicamente tres opciones posibles para los partos $(Y)$:
\begin{itemize}
	\item $Y=1:$ parto con el problema A
	\item $Y=2:$ parto con el problema B
	\item $Y=3:$ parto sin problemas
\end{itemize}
La tercera opción se toma como la opción de referencia.

Se introduce una variable binaria $X$ para representar el sexo del neonato:
\begin{itemize}
	\item $X=0:$ si es niño
	\item $X=1:$ si es niña
\end{itemize}
Planteamos un modelo de regresión logística para predecir la variable categórica $Y$ en función de $X$:\[ \log\dfrac{p_j}{p_3}=\log\dfrac{Pr[Y=j]}{Pr[Y=3]}=\theta_{j0}+\theta_{j1}x,\quad j=1,2. \]
\subsubsubsection{Interpretación de los parámetros}
Considerando la primera de las ecuaciones: \[ \log\dfrac{p_1}{p_3}=\log\dfrac{Pr[Y=1]}{Pr[Y=3]}=\theta_{10}+\theta_{11}x \]
\begin{itemize}
	\item Si $X=0,\:\dfrac{p_1}{p_3}=\exp(\theta_{10})$.
	\item Si $X=1,\:\dfrac{p_1}{p_3}=\exp(\theta_{10}+\theta_{11})$.
\end{itemize}
Por lo tanto, respecto a la probabilidad de un parto normal la probabilidad de la presencia del problema A se multiplica por $\exp(\theta_{10}+\theta_{11})$ en el caso de niños y por $\exp(\theta_{10})$ en el caso de niñas.

Si, por ejemplo, resultara $\theta_{11}=0$, el sexo no influiría sobre la probabilidad de que aparezca el problema A.

Razonando de forma análoga, si resultara $\theta_{21}>0$ se concluiría que la aparición del problema B es más probable en niñas que en niños.

\subsubsubsection{Estimador de máxima verosimilitud}
Sea $X$ una variable aleatoria, con función de densidad o \fpp $x\longmapsto f_X(x,\theta)$, donde $\theta\in\Theta\subset\R^k$.

Consideramos una \mas $(X_1,\dots,X_n)$.

Para un valor concreto de $(X_1,\dots,X_n)$, que denotamos por $(x_1,\dots,x_n)$, la \lb{función de verosimilitud} $L_n$ es una función de $\theta,\,L_n:\Theta\subset\R^k\longrightarrow\R^+$, definida como \[ L_n(\theta)=f_{X_1,\dots,X_n}(x_1,\dots,x_n;\,\theta)=\prod_{i=1}^{n}f_{\mathbf{X}}(x_i;\,\theta). \]
El \lb{estimador de máxima verosimilitud} $\hat{\theta}$ de $\theta$ es cualquier valor de $\theta$ admisible que maximiza la función $L_n(\theta)$, \[ \hat{\theta}=\arg\max_{\theta\in\Theta}L_n(\theta). \]
\subsection{Criterio de máxima de verosimilitud para nuestro modelo}
\subsubsection{Criterio}
Para estimar los parámetros del modelo utilizaremos el criterio de máxima verosimilitud:
\begin{itemize}
	\item Calculamos la función de verosimilitud.
	\item Maximizamos esta función para obtener los estimadores de máxima verosimilitud (MLE, \code{maximum likelihood estimator}).
\end{itemize}
Redefiniremos la variable $Y$ en $g$ variables indicadoras $(Y_1,\dots,Y_g)$:
\begin{itemize}
	\item $Y_j$ toma el valor 1 si la respuesta pertenece al grupo $j$ y 0 en otro caso.
	\item Tendremos que $\sum_{j=1}^{g}Y_j=1$.
\end{itemize}
\subsubsection{Función de verosimilitud}
Supongamos que disponemos de $n$ observaciones independientes de la variable $Y$ y de las variables explicativas.Para cada individuo $i$ tendremos: 
\begin{itemize}
	\item Las observaciones $(y_1^{(i)},\dots,y_g^{(i)})$, donde \[ \sum_{j=1}^{g}y_j^{(i)}=1. \]
	\item Los valores de las variables explicativas observados $\mathbf{x}^{(i)}=(x_0^{(i)},\dots,x_k^{(i)})'$.
\end{itemize}
Entonces, la \lb{función de verosimilitud} adopta la expresión \[ L(\theta_1,\dots,\theta_{g-1})\propto\prod_{i=1}^{n}\prod_{j=1}^{g}p_j(\mathbf{x}^{(i)})^{y_j^{(i)}} \] donde el símbolo $\propto$ indica \lb{proporcionalidad}.
\subsubsection{Función de log-verosimilitud}
En lugar de maximizar directamente la función de verosimilitud consideraremos su logaritmo neperiano:
\begin{itemize}
	\item Función más manejable que simplifica los cálculos.
	\item Permite utilizar métodos numéricos de optimización más eficientes y estables al transformar productos en sumas.
\end{itemize}
La log-verosimilitud adopta la expresión \[ \log L(\theta_1,\dots,\theta_{g-1})\propto\log\prod_{i=1}^{n}\prod_{j=1}^{g}p_j(\mathbf{x}^{(i)})^{y_k^{(i)}}=\sum_{i=1}^{g}\sum_{j=1}^{g}y_j^{(i)}\log p_j(\mathbf{x}^{(i)}). \]
En términos de una función costo, el criterio de máxima verosimilitud equivale a minimizar la función \[ J(\theta_1,\dots,\theta_{g-1})=-\log L(\theta_1,\dots,\theta_{g-1}). \]

En ocasiones en lugar de la función de log-verosimilitud se utiliza la función auxiliar $\Lambda=-2\log L$, denominada la \code{deviance} del modelo.

Puesto que $p_g=1-(p_1+\cdots+p_{g-1})$, la contribución del individuo $i$ en la función de la log-verosimilitud sería 
\begin{align*}
	\sum_{j=1}^{g}y_j^{(i)}\log p_j(\mathbf{x}^{(i)})&=\sum_{j=1}^{g-1}y_j^{(i)}\log p_j(\mathbf{x}^{(i)})+\left(1-\sum_{j=1}^{g-1}y_j^{(i)}\right)\log\left(1-\sum_{j=1}^{g-1}p_j(\mathbf{x}^{(i)})\right)\\
	&=\sum_{j=1}^{g-1}y_j^{(i)}\log\dfrac{p_j(\mathbf{x}^{(i)})}{1-\displaystyle\sum_{h=1}^{g-1}p_h(\mathbf{x}^{(i)})}+\log\left(1-\sum_{j=1}^{g-1}p_j(\mathbf{x}^{(i)})\right).
\end{align*}
Según el modelo logístico multinomial, \[ \log\dfrac{p_j(\mathbf{x}^{(i)})}{p_g(\mathbf{x}^{(i)})}=\mathbf{\theta_j}^{(i)}. \]
Por otra parte, $p_g(\mathbf{x}^{(i)})$ puede escribirse como \[ p_g(\mathbf{x}^{(i)})=1-\sum_{j=1}^{g-1}p_j(\mathbf{x}^{(i)})=1-p_g(\mathbf{x}^{(i)})\sum_{j=1}^{g-1}\exp(\mathbf{\theta_j'x}^{(i)}). \]
Y despejando ahora $p_g(\mathbf{x}^{(i)})$ en la expresión anterior se obtiene que \[ p_g(\mathbf{x}^{(i)})=\dfrac{1}{1+\displaystyle\sum_{h=1}^{g-1}\exp(\theta_j'\mathbf{x}^{(i)})}. \]
Y por tanto, \[ p_j(\mathbf{x}^{(i)})=\dfrac{\exp(\theta_j'\mathbf{x}^{(i)})}{1+\displaystyle\sum_{h=1}^{g-1}\exp(\theta_h'\mathbf{x}^{(i)})}. \]
Sustituyendo ahora en la función de log-verosimilitud se tendrá que
\begin{align*}
	\log L(\theta_1,\dots,\theta_{g-1})\propto\sum_{i=1}^{g}\sum_{j=1}^{g}y_j^{(i)}\log p_j(\mathbf{x}^{(i)})&=\sum_{i=1}^{n}\left[\sum_{j=1}^{n}y_j^{(i)}(\theta_j'\mathbf{x}^{(i)})-\log\left(1+\sum_{j=1}^{g-1}\exp(\theta_j'\mathbf{x}^{(i)})\right)\right]\\
	&=\sum_{i=1}^{n}\sum_{j=1}^{g-1}y_j^{(i)}(\theta_j'\mathbf{x}^{(i)})-\sum_{i=1}^{g-1}\log\left(1+\sum_{j=1}^{g-1}\exp(\theta_j'\mathbf{x}^{(i)})\right)
\end{align*}
\subsubsection{¿Cómo obtenemos en la práctica las estimaciones de $\theta_1,\dots,\theta_{g-1}$?}
Para obtener valores de los parámetros $\theta_1,\dots,\theta_{g-1}$ que maximicen la log-verosimilitud (o equivalentemente, minimicen la función costo $J$), podremos:
\begin{itemize}
	\item Aplicar algoritmos iterativos de búsqueda como el algoritmo del gradiente descendente.
	\item Hacer uso de funciones implementadas en librerías de \code{R} que permiten obtener estimaciones de los parámetros del modelo logístico multinomial, como la función \code{multinom()} de la librería \code{nnet}.
	\begin{itemize}
		\item Práctica de regresión logística multinomial.
	\end{itemize}
\end{itemize}
\subsection{Un caso sencillo}
\subsubsection{Cálculo de la verosimilitud}
Supongamos que tenemos una variable respuesta ($Y$ ) con tres categorías posibles y dos variables explicativas $X_1$ y $X_2$, cuyas observaciones están recogidas en la tabla adjunta.

\begin{center}
	\begin{tabular}{c|ccc}
		Individuo & $X_1$ & $X_2$ & $Y$ \\
		\hline
		1 & 1 & 2 & 2 \\
		2 & 2 & 1 & 2 \\
		3 & 1 & 5 & 3 \\
		4 & 2 & 4 & 3 \\
		5 & 3 & 1 & 2 \\
		6 & 2 & 2 & 2 \\
		7 & 2 & 5 & 3 \\
		8 & 5 & 1 & 1 \\
		9 & 5 & 3 & 1 \\
		10 & 3 & 2 & 2 \\
		11 & 4 & 3 & 1 \\
		12 & 4 & 4 & 1 \\
		13 & 5 & 4 & 1 \\
		14 & 1 & 4 & 3 \\
		\hline
	\end{tabular}
\end{center}

Para la implementación en \code{R} de estos cálculos introduciremos los datos en vectores y representaremos los datos gráficamente.

\begin{lstlisting}
X1 <- c(1, 2, 1, 2, 3, 2, 2, 5, 5, 3, 4, 4, 5, 1)
X2 <- c(2, 1, 5, 4, 1, 2, 5, 1, 3, 2, 3, 4, 4, 4)
Y  <- c(2, 2, 3, 3, 2, 2, 3, 1, 1, 2, 1, 1, 1, 3)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = 20, xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, Y, cex = 1)
\end{lstlisting}
\begin{center}
	\includegraphics[width=0.5\linewidth]{"Temas/Imágenes/Tema 3/screenshot009"}
\end{center}
Incluimos los valores de la variable artificial $X_0=1$.

Asociada a la variable $Y$ definimos tres variables indicadoras, $Y_j,\:j=1,2,3$, de manera que $Y_j=1$ si $Y=j$ y 0 en otro caso.
\begin{lstlisting}
library("dplyr")
X1 <- c(1, 2, 1, 2, 3, 2, 2, 5, 5, 3, 4, 4, 5, 1)
X2 <- c(2, 1, 5, 4, 1, 2, 5, 1, 3, 2, 3, 4, 4, 4)
Y  <- c(2, 2, 3, 3, 2, 2, 3, 1, 1, 2, 1, 1, 1, 3)
df = data.frame(X1, X2, Y)
df <- df %>%
	mutate(X0 = 1,
				 Y1 = ifelse(Y == 1, 1, 0),
				 Y2 = ifelse(Y == 2, 1, 0),
				 Y3 = ifelse(Y == 3, 1, 0))
df
\end{lstlisting}
\pagebreak
\begin{verbatim}
##    X1 X2 Y X0 Y1 Y2 Y3
## 1   1  2 2  1  0  1  0
## 2   2  1 2  1  0  1  0
## 3   1  5 3  1  0  0  1
## 4   2  4 3  1  0  0  1
## 5   3  1 2  1  0  1  0
## 6   2  2 2  1  0  1  0
## 7   2  5 3  1  0  0  1
## 8   5  1 1  1  1  0  0
## 9   5  3 1  1  1  0  0
## 10  3  2 2  1  0  1  0
## 11  4  3 1  1  1  0  0
## 12  4  4 1  1  1  0  0
## 13  5  4 1  1  1  0  0
## 14  1  4 3  1  0  0  1
\end{verbatim}
Empezamos implementando la función \code{J} utilizando código en \code{R}.
\begin{lstlisting}
J <- function(theta){
	C = exp(t(theta)%*%t(X))
	D = colSums(C)
	E = matrix(rep(1 + D, g - 1), nrow = g - 1, ncol = n, byrow = TRUE)
	P = C/E
	Pg = 1/(1+D)
	PT =  rbind(P, Pg)
	J = -sum(YY*log(t(PT)))
	return(J)
}
\end{lstlisting}
Introducimos los valores muestrales de las variables.
\begin{lstlisting}
library("dplyr")
n = length(Y)
g = 3
k = 2
X1 <-c(1, 2, 1, 2, 3, 2, 2, 5, 5, 3, 4, 4, 5, 1)
X2 <-c(2, 1, 5, 4, 1, 2, 5, 1, 3, 2, 3, 4, 4, 4)
Y  <-c(2, 2, 3, 3, 2, 2, 3, 1, 1, 2, 1, 1, 1, 3)
df = data.frame(X1, X2, Y)
df <- df %>%
	mutate(X0 = 1,
				 Y1 = ifelse(Y == 1, 1, 0),
				 Y2 = ifelse(Y == 2, 1, 0),
				 Y3 = ifelse(Y == 3, 1, 0))
X = as.matrix(df[, c("X0", "X1", "X2")])
YY = as.matrix(df[, c("Y1", "Y2", "Y3")])
\end{lstlisting}

Evaluamos en \[ \theta=\begin{pmatrix}
	1 & 2\\
	0 & 1\\
	2 & 3
\end{pmatrix} \]
\begin{lstlisting}
theta = matrix(c(1, 2,
								 0, 1,
								 2, 3), nrow = k+1, ncol = g-1, byrow = TRUE)
J(theta)
\end{lstlisting}
\begin{verbatim}
## [1] 111.0598
\end{verbatim}

Evaluamos en \[ \theta=\begin{pmatrix}
	-2.3 & 1.5\\
	0.5 & 2\\
	2.1 & 3.5
\end{pmatrix} \]
\begin{lstlisting}
theta = matrix(c(-2.3, 1.5,
								 0.5, 2,
								 2.1, 3.5), nrow = k+1, ncol = g-1, byrow = TRUE)
J(theta)
\end{lstlisting}
\begin{verbatim}
## [1] 155.5009
\end{verbatim}
\subsubsection{Estimación de los parámetros}
Utilizaremos la función \code{multinom()} de la librería \code{nnet} de \code{R}.
\begin{itemize}
	\item Las opciones de esta función se verán con más detalle en la práctica de regresión logística multinomial.
\end{itemize}
\begin{lstlisting}
library("nnet")
X1 <-c(1, 2, 1, 2, 3, 2, 2, 5, 5, 3, 4, 4, 5, 1)
X2 <-c(2, 1, 5, 4, 1, 2, 5, 1, 3, 2, 3, 4, 4, 4)
Y  <-c(2, 2, 3, 3, 2, 2, 3, 1, 1, 2, 1, 1, 1, 3)
df = data.frame(X1, X2, Y)
df$Y_factor = factor(df$Y, levels = c("1", "2", "3"))
df$Y_factor <- relevel(df$Y_factor, ref = "3")
mymultinom <- multinom(Y_factor ~ X1 +  X2, data = df)
\end{lstlisting}
{\small\begin{verbatim}
## # weights:  12 (6 variable)
## initial  value 15.380572 
## iter  10 value 0.194988
## iter  20 value 0.010826
## iter  30 value 0.006357
## iter  40 value 0.005358
## iter  50 value 0.004594
## iter  60 value 0.003156
## iter  70 value 0.002732
## iter  80 value 0.002396
## iter  90 value 0.002072
## iter 100 value 0.001924
## final  value 0.001924 
## stopped after 100 iterations
\end{verbatim}}
\begin{lstlisting}
summary(mymultinom)$coefficients
\end{lstlisting}
\begin{verbatim}
##    (Intercept)           X1            X2
## 1 -19.79766347 12.677891791  -5.560148528
## 2  27.10761881  2.521385121 -10.282168169
\end{verbatim}
\subsubsection{Modelo estimado}
Ecuaciones del \lb{modelo estimado}:
\[ \begin{array}{l}
	\log\dfrac{p_1}{p_3}=-19.798+12.678x_1-5.56x_2\\
	\log\dfrac{p_2}{p_3}=27.108+2.521x_1-10.282x_2
\end{array} \]
Recordemos que los coeficientes del modelo miden la variación del logaritmo de los \code{odds} por unidad de cambio en el correspondiente predictor.

Tomando exponenciales sobre los coeficientes, medimos las variaciones producidas sobre los \code{odds} directamente.

El algoritmo ha parado después de 100 iteraciones.

Valor de la $-\log L:\,0.0019242$

Valor de la \code{deviance} del modelo: $0.0038484$
\subsection{Modelo logístico multinomial con categorías ordinales}
\subsubsection{Modelo teórico}
Este tipo de modelo se utiliza cuando las categorías de la variable dependiente representan un orden lógico o jerarquía.

Expresión general del \lb{modelo}: \[ \log\dfrac{Pr[Y\le j]}{1-Pr[Y\le j]}=\theta_j'\mathbf{X},\quad j=1,\dots,g-1, \] donde $\theta_j=(\theta_{j0},\dots,\theta_{jk})'\in\R^{k+1}$ contiene los parámetros del modelo para cada categoría $j$ y $\mathbf{X}=(X_0,\dots,X_k)'$ las variables que podemos medir en nuestros individuos para predecir $Y$.

\lb{Interpretación de los coeficientes:} entender cómo un cambio en una variable predictora afecta a la razón de probabilidades de que la variable dependiente sea menor o igual a una categoría específica en comparación con las categorías superiores.