\section{Regresión logística y multinomial}
\subsection{Modelo de regresión logística}
\subsubsection{Contexto}
Deseamos predecir una variable binaria $Y$ que solo toma los valores 0 y 1.
\begin{itemize}
	\item Además, estos valores numéricos solo indicarán la pertenencia o no a un determinado grupo.
\end{itemize}
\bu{Ejemplos:}
\begin{itemize}
	\item Determinar si un paciente tiene o no una determinada enfermedad en función de diferentes variables (edad, presión arterial, nivel de colesterol, etc.).
	\begin{itemize}
		\item En este caso el valor 1 suele indica que sí la tiene y 0 que no.
	\end{itemize}
	\item Predecir si un estudiante aprueba o no un examen en función de las horas de estudio.
	\item Predecir si un mensaje de correo electrónico es spam o no en función de las palabras clave.
	\item Predecir si un cliente comprará o no un determinado producto en función de la edad y el salario.
	\item Predecir si un paciente tiene diabetes o no en función de variables como el nivel de glucosa, la presión arterial y el índice de masa corporal (IMC).
\end{itemize}
\subsubsection{Objetivo}
\lb{Objetivo:} predecir la variable respuesta $Y$ a partir de $k$ variables numéricas $X_1,\dots,X_k$ utilizando una única función \[ h_\theta(\mathbf{x})=g(\mathbf{\theta'x})=g(\theta_0+\theta_1x_1+\cdots+\theta_kx_k), \]donde $\mathbf{\theta}=(\theta_0,\dots,\theta,\theta_k)'\in\R^{k+1}$ contiene los parámetros del modelo y $\mathbf{X}=(X_0,\dots,X_k)'$ las variables que podemos medir en nuestro individuos para predecir $Y$.

Para mejorar la notación hemos incluido una variable artificial $X_0$ siempre que vale 1.
\subsubsection{¿Cómo elegir la función $g$?}
La función $g$ debe transformar esos valores numéricos (lineales) en números entre 0 y 1 que nos indicarán la \lb{probabilidad} de que el individuo pertenezca al grupo ($Y=1$): \[ g:\R\to[0,1] \]y $h_\theta(\mathbf{x})\approx Pr(Y=1|\mathbf{X=x})$, donde $\mathbf{X}=(X_0,\dots,X_k)'$.

\lb{Regla de decisión:} \[ \begin{array}{l}
	h_\theta(\mathbf{x})\ge0.5\to\hat{y}=1\\
	h_\theta(\mathbf{x})<0.5\to\hat{y}=0
\end{array} \]donde $\hat{y}$ representa el valor que predecimos para $Y$ cuando $\mathbf{X=x}$.

Existen diversas opciones para determinar $g$, la más popular es la \lb{función logística} (o \lb{sigmoide}) \[ g(z)=\dfrac{1}{1+\exp(-z)}=\dfrac{\exp(z)}{1+\exp(z)}. \]

\subsection{Función logística}
\begin{minipage}{0.5\textwidth}
	\begin{lstlisting}
par(mfrow = c(1, 1))
g <- function(x) 1/(1+exp(-x))
x <- seq(-4, 4, length = 100)
y <- g(x)
plot(x, y, xlab = 'z', ylab = 'g(z)', col = 'blue', type = "l", ylim = c(-0.1, 1.1), yaxp = c(0, 1, 5))
abline(h = 0, lty = 2)
abline(h = 1, lty = 2)
abline(v = 0, lty = 2, col = "red")
\end{lstlisting}
\end{minipage}\qquad\begin{minipage}{0.45\textwidth}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Propiedades
\end{itemize}
\begin{itemize}[label=$-$]
	\item Es continua 
	\item Estrictamente creciente.
	\item Recorrido de 0 a 1.
	\item Transformará el valor $\mathbf{\theta'x}\in\R$ en un valor $h_\theta(\mathbf{x})\in[0,1]$.
	\item $g(0)=0.5$
	\item \lb{Regla de decisión:} \[ \begin{array}{l}
		\mathbf{\theta'x}\ge0\to \hat{y}=1\\
		\mathbf{\theta'x}<0\to\hat{y}=0
	\end{array} \]
	\item La función $I_\theta(\mathbf{x})=\mathbf{\theta'x}$ define un \lb{índice de separación entre las categorías de $Y$}
\end{itemize}
\end{minipage}

\includegraphics[width=0.45\linewidth]{"Temas/Imágenes/Tema 3/screenshot001"}
\subsection{¿Cómo determinar una función costo que penalice las decisiones erróneas?}
\subsubsection{Función costo}
\begin{minipage}{0.45\textwidth}
	Para $z=h_\theta(\mathbf{x})$, definimos la función costo\[ c(z,y)=\begin{cases}
		-\log(z) & \text{si }y=1\\
		-\log(1-z) & \text{si }y=0\\
	\end{cases} \]
	O, equivalentemente, \[ c(z,y)=-y\log(z)-(1-y)\log(1-z), \] donde $y\in\{0,1\}$ y los logaritmos son neperianos.
\end{minipage}\quad\begin{minipage}{0.55\textwidth}
\begin{lstlisting}
par(mfrow = c(1, 1))
x <- seq(0, 1, length = 100)
plot(x, -log(x), xlab = 'z', ylab = 'c(z, y)', col = 'blue', type = "l",  ylim = c(-0.1, 4), yaxp = c(0, 4, 5))
text(0.1, 3.5, "y = 1", col = "blue", adj = c(0, 0))
lines(x, -log(1-x), xlab = 'z', ylab = 'c(z, y)', col = 'red', type = "l",  ylim = c(-0.1, 4), yaxp = c(0, 4, 5))
text(0.8, 3.5, "y = 0", col = "red", adj = c(0, 0))
\end{lstlisting}
\end{minipage}
\begin{flushright}
	\includegraphics[width=0.55\linewidth]{"Temas/Imágenes/Tema 3/screenshot002"}
\end{flushright}
\subsubsection{Criterio}
Minimizar el valor esperado de la función costo \[ \min_\theta J(\theta)=E[c(h_\theta(\mathbf{X}), Y)] \]
Para determinar los valores óptimos de los parámetros:
\begin{itemize}
	\item Dispondremos de una muestra (\lb{training sample}) y de individuos en los que se conozcan tanto los valores de $\mathbf{x}$ como los valores de $y$ (\lb{aprendizaje supervisado}).
	\item Calcularemos los costos en los valores muestrales.
	\item Determinaremos los valores de los parámetros que minimizan estos costos.
\end{itemize}
\subsubsection{Otra formulación del problema}
Si $p=Pr(Y=1)$ este modelo es equivalente a suponer que existe una relación lineal entre las variables $X_1,\dots,X_k$ y la función \code{log-odd de p}. \[ \log\dfrac{p}{1-p}=\mathbf{\theta'X}. \]
Puesto que esto es equivalente a suponer que \[ p=Pr(Y=1)=\dfrac{\exp(\mathbf{\theta'X})}{1+\exp(\mathbf{\theta'X})}=g(\mathbf{\theta'X}) \]
\subsection{Inferencia y predicción}
\subsubsection{Función costo empírica}
Datos muestrales: $(\mathbf{x}^{(1)},y^{(1)}),\dots,(\mathbf{x}^{(n)},y^{(n)})$.

Función costo: \[ J(\theta)\coloneq\dfrac{1}{n}\sum_{i=1}^{n}c(h_\theta(\mathbf{x}^{(i)}), y^{(i)}). \]
Desarrollando la función $c$ obtenemos \[ \begin{aligned}
	J(\theta)&=-\dfrac{1}{n}\sum_{i=1}^{n}\left[y^{(i)}\log(h_\theta(\mathbf{x}^{(i)}))+(1-y^{(i)})\log(1-h_\theta(\mathbf{x}^{(i)}))\right]\\
	&=-\dfrac{1}{n}\sum_{i=1}^{n}\left[y^{(i)}\log(g(\mathbf{\theta'x}^{(i)}))+(1-y^{(i)})\log(1-g(\mathbf{\theta'x}^{(i)}))\right]
\end{aligned} \]
\subsubsection{Función costo empírica en forma matricial}
Denotando
\begin{itemize}
	\item $M=(x_j^{(i)})$ a la matriz de datos.
	\item $\mathbf{y}=(y^{(i)})$ al vector columna con los valores de $Y$
	\item $h\coloneq g(M\mathbf{\theta})$ al vector columna con los ajustes en cada individuo, entonces \[ J(\theta)\coloneq-\dfrac{1}{n}\left[\mathbf{y}'\log(h)+(1_n-\mathbf{y})'\log(1_n-h)\right] \] donde $1_n$ representa un vector columna de dimensión $n$.
\end{itemize}
\subsubsection{Objetivo}
Ajsutar el parámetro $\theta$ para que $J$ tome el menor valor posible.
\begin{itemize}
	\item \lb{Solución:} Algoritmos iterativos de búsqueda como, por ejemplo, el algoritmo del gradiente descendente.
	\begin{itemize}
		\item Práctica complementaria de regresión logística.
	\end{itemize}
	\item Existen varias librerías de \code{R} que permiten obtener estimaciones de los parámetros del modelo logístico.
	\begin{itemize}
		\item Práctica de regresión logística.
	\end{itemize}
\end{itemize}
\subsection{Un ejemplo sencillo}
\subsubsection{Datos muestrales}
Como en técnicas anteriores usaremos un ejemplo sencillo para comprobar cómo funciona nuestro modelo.

Supongamos que tenemos dos variables predictoras $X_1$ y $X_2$ ($k = 2$) y los datos siguientes:
\begin{center}
\begin{tabular}{c|ccc}
	Individuo & $X_1$ & $X_2$ & $Y$ \\
	\hline
	1 & 1 & 2 & 0 \\
	2 & 2 & 1 & 0 \\
	3 & 3 & 1 & 0 \\
	4 & 2 & 2 & 0 \\
	5 & 5 & 1 & 1 \\
	6 & 5 & 3 & 1 \\
	7 & 3 & 2 & 0 \\
	8 & 4 & 3 & 1 \\
	9 & 4 & 4 & 1 \\
	10 & 5 & 4 & 1 \\
	\hline
\end{tabular}
\end{center}
Lo primero que tenemos que hacer (si es posible) es dibujar estos puntos añadiendo una etiqueta para distinguir los de cada grupo.

\begin{minipage}{0.45\textwidth}
	\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = 20, xlim = c(0,6), ylim = c(0,6), cex = 1.2)
text(X1 + 0.2, X2, Y, cex = 1)
	\end{lstlisting}
	\begin{center}
		\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot003"}
	\end{center}
\end{minipage}\qquad\begin{minipage}{0.5\textwidth}
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X1, X2, xlab = "X1", ylab = "X2", pch = as.integer(Y), xlim = c(0,6), ylim = c(0,6), cex = 1.2)
legend('topleft', legend = c('Y = 0','Y = 1'), pch = 0:1, cex = 1)
\end{lstlisting}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot004"}
\end{center}
\end{minipage}

En ambas gráficas podemos observar que los dos grupos se pueden separar muy bien con rectas.

Por lo tanto nuestro modelo será \[ h_\theta(\mathbf{x})=g(\theta_1+\theta_1x_1+\theta_2x_2). \]
Otra forma de analizar los grupos es calcular medidas descriptivas en cada uno de ellos.
\begin{itemize}
	\item Por ejemplo podemos calcular las medias en cada grupo:
	\pagebreak
	\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
tmp1 = tapply(X1, Y, mean)
tmp2 = tapply(X2, Y, mean)
tmp = rbind(tmp1, tmp2)
colnames(tmp) = paste0("Y = ", colnames(tmp))
rownames(tmp) = paste0("Media de ", c("X1", "X2"))
tmp
	\end{lstlisting}
	\begin{verbatim}
            Y = 0 Y = 1
Media de X1   2.2   4.6
Media de X2   1.6   3.0
	\end{verbatim}
\end{itemize}
Estas diferencias también se pueden ver representado $x^{(i)}$ frente a $Y$.

\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X1, Y, ylim = c(-0.5, 1.5), cex = 1.2, pch = 20, col = "blue")
\end{lstlisting}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot005"}
\end{center}

\end{minipage}\qquad\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
plot(X2, Y, ylim = c(-0.5, 1.5), cex = 1.2, pch = 20, col = "magenta")
\end{lstlisting}
\begin{center}
	\includegraphics[width=\linewidth]{"Temas/Imágenes/Tema 3/screenshot006"}
\end{center}

\end{minipage}

Podemos observar cómo la primera variable separa mejor a los grupos que la segunda (en los valores muestrales).

De forma similar se pueden representar histogramas o diagramas caja-bigote para comparar las variables en cada grupo.

Podemos calcular la función $J(\theta)$ en \code{R} con los datos anteriores.
\pagebreak
\begin{lstlisting}
X1 <- c(1, 2, 3, 2, 5, 5, 3, 4, 4, 5)
X2 <- c(2, 1, 1, 2, 1, 3, 2, 3, 4, 4)
Y <- c(0, 0, 0, 0, 1, 1, 0, 1, 1, 1)
n <- length(Y)
k <- 2
X0 = rep(1, n)
M <- matrix(c(X0, X1, X2), nrow = n, ncol = k + 1, byrow = FALSE)
g <- function(z){
	g = exp(z)/(1 + exp(z))
	return(g)
}
J <- function(theta){
	J = - sum(Y * log(g(M %*% theta)) + (1 - Y) * log(1 - g(M %*% theta)))/n
	return(J)
}
\end{lstlisting}
Podemos aplicar un \lb{método iterativo} para la obtención del óptimo.
\begin{itemize}
	\item Por ejemplo, el \lb{método del gradiente descendiente} (se detallará su aplicación en la práctica complementaria de regresión logística).
\end{itemize}
\begin{lstlisting}
z <- c(-3.5, 1, 0)
alpha <- 1/3
m <- 1000
J1 <- 1:m
for (i in 1:m) {
	h <- g(M %*% z)
	z <- z - (alpha/n) * t(M) %*% (h-Y)
	J1[i] <- J(z)
}
\end{lstlisting}
Partiendo del punto inicial $\theta^{(0)}=(-3.5,\,1,\,0)$ (recta vertical de separación $x_1=3.5$) y después de \lb{1000 iteraciones} obtendremos $\hat{\theta}_0=-10.7505,\,\hat{\theta}_1=2.4594$ y $\hat{\theta}_3=1.0287$, con valor de $J(\hat{\theta})=0.0597$.

En este caso,\[ I_{\hat{\theta}}(x_1,_2)=-10.7505+2.4591x_1+1.0287x_2 \]
La recta que marca la frontera de esta solución será\[ -10.7505+2.4594x_1+1.0284x_2=0 \]
Esto es, $x_2=10,4506-2.3908x_1$