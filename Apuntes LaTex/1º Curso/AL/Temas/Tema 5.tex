\section{Álgebra Lineal Computacional}
\subsection{Factorizaciones $LU,\:PLU$ y $Cholesky$}
\subsubsection{Factorización $PU\,(PLU)$}
Dada una matriz $A\in M_n(\R)$, se llama factorización $LU$ a una factorización
(o descomposición) de la forma \[ A=LU \]donde\[ \underbrace{\begin{bmatrix}
		a_{11} & \cdots & a_{1n}\\
		\vdots  & & \\
		a_{m1} & \cdots & a_{nn}
\end{bmatrix}}_A=\underbrace{\left[\begin{array}{cccccc}
		1 & 0 & \cdots & \cdots & \cdots & 0 \\
		l_{21} & 1 & 0 & \cdots & \cdots & 0 \\
		l_{31} & l_{32} & 1 & 0 & \cdots & 0 \\ \hdashline
		l_{n1} & \cdots & \cdots & \cdots & l_{n,n-1} & 1
	\end{array}\right]}_L\cdot\underbrace{\begin{bmatrix}
		u_{11} & u_{12} & \cdots & u_{1n} \\ 
		& u_{22} & \cdots & u_{2n} \\ 
		&  & \ddots &  \\ 
		0 &  &  & u_{nn }
\end{bmatrix}}_U  \]

$L$ triangular inferior con 1s en la diagonal principal\\
$U$ triangular superior\\
$L=$Lower, $U=$Upper
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Aplicaciones
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item Cálculo de determinantes\[ |A|=|L\cdot
	U|=|L|\cdot|U|=1\prod_{j=1}^{n}u_{jj} \]
	\item Resolución de sistemas lineales \[ Ax=b\Longleftrightarrow
	LUx=b\Longleftrightarrow\begin{cases}
		Ux=z\\
		Lz=b
	\end{cases}\]son dos sistemas fácilmente resolubles porque $U$ y $L$ son
	triangulares superior e inferior, respectivamente.
	
	En efecto: \[ \begin{array}{l}
		\begin{bmatrix}
			u_{11} & u_{12} & \cdots & u_{1n}\\
			0 & u_{22} & \cdots & u_{2n}\\
			\vdots & & \ddots& \\
			0 & & & u_{nn}
		\end{bmatrix}\cdot\begin{bmatrix}
			x_1\\
			\vdots\\
			\vdots\\
			x_n
		\end{bmatrix}=\begin{bmatrix}
			z_1\\
			z_2\\
			\vdots\\
			z_n
		\end{bmatrix}\\
		u_{nn}x_n=z_n\longrightarrow x_n=\dfrac{z_n}{u_{nn}}\\
		u_{n-1,n-1}x_{n-1}+u_{n-1,n}x_n=z_{n-1}\\
		x_{n-1}=\dfrac{z_{n-1}u_{n-1,n}x_n}{u_{n-1,n-1}}
	\end{array} \]
	En general, \[ x_i=\dfrac{z_1- \displaystyle\sum_{j=i+1}^{n}u_{ij}x_j}{u_{ii}} \]Para que
	esto funciones $u_{ii}\neq0$
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Coste computacional
\end{itemize}
Consideremos el caso $n=4$\[ \left.\begin{array}{r}
	u_{11}x_1+u_{12}x_2+u_{13}x_3+u_{14}x_4=z_1\\
	u_{22}x_2+u_{23}x_3+u_{24}x_4=z_2\\
	u_{33}x_3+u_{34}x_4=z_3\\
	u_{44}x_4=z_4
\end{array}\right\} \]$\begin{array}{l}
	x_4=\dfrac{z_4}{u_{44}}\qquad\text{1 operación}\\
	x_3=\dfrac{z_3-u_{34}x_4}{u_{33}}\qquad\text{3 operaciones}\\
	x_2=\dfrac{z_2-u_{23}x_3-u_{24}x_4}{u_{22}}\qquad\text{5 operaciones}\\
	x_1=\dfrac{z_2-u_{12}x_2-u_{13}x_3-u_{14}x_4}{u_{11}}\qquad\text{7
		operaciones}\\
	\text{Total} = 16=4^2=n^2
\end{array}$

En general, esto es cierto para un sistema de orden $n$, es decir, el coste
computacional de resolver un sistema triangular es del orden $n^2$, escribimos
$O(n^2)$.

\bu{¿Cómo se calculan $L$ y $U$ tales que $A=LU$?}

Haciendo eliminación Gaussiana. Veamos un ejemplo \begin{center}
	$A=\begin{bmatrix}
		2 & -1 & 0 \\
		-1 & 2 & -1 \\
		0 & -1 & 2
	\end{bmatrix}$ matriz de Toepliz de dimensión 3.
\end{center}
$\begin{bmatrix}
	2 & -1 & 0 \\
	-1 & 2 & -1 \\
	0 & -1 & 2
\end{bmatrix}\xrightarrow{F_2\to F_2+\frac{1}{2}F_1}\begin{bmatrix}
	2 & -1 & 0 \\ 
	0 & \dfrac{3}{2} & -1 \\ 
	0 & -1 & 2
\end{bmatrix}\xrightarrow{F_3\to F_3+\frac{2}{3}F_2}\begin{bmatrix}
	\bboxed{2} & -1 & 0 \\ 
	0 & \bboxed{\dfrac{3}{2}} & -1 \\ 
	0 & 0 & \bboxed{\dfrac{4}{3}}
\end{bmatrix} $

La matriz $U$ es \[U=\begin{bmatrix}
	2 & -1 & 0 \\ 
	0 & \dfrac{3}{2} & -1 \\ 
	0 & 0 & \dfrac{4}{3}
\end{bmatrix}\]
¿Dónde está $L$? Una vez conocemos el pivot de la fila $\jmath$, y la entrada
que se desea eliminar en la fila $\imath$, el \lb{multiplicador} $l_{ij}$ se
define como \[ l_{ij}=\dfrac{\text{entrada a eliminar en la fila
		$\imath$-ésima}}{\text{pivot en la fila $\jmath$}} \]Así, en el ejemplo
anterior:\[ \begin{array}{l}
	l_{21}=-\dfrac{1}{2},\quad l_{31}=\dfrac{0}{2}=0,\quad
	l_{32}=-\dfrac{1}{\frac{3}{2}}=-\dfrac{2}{3}\\
	L=\begin{bmatrix}
		1 & 0 & 0\\
		-\dfrac{1}{2} & 1 & 0 \\
		0 & -\dfrac{2}{3} & 1
	\end{bmatrix}
\end{array} \]En efecto: \[ LU=\begin{bmatrix}
	1 & 0 & 0\\
	-\dfrac{1}{2} & 1 & 0 \\
	0 & -\dfrac{2}{3} & 1
\end{bmatrix}\cdot\begin{bmatrix}
	2 & -1 & 0 \\ 
	0 & \dfrac{3}{2} & -1 \\ 
	0 & 0 & \dfrac{4}{3}
\end{bmatrix}=\begin{bmatrix}
	2 & -1 & 0 \\
	-1 & 2 & -1 \\
	0 & -1 & 2
\end{bmatrix}=A \]
\begin{tikzpicture}
	\node[red, line width=1.5, text width=\textwidth, draw=red, fill=red!10]
	{\underline{Nota 1:} (Factorización $PLU$)\\
		En ocasiones es preciso permutar algunas filas de $A$ para conseguir pivotes no
		nulos o llevar filas nulas al final. Todo ello se consigue a través de una
		matriz de permutación $P$.\\
		A la factorización $PA=LU$ se le llama factorización $PLU$. Nótese que
		$P^{-1}=P^\intercal$. Por tanto, \[ A=P^\intercal LU. \]};
\end{tikzpicture}

\begin{tikzpicture}
	\node[red, line width=1.5, text width=\textwidth, draw=red, fill=red!10]
	{\underline{Nota 2:} (Coste computacional)\\
	Se puede probar que el coste computacional de resolver un sistema lineal $Ax=b$, mediante factorización $LU$, es decir,\[ LUx=b\longrightarrow\begin{cases}
		Ux=z\\
		Lz=b
	\end{cases} \]es $\dfrac{n^3+3n^3-1}{3}$, siendo $n$ el tamaño de $A$.};
\end{tikzpicture}

Volviendo a la factorización \[ \begin{bmatrix}
	2 & -1 & 0 \\
	-1 & 2 & -1 \\
	0 & -1 & 2
\end{bmatrix}=\begin{bmatrix}
1 &  & \\
-\dfrac{1}{2} & 1 &  \\
0 & -\dfrac{2}{3} & 1
\end{bmatrix}\cdot\begin{bmatrix}
2 & -1 & 0 \\ 
 & \dfrac{3}{2} & -1 \\ 
 &  & \dfrac{4}{3}
\end{bmatrix}\]se observa que el carácter simétrico de $A$ se pierde en la factorización $LU$. Esta simétrica se puede recuperar separando los pivots (diagonal de $U$) en una matriz diagonal $D$ y dividiendo cada fila de $U$ por su correspondiente pivot. Se obtiene así: \[ \underbrace{\begin{bmatrix}
	2 & -1 & 0 \\
	-1 & 2 & -1 \\
	0 & -1 & 2
	\end{bmatrix}}_A=\underbrace{\begin{bmatrix}
	1 &  & \\
	-\dfrac{1}{2} & 1 &  \\
	0 & -\dfrac{2}{3} & 1
\end{bmatrix}}_L\cdot\underbrace{\begin{bmatrix}
2 & & \\
& \dfrac{2}{3} & \\
 & & \dfrac{4}{3}
\end{bmatrix}}_D\cdot\underbrace{\begin{bmatrix}
1 & -\dfrac{1}{2} &0  \\
 & 1 & -\dfrac{2}{3} \\
&  & 1
\end{bmatrix}}_{L^\intercal} \]
A una factorización del tipo $A=LDL^\intercal$ se le llama de Cholesky.

Además, $D$ se puede factorizar como \[ D=\underbrace{\begin{bmatrix}
		2 & & \\
		& \dfrac{2}{3} & \\
		& & \dfrac{4}{3}
\end{bmatrix}}_D=\underbrace{\begin{bmatrix}
\sqrt{2} & & \\
& \sqrt{\dfrac{2}{3}} & \\
& & \sqrt{\dfrac{4}{3}}
\end{bmatrix}}_{\sqrt{D}}\cdot\underbrace{\begin{bmatrix}
\sqrt{2} & & \\
& \sqrt{\dfrac{2}{3}} & \\
& & \sqrt{\dfrac{4}{3}}
\end{bmatrix}}_{\sqrt{D}} \]
Se llega así a la factorización \[ A=L\sqrt{D}\sqrt{D}L^\intercal \] la cual se puede simplificar como: \[ \begin{array}{l}
	L\sqrt{D}=\begin{bmatrix}
		1 &  & \\
		-\dfrac{1}{2} & 1 &  \\
		0 & -\dfrac{2}{3} & 1
	\end{bmatrix}\cdot\begin{bmatrix}
	\sqrt{2} & & \\
	& \sqrt{\dfrac{2}{3}} & \\
	& & \sqrt{\dfrac{4}{3}}
	\end{bmatrix}=\begin{bmatrix}
	\sqrt{2} &  &  \\ 
	-\dfrac{\sqrt{2}}{2} & \sqrt{\dfrac{3}{2}} &  \\ 
	0 & -\dfrac{2}{3}\sqrt{\dfrac{3}{2}} & \sqrt{\dfrac{4}{3}}
	\end{bmatrix}\\
	\sqrt{D}L^\intercal=\begin{bmatrix}
\sqrt{2} & & \\
& \sqrt{\dfrac{2}{3}} & \\
& & \sqrt{\dfrac{4}{3}}
\end{bmatrix}\cdot\begin{bmatrix}
1 & -\dfrac{1}{2} &0  \\
 & 1 & -\dfrac{2}{3} \\
&  & 1
\end{bmatrix}=\begin{bmatrix}
\sqrt{2} & -\dfrac{\sqrt{2}}{2} & 0 \\ 
 & \sqrt{\dfrac{3}{2}} & -\dfrac{2}{3}\sqrt{\dfrac{3}{2}} \\ 
 &  & \sqrt{\dfrac{4}{3}}
\end{bmatrix} 
\end{array} \] es decir, \[ \bboxed{A=\tilde{L}\tilde{L}^\intercal}, \]que es una nueva forma simplificada de la factorización de Cholesky.

\begin{tikzpicture}
	\node[red, draw=red, fill=red!10, line width=1.5, text width=\textwidth] {\underline{Nota 1:}\\
	Nótese que este último procedimiento ha funcionado porque los pivots son positivos. Esto siempre sucede para una clase particular de matrices, llamadas \textbf{definidas positivas}. Éstas son matrices que satisfacen la condición \[ x^\intercal Ax>0\:\forall x\neq0. \]Las estudiaremos en detalle más adelante.};
\end{tikzpicture}

\begin{tikzpicture}
	\node[red, draw=red, fill=red!10, line width=1.5, text width=\textwidth] {\underline{Nota 2:}\\
		Nótese que en la factorización Cholesky sólo tenemos $\tilde{L}$ y su traspuesta $\tilde{L}^\intercal$. Por tanto, el coste computacional de Cholesky es la mitad que $LU$, es decir, del orden $\dfrac{n^3}{6}$};
\end{tikzpicture}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Recomendación
\end{itemize}
$Ax=b$
\begin{itemize}
	\item Si $A$ es simétrica y definida positiva, usar \lb{Cholesky}.
	\item Para el resto, usar $LU$.
\end{itemize}
\subsection{Análisis de errores}
Analizamos la cuestión de la estabilidad en la resolución de un sistema lineal $Ax=b$.

Supongamos que $b$ se ve afectado por errores de maldición, representación y/o redondeo. La solución cambiará también, es decir, la entrada será $b+\triangle b$ y la salida $x+\triangle x$.

\bu{¿Cómo podemos estimar o cuantificar $\triangle x$ en la función de $\triangle b$?}

Obviamente, la matriz $A$ juega un papel clave.\\
Necesitamos introducir normas de matrices. Recordemos que para vectores tenemos las normas:\[ x=(x_1,\dots,x_n)\qquad\begin{array}{l}
	\|x\|_2=\sqrt{x_1^2+x_2^2+\cdots+x_n^2}\\
	\|x\|_1^p=\sum x_i\|^p,\qquad p=1,\dots\\
	\|x\|_\infty=\max\{|x_1|,|x_2|,\dots,|x_n|\}
\end{array} \]Consideraremos la norma \[ \|A\|=\underset{x\neq0}{\max}\dfrac{\|Ax\|}{\|x\|} \]con $\|\cdot\|$ cualquiera de las normas de vectores anterior, por ejemplo, la norma $\|\cdot\|_2$.

Obviamente, $\dfrac{\|Ax\|}{\|x\|}\le\|A\|,\qquad\forall x\neq0$, y así \[ \|Ax\|\le\|A\|\cdot\|x\|. \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Número de condicionamiento de $A$).
\end{itemize}
Dada $A\in M_n(\R)$ se llama número de condicionamiento de $A$ al número \[ \mathrm{c}(A)=\|A\|\cdot\|A^{-1}\| \]Obviamente, suponemos $A$ invertible.

Volvemos al sistema $Ax=b$, que suponemos compatible determinado, es decir, tiene una única solución \[ x=A^{-1}b. \]Si el término de la derecha es $b+\triangle b$, entonces se tiene \[ A(x+\triangle x)=b+\triangle b \] de modo que \[ Ax+A\triangle A=b+\triangle b \]con lo que \[ A\triangle x=\triangle b, \]es decir, \[ \triangle x=A^{-1}(\triangle b), \]y así \[ \|\triangle x\|=\|A^{-1}(\triangle b)\|\le \|A^{-1}\|\cdot\|\triangle b\|, \]lo que nos da una cota superior del error absoluto $\triangle x$. Más importante que el error absoluto es el relativo, el cual estimamos del siguiente modo: \[ \|\triangle x\|\le\|A^{-1}\|\cdot\|\triangle b\|\cdot\dfrac{\|Ax\|}{\|b\|}\le\|A^{-1}\|\cdot\|A\|\cdot\|x\|\cdot\dfrac{\|\triangle b\|}{\|b\|} \] de donde \[ \dfrac{\|\triangle b\|}{\|x\|}\le\|A\|\cdot\|A^{-1}\|\cdot\dfrac{\|\triangle b\|}{\|b\|}=\mathrm{c}(A)\cdot\dfrac{\|\triangle b\|}{\|b\|} \]
Esta desigualdad se lee: \emph{"el error relativo en la solución es del orden de $\mathrm{c}(A)$ veces el error relativo en el input $b$"}.

Si $\mathrm{c}(A)$ es grande, se dice que la matriz $A$ está mal condicionada.

La \lb{regla básica a tener en cuenta} es que para el sistema $Ax=b$, el ordenador pierde $\log_{10}(\mathrm{c}(A))$ decimales en errores de redondeo.

\begin{tikzpicture}
	\node[red, draw=red, fill=red!10, line width=1.5, text width=\textwidth] {\underline{Nota:} Se puede probar que si el error inicial viene asociado a la matriz $A$, es decir, tenemos $A+\triangle A$, la estimación anterior en términos del número de condición sigue siendo válida.};
\end{tikzpicture}

Veamos un ejemplo concreto.

\Ej

$A=\begin{bmatrix}
	1 & 7\\
	0 & 1
\end{bmatrix},\quad b=\begin{bmatrix}
7\\
1
\end{bmatrix}$

$Ax)b\Longleftrightarrow\begin{bmatrix}
	1 & 7\\
	0 & 1
\end{bmatrix}\cdot\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}=\begin{bmatrix}
7\\
1
\end{bmatrix}$

$\left.\begin{array}{r}
	x_1+7x2=7\\
	x_2=1
\end{array}\right\}\longrightarrow x_1=0$

Solución: $(x_1,x_2)=(0,1)$

Supongamos ahora que $\triangle b=\begin{bmatrix}
	0\\
	0.1
\end{bmatrix}$, es decir, una 

$\begin{array}{l}
	A(\triangle x)=\triangle b\\
	\begin{bmatrix}
		1 & 7\\
		0 & 1
	\end{bmatrix}\cdot\begin{bmatrix}
	x_1\\
	x_2
	\end{bmatrix}=\begin{bmatrix}
	0\\
	0.1
	\end{bmatrix}\\
	\left.\begin{array}{r}
		\triangle x_1+7\triangle x_2=0\\
		\triangle x_2=0.1
	\end{array}\right\}\longrightarrow\triangle x_1=-7\triangle x_2=-0.7
\end{array}$

El error relativo en $x$ es: \[ \dfrac{\|\triangle x\|}{\|x\|}=\dfrac{\sqrt{(-0.7)^2+(0.1)^2}}{\sqrt{0^2+1^2}}=0.1\sqrt{50} \]Comparando con el error relativo en $b$:
\[ \dfrac{\|\triangle b\|}{\|b\|}=\dfrac{\sqrt{0^2+0.1^2}}{7^2+1^2}=\dfrac{0.1}{\sqrt{50}} \]Por tanto:\[ \dfrac{\|\triangle x\|}{\|x\|}=0.1\sqrt{50}=0.1\sqrt{50}\cdot\dfrac{\sqrt{50}}{\sqrt{50}}=50\cdot\dfrac{0.1}{\sqrt{50}}=50\cdot\dfrac{\|\triangle b\|}{\|b\|} \]
Si calculamos en Python $\mathrm{c}(A)=51$ para la norma de Frobenius.

Observamos que $\mathrm{c}(A)$ de una estimación bastante precisa de la propagación del error.

\bu{¿Qué hacer si $\mathrm{c}(A)>>1$?}

En lugar de resolver el sistema $Ax=b$, resolvemos el sistema equivalente \[ C^{-1}Ax=C^{-1}b \]de modo que la matriz $C$ haga que \[ \mathrm{c}(C^{-1}A)<<\mathrm{c}(A) \]\bu{¿Cómo elegir $C$?}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Opción naive (ingenua):
\end{itemize}
Tomar $C=A$. En este caso, $C^{-1}A=A^{-1}A=I$ cuyo número de condición es 1. Sin embargo, calcular $A^{-1}$ es igual de costoso que resolver $Ax=b$.

El intento $C=A$ no resuelve el problema, pero nos sugiere que $C$ debe ser \lb{próxima} a $A$.

Otras dos opciones más prácticas son:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item Si $A$ no tiene cero en la diagonal, tomamos \[ C=\mathrm{diag}(a_{11},a_{22},\dots,a_{nn}) \]
	\Ej
	
	$A=\begin{bmatrix}
		8 & -2\\
		-2 & 50
	\end{bmatrix}\quad$ Python $\longrightarrow\mathrm{c}(A)=6.3371$
	
	$C=\begin{bmatrix}
		8 & 0\\
		0 & 50
	\end{bmatrix}\qquad C^{-1}=\begin{bmatrix}
	\dfrac{1}{8} & 0\\
	0 & \dfrac{1}{50}
	\end{bmatrix}$
	
	$\mathrm{c}(C^{-1}A)=2.084$ para la suma de Frobenius
	\item Tomar $C^{-1}= p(A)$, un polinomio en $A$ que resulta de \lb{truncar} la serie de $A^{-1}$. Es decir, \begin{align*}
		A^{-1}&=\left(I-(I-A)\right)^{-1}\equiv\dfrac{1}{I-(I-A)}\\
		&=I+(I-A)+(I-A)^2+\cdots
	\end{align*}que resulta de la serie geométrica\[ \dfrac{1}{1-x}=1+x+x^2+\cdots \]la cual converge si $0<x<1,\quad\|I-A\|<1$.
	
	Por tanto, \[ C^{-1}=I+\sum_{k=1}^{N}(I-A)^k \]
\end{enumerate}
Estas ideas conducen a los llamados \lb{métodos iterativos}, que estudiamos a continuación:

Dado que $Ax=b$, para toda matriz $P$ del mismo tamaño que $A$, que llamamos \lb{precondicionador}, se tiene que \[ Px=(P-A)x+b \]Esta descomposición sugiere el algoritmo iterativo siguiente:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item Inicialización: tomar $x_0\in\R^n$
	\item Iteración hasta convergencia \[ Px_{k+1}=(P-A)x_k+b. \]
\end{enumerate}
El éxito del algoritmo está garantizado si se cumple:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $x_{k+1}$ se calcula fácilmente a partir de $x_k$.
	\item Los errores $e_k=x-x_k$ se aproximan a cero cuando $k\to+\infty$, es decir, el algoritmo converge.
\end{enumerate}
Veamos qué se ha de cumplir para que el algoritmo sea convergente:\[ \begin{array}{l}
	Px_{k+1}=(P-A)x_k+b\\
	Px=(P-A)x+b
\end{array} \]Si restamos:\[ \begin{array}{l}
P(x-x_{k+1})=(P-A)\cdot(x-x_k)\\
P\cdot e_{k+1}=(P-A)e_k
\end{array} \]de donde, suponiendo que $P$ es invertible: \[ e_{k+1}=P^{-1}(P-A)e_k=\underbrace{(I-P^{-1}A)}_{M}e_k \]Repitiendo el proceso: \[ \begin{aligned}
\|e_{k+1}\|=\|M\cdot e_k\|&\le\|M\|\cdot\|e_k\|\\
&\le\|M\|\cdot\|M\|\cdot\|e_{k-1}\|\\
&\le \cdots\\
&\le\|M\|^{k+1}\cdot\|e_0\|
\end{aligned} \]lo que implica que $\|e_{k+1}\|\xrightarrow[n\to\infty]{}0$ si $\|M\|<1$.

La pregunta ahora es: \bu{¿Cómo elegir $P$?}
\begin{itemize}
	\item Jacobi propuso tomar $P=D$, la parte diagonal de $A$. Esto conduce al llamado método de Jacobi.
	\item Gauss-Seidel proponen descomponer $A$ en la forma 
	
	$\underbrace{\left[\begin{array}{ccc}
			a_{11} & \cdots & a_{1n}\\
			a_{12} & \cdots & a_{2n}\\ \hdashline
			a_{n1} & \cdots & a_{nn}
		\end{array}\right]}_A=\underbrace{\left[\begin{array}{cccc}
		0 & & & 0\\
		a_{21} & 0 & & \\
		a_{31} & a_{32} & 0 & \\ \hdashline
		a_{n1} & \cdots & a_{n,n-1} & 0
		\end{array}\right]}_L+\underbrace{\begin{bmatrix}
		a_{11} & & & \\
		& a_{22} & & \\
		& & \ddots & \\
		& & & a_{nn}
	\end{bmatrix}}_D+\linebreak\underbrace{\begin{bmatrix}
	0 & a_{2} & \cdots & a_n\\
	 & \ddots  & & \\
	 & & \ddots & \\
	 & & & 0
\end{bmatrix}}_U$

y tomar como $P=L+D$ de modo que el algoritmo queda \[ Px_{k+1}=-Ux_k+b,\quad P=L+D. \]
\Ej

$\begin{array}{l}
	A=T_4=\begin{bmatrix}
		2 & -1 & 0 & 0 \\
		-1 & 2 & -1 & 0 \\
		0 & -1 & 2 & -1 \\
		0 & 0 & -1 & 2
	\end{bmatrix}\\
	P=L+D=\begin{bmatrix}
		2& 0 & 0 & 0 \\
		-1 & 2 & 0 & 0 \\
		0 & -1 & 2 & 0 \\
		0 & 0 & -1 & 2
	\end{bmatrix},\qquad U=\begin{bmatrix}
	0 & -1 & 0 & 0\\
	0 & 0 & -1 & 0\\
	0 & 0 & 0 & -1\\
	0 & 0 & 0 & 0
	\end{bmatrix}
\end{array}$

El algoritmo queda: $Px_{k+1}=-Ux_k+b$

$\begin{bmatrix}
	2& 0 & 0 & 0 \\
	-1 & 2 & 0 & 0 \\
	0 & -1 & 2 & 0 \\
	0 & 0 & -1 & 2
\end{bmatrix}\cdot\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4
\end{bmatrix}^{\mathrm{new}}=\begin{bmatrix}
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{bmatrix}\cdot\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4
\end{bmatrix}^{\mathrm{old}}+\begin{bmatrix}
b_1\\
b_2\\
b_3\\
b_4
\end{bmatrix}$

$\begin{array}{l}
	\underbrace{\begin{bmatrix}
			2x_1\\
			-x_1+2x_2\\
			-x_2+2x_3\\
			-x_3+2x_4
	\end{bmatrix}^{\mathrm{new}}}=\begin{bmatrix}
	x_2\\
	x_3\\
	x_4
\end{bmatrix}^{\mathrm{old}}+\begin{bmatrix}
b_1\\
b_2\\
b_3\\
b_4
\end{bmatrix}\\
-1\cdot\begin{bmatrix}
	0\\
	x_1\\
	x_2\\
	x_3
\end{bmatrix}+2\cdot\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4
\end{bmatrix}^{\mathrm{new}}=\begin{bmatrix}
x_2\\
x_3\\
x_4\\
0
\end{bmatrix}^{\mathrm{old}}+\begin{bmatrix}
b_1\\
b_2\\
b_3\\
b_4
\end{bmatrix}
\end{array}$

Resolución: $x_1\longrightarrow x_2\longrightarrow x_3\longrightarrow x_4$
\end{itemize}
Recordemos que el método converge cuando \[ \|M\|<1,\quad M=(I-P^{-1}A) \]Esta condición siempre se cumple para matrices que son \lb{diagonal dominantes}.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Una matriz $A=(a_{ij})$ se dice \lb{estrictamente diagonal dominante} si para cada fila $i$ se cumple \[|a_{ii}|>\sum_{\begin{subarray}{c}
		j=1\\
		i\neq j
\end{subarray}}^{n}|a_{ij}|\]
\Ej

$A=\begin{bmatrix}
	2 & -1 & 0\\
	-1 & 4 & 2\\
	0 & 2 & 3
\end{bmatrix}\qquad\begin{array}{l}
|a_{11}|=2>1\\
|a_{22}|=4>2+1=3\\
|a_{33}|=3>2
\end{array}$
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Teorema
\end{itemize}
Supongamos que $A$ es estrictamente diagonal dominante. Entonces, los métodos de Jacobi y Gauss-Seidel son convergentes.
\subsection{Ejercicios}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item Cálculo de la inversa de una matriz usando factorización $LU$ o Cholesky.
	
	$A\in M_n(\R)$. Sea $X\in M_n(\R)$. Entonces $X=A^{-1}$ si se cumple \[ AX=I \]Si $x^1,x^2,\dots,x^n$ son las columnas de $X$, entonces se tiene:\[ \begin{array}{l}
		Ax^1=\begin{bmatrix}
			1\\
			0\\
			\vdots\\
			0
		\end{bmatrix}\\
		Ax^2=\begin{bmatrix}
			0\\
			1\\
			\vdots\\
			0
		\end{bmatrix}\\ \hdashline
		Ax^n=\begin{bmatrix}
			0\\
			\vdots\\
			0\\
			1
		\end{bmatrix}
	\end{array} \]Por tanto, para calcular $A^{-1}$ hemos de resolver $n$ sistemas lineales. Cualquiera de ellas se resuelve como \[ \begin{array}{cl}
	\begin{array}{c}
		Ax^j=b^j\\
		LUx^j=b^j\\
		\downarrow\\
		\begin{rcases}
			Ux^j=z^j\\
			Lz^j=b^j
		\end{rcases}
	\end{array} & b^j=\begin{bmatrix}
	0\\
	\vdots\\
	1\\
	0\\
	\vdots\\
	0
	\end{bmatrix}\begin{array}{l}
	\\
	\\
	\longrightarrow j\text{-ésimo}\\
	\\
	\\
	\\
	\end{array}\\
	\end{array} \]
\end{enumerate}