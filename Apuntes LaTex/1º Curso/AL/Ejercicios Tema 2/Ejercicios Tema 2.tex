\input{../../../Macros.tex}
\title{Álgebra Lineal\\ Ejercicios Tema 2: Vectores, matrices y tensores}

\begin{document}
\maketitle
\begin{enumerate}[label=\color{red}\textbf{\arabic*)}]
    \item \lb{Determina la verdad o falsedad de la siguiente afirmación: si $u_1$ es combinación lineal de $u_2$ y $u_3$, entonces $u_3$ es combinación lineal de $u_1$ y $u_2$.}

La afirmación es \textbf{falsa}  en general. Vamos a analizarla cuidadosamente:
\begin{itemize}[label=\textbullet]
    \item Supongamos que $u_2$ y $u_3$ son linealmente independientes y $u_1$ se define como una combinación lineal de $u_2$ y $u_3$. Por ejemplo, tomemos: \[
    u_1=u_2+u_3.
    \] 
\item En este caso, aunque $u_1$ es una combinación lineal de $u_2$ y $u_3$, no podemos escribir $u_3$ como una combinación lineal de $u_1$ y $u_2$, ya que: \[
u_3=u_1-u_2
\] y esta relación no es válida en todos los espacios vectoriales. La implicación original depende de la independencia lineal de los vectores involucrados.
\item Para que $u_3$ sea combinación lineal de $u_1$ y $u_2$, deben cumplorse condiciones adicionales, como que $u_1,u_2,u_3$ estén en un subespacio con dimensión menor o igual a 2.
\end{itemize}
\item \lb{Consideremos los vectores $u=(1,1,0)$ y  $v=(0,1,1)$. Encuentra un vector $w$ ortogonal a $u$ y $v$. Comprueba que $w$ es ortogonal a cualquier combinación lineal de $u$ y $v$. Encuentra ahora un vector que \underline{no} sea lineal de $u,v$ y comprueba que no es ortogonal a $w$.}

    \begin{enumerate}[label=\arabic*)]
        \item Vector $w$ ortogonal a $u$ y $v$:

            El vector  $w$ encontrado mediante el producto cruz es: \[
            w=(1,-1,1).
            \] 
        \item Verificación de la ortogonalidad:
            \begin{itemize}[label=\textbullet]
                \item $w\cdot u=0:\,w$ es ortogonal a $u$.
                \item  $w\cdot v=0:\,w$ es ortogonal a $v$.
            \end{itemize}
        \item Ortogonalidad respecto a una combinación lineal de $u$ y  $v$:

            Para una combinación lineal genérica  $c_1u+c_2v$, se cumple:  \[
           w\cdot (c_1u+c_2v)=0. 
            \] 
            Esto demuestra que $w$ es ortogonal a cualquier combinación lineal de $u$ y $v$.
        \item Vector que no es combinación lineal de $u$ y $v$:

            Un vector que no es combinación lineal de  $u$ y $v$ es:  \[
                (1,0,0)
            \] 
        \item Comprobación de no ortogonalidad con $w$:

            El producto escalar entre  $w$ y  $(1,0,0)$ es:  \[
            w\cdot (1,0,0)=1.
            \] 
            Por lo tanto, no es ortogonal a $w$.
    \end{enumerate}
\item \lb{Haz un dibujo de los siguientes conjuntos de $\R^2$: \[
\begin{array}{l}
    \{(x,y)\in \R^2\text{ tal que }\|(x,y)\|_1=1\}\\ 
    \{(x,y)\in \R^2\text{ tal que }\|(x,y)\|_2=1\}\\ 
    \{(x,y)\in \R^2\text{ tal que }\|(x,y)\|_\infty=1\}\\ 
\end{array}
\] }
\begin{center}
    \includegraphics[width=0.5\textwidth]{"figures/Figure 1"}
\end{center}
\item \lb{Prueba que $\|u\|_2\le \sqrt{\|u\|_1\|u\|_\infty} $.}

    Dado un vector $u=(u_1,u_2,\dots,u_n)\in \R^n$, las normas son: 
    \begin{enumerate}[label=\arabic*)]
        \item Norma $\|u\|_2$:  \[
        \|u\|_2=\sqrt{\sum_{i=1}^{n} u_i^2} .
        \] 
    \item Norma $\|u\|_1$:  \[
    \|u\|_1=\sum_{i=1}^{n} |u_i|.
    \] 
\item Norma $\|u\|_\infty$: \[
\|u\|_\infty=\max(|u_1|,|u_2|,\dots,|u_n|).
\] 
    \end{enumerate}
    Expandimos la definción de $\|u\|_2^2$: \[
    \|u\|_2^2=\sum_{i=1}^{n} u_i^2.
    \] 
    Usamos la desigualdad $u_i^2\le |u_i|\cdot \|u\|_\infty$ (ua que $|u_1|\le \|u\|_{\infty}$ para todo $i$): \[
    \sum_{i=1}^{n} u_i^2\le  \sum_{i=1}^{n} |u_i|\cdot \|u\|_\infty\longrightarrow \sum_{i=1}^{n} u_i^2\le \|u\|_\infty \sum_{i=1}^{n} |u_i|.
    \] 
    Por definición, $\sum_{i=1}^{n} |u_i|=\|u\|_1$, por lo que: \[
    \|u\|_2^2\le \|u\|_\infty\|u\|1.
    \] 
Tomando raíces cuadradas en ambos lados de la desigualdad: \[
\|u\|_2\le \sqrt{\|u\|_1\|u\|_\infty}. 
\] 
\item \lb{Dos vectores son ortogonales cuando su producto escalar es cero, pero ¿qué pasa si su producto escalar es próximo a cero? Sean $x=[1,-0.75]$ e $y=[0.3,0.3]$. Calcula el producto escalar  $x\cdot y$ y el ángulo que forman. ¿Qué conclusión puedes sacar?\newline Si dos vectores $x,y$ son unitarios, entonces  $-1\le x\cdot y\le 1$ (¿por qué?). En este caso, ¿qué podemos decir si $x\cdot y$ es aproximadamente $-1,1$ o cero?}

    \begin{enumerate}[label=\arabic*)]
        \item  Producto escalar: \[
        x\cdot y=0.075
        \] 
    \item Ángulo que forman los vectores:
        \begin{itemize}[label=\textbullet]
            \item Coseno del ángulo: \[
            \cos(\theta)= \dfrac{x\cdot y}{\|x\|_2\cdot \|y\|_2}=\dfrac{0.075}{1.25* \frac{3\sqrt{2} }{10} }=\dfrac{\sqrt{2} }{10}. 
            \] 
        \item Ángulo en radianes: \[
        \theta \approx \arccos \left( \dfrac{\sqrt{2} }{10} \right) \simeq 1.4289 \mathrm{rad}.
        \] 
    \item Ángulo en grados: \[
    \theta\approx 81.8699^\circ
    \] 
        \end{itemize}
    \end{enumerate}
    Cuando el producto escalar es cercano a cero, el ángulo entre los vectores se aproxima a $90^\circ$, lo que indica que los vectores son casi ortogonales. En este caso, el ángulo es aproximadamente  $81.87^\circ$, por lo que los vectores están cerca de ser ortogonales pero no completamente.

    Si $x$ e  $y$ son unitarios, sus normas son iguales a 1 $(\|x\|=\|y\|=1)$. En este caso, el producto escalar  $x\cdot y$ es: \[
    x\cdot y=\|x\|\cdot \|y\|\cdot \cos(\theta)=\cos(\theta)
    \] 
    y como $-1\le \cos(\theta)\le 1$ (por tener un comportamiento oscilante), se tiene: \[
    -1\le x\cdot y\le 1
    \] 
\item \lb{Sean $u,v$ dos vectores unitarios de  $\R^{n}$ que forman un ángulo de 60$^\circ$. Calcula $\|2u+v\|$.}

    Dado que los vectores $u$ y $v$ son unitarios ($\|u\|=1$ y  $\|v\|=1$) y forman un ángulo de $60^\circ$, podemos calcular  $\|2u+v\|$ usando las propiedades de las normas y el producto escalar.
     \begin{enumerate}[label=Paso \arabic*:]
        \item Fórmula de la norma

            La norma de $2u+v$ es:  \[
            \|2u+v\|=\sqrt{\|2u\|^2+\|v\|^2+2\left<2u,v \right>} .
            \] 
        \item Sustituir las normas
            \[
                \begin{array}{l}
            \|2u\|^2=4\|u\|^2=4,\\
            \|v\|^2=1
                \end{array}
            \] 
        \item Producto escalar $\left<2u,v \right>$ 

            El producto escalar entre $u$ y $v$ es: \[
            \left<u,v \right> = \|u\|\cdot \|v\|\cdot \cos(60^\circ)=1\cdot 1\cdot \dfrac{1}{2}=\dfrac{1}{2}.
            \] 
            Por lo tanto: \[
            \left<2u, v \right> = 2\cdot \left<u,v \right> = 2\cdot \dfrac{1}{2}=1.
            \] 
        \item Sustituir todo en la fórmula \[
        \|2u+v\|=\sqrt{\|2u\|^2+\|v\|^2+2+2\left<2u,v \right>}=\sqrt{4+1+2\cdot 1}=\sqrt{4+1+2}=\sqrt{7}    .
        \] 
        
    \end{enumerate}
\item \lb{Sean $u,v$ dos vectores de  $\R^n$ de norma $2$ y  $3$ respectivamente que forman un ángulo de  $60º$. ¿Qué ángulo forman los vectores $u$ y  $2u-v$?} 

    Para determinar el ángulo entre los vectores $u$ y $2u-v$, seguimos los siguientes pasos:
    \begin{enumerate}[label=Paso \arabic*:]
        \item Producto escalar entre $u$ y $2u-v$

            El producto escalar entre  $u$ y $2u-v$ es:  \[
            \left<u,2u-v \right> = \left<u,2u \right> - \left<u,v \right>.
            \] 
            \begin{enumerate}[label=\alph*)]
                \item Calcular $\left< u,2u\right>$: \[
                \left<u,2u \right> = 2\cdot \|u\|^2=2\cdot 2^2=8
                \] 
            \item Calcular $\left<u,v \right>$ :

                El producto escalar entre $u$ y $v$ es: \[
                \left<u,v \right> = \|u\|\cdot \|v\|\cdot \cos(60^\circ)=2\cdot 3\cdot \dfrac{1}{2}=3.
                \] 
            \end{enumerate}
            Sustituyendo: \[
            \left<u,2u-v \right> = 8-3=5.
            \] 
        \item Norma de $2u-v$

            La norma de  $2u-v$ es: \[
            \|2u-v\|=\sqrt{\|2u\|^2+\|v\|^2-2\left<2u,v \right>} .
            \] 
            \begin{enumerate}[label=\alph*)]
                \item Calcular $\|2u\|^2$: \[
                \|2u\|^2=4\|u\|^2=4\cdot 2^2=16.
                \] 
            \item Calcular $\|v\|^2$: \[
            \|v\|^2=3^2=9.
            \] 
        \item Calcular $\left<2u,v \right>$: \[
        \left<2u,v \right> = 2\left<u,v \right> = 2\cdot 3=6.
        \] 
            \end{enumerate}
            Sustituyendo todo: \[
            \|2u-v\|=\sqrt{16+9-2\cdot 6}=\sqrt{16+9-12}=\sqrt{13}   
            \] 
        \item Calcular el ángulo entre $u$ y $2u-v$

            El coseno del ángulo  $\theta$ entre $u$ y $2u-v$ es:  \[
            \cos(\theta)=\dfrac{\left< u,2u-v\right>}{\|u\|\cdot \|2u-v\|}=\dfrac{5}{2\cdot \sqrt{13} }=\dfrac{5\sqrt{3} }{26}.
            \] 
        \item Ángulo $\theta$: \[
        \theta=\arccos\left( \dfrac{5\sqrt{3} }{26} \right) \simeq 46.10^\circ.
        \] 

    \end{enumerate}
\item \lb{Calcula $A+B,(A+B)^\intercal, AB, BA, (AB)^\intercal, A^\intercal B^\intercal$ y $B^\intercal A^\intercal$ para las matrices  \[
A=\begin{bmatrix} 
    1 & 0 & 3\\
    2 & 2 & 3\\
    3 & 0 & 3
\end{bmatrix}\quad\text{y} \quad B=\begin{bmatrix} 
    2 & 1 & 0\\
    1 & 2 & 0\\
    0 & 1 & 2
\end{bmatrix} 
\] } 

$A+B=\begin{bmatrix} 
    1 & 0 & 3\\
    2 & 2 & 3\\
    3 & 0 & 3
\end{bmatrix}+\begin{bmatrix} 
    2 & 1 & 0\\
    1 & 2 & 0\\
    0 & 1 & 2
\end{bmatrix}=\begin{bmatrix} 
    3 & 1 & 3\\
    3 & 4 & 3\\
    3 & 1 & 5
\end{bmatrix}   $ 

$(A+B)^\intercal=\left( \begin{bmatrix} 
        3 & 1 & 3\\
        3 & 4 & 3\\
        3 & 1 & 5
\end{bmatrix}  \right)^\intercal=\begin{bmatrix} 
        3 & 3 & 3\\
        1 & 4 & 1\\
        3 & 3 & 5
\end{bmatrix}  $

$AB=\begin{bmatrix} 
    1 & 0 & 3\\
    2 & 2 & 3\\
    3 & 0 & 3
\end{bmatrix}\cdot\begin{bmatrix} 
    2 & 1 & 0\\
    1 & 2 & 0\\
    0 & 1 & 2
\end{bmatrix}=\begin{bmatrix} 
    2 & 4 & 6\\
    6 & 9 & 6\\
    6 & 6 & 6
\end{bmatrix} $

$BA=\begin{bmatrix} 
    2 & 1 & 0\\
    1 & 2 & 0\\
    0 & 1 & 2
\end{bmatrix}\cdot \begin{bmatrix} 
    1 & 0 & 3\\
    2 & 2 & 3\\
    3 & 0 & 3
\end{bmatrix} = \begin{bmatrix} 
    4 & 2 & 9\\
    5 & 4 & 9\\
    8 & 2 & 9
\end{bmatrix} $

$(AB)^\intercal=\left( \begin{bmatrix} 
        2 & 4 & 6\\
        6 & 9 & 6\\
        6 & 6 & 6
\end{bmatrix}  \right)^\intercal=\begin{bmatrix} 
        2 & 6 & 6\\
        4 & 9 & 6\\
        6 & 6 & 6 
\end{bmatrix}  $ 

$A^\intercal B^\intercal=\begin{bmatrix} 
    1 & 2 & 3\\
    0 & 2 & 0\\
    3 & 3 & 3
\end{bmatrix}\cdot \begin{bmatrix} 
    2 & 1 & 0\\
    1 & 2 & 1\\
    0 & 0 & 2
\end{bmatrix}=\begin{bmatrix} 
    4 & 5 & 8\\
    2 & 4 & 2\\
    9 & 9 & 9
\end{bmatrix}   $ 

$B^\intercal A^\intercal=\begin{bmatrix} 
    2 & 1 & 0\\
    1 & 2 & 1\\
    0 & 0 & 2
\end{bmatrix}\cdot \begin{bmatrix} 
    1 & 2 & 3\\
    0 & 2 & 0\\
    3 & 3 & 3
\end{bmatrix}=\begin{bmatrix} 
    2 & 6 & 6\\
    4 & 9 & 6\\
    6 & 6 & 6
\end{bmatrix} $
\item \lb{Prueba que no existe ninguna matriz $A$ tal que  $A^2=\begin{bmatrix} 
            0 & 1\\
            0 & 0
\end{bmatrix} $.}

Queremos demostrar que no existe ninguna matriz $A$ tal que: \[
A^2=\begin{bmatrix} 
    0 & 1\\
    0 & 0
\end{bmatrix} .
\] 
Sea $A$ una matriz $2\times 2$ de la forma general: \[
A=\begin{bmatrix} 
    a & b \\
    c & d
\end{bmatrix} .
\] 
Entonces, calculamos $A^2$ de la forma: \[
A^2=A\cdot A=\begin{bmatrix} 
    a & b\\
    c & d
\end{bmatrix} \cdot \begin{bmatrix} 
    a & b\\
    c & d
\end{bmatrix} =\begin{bmatrix} 
    a^2+bc & ab+bd\\
    ac+dc & bc+d^2
\end{bmatrix} .
\] 
Queremos que $A^2$ sea igual a: \[
A^2=\begin{bmatrix} 
    0 & 1\\
    0 & 0
\end{bmatrix} .
\] 
Por lo tanto, igualamos componente a componente:
\begin{enumerate}[label=\arabic*)]
    \item $a^2+bc=0$.
    \item $ab+bd=1$.
    \item  $ac+dc=0$.
    \item  $bc+d^2=0$
\end{enumerate}
\begin{enumerate}[label=Paso \arabic*:]
    \item Análisis de las ecuaciones

        De $ac+dc=0$, factorizamos:  \[
            (a+d)c=0.
        \]
        Esto implica dos posibilidades: \[
        c=0,\text{ o }a+d=0.
        \] 
        \begin{enumerate}[label=Caso \arabic*:]
            \item $c=0$
                Si $c=0$, las ecuaciones quedan:
                \begin{enumerate}[label=\arabic*.]
                    \item $a^2=0$ (de $a^2+bc=0$, con $c=0$)
                    \item $ab+bd=1$
                    \item  $d^2=0$ (de $bc+d^2=0$, con $c=0$)
                \end{enumerate}
                De $a^2=0$ y $d^2=0$, se deduce $a=0$ y $d=0$. Esto contradice la ecuación $ab+bd=1$, ya que  $ab+bd=0$.

                Por lo tanto,  $c=0$ no es posible.
            \item $a+d=0$

                Si  $a+d=0$, entonces $d=-a$. Sustituyendo en las ecuaciones: 
                \begin{enumerate}[label=\arabic*)]
                    \item $a^2+bc=0$
                    \item $ab-ab=1$, lo cual es una contradicción.
                \end{enumerate}
        \end{enumerate}
        Por lo tanto, no existe ninguna matriz $A$ tal que $A^2=\begin{bmatrix} 
            0 & 1\\
            0 & 0
        \end{bmatrix} $.
\end{enumerate}
\item \lb{¿Existen matrices reales no nula $2\times 2$ tales que $A\cdot A^\intercal=0$? ¿y si son matrices complejas?}

    \begin{enumerate}[label=Caso \arabic*:]
        \item Matrices reales $A$ tales que $A\cdot A^\intercal=0$

            Sea $A$ una matriz  $2\times 2$ real no nula: \[
            A=\begin{bmatrix} 
                a & b\\
                c & d
            \end{bmatrix} 
            \] 
            El producto $A\cdot A^\intercal=0$ es:
            \[
            A\cdot A^\intercal=\begin{bmatrix} 
                a & b\\
                c & d
            \end{bmatrix} \cdot \begin{bmatrix} 
                a & c\\
                b & d
            \end{bmatrix} =\begin{bmatrix} 
                a^2+b^2 & ac+bd\\
                ac+bd & c^2+d^2
            \end{bmatrix} 
            \] 
            Para que $A\cdot A^\intercal=0$, deben cumplirse:
            \begin{enumerate}[label=\arabic*)]
                \item $a^2+b^2=0$.
                \item $c^2+d^2=0$.
                \item $ac+bd=0$.
            \end{enumerate}
            \textbf{Análisis:}
            \begin{enumerate}[label=\arabic*)]
                \item De $a^2+b^2=0$, como $a,b\in \R$, implica $a=0$ y  $b=0$.
                \item De  $c^2+d^2=0$, como $c,d\in \R$, implica $c=0$ y  $d=0$.
            \end{enumerate}
            Esto contradice la condición de que $A$ no sea nula. Por lo tanto, \textbf{no existen matrices reales no nulas $2\times 2$ tales que $A\cdot A^\intercal=0$.}
        \item Matrices complejas $A$ tales que $A\cdot A^\intercal=0$

            Si $A$ es una matriz $2\times 2$ con entradas en $\mathbb{C}$, el análisis cambia porque en  $\mathbb{C}$, un número puede tener módulo cero sin ser necesariamente cero.

            De nuevo, sea:  \[
            A=\begin{bmatrix} 
                a & b\\
                c & d
            \end{bmatrix}, \quad\text{con $a,b,c,d\in \mathbb{C}$.}
            \] 
            El producto $A\cdot A^\intercal$ es: \[
            A\cdot A^\intercal=\begin{bmatrix} 
                a & b\\
                c & d
            \end{bmatrix} \cdot  \begin{bmatrix} 
                a & c\\
                b & d
            \end{bmatrix} =\begin{bmatrix} 
                a^2+b^2 & ac+bd\\
                ac+bd & c^2+d^2
            \end{bmatrix} .
            \] 
            Para $A\cdot A^\intercal=0$, las ecuaciones son:
            \begin{enumerate}[label=\arabic*)]
                \item $a^2+b^2=0$
                \item $c^2+d^2=0$
                \item $ac+bd=0$
            \end{enumerate}
            \textbf{Solución:}
            \begin{enumerate}[label=\arabic*)]
                \item De $a^2+b^2=0$, como $a,b\in \mathbb{C}$, no es necesario que $a$ y $b$ sean cero. Por ejemplo, si $c=1$ y $d=-i$, entonces  $c^2+d^2=0$.
                \item La tercera ecuación $ac+bd=0$ debe verificarse con los valores específicos de  $a,b,c,d$
            \end{enumerate}
            Por lo tanto, \textbf{existen matrices complejas no nulas $2\times 2$ tales que $A\cdot A^\intercal=0$}. 
    \end{enumerate}
\item \lb{Sean $A,B$ matrices tales que  $I+AB$ es invertible y sea  $S$ la inversa de  $I+AB$. Prueba que  $I+BA$ también es invertible y su inversa es  $I-BSA$.} 

    Queremos demostrar que si $S$ es la inversa de $I+AB$, es decir: \[
    S(I+AB)=(I+AB)S=I,
    \] 
    entonces $I+BA$ también es invertible y su inversa es  $I-BSA$.
     \begin{enumerate}[label=Paso \arabic*:]
        \item Demostrar que $(I-BSA)(I+BA)=I$ 

            Calculemos $(I-BSA)(I+BA)$:  \[
                (I-BSA)(I+BA)=I+BA-BSA-BSA\cdot BA=I+BA-BSA-BSABA=I+BA-B(S+SABA).
            \] 
            Por lo definición de $S$, sabemos que $S$ es la inversa de $I+AB$, por lo que  \[
            S(I+AB)=I\longrightarrow S+SAB=I
            \] 
            Sustituyendo $S+SAB=I$ en la ecuación: \[
                (I-BSA)(I+BA)=I+BA-BA=I.
            \] 
        \item Demostrar que $(I+BA)(I-BSA)=I$

            Ahora calculemos  $(I+BA)(I-BSA)$:  \[
                (I+BA)(I-BSA)=I-BSA+BA-BA\cdot BSA=I+BA-B(S+SAB)A=I+BA-BIA=I.
            \] 
        \item Conlusión 

            Hemos demostrado que: \[
                (I-BSA)(I+BA)=I\quad \text{y}\quad(I+BA)(I-BSA)=I.
            \] 
            Por lo tanto, $I+BA$ es invertible y su inversa es:  \[
                (I+BA)^{-1}=I-BSA.
            \] 
    \end{enumerate}
\item \lb{Sea $A$ una matriz  $n\times m$ y sea $B$ una matriz $m\times n$. Suponiendo que las matrices $I+AB$ y  $I+BA$ sea invertibles, prueba que se cumple la igualdad  \[
            (I+AB)^{-1}A=A(1-BA)^{-1}
\]Si $m$ es mucho más pequeño que  $n$, ¿cuál de las dos expresiones es más fácil de calcular desde el punto de vista computacional?}

Queremos demostrar: \[
    (I+AB)^{-1}A=A(I+BA)^{-1}.
\] 
\begin{enumerate}[label=Paso \arabic*:]
    \item Multiplicación por $(I+AB)$

        Multiplicamos ambos lados de la igualda por  $I+AB$ desde la izquierda para simplificar el término $(I+AB)^{-1}$. Esto da: \[
        A=(I+AB)A(I+BA)^{-1}.
        \] 
    \item Expandir $(I+AB)A$

        Multiplicamos:  \[
        (I+AB)A=A+ABA.
        \] 
        Por lo tanto, la ecuación queda como: \[
        A=(A+ABA)(I-BA)^{-1}.
        \] 
    \item Multiplicación por $(I+BA)$

        Multiplicamos ahora por  $I+BA$ desde la derecha: \[
        A(I+BA)=A+ABA.
        \] 
        Esto coincide con $(I+AB)A$, lo que confirma que:  \[
            (I+AB)^{-1}A=A(I+BA)^{-1}
        \] 
\end{enumerate}
\textbf{Análisis computacional}

Dimensiones de las matrices:
\begin{itemize}[label=\textbullet]
    \item $A$ es una matriz $n\times m$.
    \item $B$ es una matriz $m\times n$.
    \item $AB$ es una matriz $n\times n$.
    \item $BA$ es una matriz $m\times m$.
\end{itemize}
Dificultad computacional:

El cálculo de una inversa tiene una complejidad de aproximadamente $O(k^3)$, donde $k$ es la dimensión de la matriz.
\begin{itemize}[label=\textbullet]
    \item \textbf{Para $(I+AB)^{-1}$:} Necesitamos calcular la inversa de una matriz $n\times n$, lo que tiene una complejidad $O(n^3)$.
    \item \textbf{Para $(I+BA)^{-1}$:} Necesitamos calcular la inversa de una matriz $m\times m$, lo que tiene una complejidad $O(m^3)$. 
\end{itemize}
Si $m\ll n$, entonces calcular $(I+BA)^{-1}$ es significativamente más eficiente que calcular $(I+AB)^{-1}$.

\item \lb{Sea $A$ una matriz  $n\times p$ y $B$ una matriz $p\times m$. Si llamamos \textbf{flop} a una operación, ya sea una suma, resta, multiplicación o división, prueba que para calcular $AB$ son necesarios  $mn(2p-1)$ flops (haciendo la multiplicación de forma estándar). Si  $A$ es una matriz  $10\times 2$, $B$ una matriz $2\times 10$ y $C$ una matriz  $10\times 10$ y queremos calcular $ABC$, ¿qué es mejor desde el punto de vista computacional, calcular $(AB)C$ o $A(BC)$?} 

    Sea $A$ una matriz $n\times p$ y $B$ una matriz $p\times m$. Para calcular $AB$ usando la multiplicación estándar:
    \begin{enumerate}[label=\arabic*)]
        \item Cada elemento $(i,j)$ de $AB$ se obtiene como: \[
                (AB)_{ij}=\sum_{k=1}^{p} A_{ik}B_{kj}.
        \] 
        Esto requiere $p$ multiplicaciones y $p-1$ sumas, para un total de $2p-1$ flops.
    \item Hay $n\times m$ elementos en $AB$, por lo que el total de flops necesarios es: \[
    \text{Total flops}=n\cdot m\cdot (2p-1)
    \] 
    \end{enumerate}
    Dado que:
    \begin{itemize}[label=\textbullet]
        \item $A$ es una matriz $10\times 2$,
        \item $B$ es una matriz $2\cdot 10$,
        \item $C$ es una matriz $10\times 10$,
    \end{itemize}
    tenemos dos ciones para calcular $ABC$:
     \begin{enumerate}[label=\arabic*)]
        \item $(AB)C$ 
            \begin{itemize}[label=\textbullet]
                \item Primero calculamos $AB$:  $A$ es  $10\times 2$ y $B$ es  $2\times 10$, así que $AB$ es $10\times 10$. El número de flops es: \[
                10\cdot 10\cdot (2\cdot 2-1)=10\cdot 10\cdot 3=300.
                \] 
                \item Luego calculamos $(AB)C$:  $AB$ es  $10\times 10$ y $C$ es  $10\times 10$. El número de flops es: \[
            10\cdot 10\cdot (2\cdot 10-1)=10\cdot 10\cdot 19=1900.
            \]
            
                \item Total para $(AB)C$:  \[
                300+1900=2200\text{ flops. }
                \] 
            \end{itemize}
        \item $A(BC)$
            \begin{itemize}[label=\textbullet]
                \item Primero calculamos $BC$: $B$ es  $2\times 10$ y $C$ es  $10\times 10$, así que $BC$ es $2\times 10$. El número de flops es: \[
                2\cdot 10\cdot (2\cdot 10-1)=2\cdot 10\cdot 19=380.
                \] 
                \item Luego calculamos $A(BC)$:  $A$ es $10\times 2$ y $BC$ es $2\times 10$. El número de flops es: \[
                10\cdot 10\cdot (2\cdot 2-1)=10\cdot 10\cdot 3=300.
                \] 
                \item Total para $A(BC)$: \[
                    380+300=680\text{ flops}.
                 \] 
            \end{itemize}
    \end{enumerate}

    Desde el punto de vista computacional, es mejor calcular $A(BC)$.

\item \lb{Dadas dos matrices cuadradas $A,B$, se define el  \textbf{conmutador} de $A,B$ como  \[
            [A,B]=AB-BA
\]Por una parte, el spin de un electrón se suele representar a través de las siguiente tres matrices, llamadas matrices de Pauli: \[
S_x=\dfrac{1}{2}\overline{h}\begin{bmatrix} 
    0 & 1\\
    1 & 0
\end{bmatrix} \quad S_y=\dfrac{1}{2}\overline{h}\begin{bmatrix} 
    0 & -j\\
    j & 0
\end{bmatrix} \quad S_z=\dfrac{1}{2}\overline{h}\begin{bmatrix} 
    1 & 0\\
    0 & -1
\end{bmatrix} 
\]donde $\overline{h}=\dfrac{h}{2\pi}$, con $h$ constante de Plank. Comprueba que \[
[S_x,S_y]=j\overline{h}S_z,\quad [S_y,S_z]= j\overline{h} S_x, \quad [S_z,S_x]=j\overline{h}S_y
\]y que \[
S_x ^2+S_y^2+S_z^2=\dfrac{3}{4}\overline{h}^2I_3
\] con $I_3$ la matriz identidad $3\times 3$. }

Dadas las matrices: \[
S_x=\dfrac{\overline{h}}{2}\begin{bmatrix} 
    0 & 1\\
    1 & 0
\end{bmatrix} ,\quad S_y=\dfrac{\overline{h}}{2}\begin{bmatrix} 
    0 & -j\\
    j & 0
\end{bmatrix} ,\quad S_z=\begin{bmatrix} 
    1 & 0\\
    0 & -1
\end{bmatrix} ,
\] 
verificamos los commutadores:
\begin{enumerate}[label=\arabic*)]
    \item Conmutador $[S_x,S_y]$: \[
            [S_x,S_y]=S_xS_y-S_yS_x=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
                j & 0\\
                0 & -j
            \end{bmatrix}-\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
                -j & 0\\
                0 & j
            \end{bmatrix} = \dfrac{\overline{h}^2}{2}\begin{bmatrix} 
                j & 0\\
                0 & -j
            \end{bmatrix}=j\overline{h}S_z  .
    \] 
    $\begin{array}{l}
        S_xS_y=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & 1\\
            1 & 0
        \end{bmatrix}\begin{bmatrix} 
            0 & -j \\
            j & 0
        \end{bmatrix} =\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            j & 0\\
            0 & -j
        \end{bmatrix} .\\ \\
        S_yS_x=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & -j\\
            j & 0
        \end{bmatrix} \begin{bmatrix} 
            0 & 1\\
            1 & 0
        \end{bmatrix} =\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            -j & 0\\
            0 & j
        \end{bmatrix} .
    \end{array}$
\item Conmutador $[S_y,S_z]$: De maera similar, podemos calcular: \[
        [S_y,S_z]=S_yS_z - S_zS_y=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & -j\\
            -j & 0
        \end{bmatrix}-\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & j\\
            j & 0
        \end{bmatrix}=\dfrac{\overline{h}^2}{2}\begin{bmatrix} 
            0 & j\\
            j & 0
        \end{bmatrix}=j\overline{h}S_x.
\] 
$\begin{array}{l}
        
        S_yS_z=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & -j\\
            j & 0
        \end{bmatrix} \begin{bmatrix} 
            1 & 0\\
            0 & -1
        \end{bmatrix} =\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & -j\\
            -j & 0
        \end{bmatrix} .\\ \\
S_zS_y=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & -j\\
            j & 0
        \end{bmatrix}\begin{bmatrix} 
            1 & 0 \\
            0 & -1
        \end{bmatrix} =\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & j\\
            j & 0
        \end{bmatrix} .
    \end{array}$
\item Conmutador $[S_x,S_z]$:  \[
        [S_x,S_z]=S_xS_z-S_zS_x=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & -1\\
            1 & 0
        \end{bmatrix}-\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & 1\\
            -1 & 0
        \end{bmatrix}=\dfrac{\overline{h}^2}{2}\begin{bmatrix} 
            0 & 1\\
            -1 & 0
        \end{bmatrix}=j\overline{h}S_y.
\] 

$\begin{array}{l}
        S_xS_z=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & 1\\
            1 & 0
        \end{bmatrix}\begin{bmatrix} 
            1 & 0\\
            0 & -1
        \end{bmatrix} =\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & -1\\
            1 & 0
        \end{bmatrix} .\\ \\
        S_zS_x=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            1 & 0\\
            0 & -1
        \end{bmatrix} \begin{bmatrix} 
            0 & 1\\
            1 & 0
        \end{bmatrix} =\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            0 & 1\\
            -1 & 0
        \end{bmatrix} .
    \end{array}$
\end{enumerate}

Calculamos cada término al cuadrado: \[
\begin{array}{c}
    S_x^2=\left( \dfrac{\overline{h}}{2}\begin{bmatrix} 
            0 & 1\\
            1 & 0
    \end{bmatrix}  \right) ^2=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            1 & 0\\
            0 & 1
    \end{bmatrix}\\ \\
    S_y^2=\left( \dfrac{\overline{h}}{2}\begin{bmatrix} 
            0 & -j\\
            j & 0
    \end{bmatrix}  \right) ^2 = \dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            1 & 0\\
            0 & 1
    \end{bmatrix}\\ \\
    S_z^2=\left( \dfrac{\overline{h}}{2}\begin{bmatrix} 
            1 & 0\\
            0 & -1
    \end{bmatrix}  \right) ^2 = \dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            1 & 0\\
            0 & 1
    \end{bmatrix}
\end{array}
\] 
Sumando: \[
S_x^2+S_y^2+S_z^2=\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            1 & 0\\
            0 & 1
    \end{bmatrix}+\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            1 & 0\\
            0 & 1
    \end{bmatrix}+\dfrac{\overline{h}^2}{4}\begin{bmatrix} 
            1 & 0\\
            0 & 1
    \end{bmatrix}=\dfrac{3\overline{h}^2}{4}I.
\] 
\item \lb{Si $A$ es una matriz simétrica, ¿son las matrices $B^\intercal AB,A+A^\intercal$ y $A-A^\intercal$ simétricas?}

    \begin{enumerate}[label=\arabic*)]
        \item Matriz $B^\intercal AB$

            Para qu $B^\intercal AB$ sea simétrica, debe cumplirse: \[
                (B^\intercal AB)^\intercal=b^\intercal AB.
            \] 
            Usamos la propiedad de la transposición: \[
                (B^\intercal AB)^\intercal=B^\intercal A^\intercal(B^\intercal)^\intercal= B^\intercal A^\intercal B.
            \] 
            Como $A$ es simétrica ($A^\intercal =A$): \[
                (B^\intercal AB)^\intercal=B^\intercal AB.
            \]  
            Por lo tanto, $B^\intercal AB$ \textbf{es simétrica} siempre que $A$ sea simétrica.
        \item Matriz $A+A^\intercal$

            Para que $A+A^\intercal$ sea simétrica, debe cumplirse: \[
                (A+A^\intercal)^\intercal=A+A^\intercal.
            \] 
            Usamos la propiedad de la transposición:
            \[
                (A+A^\intercal)^\intercal=A^\intercal+(A^\intercal)^\intercal.
            \] 
            Dado que $A$ es simétrica  $(A^\intercal = A)$: \[
                (A+A^\intercal)^\intercal=A+A^\intercal.
            \] 
            Por lo tanto, $A+A^\intercal$ \textbf{es simétrica} siempre que $A$ sea simétrica. 
        \item Matriz $A-A^\intercal$

            Para que $A-A^\intercal$ sea simétrica, debe cumplirse: \[
                (A-A^\intercal)^\intercal=A-A^\intercal.
            \] 
            Usamos la propiedad de la transpoción: \[
                (A-A^\intercal)^\intercal=A^\intercal-(A^\intercal)^\intercal.
            \] 
            Dado que $A$ es simétrica $(A^\intercal =A)$: \[
                (A-A^\intercal)^\intercal=A-A^\intercal.
            \] 
            Esto implica que: \[
            A-A^\intercal=0.
            \] 
            Por lo tanto, $A-A^\intercal$ \textbf{no puede ser simétrica a menos que $A=0$}.  
    \end{enumerate}
\item \lb{Prueba que si $A$ es una matriz invertible simétrica, entonces  $A^{-1}$ es también simétrica.}

        \textbf{Propiedades necesarias}
        \begin{enumerate}[label=\arabic*)]
            \item \textbf{Simetría de $A$:} $A^\intercal=A$.
            \item \textbf{Propiedad de la inversa:} $A A^{-1}=A^{-1}A=I $, donde $I$ es la matriz identidad.
            \item \textbf{Transposición del producto:} Para cualquier par de matrices $X$ e  $Y$, se cumple:  \[
                    (XY)^\intercal=Y^\intercal X^\intercal.
            \]  
        \end{enumerate}
        Partimos de la definición de la inversa: \[
        A A^{-1}=I.
        \] 
        Tomamos la traspuesta de ambos lados: \[
            (A A^{-1} )^\intercal=I^\intercal.
        \] 
        Usando la propiedad de la transpoción del producto: \[
            (A A^{-1})^\intercal=(A^{-1})^\intercal A^\intercal.
        \] 
        Como $I^\intercal=I$ (la identidad es simétrica) y $A^\intercal=A$ (porque $A$ es simétrica), esto se convierte en \[
            (A^{-1})^\intercal A=I.
        \] 
        De la definición de la inversa, sabes que $A^{-1}$ es la única matriz que satisface $A^{-1} A=I$. Por lo tanto: \[
            (A^{-1})^\intercal=A^{-1}.
        \] 
\item \lb{Sean $u_1,\dots,u_m$ vectores de $\mathbb{K}^n$ y supongamos que para ciertos escalares $x_1,\dots,x_m$ se tiene $x_1u_1+\cdots+x_mu_m=0$. Expresa esta última igualdad en forma matricial.}

    La ecuación $x_1u_1+x_2u_2+\cdots+x_m u_m=0$ puede representarse en forma matricial como sigue:
    \begin{enumerate}[label=\arabic*)]
        \item \textbf{Escribir los vectores en forma de matriz}: Sean $u_1,u_2,\dots,u_m$ vectres en $\K^n$, es decir, cada $u_j$ tiene $n$ componentes: \[
        u_j=\begin{bmatrix} 
        u_{j1}\\
        u_{j 2}\\
        \vdots\\
        u_{jn}
    \end{bmatrix},\quad \text{para $j=1,2,\dots,m$. }
        \] 
        Formamos una matriz $U$ de dimensión $n\times m$ donde las columnas son los vectores $u_1,u_2,\dots,u_m$: \[
        U=\begin{bmatrix} 
            u_{11} & u_{21} & \cdots & u_{m1}\\
            u_{12} & u_{22} & \cdots & u_{m2}\\
            \vdots & \vdots & \ddots & \vdots\\
            u_{1n} & u_{2n} & \cdots & u_{mn}
        \end{bmatrix}. 
        \] 
    \item \textbf{Escribir los escalares $x_1,x_2,\dots,x_m$ como un vector columna:} \[
    x=\begin{bmatrix} 
    x_1\\
    x_2\\
    \vdots\\
    x_m
    \end{bmatrix} .
    \]  
\item \textbf{Expresar la combinación lineal:} La combinación lineal $x_1u_1+x_2u_2+\cdots+x_m u_m$ se puede escribir como el producto matricial: \[
Ux=\begin{bmatrix} 
            u_{11} & u_{21} & \cdots & u_{m1}\\
            u_{12} & u_{22} & \cdots & u_{m2}\\
            \vdots & \vdots & \ddots & \vdots\\
            u_{1n} & u_{2n} & \cdots & u_{mn}
        \end{bmatrix}\cdot\begin{bmatrix} 
    x_1\\
    x_2\\
    \vdots\\
    x_m
    \end{bmatrix}.
\]  
\item \textbf{Igualdad en forma matricial:} La igualdad $x_1u_1+x_2u_2+\cdots+x_m u_m=0$ queda en forma matricial como: \[
Ux=0,
\]  donde $U$ es la matriz de dimensión $n\times m$, $x$ es el vectore de escalares de dimensión $m$, y  $0$ es el vector nulo de dimensión $n$.
    \end{enumerate}
\item \lb{Sean $u,v$ dos vectores no nulos vistos como matrices columna  $n\times 1$. Observa que $1+v^\intercal u$ es un escalar, que suponemos no nulo. Prueba que la matriz $I+uv^\intercal$ es no singular y su inversa es \[
            (I+uv^\intercal)^{-1}=I-\dfrac{uv^\intercal}{1+v^\intercal u}
\] } 

\begin{enumerate}[label=Paso \arabic*:]
    \item Propiedad de la inversa

        Para verificar que la matriz propuesta es la inversa, debemos comprobar que: \[
            (I+uv^\intercal)\left( I-\dfrac{uv^\intercal}{1+v^\intercal u} \right) =I.
        \] 
        Expandimos el producto: \[
            (I+uv^\intercal)\left( I-\dfrac{uv^\intercal}{1+v^\intercal u} \right) =I+uv^\intercal-\dfrac{uv^\intercal}{1+v^\intercal u}-uv^\intercal\cdot \dfrac{uv^\intercal}{1+v^\intercal u}=I+uv^\intercal-\dfrac{uv^\intercal}{1+v^\intercal u}-\dfrac{u(v^\intercal u)v^\intercal}{1+v^\intercal u}.
        \] 
    \item Agrupamos términos

        Agrupamos los términos relacionados con $uv^\intercal$: \[
        uv^\intercal-\dfrac{uv^\intercal}{1+v^\intercal u}-\dfrac{u(v^\intercal u)v^\intercal}{1+v^\intercal u}.
        \] 
        Sacamos $uv^\intercal$ como factor común: \[
        uv^\intercal\left( 1-\dfrac{1}{1+v^\intercal u}-\dfrac{v^\intercal u}{1+v^\intercal u} \right)=uv^\intercal\left( 1-\dfrac{1+v^\intercal u}{1+v^\intercal u} \right) =uv^\intercal\cdot (1-1)=0 .
        \] 
        Por lo tanto, todos los términos relacionados con $uv^\intercal$ se cancelan, dejando: \[
            (I+uv^\intercal)\left( I-\dfrac{uv^\intercal}{1+v^\intercal u} \right) =I.
        \] 
\end{enumerate}
\item \lb{Sea $u$ un vector de $\R^n$ con $\|u\|=1$ y consideremos la matriz  $A=I-2uu^\intercal$. Prueba que $A$ es simétrica y  $A^2=I$.}

    \begin{enumerate}[label=Parte \arabic*:]
        \item $A$ es simétrica

            Una matriz  $A$ es simétrica si  $A^\intercal=A$. Calculemos $A^\intercal$:
            \[
            A^\intercal=(I-2uu^\intercal)^\intercal.
            \] 
            Usamos la propiedad de la transpoción: \[
            A^\intercal=I^\intercal-2(uu^\intercal)^\intercal=I-2u^\intercal u=I-2uu^\intercal.
            \] 
            Por lo tanto: \[
            A^\intercal=A.
            \] 
            Esto demuestra que la matriz $A$ \textbf{es simétrica}.
        \item $A^2=I$

            Calculemos $A^2$: \[
            A^2=(I-2uu^\intercal)(I-2uu^\intercal).=I-2uu^\intercal-2uu^\intercal+4(uu^\intercal)(uu^\intercal)=I-4uu^\intercal-4u\lbb{(u^\intercal u)}{1}u^\intercal=I-\cancel{4uu^\intercal}+\cancel{4uu^\intercal}=I.
            \] 
    \end{enumerate}
\item \lb{Sea $A$ la matriz dada por bloques  \[
A=\begin{bmatrix} 
    A_{11} & A_{12}\\
    A_{21} & A_{22}
\end{bmatrix} 
\]con $A_{11}$ invertible. Prueba que existen matrices $X,Y$ tales que  \[
A=\begin{bmatrix} 
    1 & 0\\
    X & 1
\end{bmatrix} \begin{bmatrix} 
    A_{11} & 0\\
    0 & S
\end{bmatrix} \begin{bmatrix} 
    I & Y\\
    0 & I
\end{bmatrix} 
\] donde $S=A_{22}-A_{21}A_{11}^{-1}A_{12}$ e $I$ es la matriz identidad del tamaño adecuado (la matriz  $S$ se denomina el  \textbf{complemento dde Schur} de $A_{11}$ ).}

\begin{enumerate}[label=Paso \arabic*:]
    \item Expansión del producto

        Expandimos el producto de matrices en bloques:
        \begin{enumerate}[label=\arabic*)]
            \item Multiplicamos las primeras dos matrices: 
        \[
        \begin{bmatrix} 
            1 & 0\\
            X & 1
        \end{bmatrix} \begin{bmatrix} 
            A_{11} & 0\\
            0 & S
        \end{bmatrix} =\begin{bmatrix} 
            A_{11} & 0\\
            XA_{11} & S
        \end{bmatrix} .
        \] 
    \item Multiplicamos el resultado por la tercera matriz: \[
    \begin{bmatrix} 
        A_{11} & 0\\
        XA_{11} & S
    \end{bmatrix} \begin{bmatrix} 
        I & Y\\
        0 & I
    \end{bmatrix} =\begin{bmatrix} 
        A_{11} & A_{11}Y\\
        XA_{11} & XA_{11}Y+S
    \end{bmatrix} .
    \] 
        \end{enumerate}
        Por lo tanto, tenemos: \[
        A=\begin{bmatrix} 
            A_{11} & A_{11}Y\\
            XA_{11} & XA_{11}Y+S
        \end{bmatrix} .
        \] 
    \item Identificar las submatrices

        Comparando con la matriz original: \[
        A=\begin{bmatrix} 
            A_{11} & A_{12}\\
            A_{21} & A_{22}
        \end{bmatrix} ,
        \] 
        obtenemos las siguientes ecuaciones para identificar $X,Y$ y  $S$: 
         \begin{enumerate}[label=\arabic*)]
            \item De la entrada $(1,1)$:  \[
            A_{11}=A_{11}
            \] 
            Esto es trivialmente cierto.
        \item De la entrada $(1,2)$:  \[
        A_{12}=A_{11}Y\longrightarrow Y=A_{11}^{-1}A_{12}.
        \] 
    \item De la entrada $(2,1)$:  \[
    A_{21}=XA_{11}\longrightarrow X=A_{21}A_{11}^{-1}
    \] 
\item De la entrada $(2,2)$:  \[
A_{22}=XA_{11}Y+S=(A_{21}A_{11})^{-1}A_{11}(A_{11}^{-1}A_{12})+S=A_{21}A_{11}^{-1}A_{12}+S\longrightarrow S=A_{22}-A_{21}A_{11}^{-1}A_{12}.
\] 
        \end{enumerate}
\end{enumerate}
\item \lb{Expresa la matriz $AB$ como suma de matrices de rango  $1$, donde  \[
A=\begin{bmatrix} 
    1 & -1 & 3\\
    2 & 1 & 4
\end{bmatrix},\qquad B=\begin{bmatrix} 
    1 & 2 & 3\\
    0 & 1 & 4\\
    1 & 0 & 5
\end{bmatrix}  
\] } 

Para expresar $AB$ como una suma de matrices de rango $1$, utilizamos la propiedad de la multiplicación de matrices en términos de columnas de  $B$ y filas de  $A$. El producto  $AB$ se puede descomponer como una suma de las matrices de rango 1:  \[
AB=\sum_{k=1}^{m}a_kb_k^\intercal, 
\]donde: 
\begin{itemize}[label=\textbullet]
    \item $a_k$ es la  $k$-ésima columna de  $A$.
    \item  $b_k^\intercal$ es la $k$-ésima fila de  $B$.
\end{itemize}
Dado que \[
A=\begin{bmatrix} 
    1 & -1 & 3\\
    2 & 1 & 4
\end{bmatrix},\quad B=\begin{bmatrix} 
    1 & 2 & 3\\
    0 & 1 & 4\\
    1 & 0 & 5
\end{bmatrix},  
\] identificamos que $A$ tiene 3 columnas  $(m=3)$, por lo que:  \[
AB=a_1b_1^\intercal+a_2b_2^\intercal+a_3b_3^\intercal.
\] 
\begin{enumerate}[label=Paso \arabic*:]
    \item Identificar las columnas de $A$ y filas de  $B$
         \begin{itemize}[label=\textbullet]
            \item $a_1=\begin{bmatrix} 
            1\\
            2
            \end{bmatrix},\quad a_2=\begin{bmatrix} 
            -1\\
            1
            \end{bmatrix}, a_3 =\begin{bmatrix} 
            3\\
            4
            \end{bmatrix}  $.
        \item $b_1^\intercal=\begin{bmatrix} 
                1 & 2 & 3 
        \end{bmatrix}, \quad b_2^\intercal=\begin{bmatrix} 
                0 & 1 & 4 
        \end{bmatrix},\quad b_3^\intercal=\begin{bmatrix} 
                1 & 0 & 5 
        \end{bmatrix}$.
        \end{itemize}
    \item Calcular cada matriz de rango 1
        \begin{enumerate}[label=\arabic*)]
            \item \textbf{Primera matriz:} $a_1b_1^\intercal$ \[
            a_1b_1^\intercal=\begin{bmatrix} 
            1\\
            2
            \end{bmatrix} \cdot \begin{bmatrix} 
            1 & 2 & 3 
            \end{bmatrix} =\begin{bmatrix} 
            1 & 2 & 3\\
            2 & 4 & 6
            \end{bmatrix} .
            \]  
        \item \textbf{Segunda matriz}: $a_2b_2^\intercal$ \[
        a_2b_2^\intercal=\begin{bmatrix} 
        -1\\
        1
        \end{bmatrix} \cdot \begin{bmatrix} 
        0 & 1 & 4 
        \end{bmatrix} =\begin{bmatrix} 
        0 & -1 & -4\\
        0 & 1 & 4
        \end{bmatrix} .
        \]  
    \item \textbf{Tercera matriz:} $a_3b_3^\intercal$
        \[
        a_3b_3^\intercal=\begin{bmatrix} 
        3\\
        4
        \end{bmatrix} \cdot \begin{bmatrix} 
        1 & 0 & 5 
        \end{bmatrix} =\begin{bmatrix} 
        3 & 0 & 15\\
        4 & 0 & 20
        \end{bmatrix} .
        \] 
        \end{enumerate}
    \item Sumar las matrices \[
    AB=a_1b_1^\intercal+a_2b_2^\intercal+a_3b_3^\intercal
    \] 
    Sustituyendo: \[
    AB=\begin{bmatrix} 
        1 & 2 & 3\\
        2 & 4 & 6
    \end{bmatrix}+\begin{bmatrix} 
        0 & -1 & -4\\
        0 & 1 & 4
    \end{bmatrix}+\begin{bmatrix} 
        3 & 0 & 15\\
        4 & 0 & 20
    \end{bmatrix} =\begin{bmatrix} 
        4 & 1 & 14\\
        6 & 5 & 30
    \end{bmatrix} .  
    \] 
\end{enumerate}
\item \lb{Sea $u^\intercal=\left[ \dfrac{1}{3},\dfrac{2}{3},-\dfrac{2}{3} \right], v^\intercal=\left[ \dfrac{2}{3},\dfrac{1}{3},\dfrac{2}{3} \right]  $ y $w^\intercal=[a,b,c]$. Halla $a,b,c$ para que la matriz  $Q=[u,v,w]$ sea ortogonal de determinante 1.}

    Para que $Q=[u,v,w]$ sea una matriz ortogonal de determinante 1, debe cumplir las siguientes condiciones:
     \begin{enumerate}[label=\arabic*)]
        \item \textbf{Ortogonalidad:} Las columnas $u,v,w$ de  $Q$ deben ser ortogonales entre sí:  \[
        u^\intercal v=0,\quad u^\intercal w=0,\quad v^\intercal w=0
        \]  
    \item \textbf{Norma unitaria:} Cada columna debe tener norma 1: \[
    \|w\|=1.
    \]  
\item \textbf{Determinante:} El determinante de $Q$ debe ser 1:  \[
\mathrm{det}(Q)=1.
\]   
\begin{enumerate}[label=Paso \arabic*:]
    \item Condiciones iniciales

        Dado \[
        u^\intercal=\begin{bmatrix} 
            \dfrac{1}{3} & \dfrac{2}{3} & -\dfrac{2}{3} 
        \end{bmatrix} ,\quad v^\intercal=\begin{bmatrix} 
            \dfrac{2}{3} & \dfrac{1}{3} & \dfrac{2}{3} 
        \end{bmatrix} .
        \] 
        Comprobemos que $u$ y  $v$ son ortogonales:  \[
        u^\intercal v=\dfrac{1}{3}\cdot \dfrac{2}{3}+\dfrac{2}{3}\cdot \dfrac{1}{3}+\left( -\dfrac{2}{3} \right) \cdot \dfrac{2}{3}=\dfrac{2}{9}+\dfrac{2}{9}-\dfrac{4}{9}=0.
        \] 
        Por lo tanto, $u$ y  $v$ son ortogonales. 
    \item Contruir $w$

        Para garantizar que  $Q=[u,v,w]$ sea ortogonal,  $w$ debe ser ortogonal a  $u$ y  $v$. Además,  $\|w\|=1$.

         \textbf{Condición de ortogonalidad con $u$:} \[
         u^\intercal w=\dfrac{1}{3}a+\dfrac{2}{3}b-\dfrac{2}{3}c=0.
         \]  
         \textbf{Condición de ortogonalidad con $v$:} \[
         v^\intercal w=\dfrac{2}{3}a+\dfrac{1}{3}b+\dfrac{2}{3}c=0.
         \]  
         Estas dos ecuaciones lineales son: \[
         \begin{cases}
             \dfrac{1}{3}a+\dfrac{2}{3}b-\dfrac{2}{3}c=0\\
             \dfrac{2}{3}a+\dfrac{1}{3}b+\dfrac{2}{3}c=0
         \end{cases}\longrightarrow \begin{cases}
             a+2b-2c=0\\
             2a+b+2c=0
         \end{cases} \begin{array}{l}
             \longrightarrow a=-2b+2c\\

         \end{array}
         \] 
         $\begin{aligned}
             2(-2+2c)+b+2c&=0\\
             -4b+4c+b+2c&= 0 \\
             -3b+6c&= 0 \\
             b&=2c
         \end{aligned}$ 
         Sustituyendo $b=2c$ en  $a=-2b+2c$:  \[
         a=-2(2c)+2c=-4c+2c=-2c.
         \] 
         Por lo tanto: \[
         a=-2c,\quad b=2c.
         \] 
     \item Normalizar $w$

         Para garantizar que  $\|w\|=1$, calculamos:  \[
             \begin{array}{c}
         \|w\|^2=a^2+b^2+c^2=(-2c)^2+(2c)^2+c^2=4c^2+4c^2+c^2=9c^2.\\
         \|w\|=\sqrt{9c^2}=3|c|. 
             \end{array}
         \] 
         Por lo tanto, normalizamos $w$ dividiendo por 3: \[
         w=\dfrac{1}{3}\begin{bmatrix} 
         -2c\\
         2c\\
         c
         \end{bmatrix} =\begin{bmatrix} 
         -\frac{2}{3} \\
         \frac{2}{3} \\
         \frac{1}{3} 
         \end{bmatrix}. 
         \] 
\end{enumerate}
    \end{enumerate}
\item \lb{\textbf{(Traza de una matriz)} Dada una matriz cuadrada $A$, se define la \textbf{traza} de $A$ como $\mathrm{tr}(A)=\sum_{i=1}^{n} [A]_{ii}$, es decir, como la suma de los elementos de la diagonal principal. Prueba las siguientes propiedades:}
    \begin{enumerate}[label=\color{red}\textbf{\alph*)}]
        \item \db{$\mathrm{tr}(A+B)=\mathrm{tr}(A)+\mathrm{tr}(B)$.} 

            Sea $A$ y  $B$ matrices cuadradas de tamaño $n\times n$. La traza de la suma es: \[
\mathrm{tr}(A+B)=\sum_{i=1}^{n}[(A+B)]_{ii}.
\]
Usando la propiedad de la suma de matrices, sabemos que los elementos diagonales de $A+B$ son la suma de los elementos diagonales de $A$ y $B$: \[[(A+B)]_{ii}=[A]_{ii}+[B]_{ii}.\]
Por lo tanto: \[
\mathrm{tr}(A+B)=B)=\sum_{i=1}^{n}[(A+B)]_{ii}=\sum_{i=1}^{n}[A]_{ii}+\sum_{i=1}^{n}[B]_{ii}=\mathrm{tr}(A)+\mathrm{tr}(B).
\]
        \item \db{$\mathrm{tr}(A)=\mathrm{tr}(A^\intercal)$.} 

            Sea $A$ una matriz cuadrada de tamaño  $n\times n$. La traza de $A$ es:  \[
                \mathrm{tr}(A)=\sum_{i=1}^{n} [A]_{ii}.
            \] 
            La matriz transpuesta  $A^\intercal$ tiene la misma diagonal principal que $A$, ya que  $[A^\intercal]_{ii}=[A]_{ii}$. Por lo tanto: \[
                \mathrm{tr}(A^\intercal)=\sum_{i=1}^{n} [A^\intercal]_{ii}=\sum_{i=1}^{n} [A]_{ii}=\mathrm{tr}(A).
            \] 
        \item \db{$\mathrm{tr}(AB)=\mathrm{tr}(BA)$.} 

            Sea $A$ una matriz  $n\times m$ y $B$ una matriz  $m\times n$. Usando la definición de la traza: \[
                \mathrm{tr}(AB)=\sum_{i=1}^{n} [(AB)]_{ii}.
            \] 
            El elemento $[(AB)]_{ii}$ es la suma: \[
                [(AB)]_{ii}=\sum_{k=1}^{m} [A]_{ik}[B]_{ki}.
            \] 
            Sustituyendo en la traza: \[
                \mathrm{tr}(AB)=\sum_{i=1}^{n} \sum_{k=1}^{m}[A]_{ik}[B]_{ki}. 
            \] 
            Reorganizamos los índices de la suma: \[
                \mathrm{tr}(AB)=\sum_{k=1}^{m} \sum_{i=1}^{n} [B]_{ki}[A]_{ik}.
            \] 
            El término $\sum_{i=1}^{n} [B]_{ki}[A]_{ik}$ es precisamente $[(BA)]_{kk}$, y sumando sobre $k$, obtenemos: \[
                \mathrm{tr}=\sum_{k=1}^{m} [(BA)]_{kk}=\mathrm{tr}(BA).
            \] 
        \item \db{$\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A)$ con $P$ invertible.} 

            Sea $P$ una matriz invertible de tamaño  $n\times n$ y $A$ una matriz cuadrada  $n\times n$. Usamos la propiedad de $\mathrm{tr}(AB)=\mathrm{tr}(BA)$:  \[
                \mathrm{tr}(P^{-1}AP)=\mathrm{tr}((P^{-1}A)P)=\mathrm{tr}(A(PP^{-1}))=\mathrm{tr}(AI)=\mathrm{tr}(A).
            \] 
    \end{enumerate}
\end{enumerate}
\end{document}

