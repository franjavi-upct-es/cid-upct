\section{Procesos de Poisson}
En este capítulo vamos a ver un tipo de procesos estocásticos que son utilizados para contar la ocurrencia en el tiempo o el espacio de cierto evento aleatorio. Se trata de los procesos de Poisson y, más en general, los procesos de conteo. Este tipo de procesos pueden usarse para contar las llegadas de clientes a una tienda, apariciones de defectos en una línea de producción, las solicitudes individuaes a un servidor de internet, el paso de coches por un tramo de carretera, las llegadas de emails a la bandeja de entrada, las ocurrencias de cierto tipo de delito en un gran ciudad, etc.

\subsection{Procesos de conteo}
\begin{definition}
    Un proceso estocástico en tiempo continuo $(N_t)_{t\in [0,\infty)}$ se dice que es un \textbf{proceso de conteo} si se verifica:
    \begin{enumerate}[label=\arabic*)]
        \item $N_t\ge 0$.
        \item $N_t$ toma valores enteros.
        \item Si  $s<t$ entonces  $N_s\le N_t$.
    \end{enumerate}
\end{definition}
En particular, son procesos estocásticos de tiempo continuo y estado discreto. El valor $N_t$ se interpreta como el número de veces que ocurre el evento aleatorio en estudio durante el intervalo de tiempo  $[0,t]$. En particular, si  $s<t$, entonces  $N_t-N_s$ representa el número de veces que ocurre el evento aleatorio durante el intervalo de tiempo  $(s,t]$. Para fijar ideas, vamos a considerar que estamos contando las llegadas de clientes a un establecimiento. Por ejemplo, si  $N_1=4$ tendremos que han llegado 3 clientes durante el intervalo de tiempo $(1,2]$. El tiempo podría medirse por ejemplo en horas.

\begin{figure}[h]
\centering
<<echo=FALSE, fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth'>>=
# Valores para simulación
lambda <- 2   # tasa (llegadas por unidad de tiempo)
T <- 10       # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas
set.seed(123)
tau <- NULL
i <- 1
while (sum(tau) < T) {
  tau[i] <- rexp(n = 1, rate = lambda)
  i <- i+1
}

# Calculamos los valores del proceso de Poisson
tiempos <- c(0, cumsum(tau))
N <- 0:length(tau)

# Dibujamos la trayectoria
plot(tiempos, N, type = "s",
     xlab = "Tiempo", ylab = "Llegadas", xlim = c(0, 11))
@
\label{fig:3:1}
\caption{Trayectoria de un proceso de conteo. En cada tiempo $t, N_t$ representa el número de llegadas ocurridas en el intervalo $[0,t]$}
\end{figure}
 \begin{definition}
     Un proceso estocástico $(X_t)_{t\in [0,\infty)}$ se dice que tiene \textbf{incrementos independientes} si para toda sucesión de tiempo $0\le t_1<t_2<\dots<t_n$ las variables aleatorias \[
     X_{t_2}-X_{t_1},\quad X_{t_3}-X_{t_2},\quad\dots,\quad X_{t_n}-X_{t_{n-1}},
     \] son independientes.
\end{definition}
Un ejemplo de proceso con incrementos independientes es el movimiento Browniano. Para un proceso de conteo $(N_t)_{t\in [0,\infty)}$ con incrementos independientes contado las llegadas de clientes (o cualquier otro evento aleatorio), se tendrá que los números de llegadas en intervalos disjuntos \[
    (t_1,t_2],\quad(t_2,t_3],\quad\dots,\quad(t_{n-1},t_n]
\] son independientes.

Supongamos que queremos calcular la probabilidad de que haya 2 llegadas en el intervalo $(1,2]$ y 3 llegadas en el intervalo  $(2,3]$. Como los intervalos  $(1,2]$ y  $(2,3]$ son disjuntos, tendremos  \[
    \mathbb{P}\left( \text{2 llegadas en $(1,2]$ y 3 llegadas en $(2,3]$} \right) =\mathbb{P}\left( \text{2 llegadas en $(1,2]$} \right)\cdot \mathbb{P}\left( \text{3 llegadas en $(2,3]$} \right)  
\] 
\subsection{Proceso de Poisson}
El proceso de conteo más usado es el proceso de Poisson. En esta sección veremos su definición y principales propiedades. Antes de introducir este concepto, necesitamos recordar la distribución de Poisson.
\subsubsection{Distribución de Poisson}
\begin{definition}
    Una variable aleatoria discreta $X$ se dice que tiene distribución de  \textbf{Poisson} de parámetro $\lambda>0$ si toma valores en  $\{0,1,2,3,\dots\} $ y tiene función puntual de probabilidad \[
    \mathbb{P}(X=k)=\dfrac{\lambda^ke^{-\lambda} }{k!}\quad\text{ para todo }k\in \{0,1,2,\dots\}.
    \] 
    En este caso escribimos $X\sim \mathrm{Poisson}(\lambda)$.
\end{definition}
Algunas propiedades de la distribución de Poisson que conviene recordar son las siguientes.
\begin{proposition}
    Supongamos que $X\sim \mathrm{Poisson}(\lambda)$. Entonces $\mathbb{E}(X)=\lambda$ y $\mathrm{Var}(X)=\lambda$.
\end{proposition}
La suma de variables Poisson independientes es Poisson. Más concretamente, tenemos lo siguiente.
\begin{proposition}\label{prop:2:2}
    Si $X_1,X_2,\dots,X_n$ son variables independientes tal que $X_i\sim \mathrm{Poisson}(\lambda_i)$. Entonces, \[
    X_1+X_2+\dots+X_n\sim \mathrm{Poisson}(\lambda_1+\lambda_2+\dots+\lambda_n).
    \] 
\end{proposition}
Las variables Poisson pueden ser interpretadas como el límite en distribución de variables aleatorias binomiales. Más específicamente tenemos lo siguiente.
\begin{proposition}\label{prop:3}
    Consideremos una sucesión de variables aleatorias $Y_1,Y_2,\dots$, donde $Y_n\sim \mathrm{Binomial}(n,p_n)$ con $p_n=\dfrac{\lambda}{n}$ para cierto $\lambda>0$. Entonces la función puntual de probabilidad de  $Y_n$ converge a la función puntual de probabilidad de una variable  $\mathrm{Poisson}(\lambda)$, es decir, \[
    \lim_{n \to \infty} \mathbb{P}(Y_n=k)=\dfrac{\lambda^ke^{-\lambda} }{k!}\quad\text{ para todo }k\in \{0,1,2,\dots\}.
    \] 
\end{proposition}
\textit{Demostración.} Fijado $k\in \{0,1,2,\dots\} $, como $Y_n\sim \mathrm{Binomial}(n,p_n)$, tenemos
\begin{equation}\label{eq:3:1}
	\begin{aligned}
		\mathbb{P}(Y_n=k)&= \dbinom{n}{k} p_n^k(1-p_n)^{n-k} \\
		&= \dfrac{n!}{(n-k)!k!}\left( \dfrac{\lambda}{n} \right) ^k\left( 1-\dfrac{\lambda}{n} \right) ^{n-k} \\
		&= \dfrac{n!}{(n-k)!k!}\left( \dfrac{\lambda}{n} \right) ^{k}\left( 1-\dfrac{\lambda}{n} \right) ^n\left( 1-\dfrac{\lambda}{n} \right)^{-k} \\
        &= \dfrac{n!}{(n-k)!n^k}\dfrac{\lambda^k}{k!}\left( 1-\dfrac{\lambda}{n} \right) ^n\left( 1-\dfrac{\lambda}{n} \right)^{-k}. \\
	\end{aligned}
\end{equation}
Por otro lado, observamos que \[
\begin{array}{c}
    \lim_{n \to \infty} \left( 1-\dfrac{\lambda}{n} \right) ^n=e^{-\lambda},\\
    \lim_{n \to \infty} \left( 1-\dfrac{\lambda}{n} \right) ^{-k}=1,\\
    \begin{aligned}
        \lim_{n \to \infty} \dfrac{n!}{(n-k)!n^k}&= \lim_{n \to \infty} \left( \dfrac{n}{n}\cdot \dfrac{n-1}{n}\cdot \dfrac{n-2}{n}\cdot \dots\cdot \dfrac{n-(k-1)}{n} \right)  \\
        &= \lim_{n \to \infty} \left( 1\cdot \left( 1-\dfrac{1}{n} \right) \cdot \left( 1-\dfrac{2}{n} \right) \cdot \dots\cdot \left( 1-\dfrac{k-1}{n} \right)  \right) =1, \\
    \end{aligned}
\end{array}
\] donde para calcular el último límites hemos usado que tenemos un número constante $k$ de factores que convergen a 1.

Tomando lómites cuando  $n\to \infty$ en (\ref{eq:3:1}) teniendo en cuenta las anteriores igualdades, obtenemos el resultado deseado.
\subsubsection{Definición de proceso de Poisson}
El proceso de Poisson se usa para contar la ocurrencia de un evento aleatorio cuyas observaciones se dan en el tiempo con una frecuencia promedio constante $\lambda$. Por ejemplo, supongamos que mediante la observación se llega a la conclusión de que a un establecimiento llegan, de media, 5 clientes por hora. Puesto que las llegadas son aleatorias, cada hora llegará un número de clientes que no tiene por qué ser exactamente igual a 5; sin embargo, dicha cifra se verificará en promedio al avanzar el tiempo.
 \begin{definition}
     Un proceso de conteo $(N_t)_{t\in [0,\infty)}$ se dice que es un \textbf{proceso de Poisson} con tasa $\lambda>0$ si se verifican las siguientes condiciones:
     \begin{enumerate}[label=\arabic*)]
         \item $N_0=0$.
         \item $N_t$ tiene incrementos independientes.
         \item Para cada  $t,s\ge 0$ se verifica que $N_{t+s}-N_s\sim \mathrm{Poisson}(\lambda t)$.
     \end{enumerate}
\end{definition}
Para justificar la definición anterior pensemos en los clientes que llegan al establecimiento. Supongamos que llegan de media $\lambda$ clientes por hora. Sea  $N_t$ el número de clientes que han llegadoa a la tienda tras  $t$ horas. Queremos encontrar qué distribución debería seguir la variable aleatoria  $N_t$.

Para simplificar el problema, vamos a suponer en primer lugar que, dado  $n\in \N$, los clientes van llegando en instantes de tiempo equiespaciados \[
0<\dfrac{t}{n}<\dfrac{2t}{n}<\dfrac{3t}{n}<\dots<t,
\] de forma que al final de cada intervalo $\left( \dfrac{kt}{n},\dfrac{(k+1)t}{n} \right]$ podrá llegar un solo cliente con probabilidad $p$, o no llegará ninguno con probabilidad  $1-p$. Además, consideramos que la llegada o no de un cliente en cada intervalo ocurre de forma independiente de lo que haya pasado en los intervalos anteriores.

Bajo estos supuestos, tenemos que el número total  $N_t$ de clientes llegados en el intervalo de tiempo  $[0,t]$ sigue una distribución  $\mathrm{Binomial}(n,p)$. En particular, como llegan de media $\lambda$ clientes por hora, tendremos que  \[
\lambda t=\mathbb{E}(N_t)=np.
\] 
Necesariamente, se tiene que verificar que $p=\dfrac{\lambda t}{n}$. De esta manera obtenemos la probabilidad de llegada de un cliente en cada intervalo, la cual depende del número de subdivisiones $n$ en el que dividimos el intervalo de tiempo.

Para pasar a tiempo continuo, tomemos límites cuando $n\to \infty$. De este modo, como consecuencia de la Proposición \ref{prop:3}, obtenemos que \[
\mathbb{P}(N_t=k)=\dfrac{(t+\lambda)^ke^{-\lambda t} }{k!},\quad k\in \{0,1,2,\dots\} 
\] 
Concluimos que $N_t$ sigue una distribución  $\mathrm{Poisson}(\lambda t)$, lo cual justifica el punto 3 de la definición de Proceso de Poisson.

Es lógico considerar que los clientes que llegan en dos intervalos de tiempo disjuntos sean independientes, por lo que el proceso de conteo correspondiente debería tener incrementos independientes. Por lo cual, la condición 2 de la definición anterior es natural.

Como hemos observado arriba tenemos que $\mathbb{E}(N_t)=\lambda t$, lo cual es también consecuencia de la condición 3. En particular, la tasa $\lambda$ de un proceso de Poisson nos informa del número medio de llegadas por unidad de tiempo.

 \textbf{Ejemplo.} A una gasolinera llegan de media 10 coches por hora. Supongamos que la llegada de coches a la gasolinera sigue un proceso de Poisson. Calcula
 \begin{enumerate}[label=\arabic*)]
     \item Probabilidad de que lleguen 2 coches entre las 9:00 y las 9:20.
     \item Probabilidad de que lleguen 3 coches entre las 9:00 y las 9:20 y 7 coches entre las 10:20 y las 11:00.
 \end{enumerate}
 \textbf{Solución.} Consideramos el proceso $(N_t)_{t\ge 0}$ de conteo de coches tras $t$ horas. Como llegan una media de 10 coches por hora, tendremos que  $\lambda=10$.
 \begin{enumerate}[label=\arabic*)]
     \item Sea $X=\text{"Num. de coches entre las 9:00 y las 9:20"}$. Debido a la condición 3 de la definición de proceso de Poisson, tenemos que $X\sim \mathrm{Poisson}(10/3)$ ya que entre las 9:00 y las 9:20 ha pasado $\dfrac{1}{3}$ de hora. Por tanto, \[
     \mathbb{P}(X=2)=\dfrac{e^{-\frac{10}{3} } \cdot \left( \frac{10}{3} \right) ^2}{2!}=0.1982.
     \] 
     Alternativamente, podemos usar el siguiente código en \textbf{\texttt{R}} para calcular el resultado anterior mediante la función puntual de probabilidad de la Poisson:
<<>>=
t <- 1/3
lambda <- 10
# Evaluamos la función puntual de probabilidad de la Poisson
dpois(2, t*lambda)
@
     
     \item Sea $Y=$"Núm. de coches entre las 10:20 y las 11:00". Tenemos que  $Y\sim \mathrm{Poisson}(20/3)$ y, además, $X,Y$ son independientes. Entonces,
          \[
         \begin{aligned}
             \mathbb{P}(X=3,Y=7)&= \mathbb{P}(X=3)\mathbb{P}(Y=7) \\
             &= \dfrac{e^{-\frac{10}{3} } \cdot \left( \frac{10}{3}  \right) }{3!}\dfrac{e^{-\frac{20}{3} } \cdot \left( \frac{20}{3}  \right) ^7}{7!} =0.0325 \\
         \end{aligned}
         \] 
         Alternativamente, podemos usar el siguiente código de \textbf{\texttt{R}} para calcular el resultado anterior: 
<<>>=
t <- 1/3
s <- 2/3
lambda <- 10
# Evaluamos la función puntual de probabilidad de la Poisson
dpois(3, t*lambda)*dpois(7, s*lambda)
@
         
 \end{enumerate}
 \subsection{Tiempos de ocurrencia (llegada) en un Proceso de Poisson}
 Hemos visto que los procesos de Poisson surgen de manera natural cuando se trata de contar las llegadas de clientes a un establecimiento. En esta sección vamos a estudiar qué distribución de probabilidad sigue el tiempo aleatorio que pasa entre la llegada de dos clientes consecutivos.
\subsubsection{Distribución exponencial}
Recordemos que una variable aleatoria continua $\tau$ tiene distribución exponencial de parámetro  $\lambda>0$, en cuya caso escribimos $\tau\sim \mathrm{Exp}(\lambda)$, si toma valores reales positivos y tiene función de densidad \[
f(t)=\lambda e^{-\lambda t},\quad t>0. 
\] 
Recordemos además que si $\tau\sim \mathrm{Exp}(\lambda)$, entonces la función de distribución de $\tau$ es  $F(t)=1-e^{-\lambda t} $. En particular, \[
\mathbb{P}(\tau> t)=1-F(t)=e^{-\lambda t}. 
\] 
Las variables exponenciales suelen interpretarse como el tiempo de espera para la observación de un evento aleatorio. Por ejemplo, la llegada del primer cliente a una tienda, el tiempo de fallo de un sistema, el tiempo de quiebra de una entidad financiera, tiempo en el que una acción alcanza un determinado nivel, etc.
\begin{proposition}
    Si $\tau\sim \mathrm{Exp}(\lambda)$, entonces \[
    \mathbb{E}(\tau)=\dfrac{1}{\lambda},\quad \mathrm{Var}(\tau)=\dfrac{1}{\lambda^2}.
    \] 
\end{proposition}
Si pensamos en la llegada del primer cliente a un establecimiento, la proposición anterior nos dice que el tiempo medio que esperaremos en observar llegar dicho cliente será $\dfrac{1}{\lambda}$.

Por otro lado, tendremos que $\mathbb{P}(\tau\le t)$ es la probabilidad que el primer cliente llegue en algún instante del intervalo de tiempo $[0, t]$. Por tanto,  $\mathbb{P}(\tau>t)$ es la probabilidad de que ningún cliente llegue durante el intervalo de tiempo $[0,t]$.

Una importante propiedad de las variables exponenciales es que no tienen memoria, es decir, tenemos lo siguiente.
 \begin{proposition}
    Si $\tau\sim \mathrm{Exp}(\lambda)$, entonces \[
    \mathbb{P}(\tau>s)=\mathbb{P}(\tau>t+s|\tau>t)
    \] para todo $t,s>0$.
\end{proposition}
La anterior propiedad, conocida como \textbf{falta de memoria} de la distribución exponencial, quiere decir que la probabilidad de que en el intervalo de tiempo $[0,s]$ no haya ninguna llegada, es igual a la probabilidad de que en el intervalo de tiempo  $[t,t+s]$ de igual longitud no haya ninguna llegada si en el intervalo de tiempo previo  $[0,t]$ no ha habido llegadas, es decir,  \[
    \mathbb{P}(\text{no hay llegadas en $[0,s]$})=\mathbb{P}(\text{no hay llegadas en $[t,t+s]$}|\text{no hay llegadas en $[0,t]$}).
\]  
Por ejemplo, si en 30 minutos no ha llegado ningún cliente a la tienda, la probabilidad de esperar 10 minutos más y no ver ningún cliente es la misma que si no hubiéramos esperado los 30 minutos iniciales.

\subsubsection{Modelización de tiempos de llegada y entre llegadas consecutivas}

El siguiente resultado, para el cual no damos la demostración, caracteriza a los procesos de Poisson en términos de tiempos de espera exponenciales.
\begin{theorem}
    Un proceso estocástico $(N_t)_{t\in [0,\infty)}$ es un proceso de Poisson con tasa $\lambda$ si y sólo si existe una sucesión  $\tau_1,\tau_2,\tau_3,\dots$ de variables aleatorias independientes con distribución $\mathrm{Exp}(\lambda)$ tal que
    \begin{equation}\label{eq:3:2}
        N_t=\max\{n:\tau_1+\tau_2+\dots+\tau_n\le t\}.
    \end{equation}
\end{theorem}
Podemos pensa en $\tau_i$ como el tiempo de espera entre las llegadas, es decir,  $\tau_1$ es el tiempo transcurrido hasta la primera llegada, $\tau_2$ es el tiempo transcurrido entre la priemra y la segunda llegada, $\tau_3$ es el tiempo entre la segunda y la tercera llegada, etc. De esta manera, $T_n=\tau_1+\tau_2+\dots+\tau_n$ es el tiempo trasscurrido hasta la $n$-ésima llegada. En particular, la condición (\ref{eq:3:2}) puede ser reescrita como  \[
N_t=\max \{n:T_n\le t\}. 
\] 
Si, por ejemplo, $N_t=4$, la condición anterior nos dice que  \[
T_4\le t<T_5.
\] 
En términos de llegadas, tenemos que en el instante $t$ habrán llegado los cuatro primeros clientes, pero no el quintollegado los cuatro primeros clientes, pero no el quinto.

Recordermos que $\lambda$ era el promedio de llegadas por unidad de tiempo. Por otro lado, como  $\mathbb{E}(\tau)=\dfrac{1}{\lambda}$, tendremos que esperar de media $\dfrac{1}{\lambda}$ unidades de tiempo entre cada dos llegadas sucesivas.

\textbf{Ejemplo.} A la bandeja de entrada de mi email llegan de media 2 emails por hora (o un email cada media hora). Supongamos que abro el correo a las 8:00 am. Calcula:
\begin{enumerate}[label=\arabic*)]
    \item La probabilidad de que el primer email llegue más tarde de las 8:40.
    \item La probabilidad de que el primer email llegue más tarde de las 11:00 si no ha llegado ningún correo hasta las 9:00.
\end{enumerate}
\textbf{Solución:} Consideremos los tiempos $\tau_1,\tau_2,\tau_3,\dots$ entre llegadas de emails, medidos en horas.
\begin{enumerate}[label=\arabic*)]
    \item Tenemos que $\tau_1\sim \mathrm{Exp}(2)$, luego \[
    \mathbb{P}(\tau_1>0.5)=e^{-2\cdot 0.5}=0.3679. 
    \] 
    Alternativamente, podemos usar \textbf{\texttt{R}} para calcular la probabilidad anterior mediante la función de distribución de la exponencial:
<<>>=
lambda <- 2
# Evaluamos la función de distribución de la exponencial
1 - pexp(0.5, lambda)
@

    \item Usando que $\tau_1$ no tiene memoria \[
    \mathbb{P}(\tau_1>3|\tau_1>1)=P(\tau_1>2)=e^{-2\cdot 2}=0.0183. 
    \] 
    Alternativamente, podemos usar \textbf{\texttt{R}} para calcular la probabilidad anterior.
<<>>=
lambda <- 2
# Evaluamos la función de distribución de la exponencial
1 - pexp(2, lambda)
@
    
\end{enumerate}
Recordemos que una variable aleatoria continua $T$ tiene distribución gamma, en cuyo caso escribimos  $T\sim \Gamma(k,\lambda)$, si toma valores positivos y tiene función de densidad \[
f(t)=\dfrac{\lambda^k}{\Gamma(k)}t^{k-1}e^{-\lambda t},\quad t>0, 
\] donde $\Gamma(k)$ es la función Gamma, la cual viene dada por  $\Gamma(k)=(k-1)!$ cuando  $k$ es entero.

La distribución gamma se obtiene cuando sumamos variables independientes con distribución exponencial.
 \begin{proposition}
    Supongamos que $\tau_1,\tau_2,\tau_3,\dots$ son variables independientes con distribución $\mathrm{Exp}(\lambda)$. Entonces se verifica que \[
    T_n=\tau_1+\tau_2+\dots+\tau_n\sim \Gamma(n,\lambda).
    \] 
\end{proposition}
La anterior propiedad es muy útil para calcular probabilidades relativas a tiempos de llegada, ya que nos da la distribución del tiempo de llegada $T_n$ del  $n$-ésimo cliente.

 \textbf{Ejemplo.} Volviendo al ejemplo anterior. Calcula:
 \begin{enumerate}[label=\arabic*)]
     \item La probabilidad de que el cuarto email llegue pasadas las 10:00 am.
     \item La probabilidad de que el cuarto email llegue más de 3 horas después del segundo email si el segundo email llega después de las 9:00 am.
 \end{enumerate}
 \textbf{Solución:}
 \begin{enumerate}[label=\arabic*)]
     \item Como $T_4=\tau_1+\tau_2+\tau_3+\tau_4\sim \Gamma(n=4,\lambda=2)$, tenemos \[
     \mathbb{P}(T_4>2)=\int_{2}^{\infty} \dfrac{2^4 t^3e^{-2t} }{3!}\dt=\dfrac{8}{3}\int_{2}^{\infty} t^3e^{-2t}\dt=0.4435,   
     \] donde la anterior integral puede ser calculada a mano mediante integración por partes, o con ayuda del ordenador.

     Alternativamente, usando \textbf{\texttt{R}}  tenemos.
<<>>=
n <- 4
lambda <- 2
# Evaluamos la función de distribución de la distribución gamma
1 - pgamma(2, n, lambda)
@
    \item Como $\tau_3,\tau_4$ son independientes de $\tau_2,\tau_1$, por ser las $\tau_i$ independientes entre sí, tenemos que  \[
    \begin{aligned}
        \mathbb{P}(\tau_3+\tau_4>3|\tau_1+\tau_2>1)&= \mathbb{P}(\tau_3+\tau_4>3) \\
        &= \int_{3}^{\infty} 4te^{-2t}\dt=0.0173,  \\
    \end{aligned}
    \] donde hemos usado que $\tau_3+\tau_4\sim \Gamma(n=2,\lambda=2)$. Además, la anterior integral puede ser calculada a mano mediante integración por partes, o con ayuda del ordenador.

    Alternativamente, usando \textbf{\texttt{R}}  tenemos.
<<>>=
n <- 2
lambda <- 2
1 - pgamma(3, n, lambda)
@
\end{enumerate}
\subsubsection{Simulación de procesos de Poisson}

Simular proceso de Poisson es sencillo usando los tiempos entre llegadas. Para ello basta simular los tiempos entre llegadas como variables aleatorias exponenciales independientes y contabilizar el número de llegadas ocurridas.

Veamos un ejemplo en \textbf{\texttt{R}}: 

\begin{figure}[h]
\centering
<<fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth'>>=
# Valores para simulación
lambda <- 2   # tasa (llegadas por unidad de tiempo)
T <- 10       # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas
set.seed(123)
tau <- NULL
i <- 1
while (sum(tau) < T) {
  tau[i] <- rexp(n = 1, rate = lambda)
  i <- i+1
}

# Calculamos los valores del proceso de Poisson
tiempos <- c(0, cumsum(tau))
N <- 0:length(tau)

# Dibujamos la trayectoria
plot(tiempos, N, type = "s",
     xlab = "Tiempo", ylab = "Llegadas", xlim = c(0, 10))
@
\label{fig:3:2}
\caption{Simulación de la trayectoria de un proceso de Poisson}
\end{figure}
\subsection{Transformadas de procesos de Poisson}
En esta sección vamos a ver dos tipos de transformaciones de procesos de Poisson, la superposición y la separación.
\subsubsection{Superposición de procesos de Poisson independientes}
Consideremos dos procesos de Poisson $(N_t^1)_{t\in [0,\infty)}$ y $(N_1^2)_{t\in [0,\infty)}$ independientes con tasas $\lambda_1$ y $\lambda_2$, respectivamente. Consideremos el proceso $(N_t)_{t\in [0,\infty)}$ definido como \[
    N_t=N_t^1 +N_t^2\text{ para todo }t\in [0,\infty).
\] 
Dados $t,s\ge 0$, se tiene que los incrementos \[
N_{t+s}^{1}-N_s^1\sim \mathrm{Poisson}(\lambda_1t),\quad N_{t+s}^2-N_s^2\sim \mathrm{Poisson}(\lambda_2t),
\] y son variables aleatorias independientes. Por tanto, debido a la Proposición \ref{prop:2:2} tenemos que \[
N_{t+s}-N_t=(N_{t+s}^1-N_s^1)+(N_{t+s}^2-N_s^2)
\] sigue una distribución $\mathrm{Poisson}((\lambda_1+\lambda_2)t)$. Es más, $N_0=N_0^1+N_0^2=0$, y el proceso $N_t$ tiene incrementos independientes (por tenerlos $N^1$ y  $N^2$). Dado que se verifican todas las condiciones de la definción de proceso de Poisson, podemos concluir que el proceso $(N_t)_{t\in [0,\infty)}$ es de Poisson con tasa $\lambda_1+\lambda_2$.

En general, tendremos lo siguiente.

\begin{theorem}[Superposición]
    Sean $(N_t^1),(N_t^2),\dots,(N_t^m)$ procesos de Poisson independientes con tasas $\lambda_1,\lambda_2,\dots,\lambda_m$ respectivamente. Sea además \[
        N_t=N_t^1+N_t^2+\dots+N_t^m\text{ para todo }t\in [0,\infty).
    \] 
    Entonces el proceso $(N_t)_{t\in [0,\infty)}$ es un proceso de Poisson con tasa $\lambda_1+\lambda_2+\dots+\lambda_m$.
\end{theorem}
A continuación simulamos en \textbf{\texttt{R}} la superposición de dos procesos de Poisson independientes.

<<eval=FALSE>>=
# Valores para simulación de los procesos
lambda1 <- 2  # tasa del primer proceso de Poisson
lambda2 <- 0.5  # tasa del segundo proceso de Poisson
T <- 10       # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas
set.seed(123)
tau1 <- NULL
tau2 <- NULL
i <- 1
while(sum(tau1) < T){
  tau1[i] <- rexp(n = 1, rate = lambda1)
  i <- i + 1
}

i <- 1
while (sum(tau2) < T) {
  tau2[i] <- rexp(n = 1, rate = lambda2)
  i <- i + 1
}

# Definimos los tiempos y los valores correspondientes del proceso de Poisson 1
tiempos1 <- c(0, cumsum(tau1))
N1 <- 0:length(tau1)

# Definimos los tiempos y los valores correspondientes del proceso de Poisson 2
tiempos2 <- c(0, cumsum(tau2))
N2 <- 0:length(tau2)

# Definimos los tiempos y los valores correspondientes del proceso de superposición
tiempos <- union(tiempos1, tiempos2)
tiempos <- sort(tiempos)
N <- 0:(length(tiempos) - 1)

# Dibujamos la trayectoria del proceso 1
plot(tiempos1, N1, type = "s",
     xlab = "Tiempo", ylab = "Llegadas", xlim = c(0, 10), ylim = c(0, max(N)))

# Dibujamos la trayectoria del proceso 2
lines(tiempos2, N2, type = "s", col="blue")

# Dibujamos la trayectoria del proceso superposición
lines(tiempos, N, type = "s", col="red")

# Añadimos una leyenda
legend(0, 25, legend = c("Proceso 1", "Proceso 2", "Superposición"),
       col=c("black", "blue", "red"), lty=c(1, 1, 1))
@

\begin{figure}[h]
\centering
<<echo=FALSE, fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth'>>=
# Valores para simulación de los procesos
lambda1 <- 2  # tasa del primer proceso de Poisson
lambda2 <- 0.5  # tasa del segundo proceso de Poisson
T <- 10       # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas
set.seed(123)
tau1 <- NULL
tau2 <- NULL
i <- 1
while(sum(tau1) < T){
  tau1[i] <- rexp(n = 1, rate = lambda1)
  i <- i + 1
}

i <- 1
while (sum(tau2) < T) {
  tau2[i] <- rexp(n = 1, rate = lambda2)
  i <- i + 1
}

# Definimos los tiempos y los valores correspondientes del proceso de Poisson 1
tiempos1 <- c(0, cumsum(tau1))
N1 <- 0:length(tau1)

# Definimos los tiempos y los valores correspondientes del proceso de Poisson 2
tiempos2 <- c(0, cumsum(tau2))
N2 <- 0:length(tau2)

# Definimos los tiempos y los valores correspondientes del proceso de superposición
tiempos <- union(tiempos1, tiempos2)
tiempos <- sort(tiempos)
N <- 0:(length(tiempos) - 1)

# Dibujamos la trayectoria del proceso 1
plot(tiempos1, N1, type = "s",
     xlab = "Tiempo", ylab = "Llegadas", xlim = c(0, 10), ylim = c(0, max(N)))

# Dibujamos la trayectoria del proceso 2
lines(tiempos2, N2, type = "s", col="blue")

# Dibujamos la trayectoria del proceso superposición
lines(tiempos, N, type = "s", col="red")

# Añadimos una leyenda
legend(0, 25, legend = c("Proceso 1", "Proceso 2", "Superposición"),
       col=c("black", "blue", "red"), lty=c(1, 1, 1))
@
\label{fig:3:3}
\caption{Superposición de dos procesos de Poisson}
\end{figure}
\subsubsection{Separación de procesos de Poisson}
Imaginemos una cafetería en los Pirineos donde los clientes, de España y de Francia, van llegando de acuerdo a un proceso de Poisson $(N_t)_{t\in [0,\infty)}$ con tasa de 10 clientes por hora. Supongamos además que el 60\% de los clientes son españoles y el 40\% de los clientes son franceses. De esta manera, los clientes españoles tendrán su propio proceso de conteo $(N_t^1)_{t\in [0,\infty)}$ y, de la misma manera, los clientes frances tendrán otro proceso de conteo $(N_t^2)_{t\in [0,\infty)}$. Nos preguntamos qué tipo de procesos son $(N_t^1)_{t\in [0,\infty)}$ y $(N_t^2)_{t\in [0,\infty)}$.

En general, un proceso de Poisson $(N_t)_{t\in [0,\infty)}$ puede ser separado en dos procesos $(N_t^1)_{t\in [0,\infty)}$ y $(N_t^2)_{t\in [0,\infty)}$ contando llegadas de "tipo 1" y "tipo 2", de acuerdo a una sucesión $Y_1,Y_2,\dots$ de variables aleatorias independientes entre sí, e independientes del proceso $(N_t)_{t\in [0,\infty)}$, tal que \[
\mathbb{P}(Y_i=1)=p\text{ y }\mathbb{P}(Y_{i}=2)=1-p.
\] 
De esta manera, si $T_n=\tau_1+\tau_2+\dots+\tau_n$ es el tiempo transcurrido hasta la $n$-ésima llegada, entonces los procesos  $(N_t^1)_{t\in [0,\infty)}$ contando las llegadas de tipo 2 hasta tiempo $t$ vendrán respectivamente dados por
\begin{equation}\label{eq:3:3}
 N_t^1=\# \{n:Y_n=1,T_n\le t\},\quad N_t^2=\#\{n:Y_n=2,T_n\le t\},
\end{equation}
donde $\#$ denota el cardinal de un conjunto.

Tenemos el siguiente resultado.

\begin{theorem}[Separación]
    Sea $(N_t)_{t\in [0,\infty)}$ un proceso de Poisson con tasa $\lambda$. Consideremos que cada llegada es de tipo 1 con probabilidad  $p$ o de tipo 2 con probabilidad  $1-p$, independientemente de las demás llegadas y de  $(N_t)_{t\in [0,\infty)}$. Entonces, los procesos $(N_t^1)_{t\in [0,\infty)}$ y $(N_{t}^2)_{t\in [0,\infty)}$ contando las llegadas de tipo 1 y las llegadas de tipo 2, respectivamente, verifican:
    \begin{enumerate}[label=\arabic*)]
        \item $(N_t^1)_{t\in [0,\infty)}$ es un proceso de Poisson con tasa $\lambda p$.
        \item $(N_t^2)_{t\in [0,\infty)}$ es un proceso de Poisson con tasa $\lambda(1-p)$.
        \item $(N_t^1)_{t\in [0,\infty)}$ y $(N_t^2)_{t\in [0,\infty)}$ son procesos de Poisson independientes.
    \end{enumerate}
\end{theorem}
Volvamos al ejemplo de la cafetería en los Pirineos. Dijimos que llegan una media de 10 clientes por hora, por lo que el correpondiente proceso de Poisson $(N_t)_{t\in [0,\infty)}$ tendrá tasa $\lambda=10$. Al ser el 60\% de los clientes que llegan españoles y el 40\% franceses, tendremos que los clientes españoles llegarán a la cafetería siguiendo un proceso de Poisson con tasa  $10\cdot 0.6=6$, y los clientes franceses llegarán a la cafetería siguiendo un proceso de Poisson con tasa $10\cdot 0.4=4$.

A continuación simulamos en \textbf{\texttt{R}} la separación del proceso de clientes en españoles y franceses:

<<eval=FALSE>>=
# Valores para simulación de los procesos
lambda <- 10  # tasa del proceso de Poisson
p <- 0.6  # tasa del segundo proceso de Poisson
T <- 10   # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas y tipos de llegadas
set.seed(123)
tau <- NULL
y <- NULL

i <- 1
while (sum(tau) < T) {
  tau[i] <- rexp(n = 1, rate = lambda1)
  y[i] = sample(c(1, 2), size = 1)
  i <- i+1
}

# Definimos los tiempos y los valores de N
tiempos <- c(0, cumsum(tau))
N <- 0:length(tau)

# Definimos los valores del proceso de Poisson 1
N1 <- NULL
N1[y==1] <- 1
N1[y==2] <- 0
N1 <- c(0, cumsum(N1))

# Definimos los valores del proceso de Poisson 1
N2 <- NULL
N2[y==2] <- 1
N2[y==1] <- 0
N2 <- c(0, cumsum(N2))

# Dibujamos la trayectoria del proceso 1
plot(tiempos, N1, type = "s",
     xlab = "Tiempo", ylab = "Llegadas", xlim = c(0, 10), ylim = c(0, max(N)))

# Dibujamos la trayectoria del proceso 2
lines(tiempos, N2, type = "s", col = "blue")

# Dibujamos la trayectoria del proceso 2
lines(tiempos, N, type = "s", col = "red")

# Añadimos una leyenda
legend(0, 18, legend = c("Españoles", "Franceses", "Totales"),
       col = c("black", "blue", "red"), lty = c(1, 1, 1))
@

\begin{figure}[ht]
\centering
<<echo=FALSE, fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth'>>=
# Valores para simulación de los procesos
lambda <- 10  # tasa del proceso de Poisson
p <- 0.6  # tasa del segundo proceso de Poisson
T <- 10   # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas y tipos de llegadas
set.seed(123)
tau <- NULL
y <- NULL

i <- 1
while (sum(tau) < T) {
  tau[i] <- rexp(n = 1, rate = lambda1)
  y[i] = sample(c(1, 2), size = 1)
  i <- i+1
}

# Definimos los tiempos y los valores de N
tiempos <- c(0, cumsum(tau))
N <- 0:length(tau)

# Definimos los valores del proceso de Poisson 1
N1 <- NULL
N1[y==1] <- 1
N1[y==2] <- 0
N1 <- c(0, cumsum(N1))

# Definimos los valores del proceso de Poisson 1
N2 <- NULL
N2[y==2] <- 1
N2[y==1] <- 0
N2 <- c(0, cumsum(N2))

# Dibujamos la trayectoria del proceso 1
plot(tiempos, N1, type = "s",
     xlab = "Tiempo", ylab = "Llegadas", xlim = c(0, 10), ylim = c(0, max(N)))

# Dibujamos la trayectoria del proceso 2
lines(tiempos, N2, type = "s", col = "blue")

# Dibujamos la trayectoria del proceso 2
lines(tiempos, N, type = "s", col = "red")

# Añadimos una leyenda
legend(0, 18, legend = c("Españoles", "Franceses", "Totales"),
     col = c("black", "blue", "red"), lty = c(1, 1, 1))
@
\label{fig:3:4}
\caption{Separación del proceso de Poisson}
\end{figure}
\subsection{Procesos de renovación}
Hemos visto anteriormente que los procesos de Poisson son caracterizados como el proceso de conteo cuando los tiempos entre llegadas dados por una sucesión $\tau_1,\tau_2,\tau_3,\dots$ de variables independientes y exponenciales con la misma tasa $\lambda$. Una generalización de los procesos de Poisson son los procesos de renovación donde los tiempos entre llegadas son independientes e indénticamente distribuidos, pero no necesariamente siguen una distribución exponencial. Dicho concepto es definido formalmente como sigue.
 \begin{definition}
     Sean $\tau_1,\tau_2,\tau_3,\dots$ variables aleatorias no negativas (es decir, $\tau_n\ge 0$), independientes, e idéntifcamente distribuidas, de modo que $\mathbb{P}(\tau_n=0)<1$ para todo $n$. Llamamos  \textbf{proceso de renovación} con tiempos entre llegadas $\tau_1,\tau_2,\tau_3,\dots$ al proceso estocástico $(N_t)_{t\in [0\infty)}$ verificando $N_0=0$ y 
     \begin{equation}\label{eq:3:4}
         N_t=\max \{n:T_n\le t\} 
     \end{equation}
     donde $T_n=\tau_1+\tau_2+\dots+\tau_n$.
\end{definition}
Al igual que con los proceso de Poisson interpretamos $\tau_1,\tau_2,\tau_3,\dots$ como los tiempos entre llegadas, por lo que $T_n=\tau_1+\tau_2+\dots+\tau_n$ es el tiempo de la $n$-ésima llegada. La condición (\ref{eq:3:4}) implica que si  $N_t=n$, entonces  \[
T_n\le t<T_{n+1}.
\] 
\textbf{Ejemplo:} Imaginemos una máquina que funciona durante un tiempo $\alpha$ y, tras fallar, tarda un tiempo $\beta$ en ser separada. Si la máquina falla y es reparada reiteradas veces, obtendremos variables aleatorias independientes  $\alpha_1,\beta_1,\alpha_2,\beta_2,\alpha_3,\beta_3,\dots$ con los correspondientes tiempos de fallo y reparación. El tiempo total de cada ciclo donde la máquina empieza a funcionar, falla y es reparada viene dado por $\tau_i=\alpha_i+\beta_i$. Las variables $\tau_1,\tau_2,\tau_3,\dots$ son i.i.d. Podemos, por tanto, considerar el proceso de renovación $(N_t)_{t\in [0,\infty)}$ contando el número total de ciclos hasta tiempo $t$.

Una importante propiedad de los procesos de renovación es la siguiente versión de la ley de los grandes números.
\begin{theorem}[Ley de los grandes números para procesos de renovación]
    \label{theorem:3:5:1}
    Sea $(N_t)_{t\in [0,\infty)}$ un proceso de renovación con tiempos entre llegadas $\tau_1,\tau_2,\tau_3,\dots$. Si $\mathbb{E}(\tau_i)=\mu$, entonces con probabilidad 1 se verifica que \[
    \dfrac{N_t}{t}\to \dfrac{1}{\mu}\text{ cuando }t \to \infty.
    \] 
\end{theorem}
La interpretación del anterior resultado es sencilla. Si el tiempo medio entre llegadas (ciclos consecutivos) de un proceso de renovación es de $\mu$ horas, entonces en el largo plazo se producirán $\dfrac{1}{\mu}$ llegadas (ciclos) por hora. En el caso de un proceso de Poisson con tasa $\lambda$ se tendrá que  \[
\dfrac{N_t}{t}\to \lambda\text{ cuando }t \to \infty,
\] ya que los tiempos entre llegadas tienen media $\dfrac{1}{\lambda}$.

La demostración del anterior resultado se basa en la ley fuerte de los grandes números la cual recordamos a continuación.
\begin{lemma}[Ley fuerte de los grandes números]
Sean $\tau_1,\tau_2,\tau_3,\dots$ variables aleatorias i.i.d. con  $\mathbb{E}(\tau_i)=\mu$. Entonces con probabilidad 1 se verifica que \[
\dfrac{T_n}{n}\to \mu\text{ cuando }n\to \infty,
\] donde $T_n=\tau_1+\tau_2+\dots+\tau_n$ para cada  $n\in \N$.
\end{lemma}
A continuación damos la demostración de la ley de los grandes números para procesos de renovación.

\textit{Demostración del Teorema \ref{theorem:3:5:1}} Para cada $n\in \N$, sea $T_n=\tau_1+\tau_2+\dots+\tau_n$. Como las variables  $\tau_1,\tau_2,\tau_3,\dots$ son i.i.d. con media  $\mu$, la ley de los grandes números nos dice que, con probabilidad 1, se verifica
\begin{equation}
    \label{eq:3:5}
 \dfrac{T_n}{n}\to \mu\text{ cuando }n\to \infty.
\end{equation}
Ahora, por definición de proceso de renovación tenemos que \[
T_{N_t}\le t<T_{N_t+1}.
\] 
Dividiendo entre $N_t$ tenemos que  \[
\dfrac{T_{N_t}}{N_t}\le \dfrac{t}{N_t}<\dfrac{T_{N_t+1}}{N_t}.
\] 
De ahí se sigue que
\begin{equation}\label{eq:3:6}
\dfrac{T_{N_t}}{N_t}\le \dfrac{t}{N_t}<\dfrac{T_{N_t+1}}{N_t+1} \dfrac{N_t+1}{N_t}.
\end{equation}
Puesto que las trayectorias de $N_t$ son crecientes, dando infinitos saltos de longitud 1, tendremos que  $\lim_{t \to \infty} N_t=\infty$. Como consecuencia de (\ref{eq:3:5}) se verifica, con probabilidad 1, que \[
\lim_{t \to \infty} \dfrac{T_{N_t}}{N_t}=\mu.
\] 
Por las mismas razones también se cumplirá, con probabilidad 1, que \[
\lim_{t \to \infty}\dfrac{T_{N_t+1}}{N_t+1} \dfrac{N_t+1}{N_t}=\mu.
\] 
Finalmente, tomando límite en (\ref{eq:3:6}) cuando $t \to \infty$, y teniendo en cuenta lo anterior, llegamos a la conclusión deseada, ya que $\dfrac{t}{N_t}$ se encuentra, con probabilidad 1, entre dos sucesiones las cuales convergen a $\mu$.

Volvamos al ejemplo donde una máquina se estropea y es reparada reiteradas veces. Supongamos que, de media, la máquina tarda una hora en estropearse, y quince minutos en ser reparada. Supongamos, además, que cada tiempo de fallo $\alpha_i$ y cada tiempo en reparación $\beta_i$ siguen distribuciones exponenciales. De este modo, $\alpha_i\sim \mathrm{Exp}(1)$ y $\beta_i\sim \mathrm{Exp}(4)$. El proceso de renovación resultante puede ser simulado fácilmente en \textbf{\texttt{R}}.

\begin{figure}[h]
\centering
<<fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth'>>=
# Dijemos los valores de los parámetros
lambda_1 <- 1   # tasa de fallo
lambda_2 <- 4   # tasa de reparación
T <- 10     # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas
set.seed(13)
tau <- NULL
i <- 1
while (sum(tau) < T) {
  alpha <- rexp(n = 1, rate = lambda_1)
  beta <- rexp(n = 1, rate = lambda_2)
  tau[i] <- alpha + beta
  i <- i + 1
}

# Calculamos los valores del proceso de renovación
tiempos <- c(0, cumsum(tau))
N <- 0:length(tau)

# Dibujamos la trayectoria 
plot(tiempos, N, type = "s",
     xlab = "Tiempo", ylab = "Ciclos", xlim = c(0, 10))
@
\label{fig:3:5}
\caption{Simulación del proceso de renovación donde una máquina se estropea y repara reiteradamente}
\end{figure}
El tiempo medio de cada ciclo es $\mathbb{E}(\tau_i)=\mathbb{E}(\alpha_i)+\mathbb{E}(\beta_i)=1+\dfrac{1}{4}=\dfrac{5}{4}$. Por tanto, la ley de los grandes números para procesos de renovación nos dice que \[
\dfrac{N_t}{t}\to \dfrac{4}{5}\text{ cuando }t \to \infty.
\] 
Vamos a comprobar dicho resultado simulando una trayectoria a un plazo elevado de tiempo (por ejemplo, $T=1000$), y dibujando la gráfica del cociente $\dfrac{N_t}{t}$.

<<eval=FALSE>>=
# Dijemos los valores de los parámetros
lambda_1 <- 1   # tasa de fallo
lambda_2 <- 4   # tasa de reparación
T <- 1000       # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas
set.seed(13)
tau <- NULL
i <- 1
while (sum(tau) < T) {
  alpha <- rexp(n = 1, rate = lambda_1)
  beta <- rexp(n = 1, rate = lambda_2)
  tau[i] <- alpha + beta
  i <- i + 1
}

# Calculamos los valores del proceso de renovación
tiempos <- c(0, cumsum(tau))
N <- 0:length(tau)
cociente <- N / tiempos

# Dibujamos la trayectoria 
plot(tiempos, cociente, type = "S",
     xlab = "Tiempo", ylab = "N_t/t", xlim = c(0, 1000))

# Dibujamos una línea horizontal a la altura de la inversa de la media
abline(h=4/5, col="blue")
@


\begin{figure}[h]
\centering
<<echo=FALSE,fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth'>>=
# Dijemos los valores de los parámetros
lambda_1 <- 1   # tasa de fallo
lambda_2 <- 4   # tasa de reparación
T <- 1000       # Intervalo de tiempo en el que observamos el proceso

# Simulación de los tiempos entre llegadas
set.seed(13)
tau <- NULL
i <- 1
while (sum(tau) < T) {
  alpha <- rexp(n = 1, rate = lambda_1)
  beta <- rexp(n = 1, rate = lambda_2)
  tau[i] <- alpha + beta
  i <- i + 1
}

# Calculamos los valores del proceso de renovación
tiempos <- c(0, cumsum(tau))
N <- 0:length(tau)
cociente <- N / tiempos

# Dibujamos la trayectoria 
plot(tiempos, cociente, type = "S",
     xlab = "Tiempo", ylab = "N_t/t", xlim = c(0, 1000))

# Dibujamos una línea horizontal a la altura de la inversa de la media
abline(h=4/5, col="blue")
@
\label{fig:3:6}
\caption{El cociente entre el proceso de renovación y el tiempo se va aproximando a una constante, debido a la ley de los grandes números}
\end{figure}
