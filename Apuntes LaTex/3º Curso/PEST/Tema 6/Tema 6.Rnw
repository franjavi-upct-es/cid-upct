\section{Modelos ARIMA}
\subsection{Introducción}
Tanto en los métodos de análisis clásico de series como en los métodos de alisado exponencial, partimos de un esquema establecido a priori: descomposición de la serie en las componentes tendencia-ciclo, estacionalidad e irregular. Sin embargo, a principios de 1970 aparece un nuevo enfoque en el estudio de series temporales univariantes (debido a los estadísticos Box y Jenkins) y se basa en estudiar la correlación de los datos. \textit{Este nuevo enfoque consiste en considerar que la serie temporal en estudio ha sido generada por un proceso estocástico. El objetivo en este caso es identificar el proceso estocástico que ha generado la serie, para posteriormente poder realizar predicciones.} 

Por tanto, se pretende construir un modelo que nos permita explicar la estructura y preveer la evolución, a corto y medio plazo, de una serie temporal. La variable observada puede ser económica (I.P.C., demanda de un producto, existencias en un determinado almacén, etc.), física (temperatura de un proceso, velocidad del viento en una central eólica, concentración en la atmósfera de un contaminante, etc.) o social (número de nacimientos, votos de un determinado partido, etc.).

Recordemos que la definición de serie temporal (una sucesión de valores de una variable obtenidos de manera secuencial en el tiempo) coincide con el concepto de realización de un proceso estocástico. Es decir, los datos $x_1,x_2,\dots,x_n$ de una serie temporal observados en $n$ instantes de tiempo pueden interpretarse como una trayectoria o realización de particular de un proceso estocástico  $(X_t)_{t=1,2,\dots,n}$. Teniendo en cuenta esta interpretación, la teoría de los procesos estocásticos será aplicable al estudio de series temporales.

Si dispusiéramos de muchas realizaciones de un mismo proceso estocástico, es decir, de muchas series temporales generadas por un mismo proceso, podríamos intentar obtener la función de distribución de cada variable $X_i$ del proceso, aunque no sería sencillo. En general hay que contentarse con concoer algunas características del proceso como la función de medias, la función de varianzas, etc. Supongamos, por ejemplo, que disponemos de las siguientes series qye han sido generadas por un mismo proceso:  \[
\begin{array}{c}
    \text{Serie 1: }\{x_{11},x_{12},\dots,x_{1n}\} \\
    \vdots\\
    \text{Serie k: }\{x_{k1},x_{k 2},\dots,x_{kn}\} 
\end{array}
\] entonces podemos estimar la media de cada variable $X_i$ del proceso mediante:  \[
\begin{array}{c}
    \hat{\mu}_1=\dfrac{x_{11}+x_{21}+\dots+x_{k 1}}{k}\\
    \vdots\\
    \hat{\mu}_n=\dfrac{x_{1n}+x_{2n}+\dots+x_{kn}}{k}\\
\end{array}
\] 
No obstante, nos encontraremos con una importante restricción al trabajar con series temporales: en muchos casos, la serie observada es la única realización accesible del proceso estocástico que la ha generado. Por ejemplo, en la serie de turistas que visitan España mes a mes o en la cantidad de unidades producidas diariamente en una fábrica, solo disponemos de una única trayectoria concreta del fenómeno, sin acceso a múltiples realizaciones independientes del mismo proceso.

Este problema, que a simple vista parece insalvable, requiere que apliquemos ciertas restricciones e hipótesis al tipo de proceso estocástico que genera la serie en estudio. Específicamente, necesitaremos que el proceso estocástico sea \textit{estacionario} (al menos en sentido débil) y \textit{ergódico}. Estas condiciones garantizarán que los datos observados a los largo de un período de tiempo suficientemente amplio sean representativos del comportamiento probabilístico del proceso subyacente, y que el conocimiento obtenido de los datos actuales sea útil para comprender su comportamiento en momentos futuros.  

\subsubsection{Procesos débilmente estacionarios. El correlograma.}

Tal como estudiamos en el Tema 1, un proceso estocástico $\{X_t\}_{t=1,2,\dots} $ se dice \textbf{estacionario en sentido débil} (o débilmente estacionario) si cumple que su función de medias es constante, y su función de covarianzas sólo depende del retardo o salto temporal, es decir:
\begin{itemize}[label=\textbullet]
    \item $\mu_X(t)=\mu$ para cierta constante $\mu$.
    \item $C_X(t,t+k)=\gamma_k$, para cierta cantidad  $\gamma_k$ que sólo depende de  $k$ (y no del instante $t$).
\end{itemize}
Cuando el proceso sea débilmente estacionario, se cumplirá además que la función de varianzas es constante en el tiempo, y que la función de correlaciones sólo depende del retardo o salto temporal, es decir:
\begin{itemize}[label=\textbullet]
    \item $\sigma_X(t)=\sigma$ para cierta constante  $\sigma$.
    \item  $C_X(t,t+k)=\rho_k$, para cierta cantidad  $\rho_k$ que sólo depende de  $k$ (y no del instante $t$).
\end{itemize}

\begin{observation}
    La estacionariedad en sentido débil no garantiza la estacionariedad en sentido estricto, excepto en el caso de normalidad de las variables del proceso. No obstante, la estacionariedad en sentido débil garantizará que algunas características del proceso estocástico tenga un comportamiento estable a lo largo del tiempo.
\end{observation}
En el estudio de procesos estocásticos estacionarios, la función de correlaciones (o autocorrelaciones), denotada como $\rho_X$, es de especial importancia. Como mencionamos, cuando el proceso es débilmente estacionario, esta función depende únicamente del retardo  $k$, tomando un valor  $\rho_k$ que varía sólo en función de este desfase temporal y no del instante específico. La representación gráfica de la función  $\rho_k$ en relación con el retardo  $k$ se denomina correlograma (o autocorrelograma).

\begin{figure}[h]

<<fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# Generar serie temporal (ejemplo AR(2) para obtener oscilaciones + signos negativos)
set.seed(123)
ts_data <- arima.sim(model = list(ar = c(0.7, -0.3)), n = 100)

# Correlograma (ACF)
acf(
  ts_data,
  lag.max = 20,        # mostrar hasta lag 20
  main = "",           # sin título (como en la imagen)
  xlab = "Lag",
  ylab = "ACF",
  ylim = c(-0.25, 1.05),
  ci.col = "blue",     # color de las bandas de confianza
  ci.lty = 2           # línea punteada para las bandas
)

@
    \caption{Ejemplo de correlograma}
\end{figure}

Observar que el correlograma, tal y como se ha definido, sólo tiene sentido para procesos débilmente estacionarios.

En el contexto de procesos estocásticos débilmente estacionarios, aunque dispongamos de una única realización (una única serie temporal), podemos obtener una estimación de las características del proceso del siguiente modo. Si denotamos por $\{x_1,x_2,\dots,x_n\} $ a la única serie temporal observada del proceso, se tiene:
\begin{itemize}[label=\textbullet]
    \item Estimación de la función de medias constante: \[
            \hat{\mu}=\bar{x}=\dfrac{x_1+x_2+\dots+x_n}{n}
        \] 
    \item Estimación de la función de varianzas constante: \[
            \widehat{\sigma^2}=s_x^2=\dfrac{\sum_{t=1}^{n} (x_t-\bar{x})^2}{n}
        \] 
    \item Estimación de la función de covarianzas: \[
            \hat{\gamma}_k=\dfrac{\sum_{t=1}^{n-k} (x_t-\bar{x})(x_{t+k}-\bar{x})}{n}
        \] 
    \item Estimación de la función de correlaciones: \[
    \hat{\rho}_k=\dfrac{\sum_{t=1}^{n-k}(x_t-\bar{x})(x_{t+k}-\bar{x}) }{\sum_{t=1}^{n} (x_t-\bar{x})^2}
    \] 
\end{itemize}
Si buscamos el comportamiento de estacionariedad para las series temporales, necesitaremos ver gráficas que se mantienen en un nivel constante con unas pautas estables de oscilación. En la figura se muestra un ejemplo de serie estacionaria, realización de un proceso estocástico estacionario.

\begin{figure}[h]

<<fig=TRUE, fig.width=7, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# Generar y graficar una serie estacionaria (ejemplo)
set.seed(2025)

# Simular una serie AR(2) con comportamiento oscilatorio y picos
serie_ts <- arima.sim(
  model = list(ar = c(0.6, -0.35)), # coef. AR que producen oscilaciones
  n = 100
)

# Ajustes gráficos (márgenes para dejar sitio al subtítulo)
op <- par(no.readonly = TRUE)
par(mar = c(5, 4, 2, 2) + 0.1)  # abajo, izquierda, arriba, derecha

# Dibujo
plot(
  serie_ts,
  type = "l",         # línea continua
  lwd = 1,            # grosor de línea
  col = "black",
  xlab = "Time",
  ylab = "",
  main = ""           # sin título principal
)

# Restaurar parámetros gráficos
par(op)
@
    \caption{Ejemplo de serie estacionaria (en sentido débil)}
\end{figure}
En la práctica del análisis de series encontraremos  series con problemas de estacionariedad que afectan a cualquiera de sus parámetros básicos, siendo los más frecuentes las inconstancias en media y varianza. En la figura se muestran dos ejemplos de series no estacionarias, la primera en media y la segunda en varianza.

\pagebreak

\begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ----------------------------
# Series: no estacionaria en media vs en varianza
# ----------------------------
set.seed(2025)

# parámetros
n <- 150

# 1) No estacionaria en la media: caminata aleatoria con deriva (random walk)
drift <- -0.03
rw_increments <- rnorm(n, mean = drift, sd = 0.4)
series_mean_ns <- cumsum(rw_increments)

# 2) No estacionaria en la varianza: sigma alta al inicio y disminuye
sigma <- seq(from = 2.0, to = 0.2, length.out = n)   # varianza decreciente
series_var_ns <- rnorm(n, mean = 0, sd = sigma)

# Guardar parámetros gráficos y preparar layout 1x2
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4.5, 4, 1.5, 1), oma = c(0, 0, 0, 0))

# Función auxiliar: dibuja fondo gris y luego la serie en línea
plot_with_grey_bg <- function(x, y, xlab = "Time", ylab = "", sub_caption = "") {
  plot(x, y, type = "n", xlab = xlab, ylab = ylab, xaxt = "n")
  # dibujar eje x con ticks automáticos
  axis(1)
  # dibujar fondo gris claro en el panel de trazado
  usr <- par("usr")   # (x1, x2, y1, y2)
  rect(usr[1], usr[3], usr[2], usr[4], col = "#f5f5f5", border = NA)
  # volver a dibujar ejes por encima del rect
  box()
  axis(1)
  axis(2)
  # trazar la serie encima
  lines(x, y, col = "black", lwd = 1)
  # subtítulo (tipo pie de figura)
  title(sub = sub_caption, line = 2.2, cex.sub = 1.2, font.sub = 2)
}

# Panel izquierdo: no estacionaria en la media
plot_with_grey_bg(
  x = 1:n,
  y = series_mean_ns,
  xlab = "",
  ylab = "",
  sub_caption = "Serie no estacionaria en media"
)

# Panel derecho: no estacionaria en la varianza
plot_with_grey_bg(
  x = 1:n,
  y = series_var_ns,
  xlab = "",
  ylab = "",
  sub_caption = "Serie no estacionaria en varianza"
)

# Restaurar parámetros originales
par(op)

@
\caption{Ejemplo de series no estacionarias}
\end{figure}

Además de ser estacionario, el proceso estocástico ha der ser \textit{ergódico}. Este concepto es algo más complejo y sólo indicaremos que una condición necesaria para que un proceso sea ergódico es que $\lim_{k \to \infty} \rho_k=0$, es decir, que las autocorrelaciones sean nulas para retardos altos. Esto quiere decir, que para valores altos del retardo habrá poca dependencia entre las observaciones. En caso contrario los valores de la serie alejados en el tiempo estarían altamente correlados y por tanto no se podrían obtener estimaciones consistentes de la función de medias, varianzas, etc.

En adelante, supondremos siempre que trabajamos con procesos ergódicos y nos centramos en estudiar si el proceso que ha generado la serie es o no estacionario. Llegados a este punto, cabría preguntarse si la estacionariedad resulta una condición muy restrictiva, es decir, si en la práctica existen muchas series que proceden de procesos estocásticos no estacionarios. En este sentido podemos decir que, aunque trabajemos con series no estacionarias, en general se podrá conseguir la estacionariedad mediante una transformación sencilla en los datos.

De hecho, las dos transformaciones más usuales para conseguir la estacionariedad de una serie son:
\begin{itemize}[label=\textbullet]
    \item Realizar una transformación de Box-Cox (cuando la serie no es constante en varianza), siendo la más frecuente el tomar logaritmos neperianos en los datos.
    \item Tomar diferencias en la serie (cuando la serie no es constante en media): si la tendencia es lineal se tomarán diferencias de orden 1, si la tendencia es cuadrática se tomarán diferencias de orden 2, etc. Los procesos que no son estacionarios, pero que se convierten en estacionarios al tomar diferencias, se denominan \textit{procesos integrados}. 
\end{itemize}
Finalizamos la sección indicando una propiedad para procesos estacionarios fácil de demostrar.
\begin{proposition}
    Si $\{X_t\}_t $ es un proceso estacionario, entonces el proceso primera diferencia \[
    Z_t=X_t-X_{t-1}
    \] también sigue siendo estacionario.
\end{proposition}
\subsubsection{Procesos lineales}
Los procesos lineales son una clase particular de procesos estocásticos que incluyen a los siguientes tipos de procesos, que estudiaremos con más detalle en las próximas secciones:
\begin{itemize}[label=\textbullet]
    \item Proceso puramente aleatorios o ruido blanco gaussiano (también suele denominarse simplemente ruido blanco).
    \item Procesos autorregresivos, $AR(p)$.
    \item Procesos de medias móviles, $MA(q)$.
    \item Procesos autorregresivos y de medias móviles, $ARMA(p,q)$.
    \item Procesos autorregresivos y de medias móviles no estacionarios, $ARIM(p,d,q)$.
\end{itemize}
\subsection{Proceso de ruido blanco gaussiano}
Recordemos que un ruido blanco gaussiano es un proceso estocástico $(\varepsilon_t)_{t=1,2,\dots,n}$ verificando que $\varepsilon_t\sim N(0,\sigma^2)$ para todo $t=1,2,\dots,n$ e independientes entre sí.

Podemos interpretar un ruido blanco gaussiano como una sucesión de valores sin relación alguna entre ellos, oscilando en torno al cero dentro de un margen constante. En este tipo de procesos, conocer valores pasados no proporciona ninguna información sobre el futuro ya que el proceso es "puramente aleatorio".

En el caso del proceso de ruido blanco gaussiano, los correlogramas simple y parcial no presentarán ninguna correlación significativa (salvo para el retardo 0, donde la correlación es de 1). A continuación se muestra un ejemplo de ruido blanco gaussiano y su correlograma simple.

\begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ----------------------------
# Ejemplo de ruido blanco gaussiano y su correlograma
# ----------------------------
set.seed(2025)

# Generar ruido blanco gaussiano (media 0, varianza 1)
n <- 100
ruido_blanco <- rnorm(n, mean = 0, sd = 1)

# Configurar layout 1x2
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4.5, 4, 2, 1))

# --- Panel 1: Serie temporal ---
plot(
  ruido_blanco,
  type = "l",
  xlab = "Time",
  ylab = "",
  main = "",
  ylim = c(-2, 2)
)

# --- Panel 2: Correlograma (ACF) ---
acf(
  ruido_blanco,
  lag.max = 20,
  main = "",
  xlab = "Lag",
  ylab = "ACF",
  ylim = c(-0.2, 1),
  ci.col = "blue",
  ci.lty = 2
)

# Restaurar parámetros gráficos
par(op)

@

    \caption{Ejemplo de ruido blanco gaussiano y su correlograma}
\end{figure}

\subsection{Modelos Autorregresivos de orden $p,\, AR(p)$}

Al representar la influencia de hecos pasados sobre el presente (y en consecuencia sobre el futuro) de un proceso estocástico, podemos considerar diferentes expresiones. Una de ellas consiste en colocar el valor actual del proceso dependiente linealmente de valores pasados del propio proceso, más una perturbación aleatoria que se comporta como un ruido blanco gaussiano:
\begin{equation}\label{eq:6:1}
    X_t=\delta+a_1X_{t-1}+a_2X_{t-2}+\dots+a_pX_{t-p}+\varepsilon_t
\end{equation}
donde $\delta$ representa una constante y $\varepsilon_t$ es un ruido blanco, es decir, las variables $\varepsilon_t$ son i.i.d. y todas ellas tienen distribución $N(0,\sigma^2)$.

Esta formulación se denomina autorregresiva porque en cierto modo es un modelo de regresión del proceso sobre sí mismo.

Observando el modelo propuesto en (\ref{eq:6:1}), si lo consideramos estacionario, con $E(X_t)=\mu$, se tiene que: \[
\mu=\delta+a_1\mu+a_2\mu+\dots+a_p\mu\implies\mu=\dfrac{\delta}{1-a_1-a_2-\dots-a_p}
\] por consiguiente, para que exista la media, necesitamos que: \[
a_1+a_2+\dots+a_p\neq 1
\] 
Sin perder generalidad, en el desarrollo del apartado supondremos que el proceso está centrado, esto es, $\mu=\delta=0$.

Un elemento que se suele utilizar para expresar la formulación de los procesos lineales es el llamado operador de retardo $B$. Tal operador actúa sobre un término de un proceso estocástico reduciendo el índice temporal en una unidad:  \[
B\,X_t=X_{t-1}\implies B^k\,X_t=X_{t-k}
\] y por tanto, un proceso autorregresivo de orden $p$ puede expresarse en la forma:  \[
(1-a_1B-a_2B^2-\dots-a_pB^p)X_t=\varepsilon_t
\] o equivalentemente: \[
[a_p(B)]X_t=\varepsilon_t
\] donde \[
a_p(x)=1-a_1\cdot x-a_2\cdot x^2-\dots-a_p\cdot x^p
\] se denomina \textit{polinomio característico} del proceso autorregresivo.

Como propiedad, destacaremos que la condición necesaria y suficiente para que un proceso $AR(p)$ sea estacionario es que las raíces de su polinomio característico estén fuera del círculo unidad del plano complejo.

Los dos problemas fundamentales que nos presentan los procesos autorregresivos son:
 \begin{itemize}[label=\textbullet]
    \item Determinación del orden $p$ del modelo autorregresivo.
    \item Una vez fijado éste, determinar los parámetros  $a_i$ del modelo.
\end{itemize}
\subsubsection{Determinación del orden de la autorregresión}
Determinar el orden de un proceso autorregresivo a partir de su función de autocorrelación es difícil. En general esta función es una mezcla de decrecimientos exponenciales y sinusoidales, que se amortiguan al avanzar el retardo, y no presenta rasgos fácilmente identificables con el orden del proceso. Para resolver este problema se introduce la función de autocorrelación parcial.

Si comprobamos un $AR(1)$ con un  $AR(2)$ vemos que aunque en ambos modelos cada observación está relacionada con las anteriores, el tipo de relación entre observaciones separadas dos períodos, es distinto. En el  $AR(1)$ el efecto de  $X_{t-2}$ sobre $X_t$, es siempre a través de  $X_{t-1}$, y no existe efecto directo entre ambas. Conocido $X_{t-1}$, el valor de $X_{t-2}$ es irrelevante para prever $X_t$, es decir, tendremos que  \[
\rho(X_t,X_{t-2}|X_{t-1})=0,
\] donde la notación anterior se interpreta como la correlación entre las variables $X_t$ y  $X_{t-2}$ eliminando el efecto de $X_{t-1}$.

Esta dependencia puede ilustrarse con el esquema siguiente: \[
AR(1):X_{t-3}\to X_{t-2}\to X_{t-1}\to X_t
\] donde las flechas muestras una relación de dependencia directa.

Sin embargo, en un $AR(2)$ además del efecto de  $X_{t-2}$ que se transmite a $X_t$ a través de  $X_{t-1}$, existe un efecto directo de $X_{t-2}$ sobre $X_t$, por lo que, en general,  \[
\rho(X_t,X_{t-2}|X_{t-1})\neq 0.
\] 
Por otro lado, conocidos $X_{t-1}$ y $X_{t-2}$, el valor de $X_{t-3}$ es irrelevante para predecir $X_t$, es decir,  \[
\rho(X_t,X_{t-3}|X_{t-1},X_{t-2})=0.
\] 
En este caso, podemos escribir: \[
AR(2): \begin{array}{cccc}
    \Rsh & -\longrightarrow - & \downarrow & \\
    X_{t-3} & \to X_{t-2} & \to X_{t-1} & \to X_t\\
            & \downarrow & -\longrightarrow- & \uparrow
\end{array}
\] 

La función de autocorrelación simple tiene sólo en cuenta que $X_t$ y  $X_{t-2}$ están relacionadas en ambos casos, pero si medimos la relación directa entre $X_t$ y  $X_{t-2}$, esto es, eliminando el efecto debido a $X_{t-1}$, encontraremos que para un $AR(1)$ este efecto es nulo y para un  $AR(2)$ no.

En general, un  $AR(p)$ presenta efectos directos de observaciones separadas por  $1,2,\dots,p$ retardos y los efectos directos para retardos superiores son nulos, es decir, \[
\begin{array}{ll}
    \rho(X_t,X_{t-k}|X_{t-1},X_{t-2},\dots,X_{t-k+1})\neq 0 & \text{si }k\le p,\\
    \rho(X_t,X_{t-k}|X_{t-1},X_{t-2},\dots,X_{t-k+1})= 0 & \text{si }k> p.
\end{array}
\] 
Esta idea es la clave para la utilización de la función de autocorrelación parcial, entendiendo el coeficiente de autocorrelación parcial de orden $k$ como una medida de la relación lineal entre observaciones separadas  $k$ períodos con independencia de los valores intermedios. De este concepto se deduce que un proceso  $AR(p)$ tendrá los  $p$ primeros coeficientes de autocorrelación parcial distintos de cero. Llamaremos autocorrelograma parcial a la representación de los coeficientes de correlación parcial en función del retardo.  \textit{Por tanto, para determinar el orden de un modelo autorregresivo nos fijaremos en el correlograma parcial: el número de coeficientes que sean 'significativamente' distintos de cero indica el orden del proceso de $AR$.}

A continuación mostramos cómo serían los autocorrelogramas simples y parciales \textbf{teóricos} de modelos $AR(1)$ y $AR(2)$. 
\begin{enumerate}[label=\arabic*)]
    \item \textbf{Modelos AR(1):}
        \begin{enumerate}[label=\alph*)]
            \item \textit{Con el parámetro $a_1>0$:}
                \begin{figure}[h]
                    \centering
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ACF y PACF teóricas para AR(1)
# phi = coeficiente AR(1) (a1). Si phi>0 se obtiene decaimiento positivo.
phi <- 0.7
max_lag <- 20

# ACF teórica usando ARMAacf (incluye lag 0); quitamos lag 0
acf_theo <- stats::ARMAacf(ar = phi, lag.max = max_lag)[-1]  # lags 1..max_lag

# PACF teórica para AR(1): phi en lag 1, 0 para lags > 1
pacf_theo <- c(phi, rep(0, max_lag - 1))

# Preparar layout lado a lado
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# --- ACF teórica ---
plot(1:max_lag, acf_theo,
     type = "n",
     xlab = "Lag",
     ylab = "Autocorrel.",
     ylim = c(-1, 1),
     xaxt = "n",
     main = "Autocorrel. simple teórico de AR(1)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray60")
# barras verticales estilo correlograma
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = acf_theo[i], lwd = 2)
  points(i, acf_theo[i], pch = 19, cex = 0.8)
}
# anotar phi>0
text(x = max_lag * 0.6, y = 0.9, labels = expression(a[1] > 0))

# --- PACF teórica ---
plot(1:max_lag, pacf_theo,
     type = "n",
     xlab = "Lag",
     ylab = "Autocorrel. parcial",
     ylim = c(-1, 1),
     xaxt = "n",
     main = "Autocorrel. parcial teórico de AR(1)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray60")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = pacf_theo[i], lwd = 2)
  points(i, pacf_theo[i], pch = 19, cex = 0.8)
}
text(x = max_lag * 0.6, y = 0.9, labels = expression(a[1] > 0))

# Restaurar parámetros gráficos
par(op)

@

                    \caption{Correlogramas teóricos AR(1)}
                \end{figure}
            \item \textit{Con el parámetro $a_1<0$:}
                \begin{figure}[h]
                    \centering
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ACF y PACF teóricas para AR(1)
# phi = coeficiente AR(1) (a1). Si phi>0 se obtiene decaimiento positivo.
phi <- -0.7
max_lag <- 20

# ACF teórica usando ARMAacf (incluye lag 0); quitamos lag 0
acf_theo <- stats::ARMAacf(ar = phi, lag.max = max_lag)[-1]  # lags 1..max_lag

# PACF teórica para AR(1): phi en lag 1, 0 para lags > 1
pacf_theo <- c(phi, rep(0, max_lag - 1))

# Preparar layout lado a lado
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# --- ACF teórica ---
plot(1:max_lag, acf_theo,
     type = "n",
     xlab = "Lag",
     ylab = "Autocorrel.",
     ylim = c(-1, 1),
     xaxt = "n",
     main = "Autocorrel. simple teórico de AR(1)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray60")
# barras verticales estilo correlograma
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = acf_theo[i], lwd = 2)
  points(i, acf_theo[i], pch = 19, cex = 0.8)
}
# anotar phi>0
text(x = max_lag * 0.6, y = 0.9, labels = expression(a[1] < 0))

# --- PACF teórica ---
plot(1:max_lag, pacf_theo,
     type = "n",
     xlab = "Lag",
     ylab = "Autocorrel. parcial",
     ylim = c(-1, 1),
     xaxt = "n",
     main = "Autocorrel. parcial teórico de AR(1)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray60")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = pacf_theo[i], lwd = 2)
  points(i, pacf_theo[i], pch = 19, cex = 0.8)
}
text(x = max_lag * 0.6, y = 0.9, labels = expression(a[1] < 0))

# Restaurar parámetros gráficos
par(op)

@

                    \caption{Correlogramas teóricos AR(1)}
                \end{figure}
        \end{enumerate}
    \item \textbf{Modelos AR(2):}
        \begin{enumerate}[label=\alph*)]
            \item \textit{Con los parámetros $a_1>0,\,a_2>0$:} 
            
                \begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ACF y PACF teóricas para AR(2)
# R Wizard style: nombres en snake_case y comentarios útiles

phi1 <- 0.6   # a1 > 0
phi2 <- 0.2   # a2 > 0
max_lag <- 20

# 1) Obtener la ACF teórica (incluye lag 0 en la posición 1)
acf_all <- stats::ARMAacf(ar = c(phi1, phi2), lag.max = max_lag)
# acf_all[1] = rho_0 = 1, acf_all[2] = rho_1, ..., acf_all[max_lag+1] = rho_max_lag

# 2) Calcular la PACF teórica resolviendo Yule-Walker hasta max_lag
pacf_theo <- numeric(max_lag)
for (k in 1:max_lag) {
  # matriz Toeplitz de autocorrelaciones: rho_0, rho_1, ..., rho_{k-1}
  toeplitz_mat <- toeplitz(acf_all[1:k])
  # vector RHS: rho_1, rho_2, ..., rho_k
  rhs_vec <- acf_all[2:(k+1)]
  # resolver para coeficientes phi^(k)
  phi_k <- solve(toeplitz_mat, rhs_vec)
  # la PACF en lag k es el último coeficiente de phi^(k)
  pacf_theo[k] <- phi_k[k]
}

# 3) Graficar lado a lado (ACF teórica y PACF teórica)
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# ACF teórica (barras verticales)
plot(1:max_lag, acf_all[-1], type = "n",
     xlab = "Lag", ylab = "Autocorrel.",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. simple teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = acf_all[i+1], lwd = 2)
  points(i, acf_all[i+1], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] > 0 ~~ a[2] > 0))

# PACF teórica (esperamos picos en lags 1 y 2, luego ~0)
plot(1:max_lag, pacf_theo, type = "n",
     xlab = "Lag", ylab = "Autocorrel. parcial",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. parcial teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = pacf_theo[i], lwd = 2)
  points(i, pacf_theo[i], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] > 0 ~~ a[2] > 0))

par(op)  # restaurar parámetros gráficos

@
                    \caption{Correlogramas teóricos AR(2)}
                \end{figure}
            \item \textit{Con los parámetros $a_1<0,\,a_2>0$:} 
            
                \begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ACF y PACF teóricas para AR(2)
# R Wizard style: nombres en snake_case y comentarios útiles

phi1 <- -0.6   # a1 > 0
phi2 <- 0.2   # a2 > 0
max_lag <- 20

# 1) Obtener la ACF teórica (incluye lag 0 en la posición 1)
acf_all <- stats::ARMAacf(ar = c(phi1, phi2), lag.max = max_lag)
# acf_all[1] = rho_0 = 1, acf_all[2] = rho_1, ..., acf_all[max_lag+1] = rho_max_lag

# 2) Calcular la PACF teórica resolviendo Yule-Walker hasta max_lag
pacf_theo <- numeric(max_lag)
for (k in 1:max_lag) {
  # matriz Toeplitz de autocorrelaciones: rho_0, rho_1, ..., rho_{k-1}
  toeplitz_mat <- toeplitz(acf_all[1:k])
  # vector RHS: rho_1, rho_2, ..., rho_k
  rhs_vec <- acf_all[2:(k+1)]
  # resolver para coeficientes phi^(k)
  phi_k <- solve(toeplitz_mat, rhs_vec)
  # la PACF en lag k es el último coeficiente de phi^(k)
  pacf_theo[k] <- phi_k[k]
}

# 3) Graficar lado a lado (ACF teórica y PACF teórica)
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# ACF teórica (barras verticales)
plot(1:max_lag, acf_all[-1], type = "n",
     xlab = "Lag", ylab = "Autocorrel.",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. simple teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = acf_all[i+1], lwd = 2)
  points(i, acf_all[i+1], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] < 0 ~~ a[2] > 0))

# PACF teórica (esperamos picos en lags 1 y 2, luego ~0)
plot(1:max_lag, pacf_theo, type = "n",
     xlab = "Lag", ylab = "Autocorrel. parcial",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. parcial teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = pacf_theo[i], lwd = 2)
  points(i, pacf_theo[i], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] < 0 ~~ a[2] > 0))

par(op)  # restaurar parámetros gráficos

@
                    \caption{Correlogramas teóricos AR(2)}
                \end{figure}
            \item \textit{Con los parámetros $a_1>0,\,a_2<0$:} 
            
                \begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ACF y PACF teóricas para AR(2)
# R Wizard style: nombres en snake_case y comentarios útiles

phi1 <- 0.6   # a1 > 0
phi2 <- -0.2   # a2 > 0
max_lag <- 20

# 1) Obtener la ACF teórica (incluye lag 0 en la posición 1)
acf_all <- stats::ARMAacf(ar = c(phi1, phi2), lag.max = max_lag)
# acf_all[1] = rho_0 = 1, acf_all[2] = rho_1, ..., acf_all[max_lag+1] = rho_max_lag

# 2) Calcular la PACF teórica resolviendo Yule-Walker hasta max_lag
pacf_theo <- numeric(max_lag)
for (k in 1:max_lag) {
  # matriz Toeplitz de autocorrelaciones: rho_0, rho_1, ..., rho_{k-1}
  toeplitz_mat <- toeplitz(acf_all[1:k])
  # vector RHS: rho_1, rho_2, ..., rho_k
  rhs_vec <- acf_all[2:(k+1)]
  # resolver para coeficientes phi^(k)
  phi_k <- solve(toeplitz_mat, rhs_vec)
  # la PACF en lag k es el último coeficiente de phi^(k)
  pacf_theo[k] <- phi_k[k]
}

# 3) Graficar lado a lado (ACF teórica y PACF teórica)
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# ACF teórica (barras verticales)
plot(1:max_lag, acf_all[-1], type = "n",
     xlab = "Lag", ylab = "Autocorrel.",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. simple teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = acf_all[i+1], lwd = 2)
  points(i, acf_all[i+1], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] > 0 ~~ a[2] < 0))

# PACF teórica (esperamos picos en lags 1 y 2, luego ~0)
plot(1:max_lag, pacf_theo, type = "n",
     xlab = "Lag", ylab = "Autocorrel. parcial",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. parcial teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = pacf_theo[i], lwd = 2)
  points(i, pacf_theo[i], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] > 0 ~~ a[2] < 0))

par(op)  # restaurar parámetros gráficos

@
                    \caption{Correlogramas teóricos AR(2)}
                \end{figure}
            \item \textit{Con los parámetros $a_1<0,\,a_2<0$:} 
            
                \begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
# ACF y PACF teóricas para AR(2)
# R Wizard style: nombres en snake_case y comentarios útiles

phi1 <- -0.6   # a1 > 0
phi2 <- -0.2   # a2 > 0
max_lag <- 20

# 1) Obtener la ACF teórica (incluye lag 0 en la posición 1)
acf_all <- stats::ARMAacf(ar = c(phi1, phi2), lag.max = max_lag)
# acf_all[1] = rho_0 = 1, acf_all[2] = rho_1, ..., acf_all[max_lag+1] = rho_max_lag

# 2) Calcular la PACF teórica resolviendo Yule-Walker hasta max_lag
pacf_theo <- numeric(max_lag)
for (k in 1:max_lag) {
  # matriz Toeplitz de autocorrelaciones: rho_0, rho_1, ..., rho_{k-1}
  toeplitz_mat <- toeplitz(acf_all[1:k])
  # vector RHS: rho_1, rho_2, ..., rho_k
  rhs_vec <- acf_all[2:(k+1)]
  # resolver para coeficientes phi^(k)
  phi_k <- solve(toeplitz_mat, rhs_vec)
  # la PACF en lag k es el último coeficiente de phi^(k)
  pacf_theo[k] <- phi_k[k]
}

# 3) Graficar lado a lado (ACF teórica y PACF teórica)
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# ACF teórica (barras verticales)
plot(1:max_lag, acf_all[-1], type = "n",
     xlab = "Lag", ylab = "Autocorrel.",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. simple teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = acf_all[i+1], lwd = 2)
  points(i, acf_all[i+1], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] < 0 ~~ a[2] < 0))

# PACF teórica (esperamos picos en lags 1 y 2, luego ~0)
plot(1:max_lag, pacf_theo, type = "n",
     xlab = "Lag", ylab = "Autocorrel. parcial",
     ylim = c(-1, 1), xaxt = "n",
     main = "Autocorrel. parcial teórico de AR(2)")
axis(1, at = 1:max_lag)
abline(h = 0, col = "gray70")
for (i in 1:max_lag) {
  segments(x0 = i, y0 = 0, x1 = i, y1 = pacf_theo[i], lwd = 2)
  points(i, pacf_theo[i], pch = 19, cex = 0.7)
}
text(x = max_lag * 0.5, y = 0.9, labels = expression(a[1] < 0 ~~ a[2] < 0))

par(op)  # restaurar parámetros gráficos

@
                    \caption{Correlogramas teóricos AR(2)}
                \end{figure}
        \end{enumerate}
\end{enumerate}

\subsubsection{Estimación de los parámetros del modelo}

En esta sección nos centraremos en la segunda de las cuestiones: estimar los parámetros del modelo. Así, si consideramos un proceso $AR(p)$ centrado:
\begin{equation}\label{eq:6:2}
    X_t=a_1X_{t-1}+a_2X_{t-2}+\dots+a_pX_{t-p}+E(\varepsilon_tX_{t-j})
\end{equation} multiplicando por $X_{t-j}$ y tomando esperanzas
\begin{equation}\label{eq:6:3}
    E(X_tX_{t-1})=a_1E(X_{t-1}X_{t-j})+a_2E(X_{t-2}X_{t-j})+\dots+a_pE(X_{t-p}X_{t-j})+E(\varepsilon_tX_{t-j})
\end{equation}
Obsérvese que al ser $E(\varepsilon_t\varepsilon_{t-j})=0$ para $j>0$, se tiene que  $E(\varepsilon_tX_{t-j})=0$ para $j>0$ y  $E(\varepsilon_tX_t)=\sigma_\varepsilon^2$, donde $\sigma_\varepsilon^2$ denota la varianza común de las pertubaciones $\varepsilon_t$. Por tanto, reescribiendo (\ref{eq:6:3}) en términos de covarianzas (hemos supuesto proceso de media cero), tendremos las  \textbf{ecuaciones de Yule-Walker} para el proceso $AR(p)$  \textbf{usando covarianzas}: \[
\begin{array}{l}
    \gamma_0=a_1\gamma_1+a_2\gamma_2+\dots+a_p\gamma_p+\sigma_\varepsilon^2\\
    \gamma_j=a_1\gamma_{j-1}+a_2\gamma_{j-2}+\dots+a_p\gamma_{j-p}\quad j>0
\end{array}
\]
Por otro lado, dividiendo en las ecuaciones anteriores por $\gamma_0$ (varianza común del proceso), tendremos las \textbf{ecuaciones de Yule-Walker} para un proceso $AR(p)$  \textbf{usando correlaciones:}
\[
\begin{array}{l}
    \rho_0=1\\
    \rho_j=a_1\rho_{j-1}+a_2\rho_{j-2}+\dots+a_p\rho_{j-p}\quad j>0
\end{array}
\] 
Particularizando para $j=1,2,\dots,p$, se obtiene un sistema de ecuaciones que relaciona las $p$ primeras autocorrelaciones con los parámetros del proceso. Alternativamente, se denominan ecuaciones de Yule-Walker al sistema:
 \[
\begin{array}{l}
    \rho_1=a_1+a_2\rho_1+\dots+a_p\rho_{p-1}\\
    \rho_2=a_1\rho_1+a_2+\dots+a_p\rho_{p-2}\\
    \\
    \rho_p=a_1\rho_{p-1}+a_2\rho_{p-2}+\dots+a_p
\end{array}
\] 
Llamando: \[
\begin{array}{c}
    \mathbf{a'}=(a_1,a_2,\dots,a_p)\qquad \rho'=(\rho_1,\rho_2,\dots,\rho_p)\\
    \mathbf{R}=\begin{pmatrix} 
        1 & \rho_1 & \cdots & \rho_{p-1}\\
        \vdots & \vdots & & \vdots\\
        \rho_{p-1} & \rho_{p-2} & \cdots & 1
    \end{pmatrix} 
\end{array}
\] el sistema anterior se escribe matricialmente: \[
\rho=\mathbf{R\cdot a\implies a=R^{-1}}\cdot \rho
\] por consiguiente, los valores de los parámetros $\mathbf{a}$ se pueden obtener una vez estimada la matriz de autocorrelaciones de orden $p$.

Además, las ecuaciones de Yule-Walker coinciden con el criterio de mínimos cuadrados para los residuos. Así, si consideramos un proceso  $AR(p)$:  \[
X_t=a_1X_{t-1}+a_2X_{t-2}+\dots+a_pX_{t-p}+\varepsilon_t
\] a partir de $n$ valores observados de la serie, $x_1,x_2,\dots,x_n$, los residuos $e_t$ vendrán dados por:  \[
e_t=x_t-(a_1x_{t-1}+a_2x_{t-2}+\dots+a_px_{t-p})=x_t-\hat{x}_t
\] donde $\hat{x}_t$ denotamos el valor estimado de la serie. Los parámetros $a_1,a_2,\dots,a_p$ que minimizan la suma de cuadrados de los residuos coinciden con la solución a las ecuaciones de Yule-Walker.

Con el fin de obtener los estimadores óptimos de los parámetros según mínimos cuadrados, calcularemos las derivadas parciales respecto a $a_k$. Igualando estas parciales a cero, obtenemos las llamadas ecuaciones normales del modelo  \[
\sum_{k=1}^{p} a_k \sum_{i=1}^{n} x_{i-k}x_{i-j}=\sum_{i=1}^{n} x_ix_{i-j}\qquad j=1,2,\dots,p
\] 
Si suponemos que la nedia del proceso es cero y su varianza es constante igual a la unidad, denotando por: \[
\hat{\rho}_{k-j}=\hat{\rho}_{j-k}=\sum_{i=1}^{n} x_{i-k}x_{i-j}
\] el sistema anterior se transforma en: \[
\sum_{k=1}^{p} a_k\cdot \hat{\rho}_{k-j}=\hat{\rho}_j\qquad j=1,2,\dots,p
\] sistema que se corresponde con las llamadas ecuaciones de Yule-Walker donde se ha sustituido las correlaciones teóricas por sus estimaciones.
\subsubsection{Ejemplo de modelo $AR(1)$}
Consideremos el modelo Autorregresivo de orden 1 expresado mediante: \[
X_t=0.9X_{t-1}+\varepsilon_t
\] 
Una serie temporal generada por el proceso anterior, viene dada en el gráfico de la siguiente figura.
\begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
set.seed(2025)

# Parámetros del modelo
phi <- 0.9     # coeficiente AR(1)
n <- 100       # longitud de la serie

# Simulación de la serie AR(1)
serie_ar1 <- arima.sim(model = list(ar = phi), n = n)

# --- Figura 11: Serie temporal AR(1) ---
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 1), mar = c(4.5, 4, 2, 1))

plot(
  serie_ar1,
  type = "l",
  xlab = "Time",
  ylab = "",
  main = ""
)
@

    \caption{Ejemplo de serie AR(1)}
\end{figure}

A continuación se muestran las correlaciones simples y parciales estimadas a partir de la serie AR(1) simulada.

\begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
par(mfrow = c(1, 2), mar = c(4.5, 4, 2, 1))

# Correlograma simple (ACF)
acf(
  serie_ar1,
  lag.max = 20,
  main = "",
  xlab = "Lag",
  ylab = "ACF",
  ylim = c(-0.2, 1),
  ci.col = "blue",
  ci.lty = 2
)

# Correlograma parcial (PACF)
pacf(
  serie_ar1,
  lag.max = 20,
  main = "",
  xlab = "Lag",
  ylab = "Parcial ACF",
  ylim = c(-0.2, 1),
  ci.col = "blue",
  ci.lty = 2
)
@

\caption{Correlogramas simple y parcial serie AR(1)}
\end{figure}

Como se observa, el correlograma simple representa varias correlaciones significativas decreciendo de forma sinuisoidal, pero sólo es significativa la correlación parcial correspondiente al retardo 1, lo que indica que se trata de un modelo AR(1).

\subsection{Modelos de medias móviles, $MA(q)$}

Otra alternativa de representación de la dependencia respecto al pasado consiste en considerar el valor actual como el resultado de la combinación de $q$ factores aleatorios independientes entre si más una perturbación aleatoria contemporánea al modelo:  \[
    X_t=\varepsilon_t+b_1\varepsilon_{t-1}+b_2\varepsilon_{t-2}+\dots+b_q\varepsilon_{t-q}
\] donde $\varepsilon_t$ es un ruido blanco gaussiano. Al modelo anterior se le denomina proceso de medias móviles.

 \textbf{Nota:} Dependiendo del software usado, la notación usada para representar los modelos $MA(q)$ puede variar considerando los coeficientes $b_j$ cambiados de signo. En nuestro caso, usaremos la notación usual del software  \textbf{\texttt{R}}.

 Obviamente, se tiene: \[
 \begin{aligned}
     E(X_t)&= 0 \\
     \mathrm{Var}(X_t)&= \sigma_\varepsilon^2(1+b_1^2+b_2^2+\dots+b_q^2) \\
 \end{aligned}
 \] 
 Usando el operador de retardos tendremos: \[
 X_t=(1+b_1B+b_2B^2+\dots+b_qB^q)\varepsilon_t
 \] o equivalentemente: \[
 X_t=[b_q(B)]\varepsilon_t
 \] donde el polinomio $b_q(x)=(1+b_1x+b_2x^2+\dots+b_qx^q)$ recibe el nombre del polinomio característico del proceso de medias móviles.

 Estos procesos siempre son estacionarios (no necesitan condición sobre los parámetros $b_i$). Además, un proceso $MA(q)$ se puede ver como un  $AR(\infty)$ siempre y cuando el proceso sea invertible (las raíces del polinomio característico deben estar fuera del círculo unidad). \[
     X_t=[b_q(B)]\varepsilon_t\implies[b_q(B)]^{-1}X_t=\varepsilon_t
 \] 
 \subsubsection{Determinación del orden del modelo}

 Consideremos el proceso $MA(q)$ de media nula: \[
 X_t=\varepsilon_t+b_1\varepsilon_{t-1}+b_2\varepsilon_{t-2}+\dots+b_q\varepsilon_{t-q}
 \] 
 Recordemos que la función de covarianzas viene dada por: \[
 \gamma_k=\mathrm{Cov}(X_t,X_{t-k})=E(X_tX_{t-k})-E(X_t)E(X_{t-k})
 \] 
 Entonces, teniendo en cuenta que la función de medias es nula y que $(\varepsilon_t)_t$ es un proceso de ruido blanco gaussiano, la función de covarianzas para los proceso  $MA(q)$ quedaría:  \[
\gamma_k=\begin{cases}
     \sigma_\varepsilon^2b_k+\sigma_\varepsilon^2 \sum_{j=k+1}^{q} b_{j-k}b_j & \text{ si } k=1,2,\dots,q\\
     0 & \text{si }k>q
 \end{cases}
 \] de manera que las correlaciones serán nulas para retardos mayores de $q$:  \[
 \rho_k=0\text{ si }k>q
 \] 

Este último resultado tendrá una gran importancia práctica porque nos permitirá identificar el orden del proceso $MA$ al que se ajusta una serie temporal dada.  \textit{Para ello, observaremos el autocorrelograma de la serie: el número de coeficientes que sean 'significativamente' distintos de cero indican el orden del proceso $MA$.} 

Veamos en la siguiente tabla un resumen de las características básicas de los procesos $AR$ y  $MA$ 

\begin{center}
    \begin{tabular}{|c|C{6cm}|C{6cm}|}
\hline
                     & \textbf{AR(p)}                                               & \textbf{MA(q)}                                               \\ \hline
Expresión            & $\varepsilon_t=(1-a_1B-\dots-a_pB^p)X_t$ & $X_t=(1+b_1B+\dots+b_qB^q)\varepsilon_t$ \\ \hline
Estacionario         & Raíces del polinomio característico fuera del círculo unidad & Siempre                                                      \\ \hline
Correlograma         & Infinitos valores no nulos decreciendo de manera amortiguada & Valores no nulos hasta un retardo $q$, el resto nulos          \\ \hline
Correlograma parcial & Valores no nulos hasta un retardo $p$, el resto nulos          & Infinitos valores no nulos decreciendo de manera amortiguada \\ \hline
\end{tabular}
\end{center}
\subsubsection{Estimación de los parámetros del modelo}

La estimación de los parámetros en modelos $MA$ resulta más complicada que en modelos  $AR$ puesto que las ecuaciones son no lineales en los parámetros y para resolverlas es necesario recurrir a procedimientos iterativos.

Además, rara vez se trabaja en la práctica con un sistema  $MA$ puro, sino que se utiliza una combinación de modelo  $AR$ y  $MA$ dando lugar a los llamados modelos  $ARMA$ que trataremos en la siguiente sección.
\subsubsection{Ejemplo de modelo $MA(1)$} 
Consideremos el modelo de Medias Móviles de orden 1 expresado mediante: \[
X_t=\varepsilon_t+0.8\varepsilon_{t-1}
\] 
Una serie temporal generada por el proceso anterior, viene dada en el gráfico de la siguiente figura.

\pagebreak

\begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
set.seed(2025)

# Parámetros del modelo
phi <- 0.8     # coeficiente AR(1)
n <- 300       # longitud de la serie

# Simulación de la serie AR(1)
serie_ar1 <- arima.sim(model = list(ar = phi), n = n)

# --- Figura 11: Serie temporal AR(1) ---
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 1), mar = c(4.5, 4, 2, 1))

plot(
  serie_ar1,
  type = "l",
  xlab = "Time",
  ylab = "",
  main = "",
  ylim = c(min(serie_ar1) - 0.5, max(serie_ar1) + 0.5)
)
@
    \caption{Ejemplo de serie MA(1)}
\end{figure}

A continuación se muestran las correlaciones simples y parciales estimadas a partir de la serie $MA(1)$ simulada.

\begin{figure}[h]
<<fig=TRUE, fig.width=12, fig.height=5, out.width='0.7\\textwidth', fig.align='center', echo=FALSE>>=
par(mfrow = c(1, 2), mar = c(4.5, 4, 2, 1))

# Correlograma simple (ACF)
acf(
  serie_ar1,
  lag.max = 20,
  main = "",
  xlab = "Lag",
  ylab = "ACF",
  ylim = c(-0.2, 1),
  ci.col = "blue",
  ci.lty = 2
)

# Correlograma parcial (PACF)
pacf(
  serie_ar1,
  lag.max = 20,
  main = "",
  xlab = "Lag",
  ylab = "Parcial ACF",
  ylim = c(-0.2, 1),
  ci.col = "blue",
  ci.lty = 2
)
@
    \caption{Correlogramas simple y parcial serie $MA(1)$}
\end{figure}
Como se observa, el correlograma simple presenta sólo una correlación significativa, mientras que el correlograma parcial presenta varias correlaciones parciales significativas, lo que indica que se podría tratar de un modelo $MA(1)$. 
\subsection{Modelos $ARMA(p,q)$}

Existen procesos que encuentra su representación óptima mediante una combinación de los dos modelos anteriores. Tales modelos reciben el nombre de procesos autorrgresivos de medias móviles ($ARMA$). Su expresión vendrá dada por: \[
X_t-a_1X_{t-1}-a_2X_{t-2}-\dots-a_pX_{t-p}=\varepsilon_t+b_1\varepsilon_{t-1}+b_2\varepsilon_{t-2}+\dots+b_q\varepsilon_{t-q}
\] y usando el operador de retardos: \[
(1-a_1B-a_2B^2-\dots-a_pB^p)X_t=(1+b_1B+b_2B^2+\dots+b_qB^q)\varepsilon_t
\] o equivalentemente: \[
[a_p(B)]X_t=[b_q(B)]\varepsilon_t
\] donde $a_p(x)$ y  $b_q(x)$ son los polinomios característicos.

El proceso  $ARMA$ es estacionario si las raíces del polinomio $a_p(x)$ están fuera del círculo unidad, y será invertible si las raíces del polinomio  $b_q(x)$ están fuera del círculo unidad.

Como se observa, el modelo contiene  $p$ retardos del autorregresivo y  $q$ medias móviles, por consiguiente se representará por  $ARMA(p,q)$, es decir, el primer índice indicará el orden de la autorregresión y el segundo el de las medias móviles. 

\begin{observation}
    Los modelos $AR$ y  $MA$ se pueden obtener como caso particular del modelo  $ARMA$ haciendo  $q=0$ o bien  $p=0$.
\end{observation}
Si se cumplen las condiciones para que el modelo sea considerado estacionario, todo modelo $ARMA(p,q)$ se puede considerar como un  $MA(\infty)$ de la forma:  \[
X_t=\dfrac{(1+b_1B+b_2B^2+\dots+b_qB^q)}{(1-a_1B-a_2B^2-\dots-a_pB^p)}\varepsilon_t
\]
\subsubsection{Estimación de los parámetros del modelo}
El problema de estimación de los parámetros $ARMA$ a partir de  $n$ valores observados de la serie, $x_1,x_2,\dots,x_n$, resulta mucho más complicado que en modelos $AR$ puesto que como veremos seguidamente las ecuaciones del sistema que se obtienen al aplicar mínimos cuadrados no son lineales. Veamos el caso más sencillo, un proceso  $ARMA(1,1)$. Este proceso viene caracterizado por:  \[
X_t=a_1X_{t-1}+b_1\varepsilon_{t-1}+\varepsilon_t
\] los residuos $\varepsilon_t$ vendrán dados por:  \[
  \begin{array}{rcl}
      e_1 &= & x_1-\hat{x}_1=x_1\\
      e_2 &= & x_2-\hat{x}_2=x_2-a_1x_1-b_1e_1\\
       & & \cdots \\
      e_n &= & x_n-\hat{x}_n=x_n-a_1x_{n-1}-b_1e_{n-1}\\
  \end{array}
\] 
Obviamente $e_3$ depende de  $e_2$ y este a su vez depende de $e_1$. En general, los residuos contendrán potencias y productos cruzados de los parámetros $a_1$ y $b_1$ y por tanto no se tratará de un sistema lineal.
\subsection{Procesos lineales no estacionarios: modelo ARIMA}
\subsubsection{Paseo aleatorio}
Hemos visto en las secciones anteriores que los procesos $MA$ finitos son siempre estacionarios y que los  $AR$ lo son si las raíces del polinomio característico están fuera del círculo unidad. Consideremos un proceso  $AR(1)$:  \[
X_t=a_1X_{t-1}+\varepsilon_t
\] 
Si $|a_1|>1$, el proceso resulta "explosivo" (crece rápidamente), si $|a_1|<1$, el proceso es estacionario, mientras que si $|a_1|=1$, no es ni explosivo ni estacionario. Recordemos que este tipo de procesos reciben el nombre de \textbf{paseos aleatorios}. Como veremos en el siguiente apartado, se trata de un proceso integrado de orden 1 (puesto que su primera diferencia $X_t-X_{t-1}=\varepsilon_t$ es estacionaria). 

Observar que si denotamos al operador diferencia mediante: \[
\nabla X_t=X_{t}-X_{t-1}
\] el paseo aleatorio se escribe como: \[
\nabla X_t=\varepsilon_t
\] 
\subsubsection{Procesos ARIMA}
Como acabamos de mostrar, un paseo aleatorio es un proceso $AR(1)$ que no es estacionario, pues la raíz de su polinomio característico es unitaria. Pero si tomamos diferencias de orden 1, conseguimos que el proceso resultante sea estacionario. Esta idea puede generalizarse para cualquier proceso  $ARMA$, dando lugar a lo que se conocen como  \textit{procesos autorregresivos de medias móviles (ARIMA)}.

En el ejemplo del paseo antes mencionado se ha obtenido un proceso estacionario aplicando el operador diferencia una vez.
\begin{definition}
    Diremos que un proceso sigue un modelo $ARIMA(p,d,q)$ si al aplicar  $d$ veces el operador diferencia se obtiene un proceso estacionario  $ARMA(p,q)$.
\end{definition}
Así, si denotamos por: \[
Y_t=\nabla ^dX_t=(1-B)^dX_t
\] a la serie obtenida al aplicar $d$ veces el operador diferencia, si la serie originak  $X_t$ seguía un modelo  $ARIMA(p,d,q)$ se puede expresar mediante:  \[
(1-a_1B-a_2B^2-\dots-a_pB^p)(1-B)^dX_t=(1+b_1B+b_2B^2+\dots+b_qB^q)\varepsilon_t
\] 
\begin{observation}
    Los modelos AR, MA y ARIMA se obtienen como caso particular de los modelos ARIMA, tomando como parámetro $d=0$.
\end{observation}
\subsection{Identificación del modelo, validación y predicciones}
Una vez descritas algunas de las propiedades más importantes de los modelos $AR,MA,ARMA$ y  $ARIMA$, vamos a estudiar cómo identificar el proceso estocástico del que procede la serie en estudio.

Podemos decir que el objetivo concreto perseguido a lo largo de este tema es intentar identificar el proceso  $ARIMA(p,d,q)$ que probablemente haya generado nuestra serie.Como es habitual, debemos comenzar realizando un análisis descriptivo previo de la serie que incluya detección de outliers. En el proceso de identificación del proceso generador de la serie seguiremos las siguientes etapas:
 \begin{enumerate}
    \item Analizar la estacionalidad de la serie y determinar el parámetro $d$.
    \item Determinar el orden de la parte autorregresiva (parámetro $p$) y de la parte media móvil (parámetro $q$).
    \item Estimar los coeficientes del modelo: parámetros $a_i,b_j$ y  $cte$.
    \item Determinar los residuos  $e_t=\hat{\varepsilon}_t$ y validar el modelo.
    \item Realizar predicciones.
\end{enumerate}
\subsubsection{Análisis de la estacionariedad de la serie}

La primera etapa en la identificación del proceso generador de la serie consiste en determinar si la serie (proceso generador) es estacionaria. Como ya adelantamos en un apartado anterior, los incumplimientos de la estacionariedad suelen deberse a que la función de medias o la función de varianzas no resulten constantes.

Como herramienta para verificar la estacionalidad en varianza, podemos usar la representación gráfica de la serie: si observamos que las fluctuaciones de la serie se amplifican con el tiempo o con el nivel de la serie, será indicativo de que la función de varianzas no permanece constante.

En el caso de que la serie no sea estacionaria en varianza, se suele realziar una transformación de Box-Cox, siendo la más frecuente tomar logaritmos neperianos en los datos para conseguir varianza constante.

Una vez conseguida la estacionariedad en varianza, analizaremos si es estacionaria en media. Como herramienta para verificar la estacionariedad en media, podemos usar la representación gráfica de la serie: si la trayectoria de la serie oscila aleatoriamente alrededor de un valor constante, será indicativo de que la función de medias es constante, pero si observamos que el nivel de la serie varía a lo largo del tiempo, será indicativo de no estacionariedad en media.

En el caso de que la serie no sea estacionaria en media, en general se conseguirá que se convierta en estacionaria tomando diferencias de orden $d$. Por ejemplo, si observamos tendencia lineal en la serie, tomando diferencias de orden 1 conseguiremos un nivel constante de la serie, y si observamos tendencia cuadrática, tomando diferencias de orden 2 se conseguirá la estacionariedad en media.

Por otra parte, también podemos observar el autocorrelograma de la serie: si las correlaciones estimadas no decrecen rápidamente con el retardo, podría indicarnos que el proceso generador tiene una raíz del polinomio característico igual a uno (paseo aleatorio) y por tanto no es estacionario. En estos casos se suelen tomar diferencias de orden uno y se vuelve a observar el autocorrelograma. En general basta con tomar diferencias de orden uno o dos para lograr la estacionariedad.
\subsubsection{Determinación del orden de la parte $AR$ y de la parte  $MA$} 
Como herramientas básicas para determinar los órdenes $p$ y  $q$ del modelo  $ARMA$ se suelen utilziar tanto el correlograma simple de la serie como el correlograma parcial.

Para establecer algunas pautas a la hora de determinar los valores de  $p$ y  $q$, debemos recordar las propiedades de los modelo  $AR$ y  $MA$ estudiadas en apartados anteriores.
 \begin{itemize}[label=\textbullet]
    \item En un modelo $AR(p)$, las correlaciones parciales teóricas son nulas para retardos mayores de  $p$. Sin embargo, las correlaciones teóricas nunca se hacen cero, pero decaen rápidamente a partir del retardo  $p$.

        Por tanto, si se trata de un modelo autorregresivo, el orden $p$ se puede determinar a partir del correlograma parcial identificando los valores significativos del mismo.

    \item En un modelo $MA(q)$, las correlaciones teóricas son nulas para retardos mayores de  $q$. Sin embargo, las correlaciones parciales teóricas nunca se hacen cero, pero decaen rápidamente a partir del retardo  $q$.

         Por tanto, si se trata de un modelo de medias móviles, el orden $q$ se puede determinar a partir del correlograma simple identificando los valores significativos del mismo.
     \item En un modelo  $ARMA(p,q)$ tanto las correlaciones como las correlaciones parciales teóricas nunca se hacen cero.

         Por tanto, si se trata de un modelo $ARMA$, será difícil identificar los órdenes $p$ y $q$, puesto que no se produce un corte ni en el autocorrelograma simple ni en el parcial. Podremos al menos proponer algunos modelos como candidatos de partida y posteriormente valorar si son reducibles.
 \end{itemize}
 Evidentemente, si trabajáramos con las \textit{correlaciones y correlaciones parciales teóricas} el proceso de identificación se simplificaría bastante. Sin embargo, en la práctica siempre trabajaremos con las \textit{correlaciones y correlaciones parciales estimadas} a partir de la serie en estudio, de manera que el problema de identificación del modelo resulta todavía más complejo. De hecho, en muchas ocasiones el autocorrelograma estimado a partir de la serie suele presentar ciertas oscilaciones que no se corresponden con el modelo teórico.

Otro factor importante a la hora de determinar los órdenes de la parte autorregresiva y de medias móviles es el \textit{tamaño de la serie}. La identificación será más fácil cuanto mayor sea el tamaño de la serie. En este sentido se han realizado simulaciones que muestran una gran diferencia entre las correlaciones (parciales) teóricas y las estimadas para series de tamaño pequeño (inferior a 60 observaciones).

\subsubsection{Estimación de los coeficientes del modelo}

Una vez determinados los órdenes $p$ y  $q$ del modelo  $ARMA$, necesitamos estimar los coeficientes  $a_i$ y  $b_j$ del modelo:  \[
X_t-a_1X_{t-1}-a_2X_{t-2}-\dots-a_pX_{t-p}=\varepsilon_t+b_1\varepsilon_{t-1}+b_2\varepsilon_{t-2}+\dots+b_q\varepsilon_{t-q}
\] 
Además, debemos estimar la constante del modelo, en caso de que éste la incluya.

La estimación de los parámetros se puede realizar minimizando la suma de cuadrados residual o bien maximizando la verosimilitud, dando ambos procedimientos resultados similares (aunque no iguales). Como hemos comentado en secciones anteriores, si el modelo contempla parte $MA$ será necesario resolver sistemas de ecuaciones no lineales. Por ese motivo, diferentes softwares pueden mostrar soluciones distintas para una misma serie dependiendo del algoritmo de resolución empleado.

En nuestro caso, realizaremos la estimación de los coeficientes del modelo haciendo uso del software  \textbf{\texttt{R}}, que además suele incluir (como en el caso de la regresión lineal múltiple) contrastes sobre la significación de cada uno de los parámetros $a_i,b_j$, así como de la constante. Estos contrastes nos será de gran utilidad a la hora de identificar el modelo generador de la serie. En este contexto también haremos uso del  \textit{principio de parsimonia}, según el cual debemos seleccionar como modelo óptimo aquel que contenga menor número de parámetros entre todos los modelos considerados adecuados.

\subsubsection{Validación del modelo: análisis de los residuos}

Para que el modelo propuesto en la etapa anterior sea adecuado, es necesario que los residuos (diferencia entre los valores observados de la serie y los ajustados por el modelo propuesto) se comporten como un ruido blanco gaussiano. Por tanto, la validez del modelo propuesto para por verificar las siguientes hipótesis sobre los residuos:
\begin{itemize}[label=\textbullet]
    \item Los residuos se comporan como una distribución normal (hipótesis de normalidad).
    \item Los residuos tienen varianza constante (hipóteiss de homocedasticidad).
    \item Los residuos son independientes (hipótesis de independencia).
\end{itemize}
Esta validación coincide con la realizada en el caso de los modelso de regresión, por lo que se pueden usar procedimientos similares.

Por ejemplo, la \textit{normalidad} se puede verificar mediante contrastes no paramétricos como los de Kolmogorov-Smirnov ode Shapiro-Wilks, o bien a través de métodos gráficos.

Por otra parte, podemos contrastar la \textit{homocedasticidad} observando el gráfico de dispersión de residuos frente a los valores ajustados o bien mediante el gráfico de los residuos frente al tiempo: estos gráficos deben mostrar que los residuos se situan aleatoriamente alrededor del cero, dentro de una banda de amplitud constante.

Con respecto a la \textit{independencia} de los residuos, haremos uso del autocorrelograma: éste no debe presentar ninguna correlación significativa, han de ser todas prácticamente nulas para que los residuos se supongan independientes.

\subsubsection{Bondad del ajuste y selección del mejor modelo}

Una medida usual para cuantificar la bondad de un ajuste consiste en calcular la suma de cuadrados residual del modelo ajustado. Por ejemplo, en análisis de regresión múltiple, se puede utilizar esta medida para determinar lo "bueno" que es el ajuste realizado.

Por tanto, un primer criterio para determinar el "mejor" modelo consistirá en seleccionar, dentro de los modelos considerados válidos por verificar las hipótesis de los residuos, aquel modelo que presente una menor suma de cuadrados residual. Sin embargo, este criterio no tiene en cuenta ni el número de parámetros del modelo (órdenes $p$ y  $q$), ni la magnitud de las observaciones. Es decir, si realizamos transformaciones en los datos, el modelo resultante no es comparable con el original a través de los residuos (por ejemplo, una transformación logarítmica siempre proporcionará residuos de menor magnitud que para los datos originales).

Existen otros criterios para la selección de modelos, entre los que destacaremos los criterios de información AIC, AIC corregido y BIC, basados en la verosimilitud y que tienen en cuenta el número de parámetros a estimar.

Es importante destacar que estos criterios de información no son adecuados para seleccionar el orden la diferenciación (parámetro $d$), pues los valores de la verosimilitud no son comparables para modelos con diferentes órdenes de diferenciación. Por tanto, es conveniente que la selección del orden $d$ se realice de otra forma (por ejemplo, manualmente), y posteriormente se pueden usar los criterios AIC y BIC para determinar los órdenes  $p$ y  $q$.

La  \textbf{selección del modelo} se puede realizar de manera "manual" (realizando los pasos indicados al principio de la sección), o bien utilizando funciones que realizan la selección de forma automática. En la siguiente figura, extraída de Hyndman and Athanasopoulos (2021), se muestran los pasos a seguir en las dos alternativas:

\begin{figure}[h]
    \centering

    \begin{tikzpicture}[node distance=2cm]
    \tikzstyle{startstop} = [rectangle, rounded corners, 
    minimum width=3cm, 
    minimum height=1cm,
    text centered, 
    text width=4cm, 
    draw=black, 
    fill=blue!30]

    \tikzstyle{decision} = [diamond, rounded corners,
    minimum width=3cm, 
    minimum height=1cm,
    text centered, 
    draw=black, 
    text width=3cm,
    fill=blue!30]

    \tikzstyle{option} = [ellipse, 
    minimum width=3cm, 
    minimum height=1cm, 
    text width=3cm,
    text centered, 
    draw=black, 
    fill=red!30]

    \tikzstyle{arrow} = [thick,->,>=stealth]

    \node (1) [startstop] {1. Plot the data. Identify unusual observations. Undestand patterns};
    \node (2) [startstop, below of=1] {2. If neccessary, use a Box-Cox transformation to stabilise the variance};
    \node (opt1) [option, left=1cm of 2] {Select model order yourself.};
    \node (opt2) [option, right=1cm of 2] {Use automated algorithm.};
    \node (3) [startstop, below=1cm of opt1] {3. If necessary, difference the data until it appears stationary. Use unit-root tests if you are unsure.};
    \node (ARIMA) [startstop, below=1cm of opt2] {Use \textbf{\texttt{ARIMA()}} to automatically find the best ARIMA model for your time series.};
    \node (4) [startstop, below=1cm of 3] {4. Plot the ACF/PACF of the differenced data and try to determine possible candidate models.};
    \node (5) [startstop, below=1cm of 4] {5. Try your chose model(s) and use the AICc to search for a better model.};
    \node (6) [startstop, below=11cm of 1] {6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doung a portmanteau test of the residuals.};
    \node (decision) [decision, below=1cm of 6] {Do the residuals look like white noise?};
    \node (7) [startstop, below=1cm of decision] {7. Calculate forecasts.};

    \draw[arrow] (1) -- (2);
    \draw[arrow] (2) -- (opt2);
    \draw[arrow] (2) -- (opt1);
    \draw[arrow] (opt1) -- (3);
    \draw[arrow] (opt2) -- (ARIMA);
    \draw[arrow] (3) -- (4);
    \draw[arrow] (4) -- (5);
    \draw[arrow] (5) |- (6);
    \draw[arrow] (ARIMA) |- (6);
    \draw[arrow] (6) -- (decision);
    \draw[arrow] (decision.west) |- ++(-6cm, 0) coordinate (aux_point) |- (4.west); 
    \node[below left=0.2mm of decision.west] {no};
    \draw[arrow] (decision) node[below right=2mm of decision.south] {yes} -- (7);
    \end{tikzpicture}
    \caption{Pasos en metodología ARIMA} 
\end{figure}
\subsubsection{Predicciones}

Una vez validado el modelo, el siguiente paso y fin último de nuestro estudio es realizar predicciones para valores futuros de la variable. Para obtener esas previsiones haremos uso del modelo teórico que se ha identificado como adecuado, proyectándolo hacia el futuro.

Además, las predicciones sirven para contrastar la adecuación de nuestro modelo: las discrepancias sistemáticas entre predicciones y valores observados cuestionarán la validez de nuestro modelo.

Supongamos que hemos observado la serie hasta un instante $T$, y denotemos por  $\hat{x}_{T+h}$ a la predicción de la serie en el instante $T+h$ a partir del modelo ajustado. La forma de proceder será la siguiente:
 \begin{itemize}[label=\textbullet]
    \item La estimación del término (ruido) para instantes anteriores a $T$ se corresponde con el error de previsión a un periodo vista, es decir:  \[
    \hat{\varepsilon}_t=x_t-\hat{x}_t\quad t\le T
    \] 
    \item Los términos de ruido posteriores al instante $T$ se considerarán nulos:  \[
   \hat{\varepsilon}_t=0\quad t>T 
    \] 
    \item Los valores de la serie para instantes posteriores a $T$ que se requieran en el cálculo de nuevas predicciones, se sustituyen por sus propias predicciones:  \[
    x_{T+h}=\hat{x}_{T+h}\quad h=1,2,\dots
    \] donde el parámetro $h$ representa el  \textit{horizonte de predicción}. 
\end{itemize}
Veámos a modo de ejemplo cómo proceder a la hora de realizar predicciones con dos modelos concretos:
\begin{example}
   Consideremos el modelo ARIMA (1,1,0): \[
       (1-0.6B)(1-B)X_t=\varepsilon_t
   \]  
   Supongamos que conocemos el valor de la serie hasta un instante $t_0$ y pretendemos realizar previsiones más allá de ese instante. A partir del modelo teórico, tenemos que la expresión explícita del modelo de previsión será: \[
   \hat{x}_{t_0+h}=1.6\hat{x}_{t_{0}+h-1}-0.6\hat{x}_{t_0+h-2}+\hat{\varepsilon}_{t_0+h}-0.3\hat{\varepsilon}_{t_0+h-1}-0.3\hat{\varepsilon}_{t_0+h-2}\text{ para }h=1,2,3,\dots
   \] y por consiguiente: \[
   \begin{array}{rcl}
       \hat{x}_{t_0+1} & = & 1.6x_{t_0}-0.6x_{t_0-1}-0.5\hat{\varepsilon}_{t_0}-0.3\hat{\varepsilon}_{t_0-1}\\ 
       \hat{x}_{t_0+2} & = & 1.6\hat{x}_{t_0+1}-0.6x_{t_0}-0.3\hat{\varepsilon}_{t_0}\\ 
       \hat{x}_{t_0+3} & = & 1.6\hat{x}_{t_0+2}-0.6\hat{x}_{t_0+1}\\ 
        & & \vdots \\
       \hat{x}_{t_0+h} & = & 1.6\hat{x}_{t_0+h-1}-0.6\hat{x}_{t_0+h-2}.\\ 
   \end{array}
   \] 
   Observa que para obtener las predicciones $\hat{x}_{t_0+1}$ y $\hat{x}_{t_0+1}$ necesitamos $\hat{\varepsilon}_{t_0-1}$ y $\hat{\varepsilon}_{t_0}$.

   Dichos valores los podemos sacar de la fórmula recurrente: \[
   \hat{\varepsilon}_{t_0-h}=\hat{x}_{t_0-h}-1.6\hat{x}_{t_0-h-1}+0.6\hat{x}_{t_0-h-2}+0.5\hat{\varepsilon}_{t_0-h-1}+0.3\hat{\varepsilon}_{t_0-h-2}\text{ para }h=0,1,2,\dots
   \] la cual permite obtener, a partir de los valores observados de la serie, todos los residuos de los tiempos observados $\hat{\varepsilon}_{t_0},\hat{\varepsilon}_{t_0-1},\hat{\varepsilon}_{t_0-2}, \dots$
\end{example}
Como se observa, si una serie incorpora medias móviles, la influencia directa del ruido se transmite tantos periodos hacia adelante como orden del proceso $MA$. A partir de este instante, la influencia de estos términos es indirecta a través de los valores obtenidos.

 \textbf{Nota:} Las expresiones para obtener las predicciones puntuales permiten observar que, al aumentar el horizonte de predicción, se recurre a valores estimados en lugar de datos reales, lo que se traduce en mayores errores de predicción. La metodología ARIMA permite, además de obtener predicciones puntuales como hemos descrito arriba, obtener intervalos de predicción para instantes futuros. 
