\begin{center}
    \textbf{\Large Hoja 7: Modelos de series con variables exógenas} 
\end{center}
\begin{enumerate}[label=\color{red}\textbf{\arabic*)}]
    \item \lb{Indica la expresión del modelo de regresión lineal múltiple en el contexto de series temporales. ¿En qué se diferencia del modelo de regresión lineal tradicional?} 

        En el contexto de series temporales, el modelo de \textbf{regresión lineal múltiple} se escribe como: \[
        Y_t=\omega_0+\omega_1 x_{1,t}+\cdots+\omega_k\omega_{k,t}+\varepsilon_t,\quad \varepsilon\sim N(0,\vartheta^2)\text{ independientes, }
        \] donde $Y_t$ es la observación en el instante  $t$ y $x_{i,t}$ son varaibles explicativas (exógenas observadas en $t$).

        \textbf{Diferencia respecto a la regresión lineal "tradicional"}

        La diferencia clave no es la forma algebraica, sino la \textbf{interpretación temporal y el papel de la dependencia:}
        \begin{enumerate}[label=\arabic*.]
            \item \textbf{Orden temporal:} Aquí las observaciones están indexadas por $t$ y las explicativas pueden ser también series temporales $x_{i,t}$, dummies estacionales, tendencia, intervenciones, retardos, etc.
            \item \textbf{Hipótesis sobre errores:} En regresión "clásica" se asume típicamente independencia de los errores; en series temporales esa independencia \textbf{suele fallar} por autocorrelación. Puede haber buen ajuste (p.ej. $R^2$ alto) pero \textbf{residuos incorrelados}, lo que invalida el modelo como especificación final.
            \item \textbf{Consecuencia práctica:} Si los residuos no son "ruido blanco", el tema introduce la \textbf{regresión dinámica}, donde el término de error sigue un ARIMA/SARIMA en lugar de ser i.i.d.: \[
            Y_t=\omega_0+\omega_1x_{1,t}+\cdots+\omega_kx_{k,t}+\varsigma_t,\quad\text{ con }\varsigma_t \mathrm{ARIMA}.
            \] 
        \end{enumerate}
    \item \lb{¿Por qué es últil el tiempo como predictor en un modelo de regresión para series temporales?} 

        Es útil porque el \textbf{tiempo} $t$ actúa como una variable explicativa que \textbf{captura la tendencia} (cambios sistemáticos a largo plazo) de la serie. En un modelo de regresión para series temporales, incluir $t$ permite modelar explícitamente un crecimiento o decrecimiento medio a lo largo del tiempo (tendencia lineal) y, si procede, una curvatura mediante términos como $t^2$ (tendencia cuadrática), aunque no conviene usar potencias alta, en lugar usar tendencia lineal a trozos.
    \item \lb{¿Cómo se incluye la estacionalidad en un modelo de regresión para series temporales?}

        Hay \textbf{dos formas principales} de incorporar la estacionalidad en un modelo de regresión para series temporales:
        \begin{enumerate}[label=\arabic*.]
            \item \textbf{Variables indicadoras (dummies) estacionales}

                Si la estacionalidad tiene periodo $L$ (p.ej., $L=12$ meses,  $L=4$ trimestres), se introducen $L-1$  \textbf{dummies} para representar cada "estación" (mes, trimestre, cuatrimestre, etc.), dejando una como referencia para evitar multicolinealidad. El modelo queda, por ejemplo: \[
                Y_t=\omega_0+\sum_{i=1}^{k} \omega_ix_{i,t}+\sum_{j=1}^{L-1} \delta_jD_{j,t}+\varepsilon_t.
                \]  
            \item \textbf{Términos de Fourier (seno/coseno)}

                Especialmente útil cuando $L$ es grande, en lugar de muchas dummies se añaden pares seno/coseno:  \[
                x_{2k-1,t}=\sin\left( \dfrac{2k\pi t}{L} \right),\quad x_{2k,t}=\cos\left( \dfrac{2k\pi t}{L} \right),
                \] para $k=1,\dots,K$ (con $K\le \left\lfloor L/2 \right\rfloor$). Estos términos se incluyen como regresores adicionales en el modelo.
        \end{enumerate}
    \item \lb{¿Qué ventaja ofrecen las series de Fourier para modelar la estacionalidad en series temporales?}

        Las \textbf{series de Fourier} ofrecen la ventaja de que permiten modelar la estacionalidad \textbf{con pocos parámetros} mediante funciones seno y coseno, en lugar de necesitar muchas variables indicadoras. Esto es especialmente útil cuando el período estacional $L$ es grande, porque con $K$ armónicos (pares seno/coseno) se puede capturar la forma estacional de manera flexible y parsimoniosa.
    \item \lb{¿Qué papel desempeñan las variables de calendario en los modelos de regresión para series temporales? Proporciona un ejemplo.}

        Las \textbf{variables de calendario} actúan como \textbf{regresores exógenos deterministas} que incorporan en el modelo efectos sistemáticos ligados al calendario (patrones que se repiten por estructura del tiempo) y que no quedan bien explicados solo con tendencia y ruido. Se incluyen dentro de los predictores típicos mediante \textbf{variables indicadoras (dummies)}, por ejemplo para \textbf{día de la semana, mes/estación, festivos}, o cambios asociados a un periodo concreto del calendario.

        \textbf{Ejemplo:} Si $Y_t$ son ventas diarias, se puede introducir una dummy de  \textbf{festivo}: \[
        Y_t=\omega_0+\omega_1t+\delta F_t+\varepsilon_t,
        \] donde $F_t=1$ si el día  $t$ es festivo y  $F_t=0$ si no lo es. El coeficiente  $\delta$ mide el "salto" medio en ventas atribuible a los festivos, manteniendo el resto constante.
    \item \lb{¿Qué son las variables de "intervención" o de "eventos" y cómo se usan en los modelos de regresión para series temporales?}

        Las \textbf{variables de intervención} (o de \textbf{eventos}) son predictores que se introducen en un modelo de regresión para \textbf{capturar el efecto de un suceso concreto} (p.ej., una campaña, un cambio de normativa, una avería, una subida de precio, etc.) sobre la serie $Y_t$. Se plantean como  \textbf{variables indicadoras (dummies)} diseñadas para "encenderse" en el momento del evento, distinguiendo principalmente dos casos:
        \begin{enumerate}[label=\arabic*.]
            \item \textbf{Intervención tipo "step" (efecto permanente)} 

                Modelo un cambio de empieza en un instante $T_0$ y se mantiene: \[
                I_t=\begin{cases}
                    0, & t<T_0\\
                    1, & t\ge T_0
                \end{cases}
                \] 
                Se incluye como regresor para estimar un \textbf{cambio de nivel} a partir del evento.
            \item \textbf{Intervención tipo "spike" (efecto puntual)} 

                Modelo un impacto \textbf{solo en el instante} del evento: \[
                I_t=\begin{cases}
                    1, & t=T_0\\
                    0, & t\neq T_0
                \end{cases}
                \]  
                Se incluye para recoger un \textbf{atípico puntual} o un shock transitorio. 
        \end{enumerate}
        En ambos casos, el modelo de regresión queda del tipo: \[
        Y_t=\omega_0+\sum_{i=1}^{k} \omega_ix_{i,t}+\delta I_t+\varepsilon_t,
        \] donde $\delta$ cuantifica el \textbf{efecto medio} de la intervención (salto permanente si es "step", o impacto puntual si es "spike").
    \item \lb{¿Qué significa regresión espuria en el contexto de series temporales y a qué puede deberse?}

        La \textbf{regresión espuria} en series temporales significa que \textbf{parece} existir una relación estadísticamente significativa entre dos series (p.ej., coeficientes "significativos" y $R^2$ alto), pero esa relación es \textbf{falsa o engañosa:} no responde a una relación causal real, sino a que ambas series comparten \textbf{tendencia o patrones de largo plazo} similares.

        ¿A qué puede deberse?

        \begin{itemize}[label=\textbullet]
            \item \textbf{No estacionariedad de las series:} Cuando las series son no estacionarias, pueden mostrar \textbf{alta correlación} simplemente por sus \textbf{tendencias comunes}, generando la ilusión de dependencia entre ellas.
            \item \textbf{Señales típicas:} $R^2$ \textbf{elevado} y \textbf{alta autocorrelación de los residuos}.
            \item \textbf{Mitigación habitual:} si las series no son estacionarias, una solución común es \textbf{tomar diferencias} para transformarlas en estacionarias. 
        \end{itemize}
        Además, se advierte que, aunque una regresión espuria puede dar predicciones razonables \textbf{a muy corto plazo}, en general \textbf{no será adecuada} conforme avanzamos en el horizonte de predicción.
    \item \lb{Explica la diferencia entre una relación causal y una correlación en el contexto de series temporales.}
        \begin{itemize}[label=\textbullet]
            \item \textbf{Correlación (asociación):} dos series pueden mostrar una \textbf{relación significativa} (por ejemplo, alta correlación y una regresión con $R^2$ elevado) simplemente porque ambas están influenciadas por \textbf{tendencias o patrones de largo plazo similares}, especialmente si son \textbf{no estacionarias}. Esa asociación puede dar "apariencia" de relación aunque no exista vínculo real entre ellas.
            \item \textbf{Relación causal:} implicaría que existe una \textbf{relación causal directa} entre las variables, es decir, que la evolución de una está directamente conectada con la otra. Se subraya que en casos de regresión espuria \textbf{no hay} esa relación causal directa, aunque estadísticamente parezca que sí.
        \end{itemize}
        \textbf{Ejemplo:} Se compara una serie de \textbf{pasajeros en Australia} con otra de \textbf{producción de arroz en Guinea.} Parecen relacionadas porque comparten tendencias, pero "nada tienen que ver": es un caso típico en el que hay \textbf{correlación aparente} sin \textbf{causalidad} (regresión espuria).    
    \item \lb{¿Por qué es importante verificar la autocorrelación en los residuos de un modelo de regresión?}

        Verificar la \textbf{autocorrelación de los residuos} es esencial porque, en estos modelos, se asume que los errores \textbf{deben comportarse como "ruido blanco"} (incorrelados entre sí y con los predictores). Si los residuos están autocorrelados, significa que el modelo \textbf{no ha capturado toda la estructura determinista} de la serie (queda información temporal sin explicar en los errores), y por tanto el modelo de regresión no es una especificación adecuada como modelo final.

        En ese caso, los errores \textbf{"deberían modelizarse con ARIMA"}, pasando a \textbf{regresión dinámica}, precisamente para capturar esa estructura de correlación en los residuos y \textbf{mejorar la precisión predictiva.}

        Además, una \textbf{alta autocorrelación residual} es uno de los signos clásicos de \textbf{regresión espuria} en series temporales (aparente buen ajuste, pero relación engañosa), por lo que revisarla ayuda a evitar conclusiones incorrectas.
    \item \lb{¿Cómo se eligen las variables predictoras en un modelo de regresión para series temporales?}

        La elección de variables predictoras en regresión para series temporales se plantea como un \textbf{proceso en etapas:} proponer candidatos "razonables" para la serie y después \textbf{seleccionar y validar} el subconjunto final.
        \begin{enumerate}[label=\arabic*.]
            \item \textbf{Propuesta inicial de predictores (candidatos) a partir del análisis de la serie}

                Se parte de un análisis descriptivo y se consideran predictores "útiles" típicos en series temporales, muchos ligados al \textbf{efecto calendario}, por ejemplo: \textbf{tiempo} $t$ para tendencia, \textbf{dummies} (laborable/festivo, día de la semana, estacionales), \textbf{intervenciones} (step/spike), \textbf{número de días laborables, variables retardadas} (lags) y, para estacionalidades largas, \textbf{términos de Fourier}.
            \item \textbf{Selección del modelo (qué predictores se quedan)}

                Dos enfoques principales:
                \begin{itemize}[label=\textbullet]
                    \item \textbf{Métodos iterativos:} \textit{backward, forward} y \textit{stepwise}.
                    \item \textbf{Comparación por medias de bondad/criterios} para modelos con distinto número de parámetro: $R^2$ ajustado, validación cruzada (CV) y criterios \textbf{AIC, AICc y BIC}.
                \end{itemize}
                (Además, pueden proponerse modelos no lineales con \textbf{potencias} o \textbf{interacciones} si procede.)
            \item \textbf{Validación del modelo seleccionado (imprescindibles en series temporales)} 

                La validación se hace mediante el análisis de residuos comprobando \textbf{Normalidad, homocedasticidad e independencia,} y también estudiando \textbf{multicolinealidad} y \textbf{valores influyentes}. En series temporales, es crucial verificar si los residuos son "ruido blanco" (p.ej., con ACF/PACF); si están \textbf{autocorrelados}, el modelo de regresión no es suficiente y se debe pasar a \textbf{regresión dinámica} (errores ARIMA).
            \item \textbf{Si se una regresión dinámica: condición práctica sobre estacionariedad}

                Para estimar adecuadamente un modelo de regresión dinámica, el tema recomienda que todas las series que intervienen sean \textbf{estacionarias}, salvo las variables de calendario; si no lo son, se pueden tomar \textbf{diferencias} para lograrlo. 
        \end{enumerate}

    \item \lb{¿Cómo se puede evaluar la relevancia de un predictor en un modelo de regresión?}

        \begin{enumerate}[label=\arabic*.]
            \item \textbf{Contraste de significación del coeficiente}

                Se analiza si el coeficiente asociado al predictor es estadísticamente distinto de cero ($p$-valor del contraste sobre  $\omega_i$). Si no es significativo, el predicto aporta poca evidencia de explicar $Y_t$ una vez controlado el resto.
            \item  \textbf{Selección/comparación de modelos con y sin el predictor}

                Se compara el ajuste de modelos alternativos (incluyendo o excluyendo el predictor) usando medidas/criterios que el tema recomienda para comparar modelos con distinto número de parámetros: $R^2$ \textbf{ajustado, validación cruzada (CV)} y criterios  \textbf{AIC, AICc y BIC}. Si al incluir el predictor estos criterios mejoran (p.ej., baja AIC/BIC, sube $R^2$ ajustado, mejora CV), su contribución es relevante.
            \item \textbf{Diagnóstico de multicolinealidad e influencia}

                Revisar \textbf{multicolinealidad} (que puede hacer que un predictor "parezca" irrelevante por redundancia con otros) y \textbf{valores influyentes}, porque pueden distorsionar la significación y el efecto estimado del predictor.  
        \end{enumerate}
    \item \lb{Explica la diferencia entre predicciones ex-ante y ex-post, así como la utilidad de cada una de ellas.} 

        \textbf{Diferencia:}

        \begin{itemize}[label=\textbullet]
            \item \textbf{Predicciones ex-ante:} son las que se hacen "hacia el futuro" usando \textbf{solo la información disponible hasta el presente}. En un modelo con variables exógenas, esto implica que debemos \textbf{predecir primero los predictores} (variables exógenas) para poder predecir después la variable de interés $Y_t$. Por tanto, aparecen  \textbf{dos fuentes de incertidumbre:}
                \begin{enumerate}[label=(\roman*)]
                    \item La del propio modelo de $Y_t$.
                    \item La debida a la predicción de los predictores.
                \end{enumerate}
            \item \textbf{Predicciones ex-post:} se hacen usando \textbf{valores reales observados de los predictores} en el periodo futuro que se está "prediciendo". No es un escenario realista (asume predictores "perfectamente conocidos"), pero deja \textbf{una única fuente de incertidumbre:} la del modelo que explica $Y_t$ en función de esos predictores. 
        \end{itemize}
        \textbf{Utilidad de cada una:}
        \begin{itemize}[label=\textbullet]
            \item \textbf{Ex-ante (uso principal: pronóstico operativo realista):} reproduce lo que ocurrirá en la práctica, donde no conocemos los predictores futuros y tenemos que estimarlos (por ejemplo, consumo eléctrico futuro usando temperatura, que primero habría que pronosticar y obtener de una agencia). Es la evaluación "completa" del proceso de predicción.
            \item \textbf{Ex-post (uso principal: evaluar el modelo "aislado"):} sirve para medir el rendimiento del modelo de $Y_t$  \textbf{separando} la incertidumbre que proviene de tener que predecir los predictores. Es útil para comparar modelos de forma más "limpia" cuando queremos saber qué tal explica \textit{$Y_t$ dado  $x$} (asumiendo $x$ conocido). 
        \end{itemize}
        Como alternativa práctica cuando es difícil predecir los predictores, es recomendable usar \textbf{predictores retardados} (lags), de forma que las predicciones futuras de $Y$ puedan hacerse sin necesidad de pronosticar los predictores contemporáneos. 
    \item \lb{¿Cuándo es útil emplear un modelo con retardos de las variables predictoras?}

        Es útil emplear un \textbf{modelo con retardos (lags) de los predictores} principalmente en dos situaciones:
        \begin{enumerate}[label=\arabic*.]
            \item \textbf{Cuando resulta complicado predecir los predictores exógenos en el futuro (escenario ex-ante)}

                Si predecir los predictores futuros es difícil, puede usarse una alternativa: \textbf{emplear predictores retardados} para que las predicciones futuras de $Y$ se puedan hacer \textbf{sin necesidad de pronosticar} $x$. Para un horizonte  $h$, se plantea el modelo:  \[
                Y_{t+h}=\omega_0+\omega_1x_{1,t}+\cdots+\omega_kx_{k,t}+\varepsilon_t,
                \] lo que haceel modelo \textbf{operativo en un escenario realista.}
            \item \textbf{Cuando se espera un efecto retardado del predictor sobre la variable de intereś}

                Además, destacamos que esta formulación permite capturar \textbf{efectos con retraso} de algunas variables sobre $Y$. 
        \end{enumerate}
        \textbf{Ejemplo:} En consumo eléctrico horario, es común usar como predictores \textbf{consumos retardados} (1h, 2h, ..., 24h) y también puede considerarse la \textbf{temperatura con retardo} (por ejemplo, 1 hora).   
    \item \lb{Indica la expresión del modelo de regresión dinámica. ¿En qué se diferencia del modelo de regresión para series temporales?} 

        \textbf{Expresión del modelo de regresión dinámica}

        La \textbf{regresión dinámica} se expresa como una regresión con predictores exógenos cuyos \textbf{residuos no son ruido blanco,} sino que se modelizan con un ARIMA: \[
        Y_t=\omega_0+\omega_1x_{1,t}+\cdots+\omega_kx_{k,t}+\varsigma_t
        \] y los errores $\varsigma_t$ se describen mediante un  $ARIMA(p,d,q)$:  \[
        (1-a_1B-a_2B^2-\cdots-a_pB^p)(1-B)_{\varsigma_t}^d=(1+b_1B+b_2B^2+\cdots+b_qB^q)\varepsilon_t,\quad \varepsilon\longrightarrow N(0,\vartheta^2)\text{ independientes.}
        \] 
        \textbf{Diferencia respecto al modelo de regresión para series temporales (no dinámico)} 

        El \textbf{modelo de regresión múltiple para series temporales} (sin dinámica) tiene la misma parte determinista, pero \textbf{asume errores i.i.d.} (ruido blanco) directamente: \[
        Y_t=\omega_0+\omega_1x_{1,t}+\cdots+\omega_kx_{k,t}+\varepsilon_t,\quad \varepsilon_t\longrightarrow N(0,\vartheta^2) \text{ independientes.}
        \]  
        La diferencia esencial es, por tanto, \textbf{el tratamiento de errores:}
        \begin{itemize}[label=\textbullet]
            \item En \textbf{regresión "estándar" para series}, se supone que el término de error $\varepsilon_t$ es independiente (ruido blanco).
            \item En \textbf{regresión dinámica}, se reconoce que los residuos pueden estar \textbf{correlacionados en el tiempo} y se introduce una estructura ARIMA en $\varsigma_t$; además que el modelo contempla  \textbf{dos fuentes de error:} la perturbación $\varsigma_t$ del modelo de regresión y el ruido blanco $\varepsilon_t$ del ARIMA.
        \end{itemize}
    \item \lb{Para estimar los coeficientes del modelo de regresión dinámica, podemos hacerlo de dos formas diferentes. Forma 1: estimando todos los coeficientes de forma conjunta. Forma 2: estimando primero los coeficientes del modelo de regresión y luego los coeficientes del modelo ARIMA para los residuos. ¿Se obtienen los mismos resultados? En caso negativo, indicar ventajas e inconvenientes de cada forma.}

        \textbf{No, en general no se obtienen los mismos resultados.}

        En el modelo de regresión dinámica, la estimación adecuada de los coeficientes debe hacerse \textbf{minimizando la suma de cuadrados de los errores $\varepsilon_t$} (las "innovaciones" del ARIMA) o, alternativamente, por \textbf{máxima verosimilitud.} Si en su lugar se minimizan los errores $\varsigma_t$ (los residuos "brutos" del modelo de regresión), \textbf{la estimación de los coeficientes no sería apropiada y tampoco las inferencias sobre ellos.}

        Esto implica que el procedimiento "en dos etapas" ((1) ajustar regresión ignorando la estructura ARIMA y (2) ajustar ARIMA a los residuos) \textbf{no coincide} con el ajuste conjunto salvo casos particulares.
        \begin{enumerate}[label=\textbf{Forma \arabic*:}, leftmargin=2cm]
            \item \textbf{estimación conjunta (regresión + ARIMA a la vez)}

                \textbf{Qué hace:} ajusta simultaneamente $\omega$ y los parámetros ARIMA buscando que las innovaciones $\varepsilon_t$ cumplan el modelo (min SSE de $\varepsilon_t$ o MV).

                \textbf{Ventajas}
                \begin{itemize}[label=\textbullet]
                    \item \textbf{Coherente con el modelo:} optimiza el criterio correcto (sobre $\varepsilon_t$), por lo que la estimación e inferencia son las apropiadas según el tema.
                    \item \textbf{Mejor base de predicción:} integra la parte de regresión y la parte ARIMA de los errores.
                \end{itemize}
                \textbf{Inconvenientes}
                \begin{itemize}[label=\textbullet]
                    \item \textbf{Más exigentes/modelización más delicada:} para estimar adecuadamente, conviene que las series (salvo calendario) sean \textbf{estacionarias}, lo que puede requerir transformaciones/diferencias previas.
                \end{itemize}
            \item \textbf{dos etapas (regresión primero, ARIMA después sobre residuos)} 

                \textbf{Qué hace:} estima $\omega$ como si el error fuese ruido blanco (minimizando $\varsigma_t$ de la regresión) y luego intenta explicar la autocorrelación restante con una ARIMA sobre esos residuos.

                \textbf{Ventajas}
                \begin{itemize}[label=\textbullet]
                    \item \textbf{Simplicidad y diagnóstico:} es últil como procedimiento exploratorio para detectar autocorrelación en residuos (ACF/PACF) y motivar la necesidad de una regresión dinámica.
                \end{itemize}
                \textbf{Inconvenientes} 
                \begin{itemize}[label=\textbullet]
                    \item \textbf{No optimiza el criterio correcto del modelo dinámico:} al no minimizar $\varepsilon_t$, puede dar \textbf{coeficientes distintos} y \textbf{estimaciones e inferencias no apropiadas} si se toma como estimación final.  
                \end{itemize}
                En resumen: la \textbf{estimación conjunta} es la que está alineada con el planteamiento del tema (minimizar $\varepsilon_t$ / MV) y la de \textbf{dos etapas} puede servir como aproximación o diagnóstico, pero \textbf{no garantiza} los mismos coeficientes ni inferencias válidas para el modelo dinámico. 
        \end{enumerate}
\end{enumerate}
