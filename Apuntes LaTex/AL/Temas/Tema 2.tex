\section{Vectores, matrices y tensores}
\subsection{Vectores}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Sean $n\in\mathbb{N}$ y $\mathbb{K}=\R$ o $\mathbb{C}$. Definimos $\mathbb{K}^n=\{(x_1,x_2,\dots,x_n):x_i\in\mathbb{K},1\le i\le n\}$.

A los elementos $u\in\mathbb{K}^n$ se les llama vectores de $n$-componentes. En $\mathbb{K}^n$ definimos las dos siguientes operaciones:
\begin{itemize}
	\item Suma: Si $u=(x_1,x_2,\dots,x_n),~v=(y_,y_2,\dots,y_n)\in\mathbb{K}^n$ \[ u+v=(x_1,\dots,x_n)+(y_1,\dots,y_n)=(x_1+y_1,,x_2+y_2,\dots,x_n+y_n) \]
	\item Producto por escalares: Sean $u=(x_1,\dots,x_n)\in\mathbb{K}^n$ y $\lambda\in\mathbb{K}$ \[ \lambda\cdot u=\lambda\cdot(x_1,\dots,x_n)=(\lambda x_1,\lambda x_2,\dots,\lambda x_n) \]
\end{itemize}
El conjunto $\mathbb{K}^n$ dotado de las dos operaciones anteriores se dice que es un espacio vectorial sobre $\mathbb{K}$.

\begin{center}
	\begin{tikzpicture}[baseline=(current bounding box.center), scale=1.5]
		\draw (-1,0 )-- (2,0);
		\draw (0,-1) -- (0,2);
		\draw[lightblue, dashed] (1,0) node[below] {$x_1$} -- (1,1)  node[right] {$u=(x_1,x_2)$}-- (0,1) node[left] {$x_2$};
		\draw[-latex, lightblue, line width=1.5pt] (0,0) -- (1,1);
		\node at (1.5,1.5) {$\mathbb{R}^2$};
	\end{tikzpicture}
	\qquad\qquad
	\begin{tikzpicture}[scale=2, baseline=(current bounding box.center)]
		\tdplotsetmaincoords{60}{110}  % Mover aquí para corregir el error
		\draw[thick,-latex] (0,0,0) -- (1,0,0) node[right]{$x_2$};
		\draw[thick,-latex] (0,0,0) -- (0,1,0) node[above]{$x_3$};
		\draw[thick,-latex] (0,0,0) -- (0,0,1) node[below]{$x_1$};
		\draw[lightblue, dashed] (0,0,0) -- (0.5,-0.2,0) -- (0.7,0,0)  -- (0.7,0.5, 0) -- (0,0.5,0);
		\draw[lightblue, dashed] (0.7,.5,0) -- (0.5,-0.2,0);
		\draw[lightblue, -latex] (0,0,0) -- (0.7,0.5,0);
	\end{tikzpicture}
\end{center}

\subsubsection{Dependencia linea}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Combinación lineal)
\end{itemize}
Una combinación lineal de los vectores $u_1,u_2,\dots,u_m \in\mathbb{K}^n$ es un vector que se escribe como \[ \lambda_1u_1+\lambda_2u_2+\cdots+\lambda_mu_m, \] con $\lambda_1,\lambda_2,\dots,\lambda_m$ escalares llamados \textcolor{lightblue}{coeficientes de la combinación lineal}

\Ej
\[ \begin{array}{l}
	\underset{\lambda_1}{1}\cdot\underbrace{(-1,0)}_{u_1}+\underset{\lambda_2}{2}\cdot\underbrace{(1,2)}_{u_2}=\underbrace{(1,4)}_{u} \\
	u=\lambda_1\cdot u_1+\lambda_2\cdot u_2\quad u \text{ es combinación lineal de $u_1$ y $u_2$}
\end{array}\]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Independencia lineal)
\end{itemize}
Se dice que los vectores $u_1,u_2,\dots,u_m\in\mathbb{K}^n$ son \textcolor{lightblue}{linealmente independientes} si una combinación lineal suya nula sólo puede ocurrir cuando todos los coeficientes son nulos, es decir, si $\lambda_1u_1+\lambda_2u_2+\cdots+\lambda_mu_m=0\longrightarrow\lambda_1=\lambda_2=\cdots=\lambda_m=0$.

En caso contrario, se dice que $u_1,\dots,u_m$ son \textcolor{lightblue}{linealmente dependientes}, es decir,  $0=(0,0,\dots,0)$ vector nulo.

\Ej

\begin{wrapfigure}[3]{r}{0.4\textwidth}
	\begin{tikzpicture}[baseline=(current bounding box.center), scale=2]
		\draw (-1.5,0) -- (1.5,0);
		\draw (0,-0.5)-- (0,1.5);
		\draw[lightblue, -latex, line width=1.2] (0,0) -- (1,1) node[right] {$u_1$};
		\draw[lightblue, -latex, line width=1.2] (0,0) -- (-1,1) node[left] {$u_2$};
		\draw[lightblue, dashed] (1,0) -- (1,1) -- (-1,1);
	\end{tikzpicture}
\end{wrapfigure}

$u_1=(1,1),\:u_2=(-1,1)$

¿Son linealmente independientes?

$\begin{array}{r}
	\lambda_1\cdot(1,1)+\lambda_2\cdot(-1,1)=(0,0)\\
	(\lambda_1,\lambda_1)+(-\lambda_2,\lambda_2)=(0,0)\\
	(\lambda_1-\lambda_2,\lambda_1+\lambda_2)=(0,0)\\
	\begin{rcases}
		\lambda_1-\lambda_2=0\\
		\lambda_1+\lambda_2=0
	\end{rcases}\longrightarrow\begin{array}{l}
	\lambda_1=\lambda_2\longrightarrow\lambda_2=0\\
	2\lambda_1=0\longrightarrow\lambda_1=0
	\end{array}
\end{array}$

Son linealmente independientes

\begin{wrapfigure}[3]{r}{0.3\textwidth}
	\begin{tikzpicture}[scale=1.5]
	\draw (-1,0) -- (2,0);
	\draw (0,-1) -- (0,2);
	\draw[lightblue,-latex,line width=1.5] (0,0) -- (2,2) node[right] {$u_2$};
	\draw[blue,-latex,line width=1.5] (0,0) -- (1,1) node[right] {$u_1$};
	\end{tikzpicture}
\end{wrapfigure}

$u_1=(1,1),\,u_2=(2,2)$

¿Son linealmente independientes?

$ \begin{array}{r}
	\lambda_1\cdot(1,1)+\lambda_2\cdot(2,2)=(0,0)\\
	(\lambda_1+2\lambda_2,\lambda_1+2\lambda_2)=(0,0)\\
	\begin{rcases}
		\lambda_1+2\lambda_2=0\\
		\lambda_1+2\lambda_2=0
	\end{rcases}\longrightarrow\begin{array}{l}
	\lambda_1=-2\\
	\lambda_2=1
	\end{array}
\end{array}$

Proporciona una combinación lineal nula con coeficientes no nulos.

Son linealmente dependientes.

\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Sean $S=\{u_1,u_2,\dots,u_m\}$ un conjunto linealmente independiente de vectores y $u\in\mathbb{K}^n$. Entonces:
\begin{enumerate}[label=\arabic*)]
	\item Todo subconjunto no vacío de $S$ es linealmente independiente.
	\item El conjunto $S\cup\{u\}$ es linealmente independiente si y sólo si $u$ no es combinación lineal de $u_1,\dots,u_m$.
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
\begin{enumerate}[label=\arabic*)]
	\item Trivial
	\item Si  $S\cup\{u\}$ es linealmente independiente y $u=\lambda_1u_1+\cdots+\lambda_mu_m$, entonces \[ \lambda_1+\cdots+\lambda_mu_m-1\cdot u=0 \] es una combinación lineal nula con al menos un coeficiente no nulo. Por tanto, $S\cup\{u\}$ no es linealmente independiente (contradicción).
\end{enumerate}
Recíprocamente, si $S\cup\{u\}$ es linealmente dependiente, existen escalones $r,r_1,\dots,r_m$, no todos nulos tales que $ru+r_1u_1+\cdots+r_mu_m=0$. 

$r$ no puede ser cero pues si lo fuera entonces \[ r_1u_1+r_2u_2+\cdots+r_mu_m=0 \] y así $r_1=r_2=\cdots=r_m=0$.

Por tanto, $u=-r^{-1}r_1u_1-\cdots-r^{-1}r_mu_m$ y así $u$ es combinación lineal de $u_1,\dots,u_m$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Base canónica de $\mathbb{K}^n$)
\end{itemize}
Se llama base canónica de $\mathbb{K}^n$ al conjunto de vectores \[ e_1=(1,0,\dots,0),\:e_2=(0,1,0,\dots,0),\,\dots,\:e_n=(0,\,\dots,0,1) \] Nótese que todo vector $u=(x_1,x_2,\dots,x_n)\in\mathbb{K}^n$ se puede expresar como combinación lineal de $\{e_1,e_2,\dots,e_n\}$. En efecto: \[ u=(x_1,x_2,\dots,x_n)=x_1\cdot\underbrace{(1,0,\dots,0)}_{e_1}+x_2\underbrace{(0,1,0,\dots,0)}_{e_2}+\cdots+x_n\underbrace{(0,\dots,0,1)}_{e_n}. \]

\subsubsection{Normas de vectores}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Sea $u=(x_1,x_2,\dots,x_n)\in\mathbb{K}^n$. Entonces se definen:
\begin{enumerate}[label=\arabic*)]
	\item $\|u_1\|_p=(|x_1|p+|x_2|p+\cdots+|x_n|p)^{\frac{1}{p}},\quad p\ge1$
	\begin{itemize}
		\item $p=1$
		\item $p=2$ norma euclídea
	\end{itemize}
	\item $\|u\|_\infty=\max\{|x_1|,|x_2|,\dots,|x_n|\}$ norma del máximo.
\end{enumerate}
\Ej

$\begin{array}{l}
	u=(1,-2,-1)\\
	\|u\|_1=|1|+|-2|+|-1|=4\\
	\|u_2\|=\sqrt{|1|^2+|-2|^2+|1|^2}=\sqrt{6}\\
	\|u\|_\infty=\max\{|1|,|-2|,|1|\}=2
\end{array}$
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Sean $\|\cdot\|$ cualquiera de las normas anteriores, $u\in\R^n$ y $\alpha\in\R$. Entonces:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $\|u\|\ge0$ y $\|u\|=0\longleftrightarrow u=0$.
	\item $\|\alpha u\|=|\alpha|\cdot\|u\|$.
	\item $\|u+v\|\le\|u\|+\|v\|$ (desigualdad triangular)
\end{enumerate}
\begin{center}
	\begin{tikzpicture}
		\draw (-1,0) -- (3,0);
		\draw (0,-1) -- (0,3);
		\draw[blue, -latex, line width=1.5] (0,0) -- (3,3) node[right] {$u+v$};
		\draw[lightblue, -latex, line width=1.5pt] (0,0) -- (2,1) node[right] {$u$};
		\draw[lightblue, -latex, line width=1.5pt] (2,1) -- (3,3) node[midway,right] {$v$};
	\end{tikzpicture}
\end{center}
\subsubsection{Producto escalar}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Dado $u=(x_1,x_2,\dots,x_n),\: v=(y_1,y_2,\dots,y_n)\in\R^n$, se define el \textcolor{lightblue}{producto escalar euclídeo} de $u$ por $v$ como \[ u\cdot v=(x_1,x_2,\dots,x_n)\cdot(y_1,y_2,\dots,y_n)=x_1y_1+x_2y_2+\cdots+x_ny_n \] Nótese que $\|u\|_2=\sqrt{u\cdot u}$
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición (Propiedades del producto escalar)
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $u\cdot v=v\cdot u\qquad\forall u,v\in\R^n$
	\item $u\cdot(v_1+v_2)=u\cdot v_1+u\cdot v_2\qquad \forall u,\, v_1,\, v_2\in\mathbb{K}^n$.
	\item $(\alpha\, u)\cdot v=u\cdot(\alpha\, v)\qquad\forall\alpha\in\mathbb{K},\:\forall u,\, v\in\mathbb{K}^n$
	\item $u\cdot u\ge0$ y $u\cdot u=0\longleftrightarrow u=0$.
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición (Propiedades de la norma)
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $\|u+v\|^2=\|u\|^2+\|v\|^2+2(u\cdot v)$
	\item $\|u-v\|^2=\|u\|^2+\|v\|^2-2(u\cdot v)$
	\item $|u\cdot v|\le\|u\|\cdot\|v\|$ (desigualdad de Cauchy-Shwarz).
\end{enumerate}
De la propiedad (3) se tiene que si $u,v\neq0$, entonces \[ \begin{array}{c}
	\dfrac{|u\cdot v|}{\|u\|\cdot\|v\|}\le1; \\
	-1\le\dfrac{u\cdot v}{\|u\|\cdot\|v\|}\le1.
\end{array}\]
Por tanto, existe un ángulo $0\le\varphi\le\pi$ tal que \[ \cos(\varphi)=\dfrac{u\cdot v}{\|v\|\cdot\|u\|}. \]
A este ángulo se le llama ángulo formado por los vectores $u,v$. Escribimos $\cos(u,v)$. De aquí se tiene: \[ u\cdot v=\|u\|\cdot\|v\|\cos(u,v). \]

\Ej

\begin{wrapfigure}{r}{0.4\textwidth}
	\begin{tikzpicture}
		\draw (-2,0) -- (2,0);
		\draw (0,-1) -- (0,2);
		\draw[lightblue, -latex, line width=1.5pt] (0,0) -- ({- sqrt(3)}, 1) node[left] {$v$};
		\draw[lightblue, -latex, line width=1.5pt] (0,0) -- ({ sqrt(3)}, 1) node[right] {$u$};
		\draw[lightblue, dashed] ({-sqrt(3)}, 0) -- ({- sqrt(3)}, 1) -- ({ sqrt(3)}, 1) -- ({sqrt(3)},0);
	\end{tikzpicture}
\end{wrapfigure}

$u=(\sqrt{3},1),\: v=(-\sqrt{3},1)$

$\begin{aligned}
	\cos(u,v) &=\dfrac{u\cdot v}{\|u\|\cdot\|v\|}=\dfrac{(\sqrt{3},1)\cdot(-\sqrt{3},1)}{\sqrt{(\sqrt{3})^2+(1)^2}\cdot\sqrt{(-3)^2+1^2}}\\
	 &=\dfrac{-3+1}{2\cdot2}=-\dfrac{1}{2}\longrightarrow\varphi=\dfrac{2\pi}{3}\text{ radianes.}
\end{aligned}$

\subsubsection{Ortogonalidad}

\begin{itemize}[leftmargin=*]
	\item Se dice que dos vectores $u,v\in\R^n$ son ortogonales so $u\cdot v=0$.
\end{itemize}
Por tanto, por la proposición anterior se tiene que si $u$ y $v$ son ortogonales, entonces \[ \|u\pm v\|^2=\|u\|^2+\|v\|^2\qquad\textcolor{lightblue}{\text{(Teorema de Pitágoras)}} \]
\begin{center}
	\begin{tikzpicture}[scale=2]
		\draw[-latex] (-0.5,0) -- (1.5,0);
		\draw[-latex] (0,-0.5) -- (0,1.5);
		\draw[-latex, lightblue, line width=1.5pt] (0,0) -- (0,1)
		node[left] {$v$};
		\draw[-latex, lightblue, line width=1.5pt] (0,0) -- (1,0)
		node[below] {$u$};
		\draw[-latex, lightblue, line width=1.5pt] (0,0) -- (1,1)
		node[right] {$u+v$};
		\draw[lightblue, dashed] (1,1) -- (1,0);
	\end{tikzpicture}
\end{center}

\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Un conjunto de vectores $\{u_1,u_2,\dots,u_m\}$ se dice ortogonal si $u_i\cdot u_j=0$ si $i\neq j$. Si además los vectores son unitarios $(\|u_i\|=1,\le i\le m)$, entonces el conjunto se dice ortogonal.

\Ej

La base canónica de $\R^n\: \{e_1,e_2,\dots,e_n\}$ es un conjunto ortonormal.
\begin{enumerate}[label=\arabic*)]
	\item Proposición
\end{enumerate}
Si $\{u_1,\dots,u_m\}$ es ortogonal, entonces es un conjunto linealmente independiente.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
\[ \lambda_1u_1+\lambda_2u_2+\cdots+\lambda_mu_m=0 \]
Si multiplicamos por $u_j$:\[ \cancelto{0}{\lambda_1u-1\cdot u_j}+\cdots+\cancelto{\lambda_j\|u_j\|^2}{\lambda_ju_j\cdot u_j}+\cdots+\cancelto{0}{\lambda_mu_m\cdot u_j}=0\longrightarrow\lambda_j=0\]
\subsection{Matrices}
\begin{itemize}[leftmargin=*]
	\item Una matriz es un conjunto de números reales o complejos ordenados por filas (o columnas). 
\end{itemize}
Denotaremos por $M_{m\times n}(\mathbb{K})$ al conjunto de todas las matrices de $m$-filas y $n$-columnas.

Si $m=n$ la matriz se dice cuadrada.

\Ej

{
	\renewcommand{\arraystretch}{1.5}
	\setlength{\arraycolsep}{0.5cm}
	\[ H_4 = \begin{bmatrix}
	1 & \frac{1}{2} & \frac{1}{3} & \frac{1}{4} \\
	\frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \frac{1}{5} \\
	\frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \frac{1}{6} \\
	\frac{1}{4} & \frac{1}{5} & \frac{1}{6} & \frac{1}{7}
\end{bmatrix}\qquad\text{Matriz de Hilbert} \]}
Algunos tipos de matrices destacados:
\begin{itemize}
	\item Diagonal: 
	
	$\begin{aligned}
		D & =(d_{ij}),\qquad d_{ij}=0\text{ si }i\neq j\\
		& =\mathrm{diag}[a,b,c,\dots]\\
		& =\left[\begin{tabular}{cccc}
			a                    &                      & & 0 \\
			& b                    & \multicolumn{2}{c}{}                        \\
			& & c                  &                        \\
			0 &                        &                    & $\ddots $                
		\end{tabular}\right]
	\end{aligned}$
	\item Si $a=b=c\dots$, la matriz se llama escalar.
	\item Si $a=b=c\dots,=1$, la matriz se llama identidad y se denota $I$.
	\item Matriz triangular superior (inferior) si $a_{ij}=0\qquad i>j\quad(i<j)$
	
	$\begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		& a_{22} & \cdots & a_{2n} \\
		&        & \ddots &        \\
		0      &        &        & a_{mn}
	\end{pmatrix}\longleftarrow$ Triangular superior
	
	$\begin{pmatrix}
		a_{11} &  &  & 0 \\
		a_{21}& a_{22} &  &  \\
		\vdots&  \vdots      & \ddots &        \\
		a_{m1} &   a_{m2}     &        & a_{mn}
	\end{pmatrix}\longleftarrow$ Triangular inferior
\end{itemize}
\subsubsection{Operaciones con matrices}
\begin{itemize}[label=\color{lightblue}\textbullet, leftmargin=*]
	\item Suma: 
	
	$\begin{array}{ll}
		A=(a_{ij}) & B=(b_{ij})\\
		A+B=C & c_{ij}=a_{ij}+b_{ij}\\
		A=\begin{bmatrix}
			3 & -2\\
			0 & 6
		\end{bmatrix} & B=\begin{bmatrix}
		9 & 0\\
		0 & -1
		\end{bmatrix}\\	
		\multicolumn{2}{c}{A+B=\begin{bmatrix}
				12 & -2\\
				0 & 5
		\end{bmatrix}} 
	\end{array}$

	\item Producto por escalares:  $\lambda A=(\lambda a_{ij}),\quad\lambda=j,\quad\lambda A=\begin{bmatrix}
	3j & -2j\\
	0 & 6j
	\end{bmatrix}$
	\item Producto de matrices:
	
		$\begin{array}{ll}
		A=(a_{ij}) & B=(b_{ij})\longrightarrow A\cdot B\in M_{m\times n}\\
		\rotatebox[origin=c]{-90}{$\in$} & \rotatebox[origin=c]{-90}{$\in$}\\
		M_{m\times p} & M_{p\times n}
	\end{array}$
	\begin{itemize}
		\item Si denotamos $C=A\cdot B$
		
		$c_{ij}=\sum_{k=1}^{p}a_{ik}b_{kj}$
	\end{itemize}
	\Ej
	
	$\begin{array}{ll}
		A=\begin{bmatrix}
			1 & 1 & 1
		\end{bmatrix}_{1\times 3} & B=\begin{bmatrix}
		1\\
		0\\
		1
		\end{bmatrix}_{3\times1}
	\end{array}$
	
	$\begin{array}{l}
		A\cdot B=[2]\\
		B\cdot A=\begin{bmatrix}
			1\\
			0\\
			1
		\end{bmatrix}\cdot\begin{bmatrix}
		1 & 1 & 1
		\end{bmatrix}=\begin{bmatrix}
		1 & 1 & 1\\
		0 &0 & 0\\
		1 & 1 & 1
		\end{bmatrix}
	\end{array}$
	
	El producto de matrices \textcolor{lightblue}{no es conmutativo}.
\end{itemize}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Propiedades
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $(A\cdot B)\cdot C=A\cdot(B\cdot C)$
	\item $A\cdot 0=0\cdot A=0$
	\item $A\cdot(B+C)=A\cdot B+A\cdot C$
	\item $\alpha(A\cdot B)=(\alpha A)\cdot B$
	\item $A\cdot I=I\cdot A=A$
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Matriz traspuesta y matriz simétrica)
\end{itemize}
Sea $A=(a_{ij})\in M_{m\times n}$. Se llama transpuesta de $A$, denotado $A^T$, a la matriz \[ A^T=(b_{ij})\in M_{n\times m}\text{ con }b_{ij}=a_{ji} \]
\Ej
\[A=\begin{bmatrix}
	1 & 2 & 3 \\
	4 & 5 & 6
\end{bmatrix}_{2\times 3}\qquad A^T=\begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{bmatrix}_{3\times 2} \]
\begin{itemize}[leftmargin=*]
	\item Una matriz cuadrada se dice simétrica si $A=A^T$.
\end{itemize}
\[ A=\begin{bmatrix}
	1 & 2\\
	2 & 3
\end{bmatrix}\qquad A^T=\begin{bmatrix}
1 & 2\\
2 & 3
\end{bmatrix} \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Propiedades de la traspuesta
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $(A^T)^T=A$
	\item $(A+B)^T=A^T+B^T$
	\item $(\alpha A)^T=\alpha A^T$
	\item $(A\, B)^T=B^T\,A^T$
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición 
\end{itemize}
Sea $A\in M_n(\mathbb{K})$. Se dice que $A$ es invertible si existe $B\in M_n(\mathbb{K})$ tal que \[ A\cdot B=B\cdot A=I. \] A la matriz $B$ se le llama inversa de $A$ y se denota $B=A^{-1}$.

\bu{Propiedades}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item Si $A$ y $B$ son invertibles, entonces $A\cdot B$ es invertible y $(A\, B)^{-1}=B^{-1}A^{-1}$.
	\item Si $A$ es invertible, entonces $A^T$ es invertible y \[ (A^T)^{-1}=(A^{-1})^T. \]
\end{enumerate}
\subsubsection{Operaciones con matrices por bloques}
\begin{itemize}[label=\color{lightblue}$-$, leftmargin=*]
	\item Notación \[ \underset{\begin{array}{cccc}
			v_1~ & v_2 & ~~~ & ~~v_m
	\end{array}}{\left[\begin{array}{cccc}
	a_{11} & a_{12} & \cdots & a_{1m}\\
	a_{21} & a_{22} & \cdots & a_{2m}\\
	\cdots & \cdots & \cdots & \cdots\\
	a_{n1} & a_{n2} & \cdots & a_{nm}\\
	\end{array}\right]}\begin{array}{c}
	u_1^T\\
	u_2^T\\
	\\
	u_n^T
\end{array}=\begin{bmatrix}
v_1 & v_2 & \cdots & v_m
\end{bmatrix}=\begin{bmatrix}
u_1^T\\
\vdots\\
u_n^T
\end{bmatrix}\]
	
	\item Producto como filas por columnas
	\[ \begin{array}{l}
		A= \begin{bmatrix}
		u_1^T\\
		\vdots\\
		u_n^T
	\end{bmatrix}\qquad B=\begin{bmatrix}
	v_1 & v_2 & \cdots & v_m
	\end{bmatrix}\\
	A\cdot B=\begin{bmatrix}
		u_1^T\\
		\vdots\\
		u_n^T
	\end{bmatrix}\cdot\begin{bmatrix}
	v_1 & v_2 & \cdots & v_m
	\end{bmatrix}=\underbrace{u_i^T\cdot v_j}_{\text{producto escalar}}
	\end{array}\]
	\item Producto matriz por columna
	\[ B=[v_1,\dots,v_m]_{n\times m}\qquad X=\begin{bmatrix}
		\alpha_1\\
		\vdots\\
		a_{m}
	\end{bmatrix}_{m\times 1} \]
	\[ \begin{aligned}
		B\cdot X & =\underbrace{
				 \begin{bmatrix}
				 	v_1 & v_2 & \cdots & v_m\\
				 \end{bmatrix}
				 \cdot\begin{bmatrix}
			\alpha_1\\
			\vdots\\
			\alpha_m
		\end{bmatrix}}_{n\times 1}\\
	& =\alpha v_1+\cdots+\alpha_mv_m
	\end{aligned} \]
	\[ \begin{bmatrix}
		-1 & 1\\
		2 & 3
	\end{bmatrix}_{2\times2}\cdot\begin{bmatrix}
	a\\
	b
	\end{bmatrix}_{2\times1}=\begin{bmatrix}
	-1\cdot a+1\cdot b\\
	2\cdot a+3\cdot b
	\end{bmatrix}=a\underbrace{\begin{bmatrix}
		-1\\
		2
		\end{bmatrix}}_{v_1}+b\underbrace{\begin{bmatrix}
		1\\
		3
	\end{bmatrix}}_{v_2} \]

El producto por la derecha de una matriz $A$ por una columna $X$ es una combinación lineal de las columnas de $A$ con los elementos de $X$ como coeficientes. 
\item Producto de fila por matriz
\[ \begin{aligned}
	\begin{bmatrix}
		\alpha_1 & \cdots & \alpha_m
	\end{bmatrix}\cdot\begin{bmatrix}
	u_1^T\\
	\vdots\\
	u_m^T
	\end{bmatrix} & =\alpha_1u_1+\cdots+\alpha_mu_m^T\\
	\begin{bmatrix}
		a & b
	\end{bmatrix}_{1\times2}\cdot\begin{bmatrix}
	-1 & 1\\
	2 & 3
	\end{bmatrix}_{2\times2} & =[-1\cdot a+2\cdot b,\: 1\cdot a+3\cdot b]\\
	 & =a\cdot\underbrace{\begin{bmatrix}
	 		-1 & 1
	 \end{bmatrix}}_{u_1^T}+b\cdot\underbrace{\begin{bmatrix}
	 2 & 3
 \end{bmatrix}}_{u_2^T}
\end{aligned} \]
El producto por la izquierda de una fila $X$ por una matriz $A$ es una combinación lineal de las filas de $A$ con los elementos de $X$ como coeficientes.
\item Producto como suma de matrices de rango 1

Cuando multiplicamos una columna por una fila \[ \begin{bmatrix}
	a_1\\
	\vdots\\
	a_m
\end{bmatrix}_{m\times1}\cdot\begin{bmatrix}
1 & \cdots & b_p
\end{bmatrix}_{1\times p}=\begin{bmatrix}
a_1b_1 & a_1b_2 & \cdots & a_1b_p \\
a_2b_1 & a_2b_2 & \cdots & a_2b_p \\ \hdashline
a_mb_1 & a_mb_2 & \cdots & a_mb_p
\end{bmatrix}_{m\times p} \] obtenemos una matriz cuyas filas son proporcionales, es lo que llamaremos una matriz de rango 1.
\end{itemize}
Por tanto, el producto matricial \[ A\cdot B=[v_1,\dots,v_n]_{n\times m}\cdot\begin{bmatrix}
	u_1^T\\
	\vdots\\
	a_n^T
\end{bmatrix}_{m\times n}=\underbrace{v_1u_1^T}_{\text{rango }1}+\cdots+\underbrace{v_nu_n^T}_{\text{rango }n} \] se puede expresar como suma de matrices de rango 1.
\[\begin{aligned}
	\begin{bmatrix}
	1 & 4 \\
	2 & 5 \\
	3 & 6
\end{bmatrix}_{3\times2}\cdot\begin{bmatrix}
7 & 8 & 9 \\
10 & 11 & 12
\end{bmatrix} & =\begin{bmatrix}
1\\
2\\
3
\end{bmatrix}\cdot\begin{bmatrix}
 7 & 8 & 9
\end{bmatrix}+\begin{bmatrix}
4\\
5\\
6
\end{bmatrix}\cdot\begin{bmatrix}
10 & 11 & 12
\end{bmatrix}\\
& =\begin{bmatrix}
	7 & 8 & 9 \\
	14 & 16 & 18 \\
	21 & 24 & 27
\end{bmatrix}+\begin{bmatrix}
40 & 44 & 48 \\
50 & 55 & 60 \\
60 & 66 & 72
\end{bmatrix}\\
&=\begin{bmatrix}
	47 & 52 & 57 \\
	64 & 71 & 78 \\
	81 & 90 & 99
\end{bmatrix}
\end{aligned}\]
\subsubsection{Matrices ortogonales}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Una matriz cuadrada $Q$ se dice ortogonal si es no singular y su inversa es su traspuesta, es decir, $Q^T\cdot Q=Q\cdot Q^T=I$.

Si $Q=[u_1,\dots,u_m]$

$Q^T\cdot Q=\begin{bmatrix}
	u_1^T\\
	\vdots\\
	u_n^T
\end{bmatrix}\cdots\begin{bmatrix}
u_1 & \cdots & u_n
\end{bmatrix}=\begin{bmatrix}
u_1^Tu_1 & \cdots & u_1^Tu_n \\
&  &  \\
u_n^Tu_1 & \cdots & u_n^Tu_n
\end{bmatrix}=\begin{bmatrix}
1 &  & 0 \\
& \ddots &  \\
0 &  & 1
\end{bmatrix}$

Por tanto, una matriz es ortogonal si y sólo si sus filas (columnas) forman un conjunto ortonormal de vectores
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Sean $P,\:Q$ ortogonales. Entonces:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $P\cdot Q$ es ortogonal
	\item $Q\,u\cdot Q\,v=u\cdot v\quad\forall\,u,v\in\R^4$
	\item $\|Q_u\|=\|u\|\qquad\forall\,u\in\R^4$
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $(PQ)^T=Q^TP^T=!^{-1}P^{-1}=(PQ)^{-1}$
	\item $Qu_{n\times n}\cdot Qv_{n\times1}=(Qu)_{1\times n}^T\cdot Qv_{n\times 1}=u_T\cdot Q^T\cdot Qv=u^T\cdot v=u\cdot v$
	\item $\|Qu\|^2=Qu\cdot Qu=u\cdot u =\|u\|^2$.
\end{enumerate}
\subsubsection{Tensores}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Tensor de orden 3)
\end{itemize}
Un tensor de orden 3 es una colección finita de matrices. Por tanto, el tensor $T$ lo indexamos como \[ T=(t_{ijk}),\qquad\begin{array}{l}
	1\le i\le I\\
	1\le j\le J\\
	1\le k\le K
\end{array} \] De manera recurrente se define un tensor de cualquier orden $n$. En particular, un vector es un tensor de orden 1, y una matriz un tensor de orden 2.