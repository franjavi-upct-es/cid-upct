\section{Valores y vectores propios}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Sea $A\in M_n(\R^n)$. Se dice que $\lambda\in\K$ es un valor propio o autovalor (eigenvalue) de $A$ si existe un vector \lb{no nulo} $v\in\K^n$, llamado vector propio o autovector (eigenvector) de modo que $Av=\lambda v$.

Al conjunto de todos los valores propios de $A$ se le llama espectro de $A$. Al mayor, en valor absoluto, de los autovalores se le llama radio espectral, denotado $\rho(A)$.

\bu{¿Cómo calculamos los autovalores y autovectores de una matriz?}

$Av=\lambda v\Longleftrightarrow(A-\lambda I)v=0\Longleftrightarrow v\in\nuc(A-\lambda I)\Longleftrightarrow\nuc(A-\lambda I)\neq\{0\}\Longleftrightarrow\dimn(A-\lambda I)=n-\rg(A-\lambda I)\ge1\Longleftrightarrow\rg(A-\lambda I)<n\Longleftrightarrow\det(A-\lambda I)=0$
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Polinomio característico)
\end{itemize}
Sea $A\in M_n(\K^n)$. Se llama polinomio característico de $A$, denotado $P_A(\lambda)$, al polinomio \[ P_A(\lambda)=\det(A-\lambda I) \]Por tanto, las raíces del polinomio característico son los valores propios de $A$.

\Ej

$\begin{array}{l}
	A=\begin{bmatrix}
		5 & 4\\
		4 & 5
	\end{bmatrix}\\
	\begin{aligned}
		P_A(\lambda)=\det\begin{bmatrix}
			5-\lambda & 4\\
			4 & 5-\lambda
		\end{bmatrix}&=(5-\lambda)^2-16\\
		&=25+\lambda^2-10\lambda-16\\
		&=\lambda^2-10\lambda+9=0
	\end{aligned}\\
	\lambda=\dfrac{10\pm\sqrt{100-36}}{2}=\dfrac{10\pm\sqrt{64}}{2}=\dfrac{10\pm8}{2}=\left\langle\begin{array}{l}
		\lambda_1=9\\
		\lambda_2=1
	\end{array}\right.
\end{array}$

Valores propios $\lambda_1=9,\:\lambda_2=1$\\
Espectro de $A=\{9,1\}$\\
Radio espectral $\rho(A)=9$.\\
Vectores propios.
\begin{itemize}
	\item $\lambda_1=9\qquad\mathrm{ker}(A-9I)=\mathrm{ker}\begin{bmatrix}
		-4 & 4\\
		4 & -4
	\end{bmatrix}$
	
	$\begin{array}{l}
		\begin{bmatrix}
			-4 & 4\\
			4 & -4
		\end{bmatrix}\cdot\begin{bmatrix}
			x\\
			y
		\end{bmatrix}=\begin{bmatrix}
			0\\
			0
		\end{bmatrix}\\
		\begin{rcases}
			-4x+4y=0\\
			4x-4y=0
		\end{rcases}\:y=\alpha\longrightarrow x=\alpha\\
		\mathrm{ker}(A-9I)=<(1,1)>
	\end{array}$
	\item $\lambda_2=1,\:\mathrm{ker}(A-I)=\mathrm{ker}\begin{bmatrix}
		4 & 4\\
		4 & 4
	\end{bmatrix}$
	
	$\begin{array}{l}
		\begin{bmatrix}
			4 &4\\
			4 & 4
		\end{bmatrix}\cdot\begin{bmatrix}
			x\\
			y
		\end{bmatrix}=\begin{bmatrix}
			0\\
			0
		\end{bmatrix}\\
		\begin{rcases}
			4x+4y=0\\
			4x+4y=0
		\end{rcases}\:y=\alpha\longrightarrow x=-\alpha\\
		\mathrm{ker}(A-I)=<(-1,1)>
	\end{array}$
\end{itemize}
Consideremos la matriz formada por lo valores propios anteriores, pero unitarios, es decir, \[ P=\begin{bmatrix}
	\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}\\
	\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}
\end{bmatrix} \]
Nótese que $P$ es ortogonal. Por tanto, $P^{-1}=P^\intercal$.

Además, si denotamos por $D$ la matriz diagonal formada por los valores propios de $A$, es decir, \[ D=\begin{bmatrix}
	9 & 0\\
	0 & 1
\end{bmatrix} \]entonces se cumple:\begin{align*}
	PDP^{-1}&=\begin{bmatrix}
		\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}\\
		\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}
	\end{bmatrix}\cdot\begin{bmatrix}
		9 & 0\\
		0 & 1
	\end{bmatrix}\cdot\begin{bmatrix}
		\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}\\
		-\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}
	\end{bmatrix}\\
	&=\begin{bmatrix}
		\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}\\
		\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}
	\end{bmatrix}\cdot\begin{bmatrix}
		\dfrac{9}{\sqrt{2}} & \dfrac{9}{\sqrt{2}}\\
		-\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}\\
	\end{bmatrix}\\
	&=\begin{bmatrix}
		5 & 4\\
		4 & 5
	\end{bmatrix}=A
\end{align*}
Es decir, $A=PDP^{-1}$ y también $D=P^{-1}AP$
\subsection{Interpretación en términos de aplicaciones lineales}

$\begin{array}{l}
	\begin{aligned}
		f:&\R^2\longrightarrow\R^2\\
		&(x,y)\longmapsto f(x,y)=(5x+4y.4x+5y)
	\end{aligned}\\
	C=\{(1,0),(0,1)\}\\
	\begin{array}{l}
		f(1,0)=(5,4)\\
		f(0,1)=(4,5)
	\end{array}\qquad M_{C\to C}(f)=\begin{bmatrix}
		5 &4\\
		4 & 5
	\end{bmatrix}=A\\
	B=\left\{\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right),\left(-\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)\right\}\\
\end{array}$

$\begin{aligned}
		f\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)&=\left(5\cdot\dfrac{1}{\sqrt{2}}+4\cdot\dfrac{1}{\sqrt{2}},4\cdot\dfrac{1}{\sqrt{2}}+5\cdot\dfrac{1}{\sqrt{2}}\right)\\
		&=9\cdot\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)\\
		&=9\cdot\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)+0\cdot\left(-\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)
	\end{aligned}$\\
	$\begin{aligned}
		f\left(-\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)&=\left(-5\cdot\dfrac{1}{\sqrt{2}}+4\cdot\dfrac{1}{\sqrt{2}},4\cdot-\left(\dfrac{1}{\sqrt{2}}\right)+5\cdot\dfrac{1}{\sqrt{2}}\right)\\
		&=1\cdot\left(-\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)\\
		&=0\cdot\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)+1\cdot\left(-\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)
	\end{aligned}$\\
	$M_{B\to B}(f)=\begin{bmatrix}
		9 & 0\\
		0 & 1
	\end{bmatrix}$\\
	$M_{B\to C}=\begin{bmatrix}
		\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}\\
		\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}
	\end{bmatrix}\equiv P$\\
	$M_{C\to B}=\begin{bmatrix}
		\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}\\
		-\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}
	\end{bmatrix}$
\begin{center}
	\begin{tikzcd}
		{\begin{tikzpicture}
				\draw (-1.2,0) -- (1.2,0);
				\draw (0,-1.2) -- (0,1.2);
				\draw[lightblue, -latex] (0,0) -- (1,0) node[below] {$(1,0)$};
				\draw[lightblue, -latex] (0,0) -- (0,1) node[left] {$(0,1)$};
				\node at (1.5,1.5) {$\R^2$};
				\node at (-1.5,-1.5) {$C$};
		\end{tikzpicture}} \arrow[rr, "f"] \arrow[dd] &  & {\begin{tikzpicture}
				\draw (-1.2,0) -- (1.2,0);
				\draw (0,-1.2) -- (0,1.2);
				\draw[lightblue, -latex] (0,0) -- (1,0) node[below] {$(1,0)$};
				\draw[lightblue, -latex] (0,0) -- (0,1) node[left] {$(0,1)$};
				\node at (1.5,-1.5) {$C$};
		\end{tikzpicture}}            \\
		&  &               \\
		{\begin{tikzpicture}
				\draw (-1.2,0) -- (1.2,0);
				\draw (0,-1.2) -- (0,1.2);
				\draw[lightblue, -latex] (0,0) -- (1,1);
				\draw[lightblue, -latex] (0,0) -- (-1,1);
				\draw[lightblue, dashed] (-1,1) -- (-1,0) node[below] {$-\dfrac{1}{\sqrt{2}}$};
				\draw[lightblue, dashed] (1,1) -- (1,0) node[below] {$\dfrac{1}{\sqrt{2}}$};
				\node at (1.5,1.5) {$\R^2$};
				\node at (-1.5,-1.5) {$B$};
		\end{tikzpicture}} \arrow[rr, "f"]            &  & {\begin{tikzpicture}
				\draw (-1.2,0) -- (1.2,0);
				\draw (0,-1.2) -- (0,1.2);
				\draw[lightblue, -latex] (0,0) -- (1,1);
				\draw[lightblue, -latex] (0,0) -- (-1,1);
				\draw[lightblue, dashed] (-1,1) -- (-1,0) node[below] {$-\dfrac{1}{\sqrt{2}}$};
				\draw[lightblue, dashed] (1,1) -- (1,0) node[below] {$\dfrac{1}{\sqrt{2}}$};
				\node at (1.5,-1.5) {$B$};
		\end{tikzpicture}} \arrow[uu]
	\end{tikzcd}
\end{center}
$A=M_{C\to C}(f)=M_{B\to C}M_{B\to B}(f)M_{C\to B}=PDP^{-1}$
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Conclusiones
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item Los valores propios dependen de la aplicación lineal, no de la matriz. Es decir, son invariantes por cambio de base. En efecto, las matrices $A$ y $Q^{-1}AQ$ tienen los mismos valores propios.\begin{align*}
		\det(Q^{-1}AQ-\lambda I)&=\det[Q^{-1}AQ-Q^{-1}(\lambda I)Q]\\
		&=\det[Q^{-1}(A-\lambda I)Q]\\
		&=\det[Q^{-1}]\cdot\det[A-\lambda I]\cdot\det(Q)\\
		&=\det(A-\lambda I)
	\end{align*}
	\item La base de vectores propios nos permite expresar la aplicación lineal $f$ de la forma más sencilla posible (a través de una matriz diagonal)
\end{enumerate}
\subsection{Cálculo numérico de valores propios}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Teorema (Factorización $QR$)
\end{itemize}
Sean $\{a_1,a_2,\dots,a_n\}$ un conjunto de $n$-vectores de $\R^m$ \lb{linealmente independientes}. Entonces existe un conjunto ortonormal de vectores $\{q_1,q_2,\dots,q_n\}$ de $\R^m$ de modo que \[ <a_1,a_2,\dots,a_n>=<q_1,q_2,\dots,q_n>. \]
Además, si denotamos por $A=[a_1,\dots,a_n]$ la matriz cuyas columnas son los vectores $\{a_1,\dots,a_n\}$ y por $Q$ la matriz cuyas columnas son $\{q_1,\dots,q_n\}$, entonces existe una matriz $R$, triangular superior de modo que \[ A=QR, \]es decir,\[\underset{\begin{array}{cccc}
		\downarrow & \downarrow & & \downarrow\\
		a_1 & a_2 & ~~~~ & a_n
\end{array}}{\begin{bmatrix}
		a_{11} & a_{12} & \cdots & a_{1n}\\
		\vdots & \vdots & & \vdots\\
		a_{m1} & a_{m2} & & a_{mn}
\end{bmatrix}}=\begin{bmatrix}
	q_{11} & q_{12} & \cdots & q_{1n}\\
	\vdots & \vdots & & \vdots\\
	q_{m1} & q_{m2} & & q_{mn}
\end{bmatrix}_{m\times n}\cdot\begin{bmatrix}
	r_{11} & \cdots & r_{1n}\\
	& & \\
	& & r_{nn}
\end{bmatrix}_{n\times n}  \]
\Ej\\
$A=\underset{\begin{array}{ccc}
		\downarrow & \downarrow & \downarrow\\
		a_1 & a_2 & a_3
\end{array}}{\begin{bmatrix}
		1 & 1 & 0\\
		1 & 0 & 1\\
		0 & 1 & 1
\end{bmatrix}}=\overbrace{\underset{\begin{array}{ccc}
			\downarrow & \downarrow & \downarrow\\
			q_1 ~~&~~ q_2~~ &~~ q_3
	\end{array}}{\begin{bmatrix}
			\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{}} & -\dfrac{1}{\sqrt{3}}\\
			\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{6}} & \dfrac{1}{\sqrt{3}}\\
			0 & \dfrac{2}{\sqrt{6}} & \dfrac{1}{\sqrt{3}}
\end{bmatrix}}}^{Q}\cdot\overbrace{\begin{bmatrix}
		\sqrt{2} & \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} \\ 
		0 & \dfrac{3}{\sqrt{6}} & \dfrac{1}{\sqrt{6}} \\ 
		0 & 0 & \dfrac{2}{3}\sqrt{3}
\end{bmatrix} }^R$
\begin{enumerate}[label=\color{lightblue}\arabic*{$^\circ$})]
	\item $q_1=\dfrac{a_1}{\|a_1\|},\qquad\|a_1\|=\sqrt{1^2+1^2}=\sqrt{2}$
	
	$q_1=\dfrac{1}{\sqrt{2}}a_1\longrightarrow a_1=\underbrace{\sqrt{2}}_{r_{11}}q_1+0\cdot q_2+0\cdot q_3$
	\item $v_2=a_2+\alpha q_1$
	
	$\begin{array}{l}
		\begin{aligned}
			0=v_2\cdot q_1=a_2\cdot q_1+\alpha q_1\cdot q_1\longrightarrow\alpha&=-a_2\cdot q_1\\
			&=-(1,0,1)\cdot\dfrac{1}{\sqrt{2}}(1,1,0)\\
			&=-\dfrac{1}{\sqrt{2}}
		\end{aligned}\\
		\begin{aligned}
			v_2&=a_2-\dfrac{1}{\sqrt{2}}q_1\\
			&=(1,0,1)-\left(\dfrac{1}{2},\dfrac{1}{2},0\right)\\
			&=\left(\dfrac{1}{2},-\dfrac{1}{2},1\right)
		\end{aligned}\\
		\|v_2\|=\sqrt{\dfrac{1}{4}+\dfrac{1}{4}+1}=\sqrt{\dfrac{3}{2}}\\
		q_2=\dfrac{v_2}{\|v_2\|}=\dfrac{\sqrt{2}}{\sqrt{3}}\left(\dfrac{1}{2},-\dfrac{1}{2},1\right)=\left(\dfrac{1}{\sqrt{6}},-\dfrac{1}{\sqrt{6}},\dfrac{2}{\sqrt{6}}\right)\\
		a_2=\dfrac{1}{\sqrt{2}}q_1+v_2=\underbrace{\dfrac{1}{\sqrt{2}}}_{r_{12}}q_1+\underbrace{\|v_2\|}_{\begin{subarray}{c}
				r_{22}\\
				\rotatebox{90}{=}\\
				\sqrt{\frac{3}{2}}=\frac{3}{\sqrt{6}}
		\end{subarray}}q_2+0\cdot q_3
	\end{array}$
	\item $v_3=a_3+\alpha q_1+\beta q_2$
	
	$\begin{array}{l}
		\begin{aligned}
			0=q_1\cdot v_3\longrightarrow\alpha=-a_3\cdot q_1&=-(0,1,1)\cdot\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}},0\right)\\
			&=-\dfrac{1}{\sqrt{2}}
		\end{aligned}\\
		\begin{aligned}
			0=q_2\cdot v_3\longrightarrow\beta=-a_3\cdot q_2&=-(0,1,1)\cdot\left(\dfrac{1}{\sqrt{6}},-\dfrac{1}{\sqrt{6}},\dfrac{2}{\sqrt{6}}\right)\\
			&=\dfrac{1}{\sqrt{6}}-\dfrac{2}{\sqrt{6}}=-\dfrac{1}{\sqrt{6}}
		\end{aligned}\\
		\begin{aligned}
			v_3&=(0,1,1)-\dfrac{1}{\sqrt{2}}\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}},0\right)-\dfrac{1}{\sqrt{6}}\left(\dfrac{1}{\sqrt{6}},-\dfrac{1}{\sqrt{6}},\dfrac{2}{\sqrt{6}}\right)\\
			&=\left(-\dfrac{2}{3},\dfrac{2}{3},\dfrac{2}{3}\right)
		\end{aligned}\\
		\|v_3\|=\sqrt{\dfrac{4}{9}+\dfrac{4}{9}+\dfrac{4}{9}}=\dfrac{2}{3}\sqrt{3}\\
		q_3=\dfrac{v_3}{\|v_3\|}=\dfrac{3}{2\sqrt{3}}\cdot\left(-\dfrac{2}{3},\dfrac{2}{3},\dfrac{2}{3}\right)=\left(-\dfrac{1}{\sqrt{3}},\dfrac{1}{\sqrt{3}},\dfrac{1}{\sqrt{3}}\right)\\
		a_3=\lbb{-\alpha}{r_{13}} q_1\lbb{-\beta}{r_{23}} q_2+\lbb{\|v_3\|}{r_{33}}q_3
	\end{array}$
\end{enumerate}
\subsection{Algoritmo $QR$ para el cálculo de valores propios}
\begin{enumerate}[label=\color{lightblue}\arabic*$^\circ$)]
	\item Inicialización: tomar $A_0=A$
	\item Iteración: para $k\ge0\quad d_0$:
	\begin{enumerate}[label=\color{lightblue}2.\arabic*)]
		\item $A_0=Q_1R_1$ factorización $QR$ de $A_0$
		\item $A_1=R_1Q_1$
		
		\begin{tikzpicture}
			\node[red, draw=red, line width=1.5, fill=red!10] {\underline{Nota:} $A_1=Q_1^\intercal Q_1R_1Q_1=Q_1^\intercal A_0Q_1$};
		\end{tikzpicture}
		\item $A_1=Q_2R_2$
		\item $A_2=R_2Q_2$
		
		\begin{tikzpicture}
			\node[red, draw=red, line width=1.5, fill=red!10, text width=7cm] {$\begin{aligned}
					\text{\underline{Nota:} }A_2&=Q_2^\intercal Q_2R_2Q_2=Q_2^\intercal A_1Q_2\\
					&=Q_2^\intercal Q_1^\intercal A_0Q_1 Q_2
				\end{aligned}$
				\\
				y así sucesivamente.};
		\end{tikzpicture}
	\end{enumerate}
\end{enumerate}
Se observa que $A_k$ converge a una matriz triangular superior que tiene en su diagonal principal los valores propios de $A$.

Una vez el algoritmo ha convergido, los vectores propios están en la columnas de $Q=Q_1,\dots,Q_n$.

\begin{tikzpicture}
	\node[red, draw=red, line width=1.5, fill=red!10, text width=\textwidth] {\underline{Nota:} El algoritmo es estable (los errores no se propagan) porque $Q$ es ortogonal. Por tanto, su número de condición es 1.};
\end{tikzpicture}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Factorización en valores propios)
\end{itemize}
Sea $A\in M_n(\R^n)$. Se dice que $A$ es diagonalizable o factorizable en valores propios si existen una diagonal $D$, que contiene en su diagonal los valores propios de $A$, y una matriz invertible $P$, cuyas columnas están formadas por los vectores propios de $A$, de modo que \[ \bboxed{A=PDP^{-1}} \]A esta factorización se le llama factorización en valores propios.

\bu{¿Qué matrices cuadradas son factorizables en valores propios?}

\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Sea $A\in M_n(\R)$. Si $A$ tiene $n$ valores propios distintos, entonces es diagonalizable.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Teorema
\end{itemize}
Si $A$ es simétrica, entonces \[ A=PDP^{-1} \]con $D=\mathrm{diag}(\lambda_1,\lambda_2,\dots,\lambda_n)$, $\lambda_j$ valores propios y $P=[u_1,\dots,u_n]$ es una base ortonormal de vectores propios.

\begin{tikzpicture}
	\node[red, draw=red, fill=red!10, line width=1.5, text width=\textwidth] {\underline{Nota:} Nótese que, en esta situación \begin{align*}
			\begin{bmatrix}
				u_1 & \cdots & u_b
			\end{bmatrix}\cdot\begin{bmatrix}
				\lambda_1 & & \\
				& \ddots & \\
				& & \lambda_n
			\end{bmatrix}\cdot\begin{bmatrix}
				u_1^\intercal\\
				\vdots\\
				u_m^\intercal
			\end{bmatrix}&=\begin{bmatrix}
				\lambda_1 u_1 & \cdots & \lambda_nu_n
			\end{bmatrix}\cdot\begin{bmatrix}
				u_1^\intercal\\
				\vdots\\
				u_n^\intercal
			\end{bmatrix}\\
			&=\lambda_1u_1u_1^\intercal+\cdots+\lambda_nu_nu_n^\intercal
		\end{align*}La expresión $A=\sum_{j=1}^{n}\lambda_iu_ii_i^\intercal$ se le llama descomposición espectral de $A$.\\
		Nótese que \[u_iu_i^\intercal=\begin{bmatrix}
			\hat{u}_i\\
			\vdots\\
			\hat{u}_n
		\end{bmatrix}\cdot\begin{bmatrix}
			\hat{u}_1 & \cdots & \hat{u}_n
		\end{bmatrix}=\left[\begin{array}{cccc}
			\hat{u}_1\hat{u}_1 & \hat{u}_1\hat{u}_2 & \cdots & \hat{u}_1\hat{u}_n\\
			\hat{u}_2\hat{u}_1 & \hat{u}_2\hat{u}_2 & \cdots & \hat{u}_2 \hat{u}_n\\ \hdashline
			\hat{u}_n\hat{u}_1 & \cdots & \cdots & \hat{u}_n\hat{u}_n
		\end{array}\right]\]son matrices simétricas y de rango 1};
\end{tikzpicture}
\subsection{Matrices semidefinidas positivas}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
$A\in M_n(\R)$ se dice semidefinida positiva si $x^\intercal Ax\ge0\:\forall x\in\R^n,\:x\neq0$.

Si $x^\intercal Ax>0\:\forall x\in\R^n$ no nulo. $A$ se dice definida positiva.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Sea $A\in M_n(\R)$ simétrica. Son equivalentes:
\begin{enumerate}[label=\color{lightblue}\roman*)]
	\item $A$ es definida positiva
	\item Todos los valores propios de $A$ son estrictamente positivos.
\end{enumerate}
Si $A$ es semidefinida positiva, entonces los valores propios son $\ge0$.
\subsection{Factorización en Valores Singulares $(SVD)$}
Los principales inconvenientes de la factorización en valores propios es que la matriz $A$ ha de ser cuadrada y simétrica. Sin embargo, en Ciencia de Datos aparecen muchas matrices que no son cuadradas ni simétricas. 

La factorización $SVD$ soluciona ambos problemas y, para muchos, \lb{es el resultado más importante en Ciencia de Datos}.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición
\end{itemize}
Dada $A\in M_{m\times n}(\R)$, se llama factorización en valores singulares $SVD$ (Singular  Value Descomposition) de $A$ a una factorización de la forma \[ A=U\Sigma V^\intercal \]con $U\in M_m(\R),\:V\in M_n(\R)$, ambas ortogonales y $\Sigma\in M_{m\times n}(\R)$, una matriz diagonal.

\bu{¿Cómo se obtiene?}

$A^\intercal A$ es simétrica y semidefinida positiva. En efecto, $(A^\intercal A)^\intercal=A^\intercal(A^\intercal)^\intercal=A^{\intercal}A$. \[ x^\intercal A^\intercal Ax=(Ax)^\intercal Ax=\|Ax\|^2\ge0\:\forall x\in\R^n. \]Por tanto, sus valores propios $\lambda_1,\dots,\lambda_n\ge0$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Definición (Valores Singulares)
\end{itemize}
Se llaman valores singulares de $A$, denotados $\sigma_1,\dots,\sigma_n$, a las raíces cuadradas de los valores propios de $A^\intercal A$, es decir, \[ \sigma_1=\sqrt{\lambda_1},\:\sigma_2=\sqrt{\lambda_2},\:\dots,\:\sigma_n=\sqrt{\lambda_n} \]
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Teorema (Factorización $SVD$)
\end{itemize}
Sea $A\in M_{m\times n}$, con $n\le m$. Sean $\sigma_1,\dots,\sigma_n$ los valores singulares de $A$. Entonces existen matrices ortogonales $U\in M_m(\R)$ y $V\in M_n(\R)$ tales que \[ A=U\Sigma V^\intercal \]con $\Sigma\in M_{m\times n}(\R)$ diagonal con  $\sigma_1,\dots,\sigma_n$ en la diagonal principal.

\begin{tikzpicture}
	\node[red, draw=red, fill=red!10, line width=1.5, text width=\textwidth] {\underline{Nota:} El resultado también es válido si $m\le n$. En este caso $A^\intercal=U\Sigma V^\intercal$ y por tanto, tomando traspuestas, \[ A=V\Sigma^\intercal V^\intercal \]};
\end{tikzpicture}

El esquema queda: 
\begin{center}
	\begin{tikzpicture}
		% Cuadrado A
		\node[style={minimum size=2cm,draw}] (A) at (0,0) {$A$};
		
		% Cuadrado U
		\node[style={minimum size=2cm,draw}] (U) at (3,0) {$U$};
		
		% Cuadrado matriz sigma
		\node[style={minimum size=2cm,draw}] (Sigma) at (6,0) {$\begin{matrix}
				\sigma_1 &  &  \\ 
				& \ddots &  \\ 
				&  & \sigma_n
			\end{matrix}$};
		
		% Cuadrado V^T
		\node[style={minimum size=2cm,draw}] (VT) at (9,0) {$V^T$};
		
		\node at (1.5,0) {$=$};
		\node at (4.25,0) {$\cdot$};
		\node at (7.75,0) {$\cdot$};
	\end{tikzpicture}
\end{center}
En forma compacta: si $\sigma_1,\sigma_2,\dots,\sigma_r$ son los valores singulares no nulos, entonces \begin{align*}
	U\Sigma V^\intercal&=[u_1,\dots,u_m]\cdot[\sigma_1e_1,\dots,\sigma_re_r,0,\dots,0]\begin{bmatrix}
		v_1^\intercal\\
		\vdots\\
		v_n^\intercal
	\end{bmatrix}=[\sigma_1u_1,\dots,\sigma_ru_r,0,\dots,0]\cdot\begin{bmatrix}
		v_1^\intercal\\
		\vdots\\
		v_n^\intercal
	\end{bmatrix}\\
	&=\sigma_1u_1v_1^\intercal+\cdots+\sigma_ru_rv_r^\intercal=U_r\Sigma_rV_r^\intercal
\end{align*}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración del Teorema
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\underline{Paso \arabic*:}]
	\item Como $A^\intercal A$ es simétrica y semidefinida positiva todos sus valores propios $\lambda_1,\dots,\lambda_n\ge0$ y asociados a ellos tenemos una base ortonormal de vectores propios $\{v_1,\dots,v_n\}$. Sea $V=\begin{bmatrix}
		v_1 & \cdots & v_n
	\end{bmatrix}$
	\item Definimos $u_i=\dfrac{1}{\sigma_i}Av_i,\:i=1,\dots,r$, valores singulares asociadas a los $\lambda_j\ge0$.
	\begin{enumerate}[label=\color{lightblue}2.\arabic*)]
		\item Veamos que $\{u_1,\dots,u_r\}$ es un conjunto ortonormal de vectores. En efecto: \[ \begin{aligned}
			u_i^\intercal u_j&=\dfrac{1}{\sigma_i\sigma_j}\left(Av_i\right)^\intercal(Av_j)\\
			&=\dfrac{1}{\sigma_i\sigma_j}v_i^\intercal\underbrace{A^\intercal Av_j}_{\begin{subarray}{c}
					\rotatebox{90}{=}\\
					\lambda_jv_j
			\end{subarray}}=\begin{cases}
				1 & \text{si }i=j\\
				0 & \text{si }i\neq j
			\end{cases}
		\end{aligned} \]ya que $\{v_1,\dots,v_n\}$ es una base ortonormal de vectores propios de $A^\intercal A$.
		\item Completamos (por ejemplo, usando Gram-Schmidt) el conjunto $\{u_1,\dots,u_r\}$ a una base ortonormal de $\R^m\:\{u_1,\dots,u_r,u_{r+1},\dots,u_m\}$
		
		Definimos $U=\begin{bmatrix}
			u_1&\dots&u_m
		\end{bmatrix}$
	\end{enumerate}
	\item Calculamos $U^\intercal AV$\begin{align*}
		U^\intercal AV&=\begin{bmatrix}
			u_1^\intercal\\
			\vdots\\
			u_m^\intercal
		\end{bmatrix}\cdot A\begin{bmatrix}
			v_1 & \cdots & v_n
		\end{bmatrix}\\
		&=\begin{bmatrix}
			u_1^\intercal\\
			\vdots\\
			u_m^\intercal 
		\end{bmatrix}\cdot\begin{bmatrix}
			Av_1 & \cdots & Av_n
		\end{bmatrix}\\
		&=\begin{bmatrix}
			u_1^\intercal\\
			\vdots\\
			u_m^\intercal
		\end{bmatrix}\cdot[\sigma_1u_1,\dots,\sigma_ru_r,0,\dots,0]\\
		&=(\sigma_1u_1^\intercal u_1,\dots,\sigma_ru_r^\intercal u_r,0,\dots,0)\equiv\Sigma
	\end{align*}
	Multiplicando a la derecha por $V^{-1}=V^\intercal$ y a la izquierda por $U$ se tiene \[ A=U\Sigma V^\intercal \]
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Notas importantes
\end{itemize}
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item ¿Qué son los vectores $u_1,\dots,u_n$?
	
	Son los autovectores de $AA^\intercal$. En efecto \begin{align*}
		AA^\intercal u_k&=AA^\intercal\left(\dfrac{1}{\sigma_k}Av_k\right)\\
		&=A\left(\dfrac{A^\intercal Av_k}{\sigma_k}\right)\\
		&=A\dfrac{\sigma_k^2\cdot v_k}{\sigma_k}\\
		&=\sigma_kAv_k\\
		&=\sigma_k\sigma_ku_k\\
		&=\lambda_ku_k
	\end{align*}
	\item Por tanto, los $u_s$ son los vectores propios de $AA^\intercal$ asociados a los valores propios $\lambda_1,\dots,\lambda_m$, y los $v_s$ son los vectores propios de $A^\intercal A$ asociados a los mismos valores propios $\lambda,\dots,\lambda_m$.
\end{enumerate}
\Ej

$A=\begin{bmatrix}
	1 & 0 & 0 \\
	0 & 1 & 1 \\
	1 & 1 & 1 \\
	-1 & 1 & 1
\end{bmatrix}_{4\times3}$

$A^\intercal A=\begin{bmatrix}
	1 & 0 & 1 & -1 \\
	0 & 1 & 1 & 1 \\
	0 & 1 & 1 & 1
\end{bmatrix}\cdot\begin{bmatrix}
	1 & 0 & 0 \\
	0 & 1 & 1 \\
	1 & 1 & 1 \\
	-1 & 1 & 1
\end{bmatrix}=\begin{bmatrix}
	3 & 0 & 0 \\
	0 & 3 & 3 \\
	0 & 3 & 3
\end{bmatrix}$

\bu{Valores propios de $A^\intercal A$}

$\det(A^\intercal A-\lambda I)=\det\begin{pmatrix}
	3-\lambda & 0& 0\\
	0 & 3-\lambda & 3\\
	0 & 3 & 3-\lambda
\end{pmatrix}=(3-\lambda)\begin{vmatrix}
	3-\lambda & 3 \\
	3 & 3-\lambda
\end{vmatrix}=(3-\lambda)\cdot\left((3-\lambda)^2-9\right)=0$

$\lambda_1=6>\lambda_2=3>\lambda_3=0$

\bu{Valores singulares:} $\sigma_1=\sqrt{6},\quad\sigma_2=\sqrt{3}$

\bu{Valores propios de $A^\intercal A$:}
\begin{itemize}[label=\color{lightblue}\textbullet]
	\item $\lambda_1=6$
	
	$\mathrm{ker}(A^\intercal A-6I)\Longleftrightarrow\begin{pmatrix}
		-3 & 0 & 0 \\
		0 & -3 & 3 \\
		0 & 0 & -3
	\end{pmatrix}\cdot\begin{bmatrix}
		x\\
		y\\
		z
	\end{bmatrix}=\begin{bmatrix}
		0\\
		0\\
		0
	\end{bmatrix}\longrightarrow v_1=\left(0,\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)$
	\item $\lambda_2=3$
	
	$\mathrm{ker}(A^\intercal A-3I)\Longleftrightarrow\begin{bmatrix}
		0 & 0 & 0 \\
		0 & 0 & 3 \\
		0 & 0 & 3
	\end{bmatrix}\cdot\begin{bmatrix}
		x\\
		y\\
		z
	\end{bmatrix}=\begin{bmatrix}
		0\\
		0\\
		0
	\end{bmatrix}\longrightarrow v_2=(1,0,0)$
	\item $\lambda_3=0$
	
	$\mathrm{ker}(A^\intercal A-0\cdot I)\Longleftrightarrow\begin{bmatrix}
		3 & 0 & 0 \\
		0 & 3 & 3 \\
		0 & 3 & 3
	\end{bmatrix}\cdot\begin{bmatrix}
		x\\
		y\\
		z
	\end{bmatrix}=\begin{bmatrix}
		0\\
		0\\
		0
	\end{bmatrix}\longrightarrow v_3=\left(0,\dfrac{1}{\sqrt{2}},-\dfrac{1}{\sqrt{2}}\right)$
\end{itemize}
Por tanto, \[ V=\dfrac{1}{\sqrt{2}}\begin{bmatrix}
	0 & \sqrt{2} & 0 \\
	1 & 0 & 1 \\
	1 & 0 & -1
\end{bmatrix} \]
Calculamos ahora los vectores que forman las columnas de $U$:

$u_1=\dfrac{1}{\sigma_1}Av_1=\dfrac{1}{\sqrt{6}}\begin{bmatrix}
	1 & 0 & 0 \\
	0 & 1 & 1 \\
	1 & 1 & 1 \\
	-1 & 1 & 1
\end{bmatrix}\cdot\begin{bmatrix}
0\\
\dfrac{1}{\sqrt{2}}\\
\dfrac{1}{\sqrt{2}}
\end{bmatrix}=\begin{bmatrix}
0\\
\dfrac{1}{\sqrt{3}}\\
\dfrac{1}{\sqrt{3}}\\
\dfrac{1}{\sqrt{3}}
\end{bmatrix}$

$u_2=\dfrac{1}{\sigma_2}Av_2=\dfrac{1}{\sqrt{3}}\begin{bmatrix}
	1 & 0 & 0 \\
	0 & 1 & 1 \\
	1 & 1 & 1 \\
	-1 & 1 & 1
\end{bmatrix}\cdot\begin{bmatrix}
1\\
0\\
0
\end{bmatrix}=\begin{bmatrix}
\dfrac{1}{\sqrt{3}}\\
0\\
\dfrac{1}{\sqrt{3}}\\
-\dfrac{1}{\sqrt{3}}
\end{bmatrix}$

Hemos de completar a una base ortonormal de $\R^4$:

Lo podemos hacer de manera sistemáticamente usando Gram-Schmidt a partir de los vectores \[ \left\{u_1=\left(0,\dfrac{1}{\sqrt{3}},\dfrac{1}{\sqrt{3}},\dfrac{1}{\sqrt{3}}\right),\left(\dfrac{1}{\sqrt{3}},0,\dfrac{1}{\sqrt{3}},-\dfrac{1}{\sqrt{3}}\right),(1,0,0,0),(0,1,0,0)\right\} \]o bien directamente se ve \[ u_3=\left(-\dfrac{1}{\sqrt{3}},-\dfrac{1}{\sqrt{3}},\dfrac{1}{\sqrt{3}},0\right), u_4=\left(\dfrac{1}{\sqrt{3}},-\dfrac{1}{\sqrt{3}},0,\dfrac{1}{\sqrt{3}}\right) \]La matriz $U$ es \[ U=\dfrac{1}{\sqrt{3}}\begin{bmatrix}
	0 & 1 & -1 & 1 \\
	1 & 0 & -1 & -1 \\
	1 & 1 & 1 & 0 \\
	1 & -1 & 0 & 1
\end{bmatrix} \]Por tanto,

$A=U\Sigma V^\intercal=\dfrac{1}{\sqrt{6}}\begin{bmatrix}
	0 & 1 & -1 & 1 \\
	1 & 0 & -1 & -1 \\
	1 & 1 & 1 & 0 \\
	1 & -1 & 0 & 1
\end{bmatrix}_{4\times 4}\cdot\begin{bmatrix}
\sqrt{6} & 0 & 0\\
0 & \sqrt{3} & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix}_{4\times 3}\cdot\begin{bmatrix}
0 & 1 & 1\\
\sqrt{2} & 0 & 0\\
0 & 1 & -1
\end{bmatrix}_{3\times3}$

que también podemos escribir de manera más compacta:

\[\begin{aligned}
	A=U_2\Sigma_2V_2^\intercal&=\dfrac{1}{\sqrt{6}}\underset{\begin{array}{cc}
		\downarrow & \downarrow\\
		u_1 & u_2
\end{array}}{\begin{bmatrix}
0 &1 \\
1 & 0\\
1 & 1\\
1 & -1
\end{bmatrix}}\cdot\underset{\begin{array}{cc}
\downarrow & \downarrow\\
\sigma_1 & \sigma_2
\end{array}}{\begin{bmatrix}
\sqrt{6} & 0\\
0 & \sqrt{3}
\end{bmatrix}}\cdot\begin{bmatrix}
0 & 1 & 1\\
\sqrt{2} & 0 & 0
\end{bmatrix}\begin{array}{l}
\longleftarrow v_1^\intercal\\
\longleftarrow v_2^\intercal
\end{array}\\
&=\underbrace{\begin{bmatrix}
	0\cdot0 & 0\cdot1 & 0\cdot1 \\
	1\cdot0 & 1\cdot1 & 1\cdot1 \\
	1\cdot0 & 1\cdot1 & 1\cdot1 \\
	1\cdot0 & 1\cdot1 & 1\cdot1
\end{bmatrix}}_{\sigma_1u_1v_1^\intercal}+\underbrace{\begin{bmatrix}
\dfrac{\sqrt{3}}{\sqrt{6}}\cdot\sqrt{2} & \dfrac{\sqrt{3}}{\sqrt{6}}\cdot0 & \dfrac{\sqrt{3}}{\sqrt{6}}\cdot0 \\ 
0\cdot\sqrt{2} & 0\cdot0 & 0\cdot0 \\ 
\dfrac{\sqrt{3}}{\sqrt{6}}\cdot\sqrt{2} & \dfrac{\sqrt{3}}{\sqrt{6}}\cdot0 & 1\cdot1 \\ 
-\dfrac{\sqrt{3}}{\sqrt{6}}\cdot\sqrt{2} & -\dfrac{\sqrt{3}}{\sqrt{6}}\cdot0 & -\dfrac{\sqrt{3}}{\sqrt{6}}\cdot0
\end{bmatrix} }_{\sigma_2u_2v_2^\intercal}
\end{aligned}\]

Suma de matrices de rango 1.

\bu{¿Por qué es importante la $SVD$ en Ciencia de Datos?}

\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Teorema (Eckart-Young-Mirsky)
\end{itemize}
Sea $A_k=\sum_{k=1}^{k}\sigma_iu_iv_i^\intercal$.

Entonces, para toda matriz de rango $k$ se tiene que \[ \|A-A_k\|\le\|A-B\| \]donde $\|\cdot\|$ norma de Frobenius. Es decir, $A_k$ es la mejor aproximación posible de $A$ con matrices de rango menor o igual que $k$.

\bu{Aplicaciones:} Compresión de imágenes, eliminación de ruido en señales, etc\dots

\bu{Geometría de la $SVD$:} $A=U\Sigma V^\intercal$
\begin{itemize}[label=\color{lightblue}$-$]
	\item $U$: Ortogonal
	\item $\Sigma$: Dilatación
	\item $V^\intercal$: Ortogonal
\end{itemize}
Veámoslo en dimensión 2.

\begin{center}
	\begin{tikzpicture}
	\node {\begin{tikzcd}
			~_C\quad\mathbb{R}^2 \arrow[rr, "A"] \arrow[dd] &  & \mathbb{R}^2\quad _C            \\
			&  &                                 \\
			~_V\quad\mathbb{R}^2 \arrow[rr, "\Sigma"]       &  & \mathbb{R}^2\quad _U \arrow[uu]
	\end{tikzcd}};
\draw[latex-] (-1.65,-1.2) -- (-2.5, -2) node[below] {Columnas de $V=$ base de $\R^2$.};
\draw[latex-] (1.8,-1.2) -- (4, -2) node[below, text width=5cm] {Columnas de $U$ forman una base de $\R^2$.};
\end{tikzpicture}
\end{center}

\begin{center}
	\begin{tikzcd}
	{\begin{tikzpicture}
			\draw (-1.2,0) -- (1.2,0);
			\draw (0,-1.2) -- (0,1.2);
			\draw[lightblue] (0,0) circle (1);
			\draw[-latex,blue,line width=1.5] (0,0) -- (1,0) node[below right] {$e_1$};
			\draw[-latex,blue,line width=1.5] (0,0) -- (0,1) node[above right] {$e_2$};
	\end{tikzpicture}} \arrow[rr, "A"] \arrow[dd, "V^\intercal"] &  & {{\begin{tikzpicture}
				\draw (-1.2,0) -- (1.2,0);
				\draw (0,-1.2) -- (0,1.2);
				\draw[-latex,blue,line width=1.5] (0,0) -- (1,1) node[right] {$\sigma_1e_1$};
				\draw[-latex,blue,line width=1.5] (0,0) -- (-0.5,0.5) node[above left] {$\sigma_2e_2$};
				\draw[lightblue,rotate=45] (0,0) ellipse (2cm and 1cm);
				\fill[fill=red] (1,1.6) circle (2pt) node[above] {$Ax$};
	\end{tikzpicture}}}                  \\
	&  &                     \\
	{{\begin{tikzpicture}
				\draw (-1.2,0) -- (1.2,0);
				\draw (0,-1.2) -- (0,1.2);
				\draw[lightblue] (0,0) circle (1);
				\draw[-latex,blue,line width=1.5] (0,0) -- (45:1) node[right] {$v_1$};
				\draw[-latex,blue,line width=1.5] (0,0) -- (135:1) node[left] {$v_2$};
	\end{tikzpicture}}} \arrow[rr, "\Sigma"]                      &  & {{\begin{tikzpicture}
				\draw (-1.2,0) -- (1.2,0);
				\draw (0,-1.2) -- (0,1.2);
				\draw[lightblue] (0,0) circle (1);
				\draw[-latex,blue,line width=1.5] (0,0) -- (1.5,0) node[below] {$\sigma_1e_1$};
				\draw[-latex,blue,line width=1.5] (0,0) -- (0,0.5) node[right] {$\sigma_2e_2$};
	\end{tikzpicture}}} \arrow[uu, "U"']
\end{tikzcd}
\end{center}
\[ \underbrace{\begin{bmatrix}
		a & b\\
		x & d
\end{bmatrix}}_A=\underbrace{\begin{bmatrix}
\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta
\end{bmatrix}}_U\cdot\underbrace{\begin{bmatrix}
\sigma_1 & 0\\
0 & \sigma_2
\end{bmatrix}}_{\Sigma}\cdot\underbrace{\begin{bmatrix}
\cos\phi & \sin\phi\\
-\sin\phi & \cos\phi
\end{bmatrix}}_{V^\intercal} \]
$V$ produce un cambio de base ortonormal.

$\Sigma$ define las dos direcciones principales y cuánto éstas se alargan o acortan.

$U$ es un nuevo cambio de base ortonormal que marca precisamente las direcciones principales.
\subsubsection{Los cuatro subespacios fundamentales de una matriz y la $SVD$}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Lema
\end{itemize}
Se tiene que $\nuc(A^\intercal A)=\nuc(A)$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
Sea $u\in\nuc(A^\intercal A)\longrightarrow A^\intercal Au=0$

Multiplicando por $u^\intercal$ \[ 0=u^\intercal A^\intercal Au=(Au)^\intercal Au=\|Au\|\longrightarrow Au=0\longrightarrow u\in\nuc(A) \]
Recíprocamente, si $Au=0\longrightarrow A^\intercal Au=0$.
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Proposición
\end{itemize}
Sea $A\in M_{m\times n}, \:n\le m$ y sea $r=\rg(A)$. Entonces:
\begin{enumerate}[label=\color{lightblue}\arabic*)]
	\item $\nuc(A)=<v_{r+1},\dots,v_n>$
	\item $\col(A)=<u_1,\dots,u_r>$
	\item $\fil(A)=\nuc(A)^\perp=<v_1,\dots,v_r>$
	\item $\nuc(A^\intercal)=\col(A)^\perp=<u_{r+1},\dots,u_m>$
\end{enumerate}
\begin{itemize}[label=\color{red}\textbullet, leftmargin=*]
	\item \color{lightblue}Demostración
\end{itemize}
Como $\{v_1,\dots,v_n\}$ es una base de vectores propios de $A^\intercal A$, por la factorización en valores propios se tiene que \[ V^\intercal A^\intercal AV=\mathrm{diag}[\lambda_1,\dots,\lambda_r,0,\dots,0] \]ya que $\rg(A)=\rg(U\Sigma V^\intercal)=\rg(\Sigma)$

Por el lema anterior $\{v_{r+1},\dots,v_n\}$ es una base de $\nuc(A)=\nuc(A^\intercal A)$. Nótese que $A^\intercal AV=\mathrm{diag}[\lambda_1,\dots,\lambda_r,0,\dots,0]\cdot V$.

Por la definición de los vectores $u_i=\dfrac{1}{\sigma_i}Av_i,\:1\le i\le r$, se tiene que $\col(A)=<u_1,\dots,u_r>$.

El resto se prueba teniendo en cuenta que \[ \begin{array}{l}
	\col(A)\oplus\nuc(A^\intercal)\longrightarrow\{u_{r+1},\dots,u_m\}\\
	\begin{array}{ccc}
		\fil(A) & \oplus & \nuc(A)\\
		\downarrow &  & \downarrow\\
		\{v_1,\dots,v_r\} & & \{v_{r+1},\dots,v_n\}
	\end{array}
\end{array} \]que dichos espacios son ortogonales dos a dos y que $U$ y $V$ son ortogonales.
