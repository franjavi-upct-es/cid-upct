\begin{center}
\large\textbf{\rc{Examen Convocatoria Enero 2023}}
\end{center}
\begin{enumerate}[label=\color{red}\textbf{\arabic*)}, leftmargin=*]
	\item \lb{Consideremos los números complejos \[ z_1=-1-\jmath\quad,\quad z_2=\sqrt{2}^{\frac{\pi}{4}\jmath}. \]Calcula $z_1+z_2,\,z_1\cdot z_2$ y $\dfrac{z_2}{z_1}$ y expresa el resultado en forma exponencial.}
	
	$\begin{array}{l}
	z_1=1+\jmath\\
	z_2=e^{\frac{3\pi}{4}\jmath}
	\end{array}\qquad$\begin{tikzpicture}[baseline=(current bounding box.center), scale=1.5]
	\draw[lightblue, dashed] (0,1) -| (1,0);
	\draw (-1.2,0) -- (1.2,0);
	\draw (0,-1.2) -- (0,1.2);
	\draw[lightblue] (0,0) circle (1);
	\draw[lightblue, -*, line width=1.2] (0,0) -- (1,1) node[above right] {$z_1=1+\jmath$};
	\draw[lightblue, -*, line width=1.2] (0,0) -- (135:1) node[above left] {$z_1=e^{\frac{3\pi}{4}\jmath}$};
	\end{tikzpicture}
	
	Para calcular $z_1+z_2$ escribimos $z_2$ en forma binómica:
	\[ \begin{array}{l}
	\begin{aligned}
	z_2=e^{\frac{3\pi}{4}\jmath}&=\cos\left(\dfrac{3\pi}{4}\right)+\jmath\cdot\sin\left(\dfrac{3\pi}{4}\right)\\
	&=-\dfrac{\sqrt{2}}{2}+\jmath\cdot\dfrac{\sqrt{2}}{2}\\
	&=-0.707+\jmath\cdot0.707
	\end{aligned}\\
	\begin{aligned}
	z_1+z_2&=(1+\jmath)+(-0.707+\jmath\cdot0.707)\\
	&=0.293+\jmath\cdot1.707
	\end{aligned}
	\end{array} \]
	Calculamos la forma exponencial de este número
	
	$\begin{array}{l}
	|z_1+z_2|=\sqrt{0.293^2+1.707^2}=1.732\\
	\theta=\arctan\dfrac{1.707}{0.293}=80.26^\circ=0.446\pi \text{ radianes}\\
	\bboxed{z_1+z_2=1.732\cdot e^{\jmath\cdot0.446\pi}}
	\end{array}$
	\item \lb{Sea $A$ una matriz. Explica con detalle en qué consiste la factorización en valores singulares $(SVD)$ de $A$. Por supuesto, se ha de explicar qué son los valores singulares y cómo se calculan las matrices que aparecen en dicha factorización. Pon también un ejemplo de aplicación de la factorización $SVD$ en Ciencia de Datos.}
	
	
	\item \lb{Responde a las siguientes preguntas:}
	\begin{enumerate}[label=\color{red}\alph*)]
		\item \db{Explica qué es una matriz ortogonal.}
		
		
		\item \db{Explica en qué consiste la factorización $QR$ de una matriz cuadrada $A$.}
		
		
		\item \db{¿Qué propiedad tiene que cumplir $A$ para que se pueda calcular su factorización $QR$?}
		
		El determinante de $A$ debe ser distinto de 0 $\bboxed{\det(A)\neq0}$
		\item \db{Razona si la siguiente afirmación es cierta o falsa: sea $A$ una matriz no singular. El sistema lineal $Ax=b$ siempre se puede resolver mediante factorización $LU$ y Cholesky. Además, siempre hemos de elegir el método de Cholesky porque su coste computacional es menor que el de la $LU$.}
		
		La afirmación es falsa porque aunque, en efecto, el coste computacional de Cholesky es menor que el de $LU$, en cambio no todas las matrices no singulares admiten una factorización Cholesky. Únicamente, las matrices simétricas y definidas positivas admiten una tal factorización de Cholesky.
	\end{enumerate}
	\item \lb{Consideremos la matriz \[ A=\begin{bmatrix}
	1 & 1 & 1 & 1\\
	1 & 2 & 3 & 4\\
	1 & 3 & 6 & 10\\
	1 & 4 & 10 & 20
	\end{bmatrix}. \]Calcula la factorización $LU$ de $A$ y utiliza dicha factorización para calcular el determinante de $A$. Por curiosidad, la matriz $L$ que aparece en la factorización anterior contiene el llamado \textit{triangulo de Pascal}, que se usa, por ejemplo, en Combinatoria.}
	
	$A=\begin{bmatrix}
		1 & 1 & 1 & 1\\
		1 & 2 & 3 & 4\\
		1 & 3 & 6 & 10\\
		1 & 4 & 10 & 20
		\end{bmatrix}\xrightarrow[F_4\to F_4-F_1]{\begin{array}{l}
		F_2\to F_2-F_1\\
		F_3\to F_3-F_1
		\end{array}}\begin{bmatrix}
		1 & 1 & 1 & 1 \\
		0 & 1 & 2 & 3 \\
		0 & 2 & 5 & 9 \\
		0 & 3 & 9 & 19
		\end{bmatrix}\xrightarrow[F_4\to F_3-3F_2]{F_3\to F_3-2F_2}\begin{bmatrix}
		1 & 1 & 1 & 1 \\
		0 & 1 & 2 & 3 \\
		0 & 0 & 1 & 3 \\
		0 & 0 & 3 & 10
		\end{bmatrix}\xrightarrow{F_4\to F_4-3F_3}\begin{bmatrix}
		1 & 1 & 1 & 1 \\
		0 & 1 & 2 & 3 \\
		0 & 0 & 1 & 3 \\
		0 & 0 & 0 & 1
		\end{bmatrix}=U$
		
	Nótese que $A$ es simétrica. Además, las entradas de la diagonal de $U$ son todas iguales a 1.
	
	Esto indica que $A$ también es definida positiva y que además $L=U^\intercal$ ya que en este caso particular las factorizaciones $LU$ y Cholesky coinciden.
	
	En cualquier caso, podemos calcular las entradas de $L$ mediante la fórmula. \[ l_{ij}=\dfrac{\text{entrada a eliminar en la fila $\imath$-ésima}}{\text{pivot de la fila $\jmath$}} \]Así:\\
	$\begin{array}{l}
	l_{21}=\dfrac{1}{1}=1\\
	l_{31}=\dfrac{1}{1}=1,\quad l_{32}=\dfrac{2}{1}=2\\
	l_{41}=\dfrac{1}{1}=1,\quad l_{42}=\dfrac{3}{1}=3,\quad l_{43}=\dfrac{3}{1}=3
	\end{array}$ \[ L=\begin{bmatrix}
	1 & 0 & 0 & 0\\
	1 & 1 & 0 & 0\\
	1 & 2 &1 & 0\\
	1 & 3 & 3 & 1 
	\end{bmatrix}\qquad\begin{aligned}
	\det(A)&=\det(LU)\\
	&=\det(L)\cdot\det(U)\\
	&=1\cdot 1=1
	\end{aligned} \]
	\item \lb{Consideremos la matriz $A=\begin{bmatrix}
	3 & 0\\
	4 & 5
	\end{bmatrix}$. Se pide:}
	\begin{enumerate}[label=\color{red}\alph*)]
		\item \db{Calcula los valores propios y una base ortonormal de vectores propios de la matriz $A^\intercal A$. Denotemos por $\lambda_1,\lambda_2$ dichos vectores y por $v_1,v_2$ dicha base ortornormal de vectores propios asociados a $\lambda_1 $ y $\lambda_2$, respectivamente. Llamaremos $V$ a la matriz cuyas columnas son, en este orden, $v_1$ y $v_2$.}
		
		$\begin{array}{l}
		A^\intercal A=\begin{bmatrix}
		3 & 4\\
		0 & 5
		\end{bmatrix}\cdot\begin{bmatrix}
		3 & 0\\
		4 & 5
		\end{bmatrix}=\begin{bmatrix}
		25 & 20\\
		20 & 25
		\end{bmatrix}\\
		P_{\lambda}(A^\intercal A)=\begin{vmatrix}
		25-\lambda & 20\\
		20 & 25-\lambda
		\end{vmatrix}=(25-\lambda)^2-400=625+\lambda^2-50\lambda-400\\
		\lambda^2-50\lambda+225=0\\
		\lambda=\dfrac{50\pm\sqrt{2500-900}}{2}=\dfrac{50\pm\sqrt{1600}}{2}=\dfrac{50\pm40}{2}=\left\langle\begin{array}{l}
		\lambda_1=45\\
		\lambda_2=5
		\end{array}\right.
		\end{array}$\\
		\begin{itemize}
		\item[$\bboxed{\lambda_1=45}$] $\mathrm{ker}(A^\intercal A-\lambda_1I_2)$
		
		$\begin{array}{l}
		\begin{bmatrix}
		25-45 & 20\\
		20 & 25-45
		\end{bmatrix}\cdot\begin{bmatrix}
		x_1\\
		x_2
		\end{bmatrix}=\begin{bmatrix}
		-20 & 20\\
		20 & -20
		\end{bmatrix}\cdot\begin{bmatrix}
		x_1\\
		x_2
		\end{bmatrix}=\begin{bmatrix}
		0\\
		0
		\end{bmatrix}\\
		\begin{aligned}
		\begin{rcases}
		-20x_1+20x_2=0\\
		20x_1-20x_2=0
		\end{rcases}x_2=\alpha&\longrightarrow x_1=x_2=\alpha\\
		&\longrightarrow(1,1)\\
		&\longrightarrow\|(1,1)\|=\sqrt{1^2+1^2}=\sqrt{2}\\
		&\longrightarrow\bboxed{v_1=\left(\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)}
		\end{aligned}
		\end{array}$
		\item[$\bboxed{\lambda_2=5}$] $\mathrm{ker}(A^\intercal A-\lambda_2I_2)$
		
		$\begin{array}{l}
		\begin{bmatrix}
		25-5 & 20\\
		20 & 25-5
		\end{bmatrix}\cdot\begin{bmatrix}
		x_1\\
		x_2
		\end{bmatrix}=\begin{bmatrix}
		20 & 20\\
		20 & 20
		\end{bmatrix}\cdot\begin{bmatrix}
		x_1\\
		x_2
		\end{bmatrix}=\begin{bmatrix}
		0\\
		0
		\end{bmatrix}\\
		\begin{rcases}
		20x_1+20x_2=0\\
		20x_1+20x_2=0
		\end{rcases}\longrightarrow x_1=-x_2\longrightarrow(-1,1)\longrightarrow\bboxed{x_2=\left(-\dfrac{1}{\sqrt{2}},\dfrac{1}{\sqrt{2}}\right)}
		\end{array}$\\
		
		\end{itemize}
		$V=\begin{bmatrix}
				\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}\\
				\dfrac{1}{\sqrt{2}} & \dfrac{1}{2}
				\end{bmatrix}$
		\item \db{Sean $\sigma_1=\sqrt{\lambda_1}$ y $\sigma_2=\sqrt{\lambda_2}$. Calcula los vectores $u_1=\dfrac{1}{\sigma_1}Av_1$ y $u_2=\dfrac{1}{\sigma_2}Av_2$ y comprueba que forman una base ortonormal de vectores propios asociados a $\lambda_1$ y $\lambda_2$ para la matriz $A\cdot A^{\intercal}$.}
		
		$\sigma_1=\sqrt{\lambda_1}=\sqrt{45},\quad\sigma_2=\sqrt{\lambda_2}=\sqrt{5}$
		
		$\begin{array}{l}
		u_1=\dfrac{1}{\sqrt{45}}\begin{bmatrix}
		3 & 0 \\
		4 & 5
		\end{bmatrix}\cdot\begin{bmatrix}
		\frac{1}{\sqrt{2}}\\
		\frac{1}{\sqrt{2}}
		\end{bmatrix}=\dfrac{1}{\sqrt{45}}\begin{bmatrix}
		\frac{3}{\sqrt{2}}\\
		\frac{4}{\sqrt{2}}+\frac{5}{\sqrt{2}}
		\end{bmatrix}=\bboxed{\begin{bmatrix}
		\frac{3}{\sqrt{90}}\\
		\frac{9}{\sqrt{90}}
		\end{bmatrix}}\\
		\|u_1\|=\sqrt{\dfrac{9}{90}}+\dfrac{81}{90}=\sqrt{1}=1\\
		u_2=\dfrac{1}{\sqrt{5}}\begin{bmatrix}
		2 & 0\\
		4 & 5
		\end{bmatrix}\cdot\begin{bmatrix}
		-\frac{1}{\sqrt{2}}\\
		\frac{1}{\sqrt{2}}
		\end{bmatrix}=\dfrac{1}{\sqrt{5}}\begin{bmatrix}
		-\frac{3}{\sqrt{2}}\\
		-\frac{4}{\sqrt{2}}+\frac{5}{\sqrt{2}}
		\end{bmatrix}=\bboxed{\begin{bmatrix}
		-\frac{3}{\sqrt{10}}\\
		\frac{1}{\sqrt{10}}
		\end{bmatrix}}\\
		\|u_2\|=\sqrt{\dfrac{9}{10}+\dfrac{1}{10}}=\sqrt{1}=1\\
		A\cdot A^\intercal=\begin{bmatrix}
		3 & 0\\
		4 & 5
		\end{bmatrix}\cdot\begin{bmatrix}
		3 & 4\\
		0 & 5
		\end{bmatrix}=\begin{bmatrix}
		9 & 12\\
		12 & 41
		\end{bmatrix}\\
		\begin{aligned}
		A\cdot A^\intercal u_1&=\begin{bmatrix}
		9 & 12\\
		12 & 41
		\end{bmatrix}\cdot\begin{bmatrix}
		\frac{3}{\sqrt{90}}\\
		\frac{9}{\sqrt{90}}
		\end{bmatrix}=\begin{bmatrix}
		\frac{27}{\sqrt{90}}+\frac{108}{\sqrt{90}}\\
		\frac{36}{\sqrt{90}}+\frac{369}{\sqrt{90}}
		\end{bmatrix}=\begin{bmatrix}
		\frac{135}{\sqrt{90}}\\
		\frac{405}{\sqrt{90}}
		\end{bmatrix}\\
		&=45\cdot\begin{bmatrix}
		\frac{3}{\sqrt{90}}\\
		\frac{9}{\sqrt{90}}
		\end{bmatrix};\qquad u_1\cdot u_2=\left(\dfrac{3}{\sqrt{90}},\dfrac{9}{\sqrt{90}}\right)\cdot\left(-\dfrac{3}{\sqrt{10}},\dfrac{1}{\sqrt{10}}\right)=0
		\end{aligned}\\
		A\cdot A^\intercal u_2=\begin{bmatrix}
		9 & 12\\
		12 & 41
		\end{bmatrix}\cdot\begin{bmatrix}
		-\frac{3}{\sqrt{10}}\\
		\frac{1}{\sqrt{10}}
		\end{bmatrix}=\begin{bmatrix}
		\frac{15}{\sqrt{10}}\\
		\frac{5}{\sqrt{10}}
		\end{bmatrix}=5\cdot\begin{bmatrix}
		\frac{3}{\sqrt{10}}\\
		\frac{1}{\sqrt{10}}
		\end{bmatrix}
		\end{array}$
		
		\item \db{Sean $U$ la matriz que tiene por columnas los vectores $u_1$ y $u_2$, en ese orden, y $\Sigma$ la matriz diagonal que tien en su diagonal los números $\sigma_1$ y $\sigma_2$. Comprueba que \[ A=\sigma_1u_1v_1^\intercal+\sigma_2u_2v_2^\intercal=U\Sigma V^{\intercal}. \]}
		
		$U=\begin{bmatrix}
		\dfrac{3}{\sqrt{90}} & -\dfrac{3}{\sqrt{10}}\\
		\dfrac{9}{\sqrt{90}} & \dfrac{1}{\sqrt{10}}
		\end{bmatrix}\qquad\Sigma=\begin{bmatrix}
		\sqrt{45} & 0\\
		0 & \sqrt{45}
		\end{bmatrix}$
		
		$\begin{aligned}
		\sigma_1u_1v_1^\intercal+\sigma_2u_2v_2^\intercal&=\sqrt{45}\begin{bmatrix}
		\dfrac{3}{\sqrt{90}} & \dfrac{9}{\sqrt{90}}
		\end{bmatrix}\cdot\begin{bmatrix}
		\frac{1}{\sqrt{2}}\\
		\frac{1}{\sqrt{2}}
		\end{bmatrix}+\sqrt{5}\cdot\begin{bmatrix}
		-\dfrac{3}{\sqrt{10}} & \dfrac{1}{\sqrt{10}}
		\end{bmatrix}\cdot\begin{bmatrix}
		-\frac{1}{\sqrt{2}}\\
		\frac{1}{\sqrt{2}}
		\end{bmatrix}\\
		&=\sqrt{45}\cdot\begin{bmatrix}
		\dfrac{3}{\sqrt{180}} & \dfrac{3}{\sqrt{180}}\\
		\dfrac{9}{\sqrt{180}} & \dfrac{9}{\sqrt{180}}
		\end{bmatrix}+\sqrt{5}\cdot\begin{bmatrix}
		\dfrac{3}{\sqrt{20}} & -\dfrac{3}{\sqrt{20}}\\
		-\dfrac{1}{\sqrt{20}} & \dfrac{1}{\sqrt{20}}
		\end{bmatrix}\\
		&=\begin{bmatrix}
		\dfrac{3}{2} & \dfrac{3}{2}\\
		\dfrac{9}{2} & \dfrac{9}{2}
		\end{bmatrix}+\begin{bmatrix}
		\dfrac{3}{2} & -\dfrac{3}{2}\\
		-\dfrac{1}{2} & \dfrac{1}{2}
		\end{bmatrix}=\begin{bmatrix}
		3 & 0\\
		4 & 5
		\end{bmatrix}
		\end{aligned}$
		
		$\begin{aligned}
		U\Sigma V^\intercal&=\begin{bmatrix}
		\dfrac{3}{\sqrt{90}} & -\dfrac{3}{\sqrt{10}}\\
		\dfrac{9}{\sqrt{90}} & \dfrac{1}{\sqrt{10}}
		\end{bmatrix}\cdot\begin{bmatrix}
		\sqrt{45} & 0\\
		0 & \sqrt{45}
		\end{bmatrix}\cdot\begin{bmatrix}
						\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}\\
						-\dfrac{1}{\sqrt{2}} & \dfrac{1}{2}
						\end{bmatrix}\\
			&=\begin{bmatrix}
					\dfrac{3}{\sqrt{90}} & -\dfrac{3}{\sqrt{10}}\\
					\dfrac{9}{\sqrt{90}} & \dfrac{1}{\sqrt{10}}
					\end{bmatrix}\cdot\begin{bmatrix}
											\dfrac{\sqrt{45}}{\sqrt{2}} & \dfrac{\sqrt{45}}{\sqrt{2}}\\
											-\dfrac{\sqrt{5}}{\sqrt{2}} & \dfrac{\sqrt{5}}{2}
											\end{bmatrix}\\
			&=\begin{bmatrix}
			\dfrac{3\sqrt{45}}{\sqrt{45}\sqrt{2}\sqrt{2}}+\dfrac{3\sqrt{5}}{\sqrt{10}\sqrt{2}} & \dfrac{3\sqrt{45}}{\sqrt{45}\sqrt{2}\sqrt{2}}-\dfrac{3\sqrt{5}}{\sqrt{5}\sqrt{2}\sqrt{2}}\\
			\dfrac{9\sqrt{45}}{\sqrt{45}\sqrt{2}\sqrt{2}}-\dfrac{\sqrt{5}}{\sqrt{5}\sqrt{2}\sqrt{2}} & \dfrac{9\sqrt{45}}{\sqrt{45}\sqrt{2}\sqrt{2}}+\dfrac{\sqrt{5}}{\sqrt{5}\sqrt{2}\sqrt{2}}
			\end{bmatrix}\\
			&=\begin{bmatrix}
			3 & 0\\
			4 & 5
			\end{bmatrix}
		\end{aligned}$
		\item \db{¿Cómo se llaman los números $\sigma_1$ y $\sigma_2$? ¿Cómo se llama la factorización de $A$ que hemos hecho en el apartado anterior?}
		
		$\sigma_1$ y $\sigma_2$ se llaman valores singulares, y la factorización anterior se llama factorización $SVD$.
	\end{enumerate}
	\item \lb{Consideremos la matriz $A=\begin{bmatrix}
	1 & 1 & 1\\
	1 & 2 & 3
	\end{bmatrix}$.}
	\begin{enumerate}[label=\color{red}\alph*)]
		\item \db{Calcula la dimensión y una base de los cuatro subespacios fundamentales de $A$, es decir, $\fil(A),\linebreak\col(A),\nuc(A)$ y $\nuc(A^\intercal)$.}
		
		$\fil(A)=<(1,1,1),(1,2,3)>$
		
		$(1,1,1)$ y $(1,2,3)$ son linealmente independientes pues uno no es múltiplo del otro.
		
		Por tanto, $\dimf(A)=2$\\
		$\nuc(A)=\left\{x=(x_1,x_2,x_3),Ax=0\right\}$\\
		$\begin{array}{l}
		\begin{bmatrix}
		1 & 1 & 1\\
		1 & 2 & 3
		\end{bmatrix}\cdot\begin{bmatrix}
		x_1\\
		x_2\\
		x_3
		\end{bmatrix}=\begin{bmatrix}
		x_1+x_2+x_3\\
		x_1+2x_2+3x_3
		\end{bmatrix}=\begin{bmatrix}
		0\\
		0
		\end{bmatrix}\\
		\begin{rcases}
		x_1+x_2+x_3=0\\
		x_1+2x_2+3x_3=0
		\end{rcases}
		\end{array}$
		
		Como $\rg(A)=2$, tomamos 1 parámetro. Por ejemplo $x_3=\alpha$. Nos queda:\[ \begin{rcases}
		x_1+x_2=-\alpha\\
		x_1+2x_2=-3\alpha
		\end{rcases}\quad\begin{array}{l}
		x_2=-3\alpha-(-\alpha)=-2\alpha\\
		x_1=-x_2-\alpha=2\alpha-\alpha=\alpha
		\end{array} \]
		$\nuc(A)=<(1,-2,1)>$ donde hemos tomado $\alpha=1$.\\
		$\R^3=\fil(A)\oplus\nuc(A)$\\
		$\col(A)=<(1,1),(1,2),(1,3)>$\\
		
		\item \db{En general, ¿qué relaciones puedes dar entre los subespacios anteriores?}
		
		Tres vectores en $\R^2$ siempre son linealmente dependientes.\\
				Sin embargo $\{(1,1),(1,2)\}$ son linealmente independientes. Por tanto, $\dimc(A)=2$ y una base de $\col(A)$ es $\{(1,1),(1,2)\}$
				
				Como $\R^2=\col(A)\oplus\nuc(A^\intercal)$ se tiene que $\nuc(A^\intercal)=\{0\}$
	\end{enumerate}
\end{enumerate}
