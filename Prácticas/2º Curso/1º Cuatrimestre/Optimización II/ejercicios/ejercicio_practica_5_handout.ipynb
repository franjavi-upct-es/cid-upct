{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de la Práctica 5: Automatic Differentation (AD) in Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En esta ejercicio nos centraremos en la implementación de AD en Pytorch. En concreto, en el submódulo [TORCH.AUTOGRAD](https://pytorch.org/docs/stable/autograd.html) \n",
    "\n",
    "Si no tienes instalado Pytorch, lo primero que has de hacer es instalarlo. Una forma sencilla de hacerlo es en el **Powershell Prompt** de **Anaconda**, escribiendo:\n",
    "\n",
    "**conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AD en Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga Pytorch escribiendo \n",
    "\n",
    "**import torch**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:39:39.094106Z",
     "start_time": "2024-10-15T09:39:39.084426Z"
    }
   },
   "source": [
    "# Completar aquí\n",
    "import torch as pt\n",
    "# --------------------\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este ejercicio es hacer los mismos cálculos que hicimos en la práctica de AD en TensorFlow pero ahora en Pytorch\n",
    "\n",
    "Veamos un primer ejemplo elemental: derivada de la función $y = x^2$ en $x=3$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:39:39.141620Z",
     "start_time": "2024-10-15T09:39:39.120675Z"
    }
   },
   "source": [
    "# Completar aquí\n",
    "x = pt.tensor(3.0, requires_grad=True)\n",
    "\"\"\"\n",
    "El argumento 'requires_grad=True' indica que se quiere calcular la derivada de la función con respecto a esta variable.\n",
    "\"\"\"\n",
    "\n",
    "# Definir la función\n",
    "y = x**2\n",
    "\n",
    "# Calcular la derivada\n",
    "y.backward()\n",
    "\n",
    "# Obtener el valor de la derivada\n",
    "print(f\"Derivada de y respecto de x = {x.grad.item()}\")\n",
    "# --------------------\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada de y respecto de x = 6.0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos ahora una función de dos variables\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2(x_1 + x_2)\n",
    "$$\n",
    "\n",
    "Vamos a calcular su gradiente en $x_1 = 2$, $x_2 = 3$. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:39:39.204309Z",
     "start_time": "2024-10-15T09:39:39.186211Z"
    }
   },
   "source": [
    "# Completar aquí\n",
    "# Definir las variables\n",
    "x_1 = pt.tensor(2.0, requires_grad=True)\n",
    "x_2 = pt.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Definir la función f(x1,x2)\n",
    "f = x_1**2 * (x_1 + x_2)\n",
    "\n",
    "# Calcular el gradiente\n",
    "f.backward()\n",
    "\n",
    "# Obtener el vector del gradiente en x_1 y x_2\n",
    "gradiente_x_1 = x_1.grad.item()\n",
    "gradiente_x_2 = x_2.grad.item()\n",
    "\n",
    "# Convertir en un tupla para mejor reprensentación\n",
    "gradiente_f = (gradiente_x_1, gradiente_x_2)\n",
    "\n",
    "print(f\"Gradiente de f(x1,x2) = {gradiente_f}\")\n",
    "# --------------------\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradiente de f(x1,x2) = (24.0, 4.0)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos ahora su matriz Hessiana en el mismo punto y los valores propios de dicha matriz"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:39:39.267783Z",
     "start_time": "2024-10-15T09:39:39.237859Z"
    }
   },
   "source": [
    "from torch.autograd.functional import hessian\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**2 * (x[0] + x[1])\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "hess_f = hessian(f, x)\n",
    "print(\"Hessian matrix:\\n\", hess_f)\n",
    "\n",
    "from numpy.linalg import eig\n",
    "\n",
    "eigenvalues, _ = eig(hess_f)\n",
    "\n",
    "print(f\"valores propios de la matriz hessiana de f en (2, 3) = \\n {eigenvalues}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian matrix:\n",
      " tensor([[18.,  4.],\n",
      "        [ 4.,  0.]])\n",
      "valores propios de la matriz hessiana de f en (2, 3) = \n",
      " [18.848858  -0.8488578]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora un modelo más elaborado. Consideremos la función vectorial \n",
    "$$\n",
    "y = x * w  + b\n",
    "$$\n",
    "donde $x$ es un vector fila de $4$ componentes, $w$ es una matriz $4\\times 3$ y $b$ un vector columna de $3$ componentes.\n",
    "\n",
    "Define la función anterior en **pytorch** y asigna los siguientes valores a la variables:\n",
    "\n",
    "1)  $x = [[1., 2., 3., 4.]]$\n",
    "2)  $w$ valores aleatorios\n",
    "3)  $b$ unos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:39:39.376111Z",
     "start_time": "2024-10-15T09:39:39.354279Z"
    }
   },
   "source": [
    "# Completar aquí\n",
    "x = pt.tensor([[1., 2., 3., 4.]], requires_grad=True)\n",
    "w = pt.randn((4, 3), requires_grad=True)\n",
    "b = pt.ones((3, ), requires_grad=True)\n",
    "\n",
    "# Definir la función y = x * w + b\n",
    "y = pt.matmul(x, w) + b # Multiplicación de matrices y suma del bias\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"y = {y}\")\n",
    "# --------------------\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[1., 2., 3., 4.]], requires_grad=True)\n",
      "w = tensor([[ 0.3106, -0.5961, -2.3737],\n",
      "        [-0.1981,  0.1905, -0.2271],\n",
      "        [-0.7525,  0.1824,  0.3437],\n",
      "        [-2.1758, -1.1129, -0.2312]], requires_grad=True)\n",
      "b = tensor([1., 1., 1.], requires_grad=True)\n",
      "y = tensor([[-10.0460,  -3.1197,  -1.7214]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de la función $y = x * w + b$ consideramos la función de pérdida\n",
    "\n",
    "$$\n",
    "\\text{loss } = \\frac{1}{3} \\sum_{j=1}^3 (y_j - (y_{\\text{label}})_j)^2\n",
    "$$\n",
    "donde $y_{\\text{label}} = [[1., 2., 3.]]$\n",
    "\n",
    "Define la función loss en pytorch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:42:01.771292Z",
     "start_time": "2024-10-15T09:42:01.723953Z"
    }
   },
   "source": [
    "# Completar aquí\n",
    "# Definir la etiqueta y_label\n",
    "y_label = pt.tensor([[1., 2., 3.]], requires_grad=True)\n",
    "print(f\"y_label = {y_label}\")\n",
    "\n",
    "loss = pt.mean((y - y_label) ** 2)\n",
    "print(f\"loss = {loss}\")\n",
    "# --------------------\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_label = tensor([[1., 2., 3.]], requires_grad=True)\n",
      "loss = 56.839298248291016\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula en pytorch los gradientes de la función loss, primero respecto a $y$ y después respecto a $x$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:49:52.709210Z",
     "start_time": "2024-10-15T09:49:52.684680Z"
    }
   },
   "source": [
    "# Completar aquí\n",
    "# Retener los gradientes de y\n",
    "y.retain_grad()\n",
    "\n",
    "# Recalcular la pérdida para hacer backward\n",
    "loss = pt.mean((y - y_label) ** 2)\n",
    "\n",
    "# Limpiar los gradientes de x e y\n",
    "x.grad.zero_()\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "# Calcular el backward\n",
    "loss.backward(retain_graph=True) # El parámetro permite calcular varios gradientes\n",
    "\n",
    "# Obtener los gradientes respecto a 'y' y a 'x'\n",
    "gradiente_y = y.grad.clone()\n",
    "gradiente_x = x.grad.clone()\n",
    "\n",
    "gradiente_loss = (gradiente_y, gradiente_x)\n",
    "print(f\"gradiente_loss = {gradiente_loss}\")\n",
    "# --------------------\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradiente_loss = (tensor([[-36.8201, -17.0657, -15.7380]]), tensor([[ 7.2183,  1.5234,  3.8368, 20.5487]]))\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
