---
title: "Contrastes de hipótesis"
author:
  - name: Félix Belzunce
    affiliations: Universidad de Murcia
  - name:  María del Carmen Bueso, Pilar Sanmartín
    affiliations: Universidad Politécnica de Cartagena
format: html
editor: visual
title-block-banner: true
self-contained: true
---

# Introducción

En esta práctica veremos el uso de `R` para realizar inferencia a partir de una m.a.s. de una población. Distinguiremos primero el caso de datos que provengan de una población normal o lognormal y terminaremos estudiando la inferencia para la probabilidad o proporción de un suceso en una población. Las funciones que usaremos serán las siguientes:

-   `shapiro.test`

-   `t.test`

-   `binom.test`

-   `prop.test`

# Inferencia para una muestra de una población normal o lognormal

En un gran número de situaciones, los datos suelen provenir de poblaciones con distribución normal o lognormal. La forma en que hay que proceder en cada caso es distinta, por lo que el primer paso es identificar cuál es el modelo de distribución, normal o lognormal, que se ajusta a la población de la que proceden los datos. Para el estudio en el caso de normalidad de la población vamos a hacer uso del siguiente ejemplo.

**Ejemplo 1:** Los siguientes datos se corresponden con la concentración de fósforo (P) para 20 muestras de aguas residuales:

```{r}
P<-c(1.8, 2.2, 2.1, 2.3, 2.1, 2.2, 2.1, 2.1, 1.8, 1.9, 2.4, 2.0, 1.9, 1.9, 2.2, 2.3, 
     2.2, 2.3, 2.1, 2.2)
```

En este caso la variable a considerar es P=Concentración de fósforo. El primer paso será contrastar si la población sigue una distribución normal o no.

En esta situación partimos de una variable aleatoria $X$ continua y una m.a.s. de tamaño $n$ de $X$, y nos planteamos el contraste de las hipótesis $$
\begin{array}{l}
H_0:X\text{ sigue una distribuci\'on normal} \\ H_1:X\text{ no
sigue una distribuci\'on normal}
\end{array}
$$

Para ello utilizaremos primero una herramienta gráfica para explorar la normalidad de los datos, la **gráfica Q-Q**, y después el **contraste de Shapiro-Wilk**.

La herramienta gráfica es el gráfico Q-Q que ya vimos como parte de la salida de gráficos al realizar el análisis de una distribución mediante el paquete `fitdistrplus`. En el caso de la distribución normal este gráfico se obtiene coomo vemos en el siguiente ejemplo.

**Ejemplo 1 (continuación):** Para los datos del ejemplo, esta gráfica se obtiene mediante

```{r, fig.align = 'center'}

# completar aquí



# Fin de completar aquí

```

La gráfica anterior sugiere la normalidad de los datos. Contrastaremos esa normalidad mediante el contraste de Shapiro-Wilk que describimos a continuación.

Veamos ahora el contraste de Shapiro-Wilk.

<u>**Contraste Shapiro-Wilk**</u>

**Ejemplo 1 (continuación):** Para los datos del ejemplo el contraste se obtiene mediante

```{r, fig.align = 'center'}

# completar aquí



# Fin de completar aquí

```

Como resultado del contraste obtenemos el valor del estadístico de contraste $W =$ `r shapiro.test(P)$statistic`, y como $p-valor=$ `r shapiro.test(P)$p.value`. De donde concluimos, al ser $p-valor=$ `r shapiro.test(P)$p.value` $> \alpha=$ 0.05 o 0.01, que aceptamos la hipótesis nula $H_0$, es decir $P$ sigue una distribución normal.

Una vez que hemos identificado que tenemos una población $X$ con distribución normal, es decir $X\sim N(\mu ,\sigma ^2)$, podemos estimar los parámetros puntualmente mediante los siguientes estimadores:

-   El estadístico $\overline{X}$ es el E.I.M.V. de $\mu$.

-   El estadístico $S_X^2$ es E.I. de $\sigma ^2$.

Tenemos como **intervalo de confianza al nivel** $(1-\alpha )\times 100$ para $\mu$, el intervalo $$
\left( \overline{X}-t_{1-\frac \alpha 2}\frac{S_X}{\sqrt{n}},\overline{X}%
+t_{1-\frac \alpha 2}\frac{S_X}{\sqrt{n}}\right) ,
$$ donde $t_p$, verifica que $P[T\leq t_p]=p$, siendo $T\sim t_{n-1}$ un variable con distribución t de Student con $n-1$ grados de libertad.

Podemos además a través del contraste de la t de Student hacer contrastes sobre la media de la población.

<u>**Contraste de la t de Student**</u>

**Ejemplo 1 (continuación):** Tomamos los datos del Ejemplo 1. Las estimaciones $\overline{X}$ y $S^2$ las obtenemos mediante

```{r}
mean(P)
```

y

```{r}
var(P)
```

Tanto el intervalo como el contraste de la t de Student en una distribución normal lo podemos realizar mediante el función `t.test`.

Veamos la realización de un contraste. Volviendo al ejemplo supongamos que queremos contrastar las hipótesis de que la media de $P$, la denotamos por $\mu=E[P]$, es mayor o igual que 2 con un nivel de significación $\alpha=$ 0.05. Es decir vamos a contrastar las hipótesis\
$$
H_0:\mu \geq 2
$$ frente a

$$H_1:\mu<2.
$$

```{r, fig.align = 'center'}

# completar aquí



# Fin de completar aquí

```

De donde concluimos que al ser el $p-valor=$ `r t.test(P, y = NULL, alternative = "less", mu = 2)$p.value` mayor que el nivel de significación $\alpha=$ 0.05 aceptamos la hipótesis nula, es decir, que la media de concentración de fósforo es mayor o igual que 2. El valor del estadístico de prueba viene dado por $T=$ `r t.test(P, y = NULL, alternative = "less", mu = 2)$statistic`.

**Observación:** Otras hipótesis alternativas se pueden obtener indicando `alternative = ''greater''`, con lo que la hipótesis alternativa será "mayor que'' ($\mu > \mu_0$). Si no especificamos nada, la hipótesis alternativa es "distinto que'' ($\mu \neq \mu_0$).

Para la obtención de un intervalo de confianza al nivel, por ejemplo al 99%, escribimos

```{r, fig.align = 'center'}

# completar aquí



# Fin de completar aquí

```

Que nos devuelve como intervalo ( `r t.test(P, y = NULL, conf.level = 0.99)$conf.int` ).

Cuando la variable $X$ no sigue una distribución normal, lo usual es probar otra distribución alternativa. Dependiendo del contexto existen otros modelos alternativos. Un modelo habitual es la distribución lognomal. Veamos un caso donde se produce esta situación.

**Ejemplo 2**: Los siguientes datos se corresponden con la concentración de Tetraclorobenzeno (TcCB) (en partes por $10^9$) tomadas en una determinada zona:

```{r}
tccb <- c(0.22, 0.33, 0.42, 0.51, 0.60, 0.76, 1.13, 0.23, 0.34, 0.43, 0.52, 0.62,
          0.79, 1.14, 0.26, 0.35, 0.45, 0.54, 0.63, 0.81, 1.14, 0.27, 0.38, 0.46,
          0.56, 0.67, 0.82, 1.20, 0.28, 0.39, 0.48, 0.56, 0.69, 0.84, 1.33, 0.28, 
          0.39, 0.50, 0.57, 0.72, 0.89, 0.29, 0.42, 0.50, 0.57, 0.74, 1.11)
```

Si realizamos el contraste de normalidad de los datos anteriores tenemos

```{r}

# completar aquí



# Fin de completar aquí

```

Donde observamos que se rechaza la hipótesis nula de normalidad. Nos planteamos entonces si los datos siguen una distribución lognormal.

**Ejemplo 2 (continuación):** En nuestro ejemplo tendríamos primero que hacer la transformación

```{r}
log.tccb <- log(tccb)
```

Y para la gráfica Q-Q y el contraste tendríamos

\`

```{r}

# completar aquí



# Fin de completar aquí

```

y el test de Shapiro-Wilk:

```{r}

# completar aquí



# Fin de completar aquí

```

Con los datos transformados podemos igual que en el caso anterior de la distribución normal, estimar y hacer contrastes sobre el parámetro $\mu$ y estimar el parámetro $\sigma^2$. Estos valores no son la media y la varianza de la variable de partida, en este caso la variable TcCB, si no de los datos transformados. Si queremos obtener estimaciones de la media y de la varianza de la variable TcCB tendríamos que sustituir las estimaciones de $\mu$ y $\sigma^2$ en las formulas de la media y la varianza que son

$$
E[TcCB]=e^{\mu + \frac{\sigma^2}{2}}
$$ y $$
Var[TcCB]=\left(e^{\sigma^2}-1\right)e^{2\mu + \sigma^2}
$$

**Ejemplo 2 (continuación):** Por ejemplo las estimaciones de $\mu$ y $\sigma^2$ de los datos transformados son $\overline{log(\text{TcCB})}=$`r mean(log.tccb)` y $S^2_{\text{log(TcCB)}}$=`r var(log.tccb)`, y por lo tanto las estimaciones de $E[TcCB]$ y $Var[TcCB]$ vienen dadas por

```{r}
exp( mean(log.tccb) + var(log.tccb)/2)
```

y

```{r}
(exp(var(log.tccb))-1)*exp(2 * mean(log.tccb) + var(log.tccb))
```

**Observación:** En general, si la variable $X$ no sigue una distribución normal, el intervalo y los contrastes anteriores son válidos si $n\geq 30$.

# Inferencia para una proporción

Veamos entonces con un ejemplo los cálculos con `R`. Para ello trabajamos con el siguiente ejemplo.

**Ejemplo 3:** En un estudio sobre el método de vacunación de una enfermedad se procede a la vacunación de un grupo de personas, estudiándose el número de individuos en los que la vacunación ha sido exitosa. Los datos obtenidos fueron los siguientes:

Nº de personas vacunadas: 228.\
Nº de éxitos: 223.

Si los datos están introducidos en forma de 0's y 1's la proporción muestral es simplemente la media muestral de los datos y ya hemos visto antes como obtenerla.

Para los contrastes y los intervalos, distinguiremos entre el método exacto y el método aproximado. En el primer caso trabajaremos con la función `binom.test` y en el segundo caso utilizamos `prop.test`.

Seguimos con el ejemplo.

**Ejemplo 3 (continuación):** Se quiere contrastar con un nivel de significación $\alpha=$ 0.01 si la proporción de éxitos es superior al 95%. Es decir se quiere contrastar las hipótesis $$
H_0:p \geq \text{0.95}
$$ $$H_1:p < \text{0.95}
$$ siendo $p=P(\text{Éxito})$.

El método exacto lo obtenemos mediante

```{r}

# completar aquí



# Fin de completar aquí

```

donde aceptamos la hipótesis nula al ser $p-valor=$ `r binom.test(223, 228, p = 0.95, alternative = "less")$p.value` \> 0.01.

Obtenemos un intervalo exacto mediante

```{r}

# completar aquí



# Fin de completar aquí

```

Para el método aproximado utilizamos

```{r}
prop.test(223, 228, p = 0.95, alternative = "less")
```

donde aceptamos la hipótesis nula al ser $p-valor=$ `r prop.test(223, 228, p = 0.95, alternative = "less")$p.value` \> 0.01 \$.

Obtenemos el intervalo de confianza aproximado mediante

```{r}
prop.test(223, 228, conf.level = 0.95)
```

# Inferencia para diferencias de medias de dos poblaciones

Veamos con un ejemplo el desarollo anterior.

**Ejemplo 4**: Un investigador desea evaluar el rendimiento de dos variedades híbridas de maíz, A y B. El terreno del que dispone para su experimento es bastante homogéneo. Selecciona 20 parcelas en el campo y al azar, siembra 10 parcelas con la variedad A y 10 parcelas con la variedad B. Los resultados en kg por parcela para la variedad A y la variedad B, respectivamente, fueron:

```{r}
rend.varA <- c(9.8, 8.1, 9.2, 10.1, 8.4, 8.8, 9.6, 7.4, 8.2, 10.4)

rend.varB <- c(9.3, 13.2, 12.2, 11.6, 10.0, 9.2, 10.4, 13.6, 10.5, 11.2)
```

Por la forma en que se han tomado los datos son independientes. El objetivo es comparar los valores medios de rendimiento de las variables de las que proceden los datos, para detectar diferencias entre ambas variables. Tenemos entonces dos variables:

-   $X_A$ = Rendimiento de la variedad A (en kg)
-   $X_B$ = Rendimiento de la variedad B (en kg)

Una primera aproximación al problema la podemos dar a partir de los diagramas de caja de los datos anteriores:

```{r}

# completar aquí



# Fin de completar aquí

```

Los diagramas nos muestran que posiblemente los rendimientos son distintos. Vamos a analizar esa diferencia a partir de la media de las variables.

Nuestro primer paso es analizar la normalidad de las dos variables a partir de los datos de cada una de ellas. Se realiza en este caso del contraste de Shapiro-Wilk, para cada muestra

-   primera muestra

```{r}

# completar aquí



# Fin de completar aquí

```

-   segunda muestra

```{r}

# completar aquí



# Fin de completar aquí

```

En ambos casos aceptamos la hipótesis de normalidad de las dos variables. Por lo tanto consideramos que $X_A\sim N(\mu_A,\sigma_A^2)$ y $X_B\sim N(\mu_B,\sigma_B^2)$. Señalar que estos resultados están de acuerdo con el hecho de que los diagramas de caja sean simétricos. Para poder comparar las medias debemos saber primero si las varianzas anteriores son iguales o distintas.

Realizamos primero el contraste de igualdad de varianzas mediante el contraste de la F de Snedecor:

$$
H_0:\sigma^2 _1=\sigma^2 _2
$$ $$
H_1:\sigma^2 _1\neq \sigma^2 _2
$$

Realizamos el contraste anterior mediante

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
var.test(rend.varA, rend.varB)
# fin de completar aquí
```

y obtenemos que el estadístico de contraste toma el valor $F$=`r var.test(rend.varA, rend.varB)$statistic` y $p-valor$=`r var.test(rend.varA, rend.varB)$p.value`. Como `r var.test(rend.varA, rend.varB)$p.value`$>\alpha$, con $\alpha =$ 0.05 o 0.01 aceptamos $H_0$ y por tanto las varianzas son iguales, es decir $\sigma_A^2=\sigma_B^2$.

Queremos saber ahora si los promedios de rendimiento son iguales os distintos. Realizamos entonces el contraste de la t de Student para las hipótesis: $$
\begin{array}{l}
H_0:\mu _1=\mu _2 \\ H_1:\mu _1\neq \mu _2
\end{array}
$$

Lo realizamos mediante

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
t.test(rend.varA, rend.varB, alternative = "two.sided", var.equal = TRUE)
# fin de completar aquí
```

Obteniendo que $t=$ `r t.test(rend.varA, rend.varB, alternative = "two.sided", var.equal=TRUE)$statistic` y $p-valor=$ `r t.test(rend.varA, rend.varB, alternative = "two.sided", var.equal = TRUE)$p.value` y dado que `r t.test(rend.varA, rend.varB, alternative = "two.sided", var.equal = TRUE)$p.value` $<\alpha$, con $\alpha=$ 0.05 o 0.01 aceptamos $H_1$ y por tanto los promedios de rendimiento son distintos.

**Observación:** Las hipótesis alternativas $H_1:\mu_1 < \mu_2$ y $H_1:\mu_1 > \mu_2$ se indican en `alternative` con `less` y `greater`, respectivamente. En el caso de que las varianzas sean distintas debemos indicarlo con `var.equal = FALSE`.

Veamos ahora la obtención del intervalo para la diferencia de medias.

**Ejemplo 4 (continuación):** Para obtener un intervalo de confianza para la diferencia de medias lo hacemos mediante

```{r}
t.test(rend.varA, rend.varB, conf.level = 0.99, var.equal = TRUE)
```

donde obtenemos que el intervalo de confianza al nivel $99\%$ para $\mu_A-\mu_B$ viene dado por $$(-3.7723129,-0.4676871).$$

Del intervalo obtenemos la información de que, además de ser los promedios distintos como hemos visto en el contraste, el promedio de la variedad A es menor que el promedio de la variedad B.

Veamos ahora una serie observaciones importantes.

**Observaciones:**

-   

    1.  Hemos supuesto a lo largo de todo el desarrollo anterior que las variables son normales. Por tanto tendremos que verificar esta suposición. Esto lo hemos hecho mediante el contraste de Shapiro-Wilk visto anteriormente. Tendremos que realizar un contraste por cada muestra.

-   

    2.  Si las variables no siguen una distribución normal, entonces consideraremos como alternativa la distribución lognormal para las dos variables. Si las dos variables son lognormales podremos utilizar lo anterior para realizar inferencia sobre las medias de los datos transformados.

-   

    3.  Si no nos encontramos en el caso de normalidad o de lognormalidad de las dos variables entonces tenemos dos posibilidades:

a)  Si los tamaños de muestra $n_1$, $n_2$ son mayores que 30 podemos aplicar tanto el intervalo como el contraste de la t de Student para estudiar las medias de las dos variables.

b)  Si alguno de los tamaños de muestra es menor que 30 entonces aplicaremos el contraste de Mann-Whitney que describimos a continuación.

Como solución en el caso de no poder utilizar el contrate de la t de Student en ninguna de sus formas podemos comparar dos variables independientes en términos de sus medianas mediante el **contraste de Mann-Whitney para muestras independientes**.

**Ejemplo 4 (continuación):** Vamos a plantear la comparación de las variables buscando si hay diferencias entre las medianas poblacionales. Por ejemplo vamos a contrastar como hipótesis nula que $H_0:M_A =  M_B$, siendo $M_A$ la mediana de rendimiento para la variedad A y $M_B$ la mediana a mediana de rendimiento para la variedad B. Realizamos el contraste mediante

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
wilcox.test(rend.varA, rend.varB, alternative = "two.sided")
# Fin de completar aquí
```

Donde el estadístico de contraste tomar el valor $W=$ `r wilcox.test(rend.varA, rend.varB, alternative = "two.sided")$statistic` y el $p$-valor=`r wilcox.test(rend.varA, rend.varB, alternative = "two.sided")$p.value` por lo que rechazamos al hipótesis nula y por tanto no presentan medianas iguales.

Observar que en el caso de normalidad las medias y medianas poblacionales coinciden por lo que, como ocurre en este caso en que las variables son normales, es de esperar que ambos contrastes coincidan. También hay que señalar que la mediana muestral es una estimación de la mediana poblacional y por tanto el resultado del contraste en este ejemplo reafirma la evidencia que hemos observado en los diagrama de caja sobre la diferencia de las medias.

## Caso de muestras pareadas

**Ejemplo 5:** En este ejemplo vamos a utilizar uno de los conjuntos de datos que usó William Gosset para ilustrar el desarrollo del contraste de la t de Student. Este estudio pretende mostrar los efectos de isómeros ópticos de *hiosciamina hidrobromoidal* para la mejora del sueño. Se hicieron mediciones de sueño para 10 pacientes sin ningún tratamiento, y posteriormente se hicieron mediciones después del tratamiento con *D. hiosciamina hidrobromoidal* y después del tratamiento con *L. hiosciamina hidrobromoidal*. Los datos que se anotaron fue la ganancia en promedio de horas de sueño en cada paciente y para cada tratamiento. Los datos que se obtuvieron fueron los siguientes:

```{r}
sleep.D <- c(0.7, -1.6, -0.2, -1.2, -1.0, 3.4, 3.7, 0.8, 0, 2.0)

sleep.L <- c(1.9, 08, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4)
```

Tenemos entonces dos variables:

-   $X_1 =$ Ganancia en horas para el tratamiento con *D. hiosciamina hidrobromoidal*
-   $X_2 =$ Ganancia en horas para el tratamiento con *L. hiosciamina hidrobromoidal*.

En este caso son muestras pareadas al estar medidas las dos variables sobre el mismo paciente, y por tanto cabe la posibilidad de que sean dependientes.

Una primera aproximación es pintar sobre el plano esas parejas y ver como se comportan, en particular en ver como se situan en relación a la diagonal $x=y$. En nuestro caso tenemos el siguiente gráfico:

```{r, fig.align = 'center'}
plot(sleep.D, sleep.L)
abline(0,1)
```

El gráfico anterior sugiere que hay diferencias, en concreto que los valores con el segundo tratamiento tienden a ser mayores que con el primer tratamiento.

Supongamos que queremos realizar el siguiente contraste de hipótesis: $$
\begin{array}{l}
H_0:\mu _1=\mu_2 \\ H_1:\mu _1\neq \mu _2,
\end{array}
$$ siendo $\mu_1=E[X_1]$ y $\mu_2=E[X_2]$.

Nuestro pimer paso es comprobar si la variable $X_1-X_2$ es normal. Para ello creamos la diferencia entre las dos variables

```{r}
diferencia = sleep.D - sleep.L
```

Y realizamos el contraste de Shapiro-Wilks para la diferencia anterior

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
shapiro.test(diferencia)
# fin de completar aquí
```

y obtenemos que el estadístico de contraste toma el valor $W =$ `r shapiro.test(diferencia)$statistic` y $p-valor=$ `r shapiro.test(diferencia)$p.value`, por lo que no se acepta la hipótesis de normalidad.

Por lo tanto no podríamos aplicar ni el intervalo de confianza, ni el contraste anteriores. Con el fin de ilustrar como se realiza en `R` el caso de muestras pareadas, vamos a considerar que el contraste no rechazase la hipótesis de normalidad. Podríamos en ese caso hacer el contraste de la t de Student sobre medias para muestras pareadas. Para ello utilizamos

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
t.test(sleep.D, sleep.L, alternative = "two.sided", paired = TRUE)
# fin de completar aquí
```

donde obtenemos los valores $t =$ `r t.test(sleep.D, sleep.L, alternative = "two.sided", paired = TRUE)$statistic` y \$p-valor = \$ `r t.test(sleep.D, sleep.L, alternative = "two.sided", paired = TRUE)$p.value`, por lo que con nivel de significación $\alpha$=0.05, rechazaríamos la hipótesis nula y por tanto las medias serían distintas. Y podríamos continuar el estudio para ver que media es superior.

Para obtener un intervalo de confianza, al 99%, para la diferencia de las medias, escribimos

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
t.test(sleep.D, sleep.L, conf.level = 0.99, paired = TRUE)
# fin de completar aquí
```

y obtenemos el intervalo $$(-5.2287065,  0.4487065).$$

Las hipótesis alternativas $H_1:\mu_1 < \mu_2$ y $H_1:\mu_1 > \mu_2$ se indican en `alternative` con `less` y `greater` respectivamente, como en el caso de muestras independientes.

Recordamos que en este caso los resultados no son válidos al no estar la diferencia de las variables distribuida nomralmente, lo hemos hecho solo con el fin de ilustrar como se ahce en `R`.

**Observaciones**:

\*\[1.\] Hemos supuesto a lo largo de todo el desarrollo anterior que la diferencia de las variables sigue una distribución normal. Por tanto tendremos que verificar esta suposición mediante el contraste de Shapiro-Wilk. Tendremos que realizar un contraste para la diferencia de los valores de los pares de observaciones de la muestra.

\*\[2.\] Si la diferencia no sigue una distribución normal, entonces consideraremos como alternativa la distribución normal para la diferencia de los logaritmos neperianos de las observaciones. Esto no es lo mismo que el logaritmo neperiano de las diferencias. Se trata en este caso de trabajar con las diferencias de los logaritmos neperianos.

\*\[3.\] Si no nos encontramos en los casos anteriores entonces tenemos dos posibilidades:

a)  Si el tamaño de muestra $n$ es mayor que 30 podemos aplicar lo anterior.

b)  Si el tamaño de muestra $n$ es menor que 30 entonces aplicaremos el contraste de Wilcoxon para muestras pareadas que vemos a continuación.

**Contraste de Wilcoxon:**

**Ejemplo 5 (continuación):** Si planteamos el contraste anterior para el ejemplo, en vez de para la diferencia de medias, para la mediana de la diferencia plantearíamos entonces el contraste de las hipótesis: $$
\begin{array}{l}
H_0:M =0  \\ H_1:M \neq 0,
\end{array}
$$ siendo $M$ la mediana de $X_1 - X_2$.

Lo realizamos entonces mediante

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
wilcox.test(sleep.D, sleep.L, alternative = "two.sided", paired = TRUE)
# Fin de completar aquí
```

De donde concluimos que se acepta la hipótesis alternativa y por lo tanto la diferencia $X_1 - X_2$ no se reparte un 50% positiva y en un 50% negativa, por lo que no podemos decir que son distintas de acuerdo a este criterio.

# Inferencia para diferencias de proporciones

**Ejemplo 6:** Se lleva a cabo un estudio biológico para determinar la toxicidad de unos vertidos en un río. Para ello se toman 80 ejemplares de un determinado organismo que se colocan en una pecera con agua limpia y se toman otros 80 ejemplares que se colocan en otra pecera con agua del río anterior. Para la primera muestra se tiene que, tras una semana, 72 de los organismo siguen vivos y para la segunda muestra 64 de los organismos están vivos.

Describimos primero como se lleva a cabo la inferencia con el método exacto.

**Método exacto:**

**Intervalo de confianza:** Con el método exacto podemos dar un intervalo de confianza para el cociente de las proporciones, es decir para $p_1/p_2$. Vamos a ver como obtener con `R` este intervalo.

Para ello primero tenemos que crear una matriz con la tabla de doble entrada de nuestro datos. Por ejemplo podemos crearla de la siguiente forma

```{r}
fila1 <- c(72, 64)
fila2 <- c(8, 16)
ejemplo6 <- rbind(fila1, fila2)
```

Mediante

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
fisher.test(ejemplo6, conf.level = 0.95)
# fin de completar aquí
```

obtenemos el intervalo de confianza para $p_1/p_2$ que viene dado por (0.837240, 6.469065).

**Contraste exacto para el cociente de proporciones**: Podemos realizar un contraste de hipótesis exacto para comparar proporciones mediante el contraste de Fisher.

**Ejemplo 6 (continuación):** Para nuestro ejemplo tenemos que si contrastamos las hipótesis $$
\begin{array}{l}
H_0:p_1 = p_2 \\ H_1:p_1\neq p_2
\end{array}
$$ mediante

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
fisher.test(ejemplo6, alternative = "two.sided")
# fin de completar aquí
```

obtenemos que su $p$-valor viene dado por 0.1199, por lo que para $\alpha$=0.05 o 0.01, se tiene que 0.1199$>\alpha$, por lo que se acepta $H_0$, es decir no hay diferencias entre las proporciones de supervivientes.

Pasamos al método aproximado.

**Método aproximado:**

Veamos lo anterior para los datos de nuestro ejemplo.

**Ejemplo 6 (continuación):** Vamos a contrastar, igual que antes, las hipótesis $$
\begin{array}{l}
H_0:p_1=p_2 \\ H_1:p_1\neq p_2
\end{array}
$$

Para el método aproximado escribimos

```{r}
#| eval: false
# completar aquí



# Fin de completar aquí

```

```{r}
#| warning: false
#| echo: false
# Completar aquí
prop.test(c(72, 64), c(80, 80), alternative = "two.sided")
# fin de completar aquí
```

y que para $\alpha =0.05$ o $0.01$ se tiene que $p-valor=0.1212 > \alpha$ por lo que se acepta $H_0$, es decir no hay diferencias significativas entre las proporciones de supervivientes.

Para el intervalo de confianza escribimos

```{r}
prop.test(c(72, 64), c(80, 80), conf.level = 0.95)
```

y obtenemos el siguiente intervalo de confianza al nivel $95\%$ para la diferencia de proporciones de organismos supervivientes: (-0.02206532,0.22206532)

## Ejercicio



Se trata de analizar el conjunto de datos  donde se plantean comparaciones de dos variables de acuerdo a las distintas cuestiones que aparecen a continuación de los datos. A la hora de desarrollar el análisis, debéis tener en cuenta los siguientes pasos: 

- Identificar las variables $X_1$, $X_2$ que son objeto de comparación. 

- Identificar si las muestras son independientes o pareadas. 

- Cada vez que hagáis un contraste escribir explícitamente las hipótesis que vais a contrastar, identificando la hipótesis nula y la alternativa. 

- Una vez tengáis los resultados del contraste, incluir en la respuesta el valor del estadístico de contraste, el p-valor y la conclusión final.

::: callout-warning
## Ejercicio


Los datos obtenidos por A.C. Linnerud de la Universidad Estatal de Carolina del Norte, incluidos en la tabla de datos siguiente: corresponden a los tiempos observados de la tasa de recuperación cardíaca después de realizar una carrera de $2km$ por un grupo de hombres participantes en un programa de salud, siendo la medida de la tasa de recuperación cardíaca, $TRC$, un indicador del progreso de salud de cada hombre.

Los hombres que participaron en el programa se clasificaron en dos grupos según la edad, el primer grupo con edades entre 40 y 49 años (grupo A) y el segundo grupo con edades entre 50 y 59 (grupo B).

Los datos de la columna $TRCAntes$ corresponden a los valores de la $TRC$ del conjunto de los participantes en el programa que practicaban habitualmente algún deporte, sin embargo, habían estado más de seis meses sin realizar deporte. La columna $TRCDepor$ contiene los valores de la $TRC$ de los hombres que no habían abandonado la práctica de algún deporte. Además, los componentes del programa que no practicaban deporte, tras la primera medición de la $TRC$, se sometieron a un tratamiento de ejercicio físico durante un mes, realizándose sobre ellos una segunda prueba de la $TRC$ después de dicho tratamiento ($TRCDeps$).
:::

```{r}
#| message: false
#| warning: false
library(tidyverse)
data.sport.n <- tibble(
  TRCantes = c(12.24, 12.45, 11.04, 11.22, 11.58, 8.34, 11.36, 11.52, 8.28, 12.01, 11.03, 12.01, 11.31, 14.33, 10.35, 12.51, 11.28,
               11.48, 14.05, 10.51, 18.50, 18.11),
  TRCdespues = c(11.81, 11.50, 10.55, 10.33, 10.63, 8.35, 9.66, 10.46, 8.30, 10.93, 10.34, 11.65, 9.48, 13.34, 10.40, 12.15, 10.70, 
                 10.66, 13.52, 10.25, 17.53, 18.01),
  Edad = c("A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "B", "B", "B", "B", "B", "B", "B", "B", "B")
)


data.sport.y <- tibble(
  TRCdepor = c(10.22, 9.33, 9.16, 11.28, 10.59, 13.55, 10.10, 11.46, 10.36, 10.40, 11.31, 12.55, 12.58, 10.54, 11.34, 11.15, 13.43),
  Edad = c("A", "A", "A", "A", "A", "A", "A", "A", "B", "B", "B", "B", "B", "B", "B", "B", "B")
)
```

-   Plantear, de forma razonada, un contraste para comparar la $TRC$ del grupo de hombres que no practican deporte con la del grupo de hombres que si practican algún deporte. ¿ Ha mejorado la tasa de recuperación cardiáca en el conjunto de hombres que al inicio no practicaban deporte y fueron sometidos al tratamiento físico? ¿Y en el grupo de hombres entre 40 y 49 años? ¿Y en el grupo de hombres entre 50 y 59 años?

-   Contrastar si existen diferencias significativas entre las $TRC$ del grupo de hombres que practicaban deporte al inicio del experimento y las $TRC$ después del tratamiento del grupo de hombres que no lo practicaban.

-   Contrastar si existen diferencias significativas entre las $TRC$ del grupo de hombres entre 40 y 49 años que practicaban deporte al inicio del experimento y las $TRC$ después del tratamiento del grupo de hombres entre 40 y 49 años que no lo practicaban.

\`

-   Contrastar si existen diferencias significativas entre las $TRC$ del grupo de hombres entre 50 y 59 años que practicaban deporte al inicio del experimento y las $TRC$ después del tratamiento del grupo de hombres entre 50 y 59 años que no lo practicaban.




```{r}
#| echo: true

```
