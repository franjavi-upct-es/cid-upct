{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "import math\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p output # Creación de subdirectorio auxiliar para salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos para esta práctica están disponibles como archivo .zip en este [enlace](http://web.stanford.edu/class/cs276/pa/pa3-data.zip). El _dataset_ está dividido en dos conjuntos separados:\n",
    "1. **Conjunto de entrenamiento** formado por 731 consultas (`pa3.(signal|rel).train`)\n",
    "2. **Conjunto de test** formado por 124 consultas (`pa3.(signal|rel).dev`)\n",
    "\n",
    "La idea será que, a la vez que ajustemos y maximicemos el rendimiento en el conjunto de entrenamiento, verifiquemos el rendimiento de los parámetros ajustados en el conjunto de desarrollo para asegurarnos de que no estamos sobreajustando el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos descargados y descomprimidos en pa3-data...\n",
      "\n",
      "Estructura del directorio:\n",
      "pa3-data/\n",
      "  - BSBI.dict\n",
      "  - terms.dict\n",
      "  - pa3.rel.train\n",
      "  - pa3.rel.dev\n",
      "  - pa3.signal.dev\n",
      "  - docs.dict\n",
      "  - pa3.signal.train\n"
     ]
    }
   ],
   "source": [
    "# Descarga del dataset:\n",
    "data_dir = 'pa3-data'\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/{}.zip'.format(data_dir)\n",
    "urllib.request.urlretrieve(data_url, '{}.zip'.format(data_dir))\n",
    "\n",
    "# Descomprimimos el archivo .zip:\n",
    "with zipfile.ZipFile('{}.zip'.format(data_dir), 'r') as zip_fh:\n",
    "    zip_fh.extractall()\n",
    "print('Datos descargados y descomprimidos en {}...\\n'.format(data_dir))\n",
    "\n",
    "# Imprimimos la estructura del directorio:\n",
    "print('Estructura del directorio:')\n",
    "print(data_dir + os.path.sep)\n",
    "for sub_dir in os.listdir(data_dir):\n",
    "    if not sub_dir.startswith('.'):\n",
    "        print('  - ' + sub_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de los archivos\n",
    "\n",
    "### Archivos de \"señal\"\n",
    "\n",
    "- **pa3.signal.(_train_|_test_)**: Contienen el conjunto de consultas correspondiente (_train_|_test_) junto con los documentos devueltos para cada consulta individual por un determinado motor de búsqueda. La lista de documentos se encuentra convenientemente mezclada y no está en el mismo orden que el devuelto por el motor de búsqueda empleado originalmente. Cada consulta tiene siempre 10 o menos documentos de respuesta. Por ejemplo, el formato de un par consulta-documento $(q,d)$ es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: stanford aoerc pool hours\n",
      "  url: http://events.stanford.edu/2014/February/18/\n",
      "    title: events at stanford tuesday february 18 2014\n",
      "    header: stanford university event calendar\n",
      "    header: teaching sex at stanford\n",
      "    header: rodin the complete stanford collection\n",
      "    header: stanford rec trx suspension training\n",
      "    header: memorial church open visiting hours\n",
      "    header: alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members\n",
      "    body_hits: stanford 239 271 318 457 615 642 663 960 966 971\n",
      "    body_hits: aoerc 349 401 432 530 549 578 596\n",
      "    body_hits: pool 521\n",
      "    body_length: 981\n",
      "    pagerank: 1\n",
      "  url: http://events.stanford.edu/2014/February/6/\n",
      "    title: events at stanford thursday february 6 2014\n",
      "    header: stanford university event calendar\n",
      "    header: stanford woods environmental forum featuring roz naylor\n",
      "    header: stanford school of earth sciences alumni reception at nape\n",
      "    header: an evening with stanford alumnus and pandora founder tim westergren 88\n",
      "    header: rodin the complete stanford collection\n",
      "    header: stanford rec trx suspension training\n",
      "    header: memorial church open visiting hours\n",
      "    body_hits: stanford 248 327 371 418 514 653 796 817 1081 1087 1092\n",
      "    body_hits: aoerc 545 597 628 713 732 761 779\n",
      "    body_hits: pool 704\n",
      "    body_length: 1102\n",
      "    pagerank: 0\n",
      "  url: http://events.stanford.edu/2014/March/13/\n",
      "    ...\n",
      "\n",
      "query: cardinal nights\n",
      "  url: http://events.stanford.edu/events/453/45363/\n",
      "    title: cardinal nights tie dye\n",
      "    header: cardinal nights tie dye\n",
      "    body_hits: cardinal 108\n",
      "    body_hits: nights 109\n",
      "    body_length: 282\n",
      "    pagerank: 0\n",
      "    anchor_text: cardinal nights tie dye friday october 24 2014 8 00 pm the lawn in front of\n",
      "      stanford_anchor_count: 1\n",
      "  url: https://alcohol.stanford.edu/cardinal-nights\n",
      "    title: cardinal nights office of alcohol policy and education\n",
      "    header: cardinal nights\n",
      "    header: about cardinal nights\n",
      "    body_hits: cardinal 27 77 104 148 151 190\n",
      "    body_hits: nights 28 78 105 149 152 191\n",
      "    body_length: 363\n",
      "    pagerank: 4\n",
      "    anchor_text: cardinal nights\n",
      "      stanford_anchor_count: 208\n",
      "    anchor_text: for more information about cardinal nights\n",
      "      stanford_anchor_count: 1\n",
      "  url: https://alcohol.stanford.edu/events/cardinal-nights-comedic-guest\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "# Mostramos un conjunto de líneas ilustrativo del archivo de training (la primera\n",
    "# consulta (\"query:\"), con sus primeros dos documentos (\"url:\") completos (líneas 0 a 29),\n",
    "# y otro par consulta-documentos adicional más adelante en el fichero (líneas 233\n",
    "# en adelante):\n",
    "filename = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines[0:29]:\n",
    "        print(l, end=\"\")\n",
    "    print(\"    ...\\n\")\n",
    "    for l in lines[233:256]:\n",
    "        print(l, end=\"\")\n",
    "    print(\"    ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atendiendo a la salida anterior se ejemplifica perfectamente la estructura de estos archivos de señal. El patrón se repite para cada URL, hasta que todas las URL (siempre 10 o menos) de la correspondiente consulta estén completas. A continuación este patrón general se repite para cada consulta posterior. Hay solo un campo `title`, otro `pagerank`, y otro `body_length` para cada URL, pero puede haber múltiples campos `header`, `body_hits` y `anchor_text` (junto con los correspondientes `stanford_anchor_count`). He aquí el significado de los mencionados campos:\n",
    "\n",
    "* El campo `title` es el título de la página.\n",
    "\n",
    "* El campo `pagerank` es un número entero de 0 a 9 que indica una calidad independiente de la consulta de la página (cuanto mayor sea dicho valor, mejor será la calidad de la página).\n",
    "\n",
    "* El campo `body_length` indica cuántos términos están presentes en el cuerpo del documento.\n",
    "\n",
    "* Cada campo `header` indica un subtítulo contenido en la página.\n",
    "\n",
    "* Cada campo `body_hits` especifica, para cada término de la consulta $q$, la _posting list_ posicional para ese término en el documento (posiciones de la palabra en el mismo, siempre ordenadas en orden creciente).\n",
    "\n",
    "* Cada campo `anchor_text`, siempre seguido inmediatamente por un subcampo `stanford_anchor_count` correspondiente, indica el texto de un enlace desde cualquier página del dominio de páginas web en las que está basado el dataset ([https://www.stanford.edu/](https://www.stanford.edu/)) al documento actual, junto con el número total de enlaces con dicho texto en el dataset. Por ejemplo, si el texto de anclaje (`anchor_text`) es _\"Stanford math department\"_ y el recuento (`stanford_anchor_count`) es 9, eso significa que hay nueve enlaces a la página actual (desde otras páginas en https://www.stanford.edu/) donde el texto de anclaje es exactamente _\"Stanford math department\"_. Así, en el ejemplo anterior del segundo documento de la segunda consulta, podemos ver que el anclaje _\"Cardinal nights\"_ aparece en 208 páginas del dominio https://www.stanford.edu/ apuntando al documento https://alcohol.stanford.edu/cardinal-nights en cuestión.\n",
    "\n",
    "El campo `pagerank` será empleado más adelante como ejemplo de _feature_ no textual, mientras que el resto de campos se corresponden con distintas _zonas_ del documento, que actuarán por tanto como _features_ textuales diferenciadas.\n",
    "\n",
    "### Archivos de relevancias\n",
    "\n",
    "* **pa3.rel.(train|dev)**: Estos archivos contienen una lista de juicios de relevancia (etiquetados manualmente) para cada par consulta-documento $(q,d)$ en los respectivos archivos de señal (_train|test_). El valor de relevancia es siempre un entero entre -1 y 3, con un dato mayor significando que el documento tiene más relevancia. −1 significaría que el documento ha sido simplemente ignorado. De nuevo, el patrón se repite para cada consulta, hasta que se alcanza el final del archivo.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: stanford aoerc pool hours\n",
      "url: http://events.stanford.edu/2014/February/18/ 0.0\n",
      "url: http://events.stanford.edu/2014/February/6/ 0.0\n",
      "url: http://events.stanford.edu/2014/March/13/ 0.0\n",
      "url: http://events.stanford.edu/2014/March/3/ 0.0\n",
      "url: http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf 0.0\n",
      "url: http://web.stanford.edu/group/masters/pool.html 1.0\n",
      "url: https://alumni.stanford.edu/get/page/perks/PoolAndGyms 1.5\n",
      "url: https://cardinalrec.stanford.edu/facilities/aoerc/ 2.0\n",
      "url: https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse= 0.5\n",
      "url: https://glo.stanford.edu/events/stanford-rec-open-house 0.5\n",
      "query: alumni association benefits\n",
      "url: http://alumni.stanford.edu/get/page/membership/benefits/creditcard 2.0\n",
      "url: http://alumni.stanford.edu/get/page/membership/benefits/libraries 2.0\n",
      "url: http://alumni.stanford.edu/get/page/membership/students 2.0\n",
      "url: https://alumni-esc.stanford.edu/get/page/membership/faq-general 2.0\n",
      "url: https://alumni.stanford.edu/get/page/landing/resources 2.0\n",
      "url: https://alumni.stanford.edu/get/page/membership/benefits 2.0\n",
      "url: https://alumni.stanford.edu/get/page/membership/benefits/golf 2.0\n",
      "url: https://alumni.stanford.edu/get/page/membership/benefits/rh_alums 2.0\n",
      "url: https://alumni.stanford.edu/get/page/membership/join?memb_group=SAA 2.0\n",
      "url: https://alumni.stanford.edu/get/page/perks/index 2.0\n",
      "query: cardinal nights\n",
      "url: http://events.stanford.edu/events/453/45363/ 2.0\n",
      "url: https://alcohol.stanford.edu/cardinal-nights 3.0\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las primeras filas del archivo:\n",
    "filename = os.path.join(data_dir, \"pa3.rel.train\")\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines[0:25]:\n",
    "        print(l.strip())\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, las funciones de _ranking_ requieren también ciertos estadísticos a nivel de colección de documento (tales como la frecuencia inversa de documento, o _idf_), que no están contenidos en los anteriores archivos. Para eso proporcionamos los archivos **docs.dict**, **terms.dict** y **BSBI.dict**, con los que pueden calcularse los correspondientes valores de _idf_ necesarios. En este caso, se trata simplemente de archivos binarios (`pickle` de python):\n",
    "\n",
    "## _Parsing_ de los archivos del _dataset_\n",
    "\n",
    "En primer lugar, definimos las clases `Query` y `Document` y la función `load_train_data`, necesarias para hacer el _parsing_ de los datos textuales contenidos en los ficheros de entrada:\n",
    "\n",
    "### Clase _Query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    \"\"\"Clase utilizada para almacenar una consulta.\"\"\"\n",
    "    def __init__(self, query):\n",
    "        self.query_words = query.split(\" \")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for w in self.query_words:\n",
    "            yield w\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Query):\n",
    "            return False\n",
    "        return self.query_words == other.query_words\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" \".join(self.query_words)\n",
    "\n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase _Document_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"Clase utilizada para almacenar información útil para un documento.\"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url        # Cadena\n",
    "        self.title = None     # Cadena\n",
    "        self.headers = None   # [Lista de cadenas]\n",
    "        self.body_hits = None # Diccionario: Término->[Lista de posiciones]\n",
    "        self.body_length = 0  # Entero\n",
    "        self.pagerank = 0     # Entero\n",
    "        self.anchors = None   # Diccionario: Cadena->[Conteo total de ocurrencias (anchor_counts)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for u in self.url:\n",
    "            yield u\n",
    "\n",
    "    def __str__(self):\n",
    "        result = [];\n",
    "        NEW_LINE = \"\\n\"\n",
    "        result.append(\"url: \"+ self.url + NEW_LINE);\n",
    "        if (self.title is not None): result.append(\"title: \" + self.title + NEW_LINE);\n",
    "        if (self.headers is not None): result.append(\"headers: \" + str(self.headers) + NEW_LINE);\n",
    "        if (self.body_hits is not None): result.append(\"body_hits: \" + str(self.body_hits) + NEW_LINE);\n",
    "        if (self.body_length != 0): result.append(\"body_length: \" + str(self.body_length) + NEW_LINE);\n",
    "        if (self.pagerank != 0): result.append(\"pagerank: \" + str(self.pagerank) + NEW_LINE);\n",
    "        if (self.anchors is not None): result.append(\"anchors: \" + str(self.anchors) + NEW_LINE);\n",
    "        return \" \".join(result)\n",
    "\n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función _load_train_data_\n",
    "\n",
    "La siguiente función realiza el _parsing_ de toda la información textual relevante contenida en un archivo de _training | test_ dado, ya en una estructura python (`query_dict`) fácilmente accesible programáticamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(feature_file_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        feature_file_name: Camino al fichero con las features de entrada.\n",
    "\n",
    "    Returns:\n",
    "       query_dict: Diccionario de tipo \"Consulta -> (URL -> Documento)\"\". Por ejemplo:\n",
    "        {computer science master: {'http://cs.stanford.edu/people/eroberts/mscsed/Admissions-MSInCSEducation.html':\n",
    "          {title: ms in computer science education stanford computer science\n",
    "           headers: [\"master's degree in computer science education\"]\n",
    "           body_hits: {'computer': [15], 'science': [16]}\n",
    "           body_length: 741\n",
    "           anchors: {'computer science': 2},\n",
    "          'http://scpd.stanford.edu/online-engineering-courses.jsp': title: online engineering courses stanford university\n",
    "           headers: ['computer science and information technology']\n",
    "           body_hits: {'science': [136], 'master': [188], 'computer': [223]}\n",
    "           body_length: 687,\n",
    "           }\n",
    "         ...,\n",
    "         }\n",
    "    \"\"\"\n",
    "    line = None\n",
    "    url = None\n",
    "    anchor_text = None\n",
    "    query = None\n",
    "    query_dict = {}\n",
    "    try:\n",
    "        with open(feature_file_name, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                token_index = line.index(\":\")\n",
    "                key = line[:token_index].strip()\n",
    "                value = line[token_index + 1:].strip()\n",
    "                if key == \"query\":\n",
    "                    # Nueva consulta:\n",
    "                    query = Query(value)\n",
    "                    query_dict[query] = {}\n",
    "                elif key == \"url\":\n",
    "                    # Nuevo documento encontrado para la actual consulta:\n",
    "                    url = value;\n",
    "                    query_dict[query][url] = Document(url);\n",
    "                elif key == \"title\":\n",
    "                    query_dict[query][url].title = str(value);\n",
    "                elif key == \"header\":\n",
    "                    if query_dict[query][url].headers is None:\n",
    "                        query_dict[query][url].headers = []\n",
    "                    query_dict[query][url].headers.append(value)\n",
    "                elif key == \"body_hits\":\n",
    "                    if query_dict[query][url].body_hits is None:\n",
    "                        query_dict[query][url].body_hits = {}\n",
    "                    temp = value.split(\" \",maxsplit=1);\n",
    "                    term = temp[0].strip();\n",
    "                    if term not in query_dict[query][url].body_hits:\n",
    "                        positions_int = []\n",
    "                        query_dict[query][url].body_hits[term] = positions_int\n",
    "                    else:\n",
    "                        positions_int = query_dict[query][url].body_hits[term]\n",
    "                    positions = temp[1].strip().split(\" \")\n",
    "                    for position in positions:\n",
    "                        positions_int.append(int(position))\n",
    "                elif key == \"body_length\":\n",
    "                    query_dict[query][url].body_length = int(value);\n",
    "                elif key == \"pagerank\":\n",
    "                    query_dict[query][url].pagerank = int(value);\n",
    "                elif key == \"anchor_text\":\n",
    "                    anchor_text = value\n",
    "                    if query_dict[query][url].anchors is None:\n",
    "                        query_dict[query][url].anchors = {}\n",
    "                elif key == \"stanford_anchor_count\":\n",
    "                    query_dict[query][url].anchors[anchor_text] = int(value)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fichero {feature_file_name} no encontrado!\")\n",
    "\n",
    "    return query_dict\n",
    "\n",
    "# Cargamos archivo completo de training:\n",
    "file_name = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "query_dict = load_train_data(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos (URLs) para la consulta \"stanford aoerc pool hours\":\n",
      "  http://events.stanford.edu/2014/February/18/\n",
      "  http://events.stanford.edu/2014/February/6/\n",
      "  http://events.stanford.edu/2014/March/13/\n",
      "  http://events.stanford.edu/2014/March/3/\n",
      "  http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf\n",
      "  http://web.stanford.edu/group/masters/pool.html\n",
      "  https://alumni.stanford.edu/get/page/perks/PoolAndGyms\n",
      "  https://cardinalrec.stanford.edu/facilities/aoerc/\n",
      "  https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse=\n",
      "  https://glo.stanford.edu/events/stanford-rec-open-house\n",
      "\n",
      "Documento \"http://events.stanford.edu/2014/February/18/\" correspondiente a la consulta \"stanford aoerc pool hours\":\n",
      "url: http://events.stanford.edu/2014/February/18/\n",
      " title: events at stanford tuesday february 18 2014\n",
      " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
      " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
      " body_length: 981\n",
      " pagerank: 1\n",
      "\n",
      "Campo \"body_hits\" para documento \"http://events.stanford.edu/2014/February/18/ correspondiente a la consulta \"stanford aoerc pool hours:\"\n"
     ]
    }
   ],
   "source": [
    "# Acceder a todos los documentos (dados por sus URLs) de una consulta:\n",
    "query_text = \"stanford aoerc pool hours\"\n",
    "#### COMPLETAR. Asigna el valor adecuado a la variable: \n",
    "query = query_dict[Query(query=query_text)]\n",
    "#### FIN COMPLETAR\n",
    "print(f'Documentos (URLs) para la consulta \"{query_text}\":')\n",
    "for url in query.keys():\n",
    "    print(f\"  {url}\")\n",
    "\n",
    "# Acceder a un documento completo, dado por su URL, dentro de la consulta:\n",
    "query_text = \"stanford aoerc pool hours\"\n",
    "url_text = \"http://events.stanford.edu/2014/February/18/\"\n",
    "print(f'\\nDocumento \"{url_text}\" correspondiente a la consulta \"{query_text}\":')\n",
    "#### COMPLETAR: Imprimir el documento solicitado\n",
    "print(query_dict[Query(query_text)][url_text])\n",
    "#### FIN COMPLETAR\n",
    "\n",
    "# Acceder sólo a un campo específico dentro de un documento:\n",
    "print(f'Campo \"body_hits\" para documento \"{url_text} correspondiente a la consulta \"{query_text}:\"')\n",
    "#### COMPLETAR: Imprimir el campo solicitado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentos (URLs) para la consulta \"stanford aoerc pool hours\":\n",
    "  http://events.stanford.edu/2014/February/18/\n",
    "  http://events.stanford.edu/2014/February/6/\n",
    "  http://events.stanford.edu/2014/March/13/\n",
    "  http://events.stanford.edu/2014/March/3/\n",
    "  http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf\n",
    "  http://web.stanford.edu/group/masters/pool.html\n",
    "  https://alumni.stanford.edu/get/page/perks/PoolAndGyms\n",
    "  https://cardinalrec.stanford.edu/facilities/aoerc/\n",
    "  https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse=\n",
    "  https://glo.stanford.edu/events/stanford-rec-open-house\n",
    "\n",
    "Documento \"http://events.stanford.edu/2014/February/18/\" correspondiente a la consulta \"stanford aoerc pool hours\":\n",
    "url: http://events.stanford.edu/2014/February/18/\n",
    " title: events at stanford tuesday february 18 2014\n",
    " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
    " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
    " body_length: 981\n",
    " pagerank: 1\n",
    "\n",
    "Campo \"body_hits\" para documento \"http://events.stanford.edu/2014/February/18/ correspondiente a la consulta \"stanford aoerc pool hours:\"\n",
    "{'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
    "\n",
    "## Construcción del diccionario IDF\n",
    "\n",
    "Construiremos ahora la clase que nos permitirá computar los IDF de los términos a partir de los correspondientes archivos <b>.dict</b> que usaremos también como entrada (similares a los utilizados en el _notebook_ de la práctica 1).\n",
    "\n",
    "\n",
    "### Clase _IdMap_\n",
    "\n",
    "Comenzamos creando la clase IdMap, para mapear cadenas (_tokens_) a sus correspondientes identificadores numéricos (_tokenIDs_) y viceversa. Valdrá tanto para documentos (identificados por su URL) como para términos (cadenas con un simple token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdMap:\n",
    "    \"\"\"Clase auxiliar para almacenar mapeos entre strings e identificadores numéricos de tokens.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Devuelve el número de términos almacenados en el IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "\n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Devuelve la cadena correspondiente a un determinado identificador (`i`).\"\"\"\n",
    "        return self.id_to_str[i]\n",
    "\n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Devuelve el identificador (id) correspondiente a una cadena (`s`).\n",
    "        En el caso de que `s` no esté aún en el IdMap, asigna un nuevo identificador y devuelve el nuevo id creado.\n",
    "        \"\"\"\n",
    "        if s not in self.str_to_id:\n",
    "            self.str_to_id[s] = len(self.id_to_str)\n",
    "            self.id_to_str.append(s)\n",
    "        return self.str_to_id[s]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"Acceso a un elemento por identificador o por cadena de texto.\n",
    "           Si `key` es un entero, usa el método _get_str;\n",
    "           Si `key` es una cadena, usa el método _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de diccionarios (_docs_, _terms_, _postings\\_dict_)\n",
    "\n",
    "Cargamos ahora los archivos `docs.dict`, `terms.dict` y `BSBI.dict` creados en la práctica 1, e imprimimos sus respectivas longitudes y los primeros registros para recordar sus respectivas estructuras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leídos 98998 documentos y 347071 términos\n",
      "----------- terms: -----------\n",
      "10 primeros términos:          ['3d', 'radiology', 'lab', 'stanford', 'university', 'school', 'of', 'medicine', 'and', 'quantitative']\n",
      "10 primeros mappings term->id: [('3d', 0), ('radiology', 1), ('lab', 2), ('stanford', 3), ('university', 4), ('school', 5), ('of', 6), ('medicine', 7), ('and', 8), ('quantitative', 9)]\n",
      "\n",
      "--------- docs: ------------\n",
      "10 primeros documentos:        ['0/3dradiology.stanford.edu_', '0/3dradiology.stanford.edu_patient_care_Case%2520studies_AVM.html', '0/3dradiology.stanford.edu_patient_care_case_studies.html', '0/5-sure.stanford.edu_', '0/50years.stanford.edu_', '0/a3cservices.stanford.edu_awards_nominate_', '0/a3cservices.stanford.edu_facilities_', '0/a3cservices.stanford.edu_lead_', '0/aa.stanford.edu_', '0/aa.stanford.edu_about_aviation.php']\n",
      "10 primeros mappings doc->id:  [('0/3dradiology.stanford.edu_', 0), ('0/3dradiology.stanford.edu_patient_care_Case%2520studies_AVM.html', 1), ('0/3dradiology.stanford.edu_patient_care_case_studies.html', 2), ('0/5-sure.stanford.edu_', 3), ('0/50years.stanford.edu_', 4), ('0/a3cservices.stanford.edu_awards_nominate_', 5), ('0/a3cservices.stanford.edu_facilities_', 6), ('0/a3cservices.stanford.edu_lead_', 7), ('0/aa.stanford.edu_', 8), ('0/aa.stanford.edu_about_aviation.php', 9)]\n",
      "\n",
      "------ postings_dict: ------\n",
      "10 primeros postings_dict:     [(0, (0, 849, 6792)), (1, (6792, 429, 3432)), (2, (10224, 7868, 62944)), (3, (73168, 71202, 569616)), (4, (642784, 56709, 453672)), (5, (1096456, 20874, 166992)), (6, (1263448, 85566, 684528)), (7, (1947976, 6208, 49664)), (8, (1997640, 79688, 637504)), (9, (2635144, 565, 4520))]\n",
      "10 primeros termsID:           [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Leemos los tres archivos e imprimimos sus longitudes:\n",
    "with open(\"pa3-data/terms.dict\", 'rb') as f:\n",
    "    terms = pkl.load(f)\n",
    "with open(\"pa3-data/docs.dict\", 'rb') as f:\n",
    "    docs = pkl.load(f)\n",
    "with open('pa3-data/BSBI.dict', 'rb') as f:\n",
    "    postings_dict, termsID = pkl.load(f)\n",
    "print(f\"Leídos {len(docs)} documentos y {len(terms)} términos\")\n",
    "\n",
    "# Chequeamos la consistencia de longitudes:\n",
    "assert len(terms) == len(termsID) == len(postings_dict), \\\n",
    "       \"Inconsistencia en longitudes de terms, termsID y/o postings_dict\"\n",
    "\n",
    "# Imprimimos los primeros diez valores de los idMaps docs y terms, y también de postings_dict y termsId:\n",
    "print(\"----------- terms: -----------\")\n",
    "print(\"10 primeros términos:         \", terms.id_to_str[:10])\n",
    "print(\"10 primeros mappings term->id:\", list(zip(list(terms.str_to_id.keys()),terms.str_to_id.values()))[:10])\n",
    "print(\"\\n--------- docs: ------------\")\n",
    "print(\"10 primeros documentos:       \", docs.id_to_str[:10])\n",
    "print(\"10 primeros mappings doc->id: \", list(zip(list(docs.str_to_id.keys()),docs.str_to_id.values()))[:10])\n",
    "print(\"\\n------ postings_dict: ------\")\n",
    "print(\"10 primeros postings_dict:    \", list(zip(list(postings_dict.keys()),postings_dict.values()))[:10])\n",
    "print(\"10 primeros termsID:          \", termsID[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase _Idf_\n",
    "\n",
    "A continuación, la clase `Idf` que, a partir de los diccionarios anteriores, calcula la frecuencia (absoluta) de cada término, y el correspondiente valor IDF pesado logarítmicamente, como es habitual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Idf:\n",
    "    \"\"\"Construye un diccionario para poder devolver el IDF de un término (tanto si el término consultado está\n",
    "       como si no está en el diccionario construido).\n",
    "       Recuérdese de la práctica 1 que el diccionario \"postings_dict\" mapea cada termID a una tupla\n",
    "       (posicion_de_comienzo_en_fichero_indice, numero_de_postings_en_la_lista, longitud_en_bytes_de_la_lista).\n",
    "       Dado que queremos protegernos del caso posible de consulta de un término que no aparezca en la colección,\n",
    "       aplicaremos el suavizado de Laplace típico (suma de +1 una unidad tanto en el numerador como en el\n",
    "       denominador de la proporción, evitando así la posible división por cero).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Construcción del diccionario IDF\"\"\"\n",
    "        try:\n",
    "            with open(\"pa3-data/docs.dict\", 'rb') as f:\n",
    "                docs = pkl.load(f)\n",
    "            self.total_doc_num = len(docs)\n",
    "            print(\"Número total de documentos de la colección:\", self.total_doc_num)\n",
    "\n",
    "            with open(\"pa3-data/terms.dict\", 'rb') as f:\n",
    "                terms = pkl.load(f)\n",
    "            self.total_term_num = len(terms)\n",
    "            print(\"Número total de términos:\", self.total_term_num)\n",
    "\n",
    "            with open('pa3-data/BSBI.dict', 'rb') as f:\n",
    "                postings_dict, termsID = pkl.load(f)\n",
    "\n",
    "            self.idf = {}\n",
    "            self.raw = {}\n",
    "            for key in list(postings_dict.keys()):\n",
    "                _, count, _ = postings_dict[key]\n",
    "                # Frecuencias absolutas (raw):\n",
    "                #### COMPLETA: Asigna a self.raw[terms[key]] = count\n",
    "                self.raw[terms[key]] = count\n",
    "                # Frecuencias pesadas logarítmicamente (usando suavizado Laplace):\n",
    "                self.idf[terms[key]] = math.log10((1.0+self.total_doc_num) / (1.0+count))\n",
    "        except FileNotFoundError:\n",
    "            print(\"¡Ficheros de diccionario de documentos / términos / índice no encontrados!\")\n",
    "\n",
    "    def get_raw(self, term):\n",
    "        \"\"\"Devuelve el conteo crudo de ocurrencias de documentos conteniendo un término,\n",
    "           dado, tanto si está como si no en el diccionario.\n",
    "        Args:\n",
    "            term(str) : Término del que se va a devolver su conteo crudo.\n",
    "        Return(float):\n",
    "            Idf del término.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            return self.raw[term]\n",
    "        except:\n",
    "            # Para términos fuera del corpus, simplemente devolvemos 0:\n",
    "            return 0\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        \"\"\"Devuelve el IDF de un término, tanto si está como si no en el diccionario.\n",
    "        Args:\n",
    "            term(str) : Término del que se va a devolver su IDF.\n",
    "        Return(float):\n",
    "            Idf del término.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        try:\n",
    "            return self.idf[term]\n",
    "        except:\n",
    "            # Para términos fuera del corpus, simplemente devolvemos math.log10((1.0+total_doc_num) / 1.0).\n",
    "            return math.log10((1.0+self.total_doc_num) / 1.0)\n",
    "        ### END YOUR CODE (FIXME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora una instancia de esta clase `Idf`, y probamos a consultar en ella la IDF de unos cuantos términos, tanto existentes como inexistentes. Obsérvese que, a mayor frecuencia (términos más comunes en el corpus) de un término, menor peso IDF, y viceversa (términos menos comunes tienen un mayor peso IDF). Del mismo modo, un término inexistente tendrá un IDF muy cercano a 5.0 (ya que la cantidad total de documentos es aproximadamente 100K=$10^5$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de documentos de la colección: 98998\n",
      "Número total de términos: 347071\n",
      "\n",
      "IDF(\"the\") =                  0.083  (raw count=81770)\n",
      "IDF(\"and\") =                  0.094  (raw count=79688)\n",
      "IDF(\"stanford\") =             0.143  (raw count=71202)\n",
      "IDF(\"university\") =           0.242  (raw count=56709)\n",
      "IDF(\"quantitative\") =         2.243  (raw count=565)\n",
      "IDF(\"supercalifragilistic\") = 4.996  (raw count=0)\n"
     ]
    }
   ],
   "source": [
    "theIDF = Idf()\n",
    "print()\n",
    "print(f'IDF(\"the\") =                  {theIDF.get_idf(\"the\"):5.3f}  (raw count={theIDF.get_raw(\"the\")})')\n",
    "print(f'IDF(\"and\") =                  {theIDF.get_idf(\"and\"):5.3f}  (raw count={theIDF.get_raw(\"and\")})')\n",
    "print(f'IDF(\"stanford\") =             {theIDF.get_idf(\"stanford\"):5.3f}  (raw count={theIDF.get_raw(\"stanford\")})')\n",
    "print(f'IDF(\"university\") =           {theIDF.get_idf(\"university\"):5.3f}  (raw count={theIDF.get_raw(\"university\")})')\n",
    "print(f'IDF(\"quantitative\") =         {theIDF.get_idf(\"quantitative\"):5.3f}  (raw count={theIDF.get_raw(\"quantitative\")})')\n",
    "print(f'IDF(\"supercalifragilistic\") = {theIDF.get_idf(\"supercalifragilistic\"):5.3f}  (raw count={theIDF.get_raw(\"supercalifragilistic\")})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente son unos cuantos tests adicionales, que aseguran la corrección de la implementación de la clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests de clase Idf() superados.\n"
     ]
    }
   ],
   "source": [
    "assert len(theIDF.idf) == 347071, 'Longitud incorrecta de diccionario idf.'\n",
    "assert theIDF.get_idf(\"bilibalabulu\") > 4.9, \\\n",
    "       \"Término no localizado no manejado correctamente\"\n",
    "assert theIDF.get_idf(\"data\") < theIDF.get_idf(\"radiology\"), \\\n",
    "       \"El idf de los términos más raros debe ser mayor que el de términos más comunes.\"\n",
    "assert theIDF.get_idf(\"to\") < theIDF.get_idf(\"design\"), \\\n",
    "       \"El idf de los términos más raros debe ser mayor que el de términos más comunes.\"\n",
    "print(\"Tests de clase Idf() superados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectores de conteo de términos\n",
    "\n",
    "Cada par consulta-documento $(q,d)$ de los archivos de entrada proporciona información relativa a los diferentes términos de cada consulta en cinco campos diferentes de cada documento, a saber: <b>url</b>, <b>title</b>, <b>headers</b>, <b>body</b> y <b>anchors</b> (el campo <b>pagerank</b> adicional no se utilizará hasta más adelante).\n",
    "\n",
    "Las funciones de _ranking_ construirán inicialmente los correspondientes cinco vectores de conteo de términos crudos ($rs$, de _raw score_) para cada uno de estos pares $(q,d)$, precisamente a partir de las coincidencias (_hits_) en estos cinco diferentes campos. Estos vectores $rs$ simplemente contarán cuantas veces ocurre cada término de búsqueda en un determinado campo. Para el campo <b>anchor</b>, se asumirá la simplificación de que hay un gran documento que contiene todos los enlaces, con el texto del enlace multiplicado por el campo <b>stanford_anchor_count</b>. Seguiremos un enfoque análogo también para el campo <b>header</b>.\n",
    "    \n",
    "Así, para el par $(q,d)$ de ejemplo donde la consulta era $q=[\\text{stanford aoerc pool hours}]^T$ y el documento $d=$\"http://events.stanford.edu/2014/February/18/\" (mostrado en el ejemplo de una celda anterior de este mismo notebook), el vector ${rs}_{b}$ correspondiente al campo <b>body</b> será $[{10 \\ 7 \\ 1 \\ 0}]^T$, dado que había 10 apariciones del término \"stanford\" en dicho campo, 7 para el término \"aoerc\", 1 para el término \"pool\" y ninguna para el término \"hours\". Análogamente, el vector ${rs}_{a}$ para el campo <b>anchor</b> será simplemente $[\\text{0 0 0 0}]^T$, dado que no hay ningún enlace (<b>anchor</b>) para este documento. Finalmente el vector ${rs}_{t}$ para el campo <b>title</b> será $[\\text{1 0 0 0}]^T$, $[\\text{1 0 0 0}]^T$ también para el vector ${rs}_{u}$ del campo <b>url</b> (estos dos fácilmente deducibles desde los correspondientes campos `title` y `url`, que son únicos en cada documento), y ${rs}_{h}$ $[\\text{5 0 0 1}]^T$ para el campo <b>header</b> (este último acumulado para las distintas cabeceras --esto es, \"subtítulos\"-- presentes en el documento). Todo esto se puede corroborar fácilmente observando el contenido de dicho documento en el archivo de señal original:\n",
    "    \n",
    "```   \n",
    "query: stanford aoerc pool hours\n",
    " url: http://events.stanford.edu/2014/February/18/\n",
    "  title: events at stanford tuesday february 18 2014\n",
    "  header: stanford university event calendar\n",
    "  header: teaching sex at stanford\n",
    "  header: rodin the complete stanford collection\n",
    "  header: stanford rec trx suspension training\n",
    "  header: memorial church open visiting hours\n",
    "  header: alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members\n",
    "  body_hits: stanford 239 271 318 457 615 642 663 960 966 971\n",
    "  body_hits: aoerc 349 401 432 530 549 578 596\n",
    "  body_hits: pool 521\n",
    "  body_length: 981\n",
    "  pagerank: 1    \n",
    "```\n",
    "    \n",
    "Un ejemplo adicional, usando la misma consulta, pero un documento de respuesta diferente, para ilustrar los vectores correspondientes a los campos <b>anchor</b>, no presentes en el documento anterior:\n",
    "```\n",
    "  url: https://cardinalrec.stanford.edu/facilities/aoerc/\n",
    "    ...\n",
    "    anchor_text: gyms aoerc\n",
    "      stanford_anchor_count: 3\n",
    "    anchor_text: aoerc\n",
    "      stanford_anchor_count: 13\n",
    "    anchor_text: http cardinalrec stanford edu facilities aoerc\n",
    "      stanford_anchor_count: 4\n",
    "    anchor_text: arrillaga outdoor education and recreation center aoerc link is external\n",
    "      stanford_anchor_count: 1\n",
    "    anchor_text: the arrillaga outdoor education and research center aoerc\n",
    "      stanford_anchor_count: 2\n",
    "    anchor_text: aoerc will shutdown for maintenance\n",
    "      stanford_anchor_count: 2\n",
    "```\n",
    "\n",
    "Aquí, el vector para  <b>anchor</b> será $[\\text{4 25 0 0}]^T$, dado que sólo hay un total de 4 enlaces (campo <b>stanford_anchor_count</b>) para el término \"stanford\", pero 25 (=3+13+4+1+2+2) para el término “aoerc”.\n",
    "\n",
    "(Nótese que, en lo referente al campo  <b>url</b> es necesario _\"tokenizar\"_ previamente sólo los caracteres alfanuméricos. Nótese también que, al calcular los conteos de términos crudos, todo se hace convirtiendo previamente los _tokens_ a minúsculas).\n",
    "\n",
    "# Clase base para todos los _scorers_\n",
    "\n",
    "# Clase _AbstractScorer_\n",
    "\n",
    "Construimos ahora una clase base abstracta, de la que habrá que ir reimplementando métodos en las clases hijas, conforme vayamos implementando los diferentes métodos de _scoring_. En todo caso, esta clase AbstractScorer recogerá una funcionalidad básica común a todos. Lo fundamental serán los distintos métodos `parse_`_field_, que transforman la información textual disponible en el archivo de señal para cada campo en la correspondiente información numérica. Aprovechamos también aquí para añadir los posibles esquemas de _weighting_ SMART vistos en teoría (si bien en esta práctica no programaremos todas las posibilidades disponibles, sino sólo alguna de las más comúnmente utilizadas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractScorer:\n",
    "    \"\"\" Una clase básica abstracta para un scorer.\n",
    "        Implementa una funcionalidad básica de construcción de vectores de consulta y de documento.\n",
    "        Tendrá que ser extendida adecuadamente por cada scorer específico.\n",
    "    \"\"\"\n",
    "    def __init__(self, idf, query_weight_scheme=None, doc_weight_scheme=None):\n",
    "        self.idf = idf\n",
    "        self.TFTYPES = [\"url\", \"title\", \"body_hits\", \"header\", \"anchor\"]\n",
    "        # Esquemas por defecto:\n",
    "        self.default_query_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": None} # Esquema natural, none, none\n",
    "        self.default_doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": None}   # Esquema natural, none, none\n",
    "        self.query_weight_scheme = query_weight_scheme if query_weight_scheme is not None \\\n",
    "                                   else self.default_query_weight_scheme\n",
    "        self.doc_weight_scheme = doc_weight_scheme if doc_weight_scheme is not None \\\n",
    "                                 else self.default_doc_weight_scheme\n",
    "\n",
    "    def parse_url(self, url):\n",
    "        \"\"\"Parsea la URL del documento, devolviendo un Counter de los tokens encontrados en la URL.\n",
    "        Args:\n",
    "            url: el url del que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            Lista de tokens del URL (una vez limpios), y Counter resultado.\n",
    "        \"\"\"\n",
    "        if url:\n",
    "            url_token_in_term = url.replace(\"http:\",\".\").replace('/','.').replace('?','.') \\\n",
    "                                   .replace('=','.').replace(\"%20\",\".\").replace(\"...\",\".\").replace(\"..\",\".\")\\\n",
    "                                   .replace('-','.').lower();\n",
    "            url_token = url_token_in_term.strip(\".\").split('.')\n",
    "            return url_token, Counter(url_token)\n",
    "        else:\n",
    "            return [], Counter([])\n",
    "\n",
    "    def parse_title(self, title):\n",
    "        \"\"\"Parsea el campo title del documento, devolviendo un Counter de los tokens encontrados en el mismo.\n",
    "        Args:\n",
    "            title: el title del que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        if title:\n",
    "            return Counter(title.split(\" \"))\n",
    "        else:\n",
    "            return Counter([])\n",
    "\n",
    "    def parse_headers(self, headers):\n",
    "        \"\"\"Parsea los campos headers del documento, devolviendo un Counter de los tokens encontrados en los mismos.\n",
    "        Args:\n",
    "            headers: la lista de headers sobre los que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        headers_token = []\n",
    "        if headers is not None:\n",
    "            for header in headers:\n",
    "                header_token = header.split(\" \")\n",
    "                headers_token.extend(header_token)\n",
    "        return Counter(headers_token)\n",
    "\n",
    "    def parse_anchors(self, anchors):\n",
    "        \"\"\"Parsea los campos anchors del documento, devolviendo un Counter de los tokens encontrados en los mismos.\n",
    "        Args:\n",
    "            anchors: la lista de anchors sobre los que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        anchor_count_map = Counter({})\n",
    "        if anchors is not None:\n",
    "            for anchor in anchors:\n",
    "                count = anchors[anchor]\n",
    "                anchor_tokens = anchor.split(\" \")\n",
    "                for anchor_token in anchor_tokens:\n",
    "                    if(anchor_token in anchor_count_map.keys()):\n",
    "                        anchor_count_map[anchor_token] += count\n",
    "                    else:\n",
    "                        anchor_count_map[anchor_token] = count\n",
    "        return anchor_count_map\n",
    "\n",
    "    def parse_body_hits(self, body_hits):\n",
    "        \"\"\"Parsea los campos body_hits del documento, devolviendo un Counter de los tokens encontrados en los mismos.\n",
    "        Args:\n",
    "            body_hits: la lista de elementos del body.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        body_hits_count_map = Counter({})\n",
    "        if body_hits is not None:\n",
    "            for body_hit in body_hits:\n",
    "                body_hits_count_map[body_hit] = len(body_hits[body_hit])\n",
    "        return body_hits_count_map\n",
    "\n",
    "    def get_query_vector(self, q, query_weight_scheme = None):\n",
    "        \"\"\" Obtiene un vector numérico para la consulta q.\n",
    "        Args:\n",
    "            q (Query): Query(\"Una consulta determinada\")\n",
    "        Returns:\n",
    "            query_vec (dict): El vector resultado.\n",
    "        \"\"\"\n",
    "        # En subclases de esta AbstractScorer, podrían tenerse en cuenta todas las\n",
    "        # posibilidades SMART, usando diferentes esquemas de frecuencia del término (tf),\n",
    "        # frecuencia de documento (idf) y normalización. En todo caso, nótese que en\n",
    "        # general no se suele necesitar normalización para la consulta en ningún caso, ya que\n",
    "        # dicha normalización no variaría con respecto a todos los documentos resultados de una\n",
    "        # misma consulta, lo que resultaría en un simple factor de escalado común que no\n",
    "        # afectaría al posterior ranking de los mismos.\n",
    "        #\n",
    "        # if query_weight_scheme is None:\n",
    "        #     query_weight_scheme = self.query_weight_scheme\n",
    "\n",
    "        query_vec = {}\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        # En nuestro caso base, usaremos simplemente el contador básico de términos, sin\n",
    "        # normalización ni uso de idf:\n",
    "        query_vec = Counter(q.query_words)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return query_vec\n",
    "\n",
    "    def get_doc_vector(self, q, d, doc_weight_scheme=None):\n",
    "        \"\"\" Obtiene un vector numérico para el documento d.\n",
    "        Args:\n",
    "        q (Query) : Query(\"Una consulta\")\n",
    "        d (Document) : Query(\"Una consulta\")[\"Un URL\"]\n",
    "        Returns:\n",
    "        doc_vec (dict) : Un diccionario de conteo de la frecuencia de términos, con un subdiccionario para\n",
    "                         cada tipo de campo (tipo_de_campo -> (término -> conteo))\n",
    "                    Ejemplo: \"{'url':   {'stanford': 1, 'aoerc': 0, 'pool': 0, 'hours': 0},\n",
    "                               'title': {'stanford': 1, 'aoerc': 0, 'pool': 0, 'hours': 0},\n",
    "                               ...\n",
    "                               }\"\n",
    "        \"\"\"\n",
    "        # De nuevo, podrían considerarse todas las posibilidades SMART en las subclases\n",
    "        # de esta AbstractScorer, si bien en esta clase base nos contentaremos con un simple\n",
    "        # conteo crudo de los términos en los distintos campos:\n",
    "        #\n",
    "        # if doc_weight_scheme is None:\n",
    "        #    doc_weight_scheme = self.doc_weight_scheme\n",
    "\n",
    "        doc_vec = {}\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        # Sólo para depurar:\n",
    "        # print(f\"URL:        {d.url}          ->   {self.parse_url(d.url)}\")\n",
    "        # print(f\"TITLE:      {d.title}        ->   {self.parse_title(d.title)}\")\n",
    "        # print(f\"HEADERS:    {d.headers}      ->   {self.parse_headers(d.headers)}\")\n",
    "        # print(f\"ANCHORS:    {d.anchors}      ->   {self.parse_anchors(d.anchors)}\")\n",
    "        # print(f\"BODY_HITS:  {d.body_hits}    ->   {self.parse_body_hits(d.body_hits)}\")\n",
    "        #\n",
    "        # Simple conteo crudo de los términos por campos:\n",
    "        _, doc_vec[\"url\"] = self.parse_url(d.url)\n",
    "        doc_vec[\"title\"] = self.parse_title(d.title)\n",
    "        doc_vec[\"headers\"] = self.parse_headers(d.headers)\n",
    "        doc_vec[\"anchors\"] = self.parse_anchors(d.anchors)\n",
    "        doc_vec[\"body_hits\"] = self.parse_body_hits(d.body_hits)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return doc_vec\n",
    "\n",
    "    # Métodos no implementados en la clase base; en su caso, serán reimplementados en cada scorer concreto:\n",
    "\n",
    "    def normalize_doc_vec(self, q, d, doc_vec):\n",
    "        \"\"\" Normalizar el vector de documento.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            doc_vec (dict) : El vector de documento\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_sim_score(self, q, d):\n",
    "        \"\"\" Devuelve la puntuación para una consulta q y documento d dados.\n",
    "        Args:\n",
    "            q (Query): la consulta.\n",
    "            d (Document) : el documento.\n",
    "        Returns:\n",
    "            La puntuación para el par (q,d).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_net_score(self, q, d):\n",
    "        \"\"\" Calcular el scoring neto entre la consulta y el documento.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "        Return:\n",
    "            score (float) : La puntuación resultado.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la clase abstracta con una consulta y un documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query q:  stanford aoerc pool hours\n",
      "\n",
      "Document d:  url: http://events.stanford.edu/2014/February/18/\n",
      " title: events at stanford tuesday february 18 2014\n",
      " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
      " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
      " body_length: 981\n",
      " pagerank: 1\n",
      "\n",
      "Vector consulta:\n",
      "  Counter({'stanford': 1, 'aoerc': 1, 'pool': 1, 'hours': 1})\n",
      "\n",
      "Vector documento:\n",
      "  url        -> Counter({'events': 1, 'stanford': 1, 'edu': 1, '2014': 1, 'february': 1, '18': 1})\n",
      "  title      -> Counter({'events': 1, 'at': 1, 'stanford': 1, 'tuesday': 1, 'february': 1, '18': 1, '2014': 1})\n",
      "  headers    -> Counter({'stanford': 5, 'university': 1, 'event': 1, 'calendar': 1, 'teaching': 1, 'sex': 1, 'at': 1, 'rodin': 1, 'the': 1, 'complete': 1, 'collection': 1, 'rec': 1, 'trx': 1, 'suspension': 1, 'training': 1, 'memorial': 1, 'church': 1, 'open': 1, 'visiting': 1, 'hours': 1, 'alternative': 1, 'transportation': 1, 'counseling': 1, 'tm': 1, '3': 1, 'hour': 1, 'univ': 1, 'shc': 1, 'employees': 1, 'retirees': 1, 'family': 1, 'members': 1})\n",
      "  anchors    -> Counter()\n",
      "  body_hits  -> Counter({'stanford': 10, 'aoerc': 7, 'pool': 1})\n"
     ]
    }
   ],
   "source": [
    "q = Query(\"stanford aoerc pool hours\")\n",
    "d = query_dict[q]['http://events.stanford.edu/2014/February/18/']\n",
    "print(\"Query q: \", q)\n",
    "print()\n",
    "print(\"Document d: \", d)\n",
    "\n",
    "a_scorer = AbstractScorer(theIDF)\n",
    "query_vec = a_scorer.get_query_vector(q)\n",
    "print(f\"Vector consulta:\")\n",
    "print(f\"  {query_vec}\")\n",
    "print()\n",
    "doc_vec = a_scorer.get_doc_vector(q, d)\n",
    "print(f\"Vector documento:\")\n",
    "for k, v in doc_vec.items():\n",
    "    print(f\"  {k:10s} -> {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase _BaselineScorer_\n",
    "\n",
    "Definimos aquí un simple _\"scorer baseline\"_, que nos servirá para probar la funcionalidad de la clase abstracta base. La clase `BaselineScorer` heredará directamente de la clase `AbstractScorer`, reimplementando solamente el método `get_sim_score`, que simplemente acumulará, para aquellos términos en la consulta, los contadores absolutos de dichos términos en el vector de documento correspondiente, utilizando en cada caso el campo `url`, `title`, `headers`, `anchors`, o `body_hits` que se le indique en cada momento mediante el método `set_field_type(...)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineScorer(AbstractScorer):\n",
    "    def __init__(self, idf):\n",
    "        super().__init__(idf)\n",
    "        self.field_type = \"url\"  # Campo por defecto de inicialización de la clase.\n",
    "\n",
    "    def set_field_type(self, field_type):\n",
    "        self.field_type = field_type\n",
    "\n",
    "    def get_sim_score(self, q, d):\n",
    "        q_vec = self.get_query_vector(q)\n",
    "        d_vec = self.get_doc_vector(q, d)\n",
    "        score = 0\n",
    "        if self.field_type in d_vec.keys():\n",
    "            for term in d_vec[self.field_type].keys():\n",
    "                if term in q_vec.keys():\n",
    "                    score += d_vec[self.field_type][term]\n",
    "        return score\n",
    "\n",
    "    # En este caso el scoring neto coincide con el devuelto por get_sim_score\n",
    "    # (no se combinan los distintos campos, simplemente se tiene en cuenta el\n",
    "    #  campo fijado previamente con set_field_type):\n",
    "    def get_net_score(self, q, d):\n",
    "        return self.get_sim_score(q, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el _baseline scorer_ con una consulta y un par de documentos de ejemplo, para todos los campos posibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "\n",
      "Consulta:  stanford aoerc pool hours\n",
      "Vector de consulta:\n",
      "Counter({'stanford': 1, 'aoerc': 1, 'pool': 1, 'hours': 1})\n",
      "\n",
      "---------\n",
      "\n",
      "Documento:\n",
      " url: http://events.stanford.edu/2014/February/18/\n",
      " title: events at stanford tuesday february 18 2014\n",
      " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
      " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
      " body_length: 981\n",
      " pagerank: 1\n",
      "\n",
      "Vectores de documento:\n",
      "  url vector (computed similarity=1):\n",
      "  Counter({'events': 1, 'stanford': 1, 'edu': 1, '2014': 1, 'february': 1, '18': 1})\n",
      "\n",
      "  title vector (computed similarity=1):\n",
      "  Counter({'events': 1, 'at': 1, 'stanford': 1, 'tuesday': 1, 'february': 1, '18': 1, '2014': 1})\n",
      "\n",
      "  headers vector (computed similarity=6):\n",
      "  Counter({'stanford': 5, 'university': 1, 'event': 1, 'calendar': 1, 'teaching': 1, 'sex': 1, 'at': 1, 'rodin': 1, 'the': 1, 'complete': 1, 'collection': 1, 'rec': 1, 'trx': 1, 'suspension': 1, 'training': 1, 'memorial': 1, 'church': 1, 'open': 1, 'visiting': 1, 'hours': 1, 'alternative': 1, 'transportation': 1, 'counseling': 1, 'tm': 1, '3': 1, 'hour': 1, 'univ': 1, 'shc': 1, 'employees': 1, 'retirees': 1, 'family': 1, 'members': 1})\n",
      "\n",
      "  anchors vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "  body_hits vector (computed similarity=18):\n",
      "  Counter({'stanford': 10, 'aoerc': 7, 'pool': 1})\n",
      "\n",
      "---------\n",
      "\n",
      "Documento:\n",
      " url: https://cardinalrec.stanford.edu/facilities/aoerc/\n",
      " title: \n",
      " pagerank: 4\n",
      " anchors: {'gyms aoerc': 3, 'aoerc': 13, 'http cardinalrec stanford edu facilities aoerc': 4, 'arrillaga outdoor education and recreation center aoerc link is external': 1, 'the arrillaga outdoor education and research center aoerc': 2, 'aoerc will shutdown for maintenance': 2}\n",
      "\n",
      "Vectores de documento:\n",
      "  url vector (computed similarity=2):\n",
      "  Counter({'https:': 1, 'cardinalrec': 1, 'stanford': 1, 'edu': 1, 'facilities': 1, 'aoerc': 1})\n",
      "\n",
      "  title vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "  headers vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "  anchors vector (computed similarity=29):\n",
      "  Counter({'aoerc': 25, 'http': 4, 'cardinalrec': 4, 'stanford': 4, 'edu': 4, 'facilities': 4, 'gyms': 3, 'arrillaga': 3, 'outdoor': 3, 'education': 3, 'and': 3, 'center': 3, 'the': 2, 'research': 2, 'will': 2, 'shutdown': 2, 'for': 2, 'maintenance': 2, 'recreation': 1, 'link': 1, 'is': 1, 'external': 1})\n",
      "\n",
      "  body_hits vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "---------\n",
      "\n",
      "Tests de clase BaselineScorer() superados.\n"
     ]
    }
   ],
   "source": [
    "baseline_scorer = BaselineScorer(theIDF)\n",
    "\n",
    "q = Query(\"stanford aoerc pool hours\")\n",
    "print(\"---------\\n\")\n",
    "print(\"Consulta: \", q)\n",
    "print(f\"Vector de consulta:\\n{baseline_scorer.get_query_vector(q)}\")\n",
    "print(\"\\n---------\\n\")\n",
    "\n",
    "d1 = query_dict[q]['http://events.stanford.edu/2014/February/18/']          # Ejemplo que tiene \"body_hits\".\n",
    "d2 = query_dict[q]['https://cardinalrec.stanford.edu/facilities/aoerc/']    # Ejemplo que tiene \"anchors\".\n",
    "\n",
    "for i,d in enumerate([d1,d2]):\n",
    "    doc_vectors = baseline_scorer.get_doc_vector(q,d)\n",
    "    print(\"Documento:\\n\", d)\n",
    "    print(f\"Vectores de documento:\")\n",
    "    similarities = {}\n",
    "    for k in doc_vectors.keys(): # Para cada campo:\n",
    "        baseline_scorer.set_field_type(k)\n",
    "        similarity = baseline_scorer.get_sim_score(q,d)\n",
    "        print(f\"  {k} vector (computed similarity={similarity}):\\n  {doc_vectors[k]}\\n\")\n",
    "        similarities[k] = similarity\n",
    "    # print(similarities)\n",
    "    if i==0:\n",
    "        assert similarities == {'url': 1, 'title': 1, 'headers': 6, 'anchors': 0, 'body_hits': 18}, \\\n",
    "          \"Scorer de similaridad baseline utilizando pesos por defecto no obtiene resultado esperado para d1\"\n",
    "    elif i==1:\n",
    "        assert similarities == {'url': 2, 'title': 0, 'headers': 0, 'anchors': 29, 'body_hits': 0}, \\\n",
    "          \"Scorer de similaridad baseline utilizando pesos por defecto no obtiene resultado esperado para d2\"\n",
    "    print(\"---------\\n\")\n",
    "\n",
    "print(\"Tests de clase BaselineScorer() superados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de _ranking_ 1: Similaridad del coseno\n",
    "\n",
    "Mas allá del _scoring_ _baseline_ (un tanto _naive_) anterior, la primera función de _ranking_ más elaborada que aplicaremos consistirá en una variante clásica de la similitud coseno (con la norma L1). Se trata esencialmente de construir el vector del documento y el vector de la consulta para luego tomar el producto escalar como resultado.\n",
    "En realidad, es exactamente lo mismo que hemos hecho ya para el simple conteo de términos anterior (suponiendo que el vector consulta era siempre un vector binario con 1 en los términos contenidos en la consulta, y 0 en el resto). Ahora, sin embargo, en lugar de tomar simplemente los conteos de términos (tanto en la consulta como en el documento), podrían considerarse varias alternativas para calcular el peso (=componente) de cada término, decidiendo:\n",
    "\n",
    "1. Cómo se calcula exactamente la frecuencia de cada término.\n",
    "2. Cómo se realiza la ponderación por frecuencia de documento de cada término.\n",
    "3. La estrategia de normalización seguida.\n",
    "\n",
    "De nuevo, la figura 6.15 de la página 128 del libro de Manning ([enlace](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf)) nos recuerda todas las posibles alternativas para ello, según la notación SMART.\n",
    "\n",
    "En lo que sigue discutiremos las opciones para ambos vectores por separado.\n",
    "\n",
    "## Vectores de consulta\n",
    "\n",
    "* **Frecuencia del término** (_tf_):\n",
    "Las frecuencias crudas pueden computarse de forma sencilla a partir de los términos de la consulta. Deberían corresponderse, en la mayoría de los casos, con un simple 1 para cada término que apareciese en la consulta, equivalente a la opción _\"boolean\"_ (pero no necesariamente, ya que algún término podría aparecer repetido). Dicha frecuencia cruda podría, si se desease, ser escalada sublinealmente (usando el logaritmo). En todo caso, en este notebook mantendremos el enfoque simple del conteo natural de términos, muy similar al vector booleano, dado que la inmensa mayoría de las consultas del _dataset_ no contienen términos repetidos.\n",
    "\n",
    "* **Frecuencia del documento** (_df_):\n",
    "Cada uno de los términos en el vector de la consulta deberá entonces ser pesado (=multiplicado) usando el valor de IDF correspondiente para cada término. Usaremos, como ya se comentó antes en este mismo notebook, el IDF computado a partir del corpus de la práctica 1. Recuérdese que, para el caso de palabras que no apareciesen en dicho corpus, se usará la técnica del suavizado Laplaciano (es decir, simplemente sumar 1 en el numerador y en el denominador; esto equivale esencialmente a asumir la existencia de un hipotético documento _\"dummy\"_ que contiene todos los posibles términos)\n",
    "\n",
    "* **Normalización** (_norm_):\n",
    "En ningún caso será necesario normalizar el vector de consulta, ya que cualquier posible normalización se aplicaría por igual al cruzarlo con todos los correspondientes documentos resultado, obteniéndose un simple escalado uniforme de los valores de _scoring_, lo que obviamente no influiría en absoluto en el correspondiente _ranking_ de resultados.\n",
    "\n",
    "## Vectores de documento\n",
    "\n",
    "* **Frecuencia del término** (_tf_):\n",
    "Al igual que con el vector de consulta, podremos utilizar directamente las frecuencias crudas, o, alternativamente, aplicar algún tipo de escalado sublineal. En particular, el escalado sublineal típico es $tf_i = 1 + log(rs_i)$ si $rs_i > 0$, o simplemente $0$ en caso contrario. Así, por ejemplo, para el anterior vector _tf_ del campo **body** del documento _d_, el vector resultante sería $[\\text{1+log(10)  1+log(7)  1+log(1)  0}]^T$ (de nuevo, puede encontrarse más información sobre el escalado sublineal del término _tf_ en la página 126, sección 6.4.1 del [libro de Manning](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf)).\n",
    "\n",
    "* **Frecuencia del documento** (_df_):\n",
    "No utilizaremos ningún tipo de frecuencia del documento en el vector de documento. En lugar de ello, se incorporará este peso únicamente en el vector de consulta, como se describía en el apartado anterior.\n",
    "\n",
    "* **Normalización** (_norm_):\n",
    "En este caso, no podemos usar la normalización del coseno, dado que nuestros archivos de entrenamiento no proporcionan acceso al contenido del documento en sí, sino solo un resumen de campos. Por lo tanto, no sabemos ni qué otros términos aparecen, ni el recuento de los mismos, en el campo **body**. En lugar de ello, por tanto, utilizaremos la normalización de longitud. Además, dado que puede haber enormes discrepancias entre las longitudes de los diferentes campos, dividimos todos los campos por el mismo factor de normalización, la propia longitud del campo **body**. Dado, además, el hecho de que algunos documentos aparecen con una longitud de 0, una buena estrategia es, de nuevo, agregar un valor (p.e. 500), a la longitud del cuerpo de cada documento. El valor concreto a utilizar, además, puede ser también utilizado para experimentar con ésta u otras estrategias de suavizado, y observar su posible influencia en los resultados de _ranking_ obtenidos.\n",
    "\n",
    "## Pesado relativo de los diferentes campos\n",
    "\n",
    "Dado un documento $d$ y una consulta $q$, si $qv_q$ es el vector resultante de la consulta y $tf_{d,u}$, $tf_{d,t}$, $tf_{d,b}$, $tf_{d,h}$ y $tf_{d,a}$ son los vectores resultantes para cada uno de los campos **url**, **title**, **body**, **header** and **anchor**, respectivamente, definimos el _scoring_ global neto como (nótese que el símbolo $\\cdot$ se usa en la siguiente expresión tanto para el producto escalar entre vectores como para el producto de un escalar por un vector):\n",
    "\n",
    "$$qv_q \\cdot (c_u \\cdot tf_{d,u} + c_t \\cdot tf_{d,t} + c_b \\cdot tf_{d,b} + c_h \\cdot tf_{d,h} + c_a \\cdot tf_{d,a})$$\n",
    "\n",
    "Donde $c_u$, $c_t$, $c_b$, $c_h$ y $c_a$ son los pesos dados a los campos **url**, **title**, **body**, **header** y **anchor**, respectivamente.\n",
    "\n",
    "Por supuesto, para usar la expresión anterior necesitamos determinar de una forma sensata los pesos para cada uno de estos cinco campos. En este sentido, trataremos de escogerlos de forma que la función NDCG de evaluación (que describiremos más adelante) obtenga un valor lo más optimizado posible cuando sea aplicada al conjunto de test completo. Usaremos el conjunto de _training_ para intentar derivar dicho valor óptimo de los cinco parámetros mencionados, para luego evaluar su rendimiento en el conjunto de _test_. En una primera instancia, lo intentaremos hacer manualmente, intentando razonar simplemente sobre la importancia relativa de los diferentes pesos. Al final del notebook sustituiremos esta suerte de \"razonamiento manual\" por una śencilla técnica de _machine learning_.\n",
    "\n",
    "Nótese que el valor absoluto de dichos pesos no importará, sólo la relación (ratio) entre ellos (valores relativos), dado que si multiplicamos todos los pesos por la misma constante, el _scoring_ final quedará simplemente multiplicado por dicha constante para todos los documentos por igual, lo que obviamente no afectará en absoluto a la ordenación.\n",
    "\n",
    "### Esquema de _weighting_ inicial\n",
    "\n",
    "Proporcionamos aquí un esquema de pesado por defecto de partida, que puede después variarse para intentar mejorar el rendimiento (medido con NDGC). Nótese que:\n",
    "* En estos primeros pesos por defecto se asigna una importancia del peso de la URL 100 veces superior a la del resto de pesos, a los que, por otro lado, se da un peso equivalente.\n",
    "* Se añade al conjunto de parámetros ajustables un parámetro de suavizado de la longitud del documento (`smoothing_body_length`), término que podrá ser modificado para influir en la función de _scoring_ final. Dicho término será simplemente sumado a la longitud original de cada documento (con lo que, en todo caso, y como se comentó anteriormente, se evitará siempre la posible división por cero para documentos en los cuales la longitud indicada en el documento del dataset de entrada sea cero). Se deduce, pues, que dar un valor cada vez mayor para este parámetro implicará una influencia progresivamente menor del campo `body_length` original de cada documento particular, ya que el correspondiente factor de influencia de la longitud tenderá con ello a homogeneizarse más para todos los documentos, al disminuir progresivamente el peso relativo del valor inicial de `body_length` en la suma total del denominador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cosine = {\n",
    "    \"url_weight\" : 10,\n",
    "    \"title_weight\": 0.1,\n",
    "    \"body_hits_weight\" : 0.1,\n",
    "    \"header_weight\" : 0.1,\n",
    "    \"anchor_weight\" : 0.1,\n",
    "    \"smoothing_body_length\" : 500,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase _CosineSimilarityScorer_\n",
    "\n",
    "He aquí la definición de una clase para realizar un _scoring_ basado en la similaridad del coseno (basada en la clase `AbstractScorer` definida anteriormente, y reimplementando los métodos adecuados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityScorer(AbstractScorer):\n",
    "\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None):\n",
    "        # Inicializamos clase base \"AbstractScorer\", ...\n",
    "        super().__init__(idf, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme)\n",
    "        self.query_dict = query_dict\n",
    "        # ... y añadimos los parámetros necesarios (5 pesos de los 5 campos + factor suavizado longitud):\n",
    "        try:\n",
    "            self.smoothing_body_length = params[\"smoothing_body_length\"]\n",
    "        except:\n",
    "            self.smoothing_body_length = 0\n",
    "        self.weights = {\"url\": params[\"url_weight\"], \"title\": params[\"title_weight\"],\n",
    "                        \"headers\": params[\"header_weight\"], \"anchors\": params[\"anchor_weight\"],\n",
    "                        \"body_hits\": params[\"body_hits_weight\"]}\n",
    "\n",
    "    def get_query_vector(self, q):\n",
    "        \"\"\" Usando los vectores de conteo crudos de la clase base, aplica diferentes variantes\n",
    "            SMART para obtener el correspondiente vector numéricos de consulta modificado.\n",
    "        Args:\n",
    "            q (Query): Query(\"Una consulta determinada\")\n",
    "        Returns:\n",
    "            query_vec (dict): El vector resultado.\n",
    "        \"\"\"\n",
    "        # Método de conteo de la clase base:\n",
    "        query_vec = super().get_query_vector(q)\n",
    "\n",
    "        # Frecuencia de documento (implementadas las variantes n, b, y t SMART):\n",
    "        if self.query_weight_scheme[\"df\"] == \"n\":     # No se modifica query_vec:\n",
    "            pass\n",
    "        elif self.query_weight_scheme[\"df\"] == \"b\":   # Vector query_vec booleano:\n",
    "            for key_term in query_vec.keys():\n",
    "                query_vec[key_term] = 1\n",
    "        elif self.query_weight_scheme[\"df\"] == \"t\":   # Modificación IDF de query_vec:\n",
    "            for key_term in query_vec.keys():\n",
    "                query_vec[key_term] = self.idf.get_idf(key_term)\n",
    "\n",
    "        return query_vec\n",
    "\n",
    "    def get_doc_vector(self, q, d):\n",
    "        \"\"\" Usando los vectores de conteo crudos de la clase base, aplica diferentes variantes\n",
    "            SMART para obtener los correspondientes vectores numéricos de documento modificados.\n",
    "        Args:\n",
    "        q (Query) : Query(\"Una consulta\")\n",
    "        d (Document) : Query(\"Una consulta\")[\"Un URL\"]\n",
    "        Returns:\n",
    "        doc_vec (dict) : Vectores numéricos modificados, de nuevo con esquema (tipo_de_campo -> (término -> conteo))\n",
    "                    Ejemplo: \"{'url':   {'stanford': 0.13, 'aoerc': 0, 'pool': 0, 'hours': 0},\n",
    "                               'title': {'stanford': 0.11, 'aoerc': 0, 'pool': 0, 'hours': 0},\n",
    "                               ...\n",
    "                               }\"\n",
    "        \"\"\"\n",
    "        # Método de conteo de la clase base:\n",
    "        doc_vec = super().get_doc_vector(q, d)\n",
    "\n",
    "        # Frecuencia de término (implementadas las variantes n y l SMART):\n",
    "        if self.doc_weight_scheme[\"tf\"] == \"n\":   # No se modifica doc_vec:\n",
    "            pass\n",
    "        elif self.doc_weight_scheme[\"tf\"] == \"l\": # Modificación logarítmica (sublineal) de doc_vec\n",
    "            for key_field in doc_vec.keys():\n",
    "                for key_term in doc_vec[key_field].keys():\n",
    "                    doc_vec[key_field][key_term] = 1 + math.log(doc_vec[key_field][key_term])\n",
    "\n",
    "        # Normalización:\n",
    "        if self.doc_weight_scheme['norm'] == \"default\":\n",
    "            doc_vec = self.normalize_doc_vec(q, d, doc_vec)\n",
    "\n",
    "        return doc_vec\n",
    "\n",
    "    def get_sim_score(self, q, d, field_type):\n",
    "        \"\"\" Cálculo del scoring para un campo individual:\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            field_type (str) : El campo del que se usará el vector de documento.\n",
    "        Return:\n",
    "            score (float) : El scoring individual (para el campo field_type) del par (q,d):\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        q_vec = self.get_query_vector(q)\n",
    "        d_vec = self.get_doc_vector(q, d)\n",
    "        score = 0\n",
    "        if field_type in d_vec.keys():\n",
    "            for term in d_vec[field_type].keys():\n",
    "                if term in q_vec.keys():\n",
    "                    score += d_vec[field_type][term] * q_vec[term]\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return score\n",
    "\n",
    "    def get_net_score(self, q, d):\n",
    "        \"\"\" Cálculo del scoring global (neto), usando los cinco pesos:\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "        Return:\n",
    "            score (float) : El scoring global (neto, sumando todos los campos) del par (q,d):\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        score = 0\n",
    "        for field_type in [\"url\", \"title\", \"headers\", \"anchors\", \"body_hits\"]:\n",
    "            score +=  self.weights[field_type] * self.get_sim_score(q, d, field_type)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return score\n",
    "\n",
    "    ## Normalización\n",
    "    def normalize_doc_vec(self, q, d, doc_vec):\n",
    "        \"\"\" Normalización del vector de documento:\n",
    "        Damos una normalización uniforme basada en la longitud del documento, tal\n",
    "        y como se discutió en el item \"Normalización\" del anterior apartado\n",
    "        \"Vectores de documento\".\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            doc_vec (dict) : El vector de documento.\n",
    "        Return:\n",
    "            doc_vec (dict) : El vector de documento tras la normalización.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        for key_field in doc_vec.keys():\n",
    "            for key_term in doc_vec[key_field].keys():\n",
    "                doc_vec[key_field][key_term] /= (d.body_length + self.smoothing_body_length)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        # print(d.body_length, self.smoothing_body_length)\n",
    "\n",
    "        return doc_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He aquí una primera prueba sencilla de _scoring_ de un par ($q$,$d$) usando esta similaridad del coseno, en particular usando el esquema SMART _ddd.qqq_ = _nnn.bnn_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector consulta:  Counter({'stanford': 1, 'aoerc': 1, 'pool': 1, 'hours': 1}) \n",
      "\n",
      "Vector de documento original:\n",
      " {'url': Counter({'events': 1, 'stanford': 1, 'edu': 1, '2014': 1, 'february': 1, '18': 1}), 'title': Counter({'events': 1, 'at': 1, 'stanford': 1, 'tuesday': 1, 'february': 1, '18': 1, '2014': 1}), 'headers': Counter({'stanford': 5, 'university': 1, 'event': 1, 'calendar': 1, 'teaching': 1, 'sex': 1, 'at': 1, 'rodin': 1, 'the': 1, 'complete': 1, 'collection': 1, 'rec': 1, 'trx': 1, 'suspension': 1, 'training': 1, 'memorial': 1, 'church': 1, 'open': 1, 'visiting': 1, 'hours': 1, 'alternative': 1, 'transportation': 1, 'counseling': 1, 'tm': 1, '3': 1, 'hour': 1, 'univ': 1, 'shc': 1, 'employees': 1, 'retirees': 1, 'family': 1, 'members': 1}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 10, 'aoerc': 7, 'pool': 1})} \n",
      "\n",
      "---\n",
      "Scoring campo url: 1\n",
      "Scoring campo title: 1\n",
      "Scoring campo headers: 6\n",
      "Scoring campo anchors: 0\n",
      "Scoring campo body_hits: 18\n",
      "\n",
      "Scoring neto: 12.5\n"
     ]
    }
   ],
   "source": [
    "q = Query(\"stanford aoerc pool hours\")\n",
    "d = query_dict[q]['http://events.stanford.edu/2014/February/18/']\n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": None}\n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 'n', \"norm\": None}\n",
    "cs = CosineSimilarityScorer(theIDF, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "print('Vector consulta: ', cs.get_query_vector(q), '\\n')\n",
    "print('Vector de documento original:\\n', cs.get_doc_vector(q, d), '\\n')\n",
    "print(\"---\")\n",
    "print(\"Scoring campo url:\", cs.get_sim_score(q,d,\"url\"))\n",
    "print(\"Scoring campo title:\", cs.get_sim_score(q,d,\"title\"))\n",
    "print(\"Scoring campo headers:\", cs.get_sim_score(q,d,\"headers\"))\n",
    "print(\"Scoring campo anchors:\", cs.get_sim_score(q,d,\"anchors\"))\n",
    "print(\"Scoring campo body_hits:\", cs.get_sim_score(q,d,\"body_hits\"))\n",
    "print(\"\\nScoring neto:\", cs.get_net_score(q,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y he aquí, para el mismo par ($q$,$d$), algunos posibles vectores alternativos, obtenidos usando diferentes variantes SMART, tanto para la consulta $q$ como para el documento $d$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector consulta original:  Counter({'stanford': 1, 'aoerc': 1, 'pool': 1, 'hours': 1}) \n",
      "\n",
      "Vector consulta IDF:  Counter({'aoerc': 4.995630807762446, 'pool': 2.446627545736658, 'hours': 1.2882309766291968, 'stanford': 0.14313251558629017}) \n",
      "\n",
      "Vector de documento original:\n",
      " {'url': Counter({'events': 1, 'stanford': 1, 'edu': 1, '2014': 1, 'february': 1, '18': 1}), 'title': Counter({'events': 1, 'at': 1, 'stanford': 1, 'tuesday': 1, 'february': 1, '18': 1, '2014': 1}), 'headers': Counter({'stanford': 5, 'university': 1, 'event': 1, 'calendar': 1, 'teaching': 1, 'sex': 1, 'at': 1, 'rodin': 1, 'the': 1, 'complete': 1, 'collection': 1, 'rec': 1, 'trx': 1, 'suspension': 1, 'training': 1, 'memorial': 1, 'church': 1, 'open': 1, 'visiting': 1, 'hours': 1, 'alternative': 1, 'transportation': 1, 'counseling': 1, 'tm': 1, '3': 1, 'hour': 1, 'univ': 1, 'shc': 1, 'employees': 1, 'retirees': 1, 'family': 1, 'members': 1}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 10, 'aoerc': 7, 'pool': 1})} \n",
      "\n",
      "-----\n",
      "Vector de documento normalizado:\n",
      " {'url': Counter({'events': 0.0006752194463200541, 'stanford': 0.0006752194463200541, 'edu': 0.0006752194463200541, '2014': 0.0006752194463200541, 'february': 0.0006752194463200541, '18': 0.0006752194463200541}), 'title': Counter({'events': 0.0006752194463200541, 'at': 0.0006752194463200541, 'stanford': 0.0006752194463200541, 'tuesday': 0.0006752194463200541, 'february': 0.0006752194463200541, '18': 0.0006752194463200541, '2014': 0.0006752194463200541}), 'headers': Counter({'stanford': 0.00337609723160027, 'university': 0.0006752194463200541, 'event': 0.0006752194463200541, 'calendar': 0.0006752194463200541, 'teaching': 0.0006752194463200541, 'sex': 0.0006752194463200541, 'at': 0.0006752194463200541, 'rodin': 0.0006752194463200541, 'the': 0.0006752194463200541, 'complete': 0.0006752194463200541, 'collection': 0.0006752194463200541, 'rec': 0.0006752194463200541, 'trx': 0.0006752194463200541, 'suspension': 0.0006752194463200541, 'training': 0.0006752194463200541, 'memorial': 0.0006752194463200541, 'church': 0.0006752194463200541, 'open': 0.0006752194463200541, 'visiting': 0.0006752194463200541, 'hours': 0.0006752194463200541, 'alternative': 0.0006752194463200541, 'transportation': 0.0006752194463200541, 'counseling': 0.0006752194463200541, 'tm': 0.0006752194463200541, '3': 0.0006752194463200541, 'hour': 0.0006752194463200541, 'univ': 0.0006752194463200541, 'shc': 0.0006752194463200541, 'employees': 0.0006752194463200541, 'retirees': 0.0006752194463200541, 'family': 0.0006752194463200541, 'members': 0.0006752194463200541}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 0.00675219446320054, 'aoerc': 0.004726536124240378, 'pool': 0.0006752194463200541})} \n",
      "\n",
      "-----\n",
      "Vector de documento con escalado logarítmico de tf:\n",
      " {'url': Counter({'events': 1.0, 'stanford': 1.0, 'edu': 1.0, '2014': 1.0, 'february': 1.0, '18': 1.0}), 'title': Counter({'events': 1.0, 'at': 1.0, 'stanford': 1.0, 'tuesday': 1.0, 'february': 1.0, '18': 1.0, '2014': 1.0}), 'headers': Counter({'stanford': 2.6094379124341005, 'university': 1.0, 'event': 1.0, 'calendar': 1.0, 'teaching': 1.0, 'sex': 1.0, 'at': 1.0, 'rodin': 1.0, 'the': 1.0, 'complete': 1.0, 'collection': 1.0, 'rec': 1.0, 'trx': 1.0, 'suspension': 1.0, 'training': 1.0, 'memorial': 1.0, 'church': 1.0, 'open': 1.0, 'visiting': 1.0, 'hours': 1.0, 'alternative': 1.0, 'transportation': 1.0, 'counseling': 1.0, 'tm': 1.0, '3': 1.0, 'hour': 1.0, 'univ': 1.0, 'shc': 1.0, 'employees': 1.0, 'retirees': 1.0, 'family': 1.0, 'members': 1.0}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 3.302585092994046, 'aoerc': 2.9459101490553135, 'pool': 1.0})} \n",
      "\n",
      "-----\n",
      "Vector de documento con escalado logarítmico y normalizado:\n",
      " {'url': Counter({'events': 0.0006752194463200541, 'stanford': 0.0006752194463200541, 'edu': 0.0006752194463200541, '2014': 0.0006752194463200541, 'february': 0.0006752194463200541, '18': 0.0006752194463200541}), 'title': Counter({'events': 0.0006752194463200541, 'at': 0.0006752194463200541, 'stanford': 0.0006752194463200541, 'tuesday': 0.0006752194463200541, 'february': 0.0006752194463200541, '18': 0.0006752194463200541, '2014': 0.0006752194463200541}), 'headers': Counter({'stanford': 0.001761943222440311, 'university': 0.0006752194463200541, 'event': 0.0006752194463200541, 'calendar': 0.0006752194463200541, 'teaching': 0.0006752194463200541, 'sex': 0.0006752194463200541, 'at': 0.0006752194463200541, 'rodin': 0.0006752194463200541, 'the': 0.0006752194463200541, 'complete': 0.0006752194463200541, 'collection': 0.0006752194463200541, 'rec': 0.0006752194463200541, 'trx': 0.0006752194463200541, 'suspension': 0.0006752194463200541, 'training': 0.0006752194463200541, 'memorial': 0.0006752194463200541, 'church': 0.0006752194463200541, 'open': 0.0006752194463200541, 'visiting': 0.0006752194463200541, 'hours': 0.0006752194463200541, 'alternative': 0.0006752194463200541, 'transportation': 0.0006752194463200541, 'counseling': 0.0006752194463200541, 'tm': 0.0006752194463200541, '3': 0.0006752194463200541, 'hour': 0.0006752194463200541, 'univ': 0.0006752194463200541, 'shc': 0.0006752194463200541, 'employees': 0.0006752194463200541, 'retirees': 0.0006752194463200541, 'family': 0.0006752194463200541, 'members': 0.0006752194463200541}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 0.002229969677916304, 'aoerc': 0.0019891358197537566, 'pool': 0.0006752194463200541})} \n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "q = Query(\"stanford aoerc pool hours\")\n",
    "d = query_dict[q]['http://events.stanford.edu/2014/February/18/']\n",
    "\n",
    "query_weight_scheme, doc_weight_scheme = None, None\n",
    "\n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 'n', \"norm\": None}\n",
    "cs = CosineSimilarityScorer(theIDF, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "print('Vector consulta original: ', cs.get_query_vector(q), '\\n')\n",
    "\n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None}\n",
    "cs = CosineSimilarityScorer(theIDF, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "print('Vector consulta IDF: ', cs.get_query_vector(q), '\\n')\n",
    "\n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": None}\n",
    "cs = CosineSimilarityScorer(theIDF, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "print('Vector de documento original:\\n', cs.get_doc_vector(q, d), '\\n')\n",
    "print(\"-----\")\n",
    "\n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": \"default\"}\n",
    "cs = CosineSimilarityScorer(theIDF, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "print('Vector de documento normalizado:\\n', cs.get_doc_vector(q, d), '\\n')\n",
    "print(\"-----\")\n",
    "\n",
    "doc_weight_scheme = {\"tf\": 'l', \"df\": 'n', \"norm\": None}\n",
    "cs = CosineSimilarityScorer(theIDF, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "print('Vector de documento con escalado logarítmico de tf:\\n', cs.get_doc_vector(q, d), '\\n')\n",
    "print(\"-----\")\n",
    "\n",
    "doc_weight_scheme = {\"tf\": 'l', \"df\": 'n', \"norm\": \"default\"}\n",
    "cs = CosineSimilarityScorer(theIDF, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "print('Vector de documento con escalado logarítmico y normalizado:\\n', cs.get_doc_vector(q, d), '\\n')\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de _ranking_ 2: BM25F\n",
    "\n",
    "Usaremos aquí la frecuencia de términos normalizada por campos ($ftf$, de _Field dependent normalized Term Frequency_). Así, para un término $t$ dado, y un campo $f \\in \\{url, header, body, title, anchor\\}$ en el documento $d$, usaremos:\n",
    "\n",
    "\\begin{equation}\n",
    "ftf_{d,f,t} = \\frac{tf_{d,f,t}}{1 + B_f((\\text{len}_{d,f} / \\text{avlen}_f) - 1)}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $tf_{d,f,t}$ es la frecuencia cruda de $t$ en el campo $f$ del documento $d$, $len_{d,f}$ es la longitud del campo $f$ en $d$, y $avlen_f$ es la longitud media del mismo campo en toda la colección.\n",
    "\n",
    "Por supuesto, las correspondientes variables $avlen_{body}$, $avlen_{url}$, $avlen_{title}$, $avlen_{header}$ y $avlen_{anchor}$ se deberán computar usando de nuevo el conjunto de _training_. Los valores de $B_f$ serán parámetros adicionales dependientes de cada uno de los campos $f$, y al igual que los $c_f$ de la sección anterior, deberán ser ajustados. Si $avlen_f$ fuese cero, entonces definiremos $ftf_{d,f,t} = 0$ (si bien esto no debería ser necesario en este _dataset_ en concreto).\n",
    "\n",
    "El peso global para el término $t$ en el documento $d$, usando ya todos los campos, sería:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{d,t} = \\sum_{f} W_f \\cdot ftf_{d,f,t}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Siendo, de nuevo, los distintos $W_f$ parámetros que determinan los pesos de importancia relativos dados a cada uno de los campos.\n",
    "\n",
    "Puesto que, además, tenemos también una característica adicional no textual (el <b>pagerank</b>), la incorporaremos también en nuestra función de _ranking_ usando el método descrito en las transparencias de teoría.\n",
    "\n",
    "En concreto, pues, y resumiendo, el _scoring_ global para el $d$ respecto a la consulta $q$ quedará definido como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{t \\in q} \\frac{w_{d,t}}{K_1 + w_{d,t}}idf_t + \\lambda V_{j}(f)\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "De nuevo aquí $K_1$ es un parámetro libre, y la función $V_{j}$ podría ser cualquier de las funciones logarítmicas, de saturación o sigmoide mencionadas en las transparencias de teoría, y que en este caso fijaremos simplemente como $V_{pagerank}(pr) = log(\\lambda'+pr)$.\n",
    "\n",
    "$\\lambda$ y $\\lambda'$ son los dos últimos parámetros libres adicionales para este modelo.\n",
    "\n",
    "## Clase _BM25FScorer_\n",
    "\n",
    "Definimos aquí la clase `BM25FScorer`, basada en la `CosineSimilarityScorer` anterior, pero incorporando en este caso todas las modificaciones necesarias para implementar la nueva funcionalidad descrita en los párrafos anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25FScorer(CosineSimilarityScorer):\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None):\n",
    "        super().__init__(idf, query_dict, params, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme)\n",
    "\n",
    "        # Añadimos aquí los pesos ya específicos para BM25, y los nuevos parámetros libres...\n",
    "        self.b_url = params['b_url']\n",
    "        self.b_title = params['b_title']\n",
    "        self.b_header = params['b_header']\n",
    "        self.b_body_hits = params['b_body_hits']\n",
    "        self.b_anchor = params['b_anchor']\n",
    "        self.k1 = params['k1']\n",
    "        self.pagerank_lambda = params['pagerank_lambda']\n",
    "        self.pagerank_lambda_prime = params['pagerank_lambda_prime']\n",
    "\n",
    "        # ... y aqui tres estructuras de datos adicionales, necesarias para la implementación\n",
    "        # (relativas al cálculo de longitudes totales de cada documento, longitudes medias\n",
    "        # para cada campo, y scorings previos de cada documento por su pagerank):\n",
    "        self.lengths = {} # Document -> field -> length\n",
    "        self.avg_length = {}\n",
    "        self.pagerank_scores = {}\n",
    "\n",
    "        # Cálculo inicial de las longitudes medias por campo (ver definición de método\n",
    "        # calc_avg_length() justo a continuación):\n",
    "        self.calc_avg_length()\n",
    "\n",
    "    def calc_avg_length(self, debug=False):\n",
    "        \"\"\" Computa las longitudes medias de cada campo en la colección.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        if(debug):\n",
    "            print(\"------ Computando las longitudes medias de cada campo en la colección ...   -------\")\n",
    "        count_docs, count_queries = 0, 0\n",
    "        count_docs_with_title, cum_len_title = 0, 0\n",
    "        count_docs_with_headers, cum_len_headers = 0, 0\n",
    "        count_docs_with_anchors, cum_len_anchors = 0, 0\n",
    "        count_docs_with_url, cum_len_url = 0, 0\n",
    "        count_docs_with_body_hits, cum_len_body_hits = 0, 0\n",
    "        self.lengths = {}\n",
    "        for i,q in enumerate(query_dict):\n",
    "            docs = query_dict[q]\n",
    "            count_queries += 1\n",
    "            for j,d in enumerate(docs):\n",
    "                count_docs += 1\n",
    "                doc = query_dict[q][d]\n",
    "                self.lengths[doc.url] = {}\n",
    "                # Campo 'title':\n",
    "                len_title = 0\n",
    "                if doc.title:\n",
    "                    count_docs_with_title += 1\n",
    "                    len_title = len(doc.title.split())\n",
    "                    cum_len_title += len_title\n",
    "                self.lengths[doc.url][\"title\"] = len_title\n",
    "                # Campo 'header':\n",
    "                len_headers = 0\n",
    "                if doc.headers:\n",
    "                    count_docs_with_headers += 1\n",
    "                    for header in doc.headers:\n",
    "                        len_headers += len(header.split())\n",
    "                    cum_len_headers += len_headers\n",
    "                self.lengths[doc.url][\"header\"] = len_headers\n",
    "                # Campo 'anchors'\n",
    "                len_anchors = 0\n",
    "                if doc.anchors:\n",
    "                    count_docs_with_anchors += 1\n",
    "                    for anchor in doc.anchors.keys():\n",
    "                        len_anchors += len(anchor.split()) * doc.anchors[anchor]\n",
    "                    cum_len_anchors += len_anchors\n",
    "                self.lengths[doc.url][\"anchors\"] = len_anchors\n",
    "                # Campo 'url'\n",
    "                len_url = 0\n",
    "                if doc.url:\n",
    "                    count_docs_with_url += 1\n",
    "                    _, dict_counters_url = self.parse_url(doc.url)\n",
    "                    for c in dict_counters_url.keys():\n",
    "                        len_url += dict_counters_url[c]\n",
    "                    cum_len_url += len_url\n",
    "                self.lengths[doc.url][\"url\"] = len_url\n",
    "                # Campo 'body_hits'\n",
    "                len_body_hits = 0\n",
    "                if doc.body_length:\n",
    "                    count_docs_with_body_hits += 1\n",
    "                    len_body_hits = doc.body_length\n",
    "                    cum_len_body_hits += len_body_hits\n",
    "                self.lengths[doc.url][\"body_hits\"] = len_body_hits\n",
    "                # Sólo para depurar:\n",
    "                # if i<=8 and j==4:\n",
    "                    # print(f\"TITLE={doc.title}, LEN={len(doc.title.split())}\")\n",
    "                    # print(f\"HEADERS={doc.headers}, LEN={len_headers}\")\n",
    "                    # print(f\"ANCHORS={doc.anchors}, LEN={len_anchors}\")\n",
    "                    # print(f\"URL={doc.url}={self.parse_url(doc.url)}, LEN={len_url}\")\n",
    "                    # print(f\"BODY_LENGTH={doc.body_length}, LEN={len_body_hits}\")\n",
    "        avg_len_title = cum_len_title/count_docs_with_title\n",
    "        avg_len_headers = cum_len_headers/count_docs_with_headers\n",
    "        avg_len_anchors = cum_len_anchors/count_docs_with_anchors\n",
    "        avg_len_url = cum_len_url/count_docs_with_url\n",
    "        avg_len_body_hits = cum_len_body_hits/count_docs_with_body_hits\n",
    "\n",
    "        if(debug):\n",
    "            print(f\"count_queries={count_queries}\\n\"\n",
    "                f\"count_docs={count_docs}\\n\"\n",
    "                f\"count_docs_with_title={count_docs_with_title}\\n\"\n",
    "                f\"count_docs_with_headers={count_docs_with_headers}\\n\"\n",
    "                f\"count_docs_with_anchors={count_docs_with_anchors}\\n\"\n",
    "                f\"count_docs_with_url={count_docs_with_url}\\n\"\n",
    "                f\"count_docs_with_body_hits={count_docs_with_body_hits}\\n\")\n",
    "            print(f\"avg_len_title={avg_len_title}\")\n",
    "            print(f\"avg_len_headers={avg_len_headers}\")\n",
    "            print(f\"avg_len_anchors={avg_len_anchors}\")\n",
    "            print(f\"avg_len_url={avg_len_url}\")\n",
    "            print(f\"avg_len_body_hits={avg_len_body_hits}\")\n",
    "\n",
    "        self.avg_length = {\"title\": avg_len_title, \"headers\": avg_len_headers,\n",
    "                           \"anchors\": avg_len_anchors, \"url\": avg_len_url,\n",
    "                           \"body_hits\": avg_len_body_hits}\n",
    "        # self.length = {}\n",
    "        # self.pagerank_scores = {}\n",
    "        if(debug):\n",
    "            print(\"------ ...Completado (longitudes medias de la colección computadas) -------\")\n",
    "        ### END YOUR CODE (FIXME)\n",
    "\n",
    "    def normalize_doc_vec(self, q, d, doc_vec, debug=False):\n",
    "        \"\"\" Normalizar las frecuencias crudas de los diferentes campos en el documento\n",
    "            d usando la ecuación (1) especificada más arriba.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            doc_vec (dict) : El vector de documento a normalizar.\n",
    "        Return:\n",
    "            doc_vec (dict) : El vector de documento normalizado.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        if debug:\n",
    "            print(\"NORM BEFORE:\", doc_vec)\n",
    "        for key_field in doc_vec.keys():\n",
    "            if key_field == \"url\":\n",
    "                Bf = self.b_url\n",
    "                lendf = self.lengths[d.url][\"url\"]\n",
    "                avlenf = self.avg_length[\"url\"]\n",
    "            elif key_field == \"title\":\n",
    "                Bf = self.b_title\n",
    "                lendf = self.lengths[d.url][\"title\"]\n",
    "                avlenf = self.avg_length[\"title\"]\n",
    "            elif key_field == \"header\":\n",
    "                Bf = self.b_header\n",
    "                lendf = self.lengths[d.url][\"headers\"]\n",
    "                avlenf = self.avg_length[\"headers\"]\n",
    "            elif key_field == \"body_hits\":\n",
    "                Bf = self.b_body_hits\n",
    "                lendf = self.lengths[d.url][\"body_hits\"]\n",
    "                avlenf = self.avg_length[\"body_hits\"]\n",
    "            elif key_field == \"anchor\":\n",
    "                Bf = self.b_anchor\n",
    "                lendf = self.lengths[d.url][\"anchors\"]\n",
    "                avlenf = self.avg_length[\"anchors\"]\n",
    "            for key_term in doc_vec[key_field].keys():\n",
    "                doc_vec[key_field][key_term] /= (1+Bf*((lendf/avlenf)-1))\n",
    "        if debug:\n",
    "            print(\"NORM AFTER:\", doc_vec)\n",
    "\n",
    "        return doc_vec\n",
    "        ### END YOUR CODE (FIXME)\n",
    "\n",
    "    def get_net_vector(self, q, d):\n",
    "        \"\"\" Obtener el vector neto global para el par (q,d), usando la ecuación (2) anterior.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "        Return:\n",
    "            doc_vec (dict) : El vector de documento normalizado (ya sólo uno, incluyendo todos los términos incluídos en todos los campos).\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        n_vec = {}\n",
    "        d_vec = self.get_doc_vector(q, d)\n",
    "        for field_type in [\"url\", \"title\", \"headers\", \"anchors\", \"body_hits\"]:\n",
    "            if field_type in d_vec.keys():\n",
    "                for term in d_vec[field_type].keys():\n",
    "                    if term not in n_vec.keys():\n",
    "                        n_vec[term] = 0\n",
    "                    n_vec[term] += d_vec[field_type][term] * self.weights[field_type]\n",
    "        return n_vec\n",
    "        ### END YOUR CODE (FIXME)\n",
    "\n",
    "    def get_net_score(self, q, d):\n",
    "        \"\"\" Obtener la puntuación global BM25F para el par (q,d), usando la ecuación (3) anterior.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "        Return:\n",
    "            doc_vec (dict) : El scoring neto global, incluyendo ya también la puntuación por pagerank.\n",
    "        \"\"\"\n",
    "        q_vec = self.get_query_vector(q)\n",
    "        n_vec = self.get_net_vector(q, d)\n",
    "        score = 0\n",
    "        for term in n_vec.keys():\n",
    "            if term in q_vec.keys():\n",
    "                score += (n_vec[term]/(self.k1+n_vec[term])) * self.idf.get_idf(term)\n",
    "        score += self.pagerank_lambda * math.log(self.pagerank_lambda_prime + d.pagerank)\n",
    "        # print(\"PAGERANK:\", d.pagerank)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la nueva clase _BM25FScorer_, primero con unos ciertos parámetros iniciales en los que hemos fijado $b_f=0 \\quad \\forall f$, así como $k_1=\\lambda = \\lambda'=0$, simplemente para depurar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSULTA: stanford aoerc pool hours \n",
      "\n",
      "DOCUMENTO: url: http://events.stanford.edu/2014/February/18/\n",
      " title: events at stanford tuesday february 18 2014\n",
      " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
      " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
      " body_length: 981\n",
      " pagerank: 1\n",
      "\n",
      "\n",
      "Vector de consulta: Counter({'aoerc': 4.995630807762446, 'pool': 2.446627545736658, 'hours': 1.2882309766291968, 'stanford': 0.14313251558629017})\n",
      "\n",
      "Vector de documento: {'url': Counter({'events': 1.0, 'stanford': 1.0, 'edu': 1.0, '2014': 1.0, 'february': 1.0, '18': 1.0}), 'title': Counter({'events': 1.0, 'at': 1.0, 'stanford': 1.0, 'tuesday': 1.0, 'february': 1.0, '18': 1.0, '2014': 1.0}), 'headers': Counter({'stanford': 5.0, 'university': 1.0, 'event': 1.0, 'calendar': 1.0, 'teaching': 1.0, 'sex': 1.0, 'at': 1.0, 'rodin': 1.0, 'the': 1.0, 'complete': 1.0, 'collection': 1.0, 'rec': 1.0, 'trx': 1.0, 'suspension': 1.0, 'training': 1.0, 'memorial': 1.0, 'church': 1.0, 'open': 1.0, 'visiting': 1.0, 'hours': 1.0, 'alternative': 1.0, 'transportation': 1.0, 'counseling': 1.0, 'tm': 1.0, '3': 1.0, 'hour': 1.0, 'univ': 1.0, 'shc': 1.0, 'employees': 1.0, 'retirees': 1.0, 'family': 1.0, 'members': 1.0}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 10.0, 'aoerc': 7.0, 'pool': 1.0})}\n",
      "\n",
      "Vector neto: {'events': 0.25, 'stanford': 3.5, 'edu': 0.1, '2014': 0.25, 'february': 0.25, '18': 0.25, 'at': 0.4, 'tuesday': 0.15, 'university': 0.25, 'event': 0.25, 'calendar': 0.25, 'teaching': 0.25, 'sex': 0.25, 'rodin': 0.25, 'the': 0.25, 'complete': 0.25, 'collection': 0.25, 'rec': 0.25, 'trx': 0.25, 'suspension': 0.25, 'training': 0.25, 'memorial': 0.25, 'church': 0.25, 'open': 0.25, 'visiting': 0.25, 'hours': 0.25, 'alternative': 0.25, 'transportation': 0.25, 'counseling': 0.25, 'tm': 0.25, '3': 0.25, 'hour': 0.25, 'univ': 0.25, 'shc': 0.25, 'employees': 0.25, 'retirees': 0.25, 'family': 0.25, 'members': 0.25, 'aoerc': 1.4000000000000001, 'pool': 0.2}\n",
      "\n",
      "Scoring neto: 8.873621845714592\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos la consulta y el documento de ejemplo:\n",
    "q = Query(\"stanford aoerc pool hours\")\n",
    "d = query_dict[q]['http://events.stanford.edu/2014/February/18/']\n",
    "print(\"CONSULTA:\", q,\"\\n\")\n",
    "print(\"DOCUMENTO:\", d)\n",
    "\n",
    "# Usamos consulta booleana, sin normalizar, e incluyendo en ella el IDF...\n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None}\n",
    "# ... y con conteo de frecuencias crudas iniciales para el documento, normalizados por\n",
    "# zonas de acuerdo a la ecuación (1):\n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": \"default\"}\n",
    "\n",
    "# Creamos el scorer BM25F con los anteriores parámetros, e inicialmente con los respectivos b_f\n",
    "# inicializados a 0.0 para comprobar la corrección de los cálculos en los vectores separados\n",
    "# por campos:\n",
    "params_bm25f = {\n",
    "    \"url_weight\" : 0.1,\n",
    "    \"title_weight\": 0.15,\n",
    "    \"body_hits_weight\" : 0.2,\n",
    "    \"header_weight\" : 0.25,\n",
    "    \"anchor_weight\" : 0.30,\n",
    "    \"b_url\" : 0.0,\n",
    "    \"b_title\" : 0.0,\n",
    "    \"b_header\" : 0.0,\n",
    "    \"b_body_hits\" : 0.0,\n",
    "    \"b_anchor\" : 0.0,\n",
    "    \"k1\": 0.0,\n",
    "    \"pagerank_lambda\" : 0.0,\n",
    "    \"pagerank_lambda_prime\" : 0.0,\n",
    "}\n",
    "bm25f_scorer = BM25FScorer(theIDF, query_dict, params_bm25f, query_weight_scheme, doc_weight_scheme)\n",
    "\n",
    "print('\\nVector de consulta:', bm25f_scorer.get_query_vector(q))\n",
    "print('\\nVector de documento:', bm25f_scorer.get_doc_vector(q, d))\n",
    "print('\\nVector neto:', bm25f_scorer.get_net_vector(q, d))\n",
    "print('\\nScoring neto:', bm25f_scorer.get_net_score(q, d))\n",
    "\n",
    "assert bm25f_scorer.get_net_score(q, d)  == sum([theIDF.get_idf(term) if term in q else 0 for term, val in bm25f_scorer.get_net_vector(q, d).items()]), \\\n",
    "       \"Fallo en chequeo de la clase BM25FScorer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si observamos la información impresa por la celda anterior, y prestamos atención al vector de documento impreso (separado por campos), comprobamos que los vectores resultantes coinciden con los de conteo originales (como corresponde a los valores $b_f=0 \\quad \\forall f$ usados para depurar en primera instancia).\n",
    "\n",
    "Obsérvese también que el vector neto combina ya todos los términos en un sólo vector, según los pesos indicados en los parámetros usados (p.e., para el término _\"events\"_, que aparece con un valor de 1.0 tanto en el campo **url** como en el campo **title**, el valor es de 0.25, como corresponde a la suma $0.1*1.0+0.15*1.0 = 0.25$, con $w_u=0.1$ y $w_t=0.15$. El término _\"aoerc\"_, por su parte, aparece con un valor 1.4, correspondiente en este caso a la suma $0.2*7.0 = 1.4$, al aparecer únicamente en el campo **body_hits**, con $w_b=0.2$ y un conteo de apariciones de exactamente 7.0 en dicho campo.\n",
    "\n",
    "Finalmente, puede comprobarse también que el _scoring_ neto obtenido coincide en este caso con la simple suma de los IDF de todos los términos incluidos en la consulta, como debe ser el caso al aplicar la ecuación (3) con $k_1=\\lambda = \\lambda'=0$.\n",
    "\n",
    "Un ejercicio interesante es cambiar ahora los parámetros libres con otros valores con más sentido, para observar sus respectivas influencias en los resultados. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector de consulta: Counter({'aoerc': 4.995630807762446, 'pool': 2.446627545736658, 'hours': 1.2882309766291968, 'stanford': 0.14313251558629017})\n",
      "\n",
      "Vector de documento: {'url': Counter({'events': 1.1888553231609935, 'stanford': 1.1888553231609935, 'edu': 1.1888553231609935, '2014': 1.1888553231609935, 'february': 1.1888553231609935, '18': 1.1888553231609935}), 'title': Counter({'events': 0.9730647184709617, 'at': 0.9730647184709617, 'stanford': 0.9730647184709617, 'tuesday': 0.9730647184709617, 'february': 0.9730647184709617, '18': 0.9730647184709617, '2014': 0.9730647184709617}), 'headers': Counter({'stanford': 4.865323592354809, 'university': 0.9730647184709617, 'event': 0.9730647184709617, 'calendar': 0.9730647184709617, 'teaching': 0.9730647184709617, 'sex': 0.9730647184709617, 'at': 0.9730647184709617, 'rodin': 0.9730647184709617, 'the': 0.9730647184709617, 'complete': 0.9730647184709617, 'collection': 0.9730647184709617, 'rec': 0.9730647184709617, 'trx': 0.9730647184709617, 'suspension': 0.9730647184709617, 'training': 0.9730647184709617, 'memorial': 0.9730647184709617, 'church': 0.9730647184709617, 'open': 0.9730647184709617, 'visiting': 0.9730647184709617, 'hours': 0.9730647184709617, 'alternative': 0.9730647184709617, 'transportation': 0.9730647184709617, 'counseling': 0.9730647184709617, 'tm': 0.9730647184709617, '3': 0.9730647184709617, 'hour': 0.9730647184709617, 'univ': 0.9730647184709617, 'shc': 0.9730647184709617, 'employees': 0.9730647184709617, 'retirees': 0.9730647184709617, 'family': 0.9730647184709617, 'members': 0.9730647184709617}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 14.09836268453252, 'aoerc': 9.868853879172764, 'pool': 1.409836268453252})}\n",
      "\n",
      "Vector neto: {'events': 0.21619200416319553, 'stanford': 2.1125606318519283, 'edu': 0.11888553231609936, '2014': 0.21619200416319553, 'february': 0.21619200416319553, '18': 0.21619200416319553, 'at': 0.19461294369419235, 'tuesday': 0.09730647184709618, 'university': 0.09730647184709618, 'event': 0.09730647184709618, 'calendar': 0.09730647184709618, 'teaching': 0.09730647184709618, 'sex': 0.09730647184709618, 'rodin': 0.09730647184709618, 'the': 0.09730647184709618, 'complete': 0.09730647184709618, 'collection': 0.09730647184709618, 'rec': 0.09730647184709618, 'trx': 0.09730647184709618, 'suspension': 0.09730647184709618, 'training': 0.09730647184709618, 'memorial': 0.09730647184709618, 'church': 0.09730647184709618, 'open': 0.09730647184709618, 'visiting': 0.09730647184709618, 'hours': 0.09730647184709618, 'alternative': 0.09730647184709618, 'transportation': 0.09730647184709618, 'counseling': 0.09730647184709618, 'tm': 0.09730647184709618, '3': 0.09730647184709618, 'hour': 0.09730647184709618, 'univ': 0.09730647184709618, 'shc': 0.09730647184709618, 'employees': 0.09730647184709618, 'retirees': 0.09730647184709618, 'family': 0.09730647184709618, 'members': 0.09730647184709618, 'aoerc': 0.9868853879172765, 'pool': 0.1409836268453252}\n",
      "\n",
      "Scoring neto: 4.093638107806798\n"
     ]
    }
   ],
   "source": [
    "params_bm25f = {\n",
    "    \"url_weight\" : 0.1,\n",
    "    \"title_weight\": 0.1,\n",
    "    \"body_hits_weight\" : 0.1,\n",
    "    \"header_weight\" : 0.1,\n",
    "    \"anchor_weight\" : 0.1,\n",
    "    \"b_url\" : 0.5,\n",
    "    \"b_title\" : 0.5,\n",
    "    \"b_header\" : 0.5,\n",
    "    \"b_body_hits\" : 0.5,\n",
    "    \"b_anchor\" : 0.5,\n",
    "    \"k1\": 1.0,\n",
    "    \"pagerank_lambda\" : 1.0,\n",
    "    \"pagerank_lambda_prime\" : 2.0,\n",
    "}\n",
    "bm25f_scorer = BM25FScorer(theIDF, query_dict, params_bm25f, query_weight_scheme, doc_weight_scheme)\n",
    "\n",
    "print('\\nVector de consulta:', bm25f_scorer.get_query_vector(q))\n",
    "print('\\nVector de documento:', bm25f_scorer.get_doc_vector(q, d))\n",
    "print('\\nVector neto:', bm25f_scorer.get_net_vector(q, d))\n",
    "print('\\nScoring neto:', bm25f_scorer.get_net_score(q, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante reevaluar varias veces la celda anterior jugando con pequeños cambios aislados en los diferentes parámetros libres, e interpretar de esta forma su efecto inmediato tanto en los vectores de documento separados por campos como en el vector neto resultado, y el correspondiente scoring neto final.\n",
    "\n",
    "# Función de ranking 3: ventana más pequeña\n",
    "\n",
    "Finalmente, añadiremos la influencia de los tamaños de ventana al algoritmo BM25F. Para una consulta deda $q$, definimos la _ventana más pequeña_ $w_{q,d}$ como la secuencia más corta de tokens en el documento $d$ tal que todos los términos en la consulta $q$ están presentes en dicha secuencia. Una ventana sólo puede especificarse para un campo en particular, y para el caso concreto del campo _anchor\\_text_, se exige que todos los términos en $q$ estén presentes en un enlace particular (esto es, si un término ocurre en el texto de un enlace, y otro en el de otro enlace diferente al mismo documento), no se considerará una misma ventana. Si, por otro lado, $d$ no contiene alguno de los términos de la consulta y, por tanto, no se puede encontrar dicha ventana, entonces definimos $w_{q,d} = \\infty$.\n",
    "\n",
    "Intuitivamente, cuanto más pequeña sea la ventana $w_{q,d}$, más relevante debería ser el documento $d$ para la consulta $q$. Por lo tanto, multiplicaremos el _scoring_ BM25F del documento por un factor de _boost_ basado en $w_{q,d}$, de forma que:\n",
    "\n",
    "* Si $w_{q,d} = \\infty$, entonces el factor de _boost_ es 1.0.\n",
    "* Si $w_{q,d} = |Q|$, siendo $Q$ el número de términos únicos en $q$, entonces multiplicaremos el _scoring_ original por un factor predeterminado máximo $B$, estrictamente mayor que uno.\n",
    "* Para valores de $w_{q,d}$ entre la longitud de la consulta e infinito, el factor de _boost_ deberá moverse entre B (valor máximo) y 1.0 (valor mínimo), decrementándose progresivamente conforme crece el tamaño de $w_{q,d}$.\n",
    "\n",
    "Para esto último, podría aquí usarse un decrecimiento de tipo exponencial, o bien del tipo $\\frac{1}{x}$. La siguiente gráfica ilustra una posible implementación de este último tipo de decrecimiento, para 4 valores diferentes de B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    }
   },
   "outputs": [],
   "source": [
    "# Ilustramos cuatro funciones de decrecimiento basado en 1/x para los valores\n",
    "# máximos de B = {4.0, 2.0, 1.75, 1.25}:\n",
    "len_q_list = 10.0\n",
    "max_win_len = 50\n",
    "len_min = np.arange(len_q_list,max_win_len+1)\n",
    "for B in [4.0, 2.0, 1.75, 1.25]:\n",
    "    factor_win = 1.0+(B-1.0)*len_q_list/len_min\n",
    "    plt.plot(len_min,factor_win,'+-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clase _WindowScorer_\n",
    "\n",
    "He aquí la definición de una clase _SmallestWindowScorer_ para implementar la técnica anterior. Se basa en la clase anterior _BM25Scorer_, ampliándola fundamentalmente con el método `get_boost_score`, que realiza el trabajo principal apoyándose a su vez en el método `min_sublist_with_all`. Éste último es el que en última instancia realiza la búsqueda efectiva de la ventana de texto más pequeña que contiene a todos los términos de la consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallestWindowScorer(BM25FScorer):\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None):\n",
    "        super().__init__(idf, query_dict, params, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme) #modified\n",
    "        # Añadimos el parámetro \"B\" (máximo boosting alcanzable):\n",
    "        self.B = params[\"B\"]\n",
    "\n",
    "    ### BEGIN YOUR CODE (FIXME)\n",
    "    def min_sublist_with_all(self, A, B):\n",
    "        \"\"\" Calcula el tamaño de la mínima sublista de B que contiene a toda la\n",
    "            lista de términos de A.\n",
    "        Args:\n",
    "            A (lista de términos) : Lista de términos en la consulta.\n",
    "            B (lista de términos) : Lista de términos en la que realizar la\n",
    "                                    búsqueda de la sublista más pequeña.\n",
    "        Return:\n",
    "            min_length (dict) : La longitud de la sublista más pequeña encontrada\n",
    "                                (float('inf') si no existe tal sublista).\n",
    "        \"\"\"\n",
    "        required_elements = len(A)        # Número de elementos a encontrar\n",
    "        element_count = defaultdict(int)  # Para contar elementos encontrados\n",
    "        left = 0                          # Borde izquierdo\n",
    "\n",
    "        # Convertimos A en diccionario para hacer búsquedas rápidas:\n",
    "        required = {element: 0 for element in A}\n",
    "\n",
    "        min_length = float('inf')\n",
    "        min_start = 0\n",
    "        elements_found = 0\n",
    "\n",
    "        # Vamos aumentando el borde derecho:\n",
    "        for right in range(len(B)):\n",
    "            if B[right] in required:\n",
    "                # Encontramos elemento de A en posición 'right' de B.\n",
    "                element_count[B[right]] += 1\n",
    "                if element_count[B[right]] == 1:\n",
    "                    elements_found += 1\n",
    "\n",
    "            # Intentamos hacer la longitud de la lista lo menor posible...\n",
    "            while elements_found == required_elements:\n",
    "                if right - left + 1 < min_length:\n",
    "                    min_length = right - left + 1\n",
    "                    min_start = left\n",
    "                # ... recortando por la izquierda:\n",
    "                if B[left] in required:\n",
    "                    element_count[B[left]] -= 1\n",
    "                    if element_count[B[left]] == 0:\n",
    "                        elements_found -= 1\n",
    "                left += 1\n",
    "\n",
    "        # Devolvemos la longitud de la mínima sublista encontrada:\n",
    "        return min_length\n",
    "    ### END YOUR CODE (FIXME)\n",
    "\n",
    "    def get_boost_score(self, q, d, debug=False):\n",
    "        \"\"\" Calcula el factor de boost basado en la técnica 'smallest window'.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            debug (bool) : Flag para imprimir posible información de depuración.\n",
    "        Return:\n",
    "            factor_win (float) : Factor de boost, entre 1 y B.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        # Lista de términos de la consulta de entrada:\n",
    "        q_list = str(q).split()\n",
    "\n",
    "        # Extraemos todas las listas de términos del documento, separadas\n",
    "        # y procesadas debidamente según campos:\n",
    "        all_lists = []\n",
    "        try: # Campo url:\n",
    "            d_url_list, _ = self.parse_url(d.url)\n",
    "        except:\n",
    "            d_url_list = []\n",
    "        try: # Campo title:\n",
    "            d_title_list = d.title.split()\n",
    "        except:\n",
    "            d_title_list = []\n",
    "        try: # Campo headers:\n",
    "            d_headers_lists = [h.split() for h in d.headers]\n",
    "        except:\n",
    "            d_headers_lists = []\n",
    "        try: # Campo anchors:\n",
    "            d_anchors_lists = [a.split() for a in d.anchors]\n",
    "        except:\n",
    "            d_anchors_lists = []\n",
    "        try: # Campo body:\n",
    "            # Construimos lista ficticia de términos a partir de body_hits,\n",
    "            # rellenando con \"-\" donde no se conoce el término:\n",
    "            max_pos = -1\n",
    "            for k in d.body_hits.keys():\n",
    "                max_pos_k = max(d.body_hits[k])\n",
    "                if max_pos_k > max_pos:\n",
    "                    max_pos = max_pos_k\n",
    "            d_body_hits_list = (max_pos+1)*[\"-\"]\n",
    "            for k in d.body_hits.keys():\n",
    "                for pos in d.body_hits[k]:\n",
    "                    d_body_hits_list[pos] = k\n",
    "        except:\n",
    "            d_body_hits_list = []\n",
    "\n",
    "        # Lista de todas las listas de términos a procesar para este documento:\n",
    "        all_lists += [d_url_list] + [d_title_list] + d_headers_lists + d_anchors_lists + [d_body_hits_list]\n",
    "        if debug:\n",
    "            print(f\"\\nq_list: {q_list}\")\n",
    "            print(\"\\nall_lists:\")\n",
    "            for l in all_lists:\n",
    "                print(f\" {l}\")\n",
    "\n",
    "        # Cómputo de la mínima ventana para todas las listas de todos los campos\n",
    "        # (tamaño definitivo de la mínima ventana para este documento):\n",
    "        if debug:\n",
    "            print(f\"\\n Ternas (q_list, lista, min_dist):\")\n",
    "        len_min = float(\"inf\")\n",
    "        for i,lt in enumerate(all_lists):\n",
    "            min_dist = self.min_sublist_with_all(q_list,lt)\n",
    "            if debug:\n",
    "               print(f\"{q_list}     {lt}     {min_dist}\")\n",
    "            if min_dist < len_min:\n",
    "                len_min = min_dist\n",
    "        factor_win = 1.0+(self.B-1)*len(q_list)/len_min if len_min != float('inf') else 1.0\n",
    "        return factor_win\n",
    "        ### END YOUR CODE (FIXME)\n",
    "\n",
    "    def get_net_score(self, q, d):\n",
    "        \"\"\" Obtener el scoring neto para un par consulta - documento utilizando\n",
    "            un factor de boosting computado usando la similaridad por la técnica\n",
    "            del mínimo tamaño de ventana.\n",
    "        Args:\n",
    "            d (Document) : El documento.\n",
    "            q (Query) : La consulta.\n",
    "\n",
    "        Return:\n",
    "            El scoring crudo multiplicado por el factor de boost.\n",
    "        \"\"\"\n",
    "        boost = self.get_boost_score(q, d)\n",
    "        raw_score = super().get_net_score(q, d)\n",
    "        return boost * raw_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos en la celda siguiente la clase anterior. Definimos unos parámetros por defecto para la clase (incluyendo un $B$ máximo de 2.0), elegimos una consulta _q_ y un documento _d_ de prueba, y calculamos un factor de _boost_ en modo `debug=True`, para comprobar la corrección de los cómputos intermedios para calcularlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Parámetros para la creación de la clase:\n",
    "params_window = {\n",
    "    \"B\": 2.0,\n",
    "    \"url_weight\" : 1.0,\n",
    "    \"title_weight\": 0.1,\n",
    "    \"body_hits_weight\" : 0.25,\n",
    "    \"header_weight\" : 0.5,\n",
    "    \"anchor_weight\" : 0.3,\n",
    "    \"b_url\" : 0.0,\n",
    "    \"b_title\" : 0.0,\n",
    "    \"b_header\" : 0.0,\n",
    "    \"b_body_hits\" : 0.0,\n",
    "    \"b_anchor\" : 0.0,\n",
    "    \"k1\": 2.0,\n",
    "    \"pagerank_lambda\" : 0.1,\n",
    "    \"pagerank_lambda_prime\" : 1.0,\n",
    "}\n",
    "\n",
    "# Consulta y documento de prueba:\n",
    "q = Query(\"stanford parking\")\n",
    "d = query_dict[q][\"https://transportation.stanford.edu/\"]\n",
    "\n",
    "# Definición de instancia de la clase:\n",
    "smallest_window_scorer = SmallestWindowScorer(theIDF, query_dict, params_window)\n",
    "\n",
    "# Prueba de funcionamiento interno del método get_boost_score(...):\n",
    "smallest_window_scorer.get_boost_score(q, d, debug=True)\n",
    "\n",
    "# Prueba del método get_net_score(...) que calcula el scoring neto:\n",
    "print(f\"\\nScoring neto tras usar el factor de boost: {smallest_window_scorer.get_net_score(q, d):5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking del dataset\n",
    "\n",
    "## Clase _Rank_\n",
    "\n",
    "Definimos una sencilla clase conteniendo sólo métodos de clase, que agrupa varias funciones de utilidad en la construcción de _rankings_ de documentos resultado de la búsqueda para una determinada consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Rank:\n",
    "    # Sólo métodos de clase:\n",
    "    def score(query_dict, score_type, idf, params):\n",
    "        \"\"\" Llamar a esta función para puntuar (y ordenar según esta puntuación)\n",
    "            todos los documentos correspondientes a una consulta, en un conjunto\n",
    "            completo (dado en forma de mapping consultas -> {documentos}).\n",
    "        Args:\n",
    "            query_dict (dict) :  Mapeo Query->url->Document.\n",
    "            score_type (str) : Tipo de scorer a usar (\"baseline\", \"cosine\", \"bm25f\", \"window\").\n",
    "            idf (dict) : Diccionario IDF.\n",
    "            params(dict) : Parámetros para el scorer usado.\n",
    "        Return\n",
    "            query_rankings (dict) : Un mapeo Query->Document->(r,s) (r=ranking=entero, comenzando en 1; s=score=float).\n",
    "        \"\"\"\n",
    "        # Seleccionar subclase concreta de AbstractScorer para crear el tipo de instancia\n",
    "        # concreta de scorer a utilizar:\n",
    "        if score_type == \"baseline\": scorer = BaselineScorer(idf)\n",
    "        elif score_type == \"cosine\": scorer = CosineSimilarityScorer(idf, query_dict, params)\n",
    "        elif score_type == \"bm25f\": scorer = BM25FScorer(idf, query_dict, params)\n",
    "        elif score_type == \"window\": scorer = SmallestWindowScorer(idf, query_dict, params)\n",
    "        else: print('Tipo erróneo de scorer (debe ser \"baseline\", \"cosine\", \"bm25f\" o \"window\")!')\n",
    "\n",
    "        # Diccionario donde se almacenará el mapping consultas->rankings devuelto:\n",
    "        query_rankings = {}\n",
    "        # Bucle que recorre todas las consultas en el diccionario de entrada:\n",
    "        for i, query in enumerate(query_dict.keys()):\n",
    "            q = query\n",
    "            doc_and_scores = {}\n",
    "            ### BEGIN YOUR CODE (FIXME)\n",
    "            # Bucle que recorre todos los urls para cada consulta, obteniendo el score correspondiente:\n",
    "            for doc in query_dict[query]:\n",
    "                d = query_dict[q][doc]\n",
    "                score = scorer.get_net_score(q, d)\n",
    "                doc_and_scores[d] = score\n",
    "            # Ordenamos los documentos por sus scorings...\n",
    "            sorted_doc_scores = sorted([(d,s) for d,s in doc_and_scores.items()],\n",
    "                                       key=lambda _: _[1], reverse=True)\n",
    "            # ... y asignamos el mapeo Document->(ranking,score) resultante de todos los documentos\n",
    "            # a la consulta correspondiente en el mapeo de salida Query->Document->ranking:\n",
    "            query_rankings[query] = {d:(r+1,s) for r,(d,s) in enumerate(sorted_doc_scores)}\n",
    "            ### END YOUR CODE (FIXME)\n",
    "\n",
    "        return query_rankings\n",
    "\n",
    "    def write_ranking_to_file(query_rankings, ranked_result_filename):\n",
    "        \"\"\" Función que exporta los rankings obtenidos sobre un dataset de\n",
    "           consultas-documentos a un fichero de texto.\n",
    "        Args:\n",
    "            query_rankings (dict) : Un mapeo Query->Document->ranking (ranking=entero, comenzando en 1).\n",
    "            ranked_result_filename (str): Ruta al archivo de salida.\n",
    "        \"\"\"\n",
    "        with open(ranked_result_filename, \"w\") as f:\n",
    "            for query, docs in query_rankings.items():\n",
    "                f.write(\"query: \"+ query.__str__() + \"\\n\")\n",
    "                for doc, rank in docs.items():\n",
    "                    output_info = \"  url: \" + doc.url + \"\\n\" + \\\n",
    "                                  \"    title: \" + doc.title + \"\\n\" + \\\n",
    "                                  \"    rank:  \" + str(rank[0]) + \"\\n\" + \\\n",
    "                                  \"    score: \" + str(rank[1]) + \"\\n\"\n",
    "                    f.write(output_info)\n",
    "        print(f\"¡Escritura de archivo {ranked_result_filename} realizada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de archivos de rankings\n",
    "\n",
    "Usando la clase `Rank` anterior, realizamos las ordenaciones de todas las consultas contenidas en `query_dict` (provenientes del archivo de _training_ inicial). Realizamos cuatro _rankings_ usando las cuatro técnicas desarrolladas (_\"baseline\"_, _\"cosine\"_, _\"bm25f\"_, _\"window\"_), guardando cada una de ellas en el correspondiente archivo `output/ranked_train_{tecnica}.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "for method, params in zip([\"baseline\", \"cosine\", \"bm25f\", \"window\"], [None, params_cosine, params_bm25f, params_window]):\n",
    "    query_dict = load_train_data(os.path.join(data_dir, \"pa3.signal.train\"))\n",
    "    query_rankings = Rank.score(query_dict, method, theIDF, params)\n",
    "    Rank.write_ranking_to_file(query_rankings, os.path.join(\"output\", \"ranked_train_\"+method+\".txt\"))\n",
    "    print(f\"Rankings realizados para {len(query_rankings)} consultas, (usando el {method} scorer)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A título de ejemplo de los resultados obtenidos, a continuación mostramos los _rankings_ realizados por los cuatro métodos para los diez documentos obtenidos para la primera consulta del _dataset_ de _training_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "!echo RANKING 1ª CONSULTA, BASELINE:\n",
    "!head -41 output/ranked_train_baseline.txt\n",
    "!echo\n",
    "\n",
    "!echo RANKING 1ª CONSULTA, COSINE:\n",
    "!head -41 output/ranked_train_cosine.txt\n",
    "!echo\n",
    "\n",
    "!echo RANKING 1ª CONSULTA, BM25F:\n",
    "!head -41 output/ranked_train_bm25f.txt\n",
    "!echo\n",
    "\n",
    "!echo RANKING 1ª CONSULTA, WINDOW:\n",
    "!head -41 output/ranked_train_window.txt\n",
    "!echo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación\n",
    "\n",
    "## Métrica _Normalized Discounted Cumulative Gain_ (NDCG)\n",
    "\n",
    "Sea cual sea la función de _ranking_ empleada, para evaluar su eficacia necesitaremos una métrica adecuada, basada en algún tipo de medida de relevancia previa disponible para cada documento en el contexto de una consulta.\n",
    "Una vez dicha métrica esté disponible, la usaremos para evaluar cada función de _ranking_ ${rf}$ con los datos de test ordenados según la misma.\n",
    "\n",
    "Usaremos en particular la métrica [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (_Normalized Discounted Cumulative Gain_). Se trata de una métrica en la que, con un valor obtenido siempre positivo y menor o igual que uno, cuanto más se acerque el valor a uno, mejor será la función de ranking con respecto a los valores de relevancia etiquetados iniciales.\n",
    "\n",
    "Según esta métrica, para una consulta particular, se define el NDGC de una determinada consulta como:\n",
    "\n",
    "$$\n",
    "NDCG(q) = \\frac{1}{Z} \\sum_{m=1}^{p}\\frac{2^{R(q,m)}-1}{log_{2}(1+m)}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "(Nótese que en esta métrica utilizamos la 2ª definición de $DCG_p$ de la página correspondiente en wikipedia, que hace que el numerador se anule para una relevancia=0; con ello se pone mayor énfasis en los documentos ubicados en las primeras posiciones).\n",
    "\n",
    "Aquí $R(q, m)$ es el juicio de relevancia proporcionado manualmente (preetiquetado en el conjunto _training_) dado al documento$m$ para la consulta $q$. $Z$ es simplemente un factor de normalización, consistente en el valor NDCG ideal para esa consulta. Dicho valor ideal (iNDCG) se calcula simplemente ordenando los documentos por orden decreciente de relevancia, y calculando entonces el NDCG correspondiente usando simplemente $Z=1$. Para el caso particular en que $iNDCG=0$ (correspondiente, según la fórmula anterior, a todos los valores de relevancia iguales a cero), se define simplemente $NDCG(q) = 1$. Finalmente, $p$ es el número de documentos totales que son posibles resultados (_matchings_) para la consulta (un máximo de 10 en nuestro caso).\n",
    "\n",
    "Con dicha definición de NDCG(q) para una determinada consulta, podemos compuar el NDCG para un conjunto de consultas $Q = \\{q_1,...,q_m\\}$ simplemente tomando la media de los NDCGs obtenidos para todas las consultas individuales $q_i \\in Q$.\n",
    "\n",
    "## Clase _NDCG_\n",
    "\n",
    "A continuación definimos una clase específica para implementar la métrica NDCG que acabamos de describir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCG:\n",
    "    def get_rel_scores_from_file(self, filename):\n",
    "        \"\"\" Función que obtiene los valores ground-truth de relevancia para un dataset a\n",
    "            partir de un fichero de texto correspondiente.\n",
    "        Args:\n",
    "            filename (str): Ruta al archivo de entrada.\n",
    "        \"\"\"\n",
    "        # Guardaremos las relevancias de cada documento en una variable de instancia\n",
    "        # self.rel_scores, de tipo mapeo (diccionario) url->float, con url=str:\n",
    "        self.rel_scores = {}\n",
    "        query = \"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"query\"):\n",
    "                    query = line.split(\":\")[-1].strip()\n",
    "                    url_score = {}\n",
    "                    self.rel_scores[query] = url_score\n",
    "                else: # line.startswith(\"url\")\n",
    "                    tokens = line[line.index(\":\")+1:].strip().split(\" \")\n",
    "                    url = tokens[0]\n",
    "                    rel = tokens[1]\n",
    "                    if float(rel) < 0:\n",
    "                        rel = 0\n",
    "                    if url_score is not None:\n",
    "                        url_score[url] = float(rel)\n",
    "\n",
    "    def calc_ndcg(self, rels):\n",
    "        \"\"\" Calcula el valor NDCG para una ordenación dada, tomando como entrada la\n",
    "            lista de relevancias de ground-truth de los documentos, ordenada según la\n",
    "            función de ranking a evaluar. Se aplica entonces la ecuación (4) anterior, que\n",
    "            tomará un valor más cercano a uno según la ordenación de dicha lista se acerque\n",
    "            más a la ordenación natural, de mayor a menor, de la lista de relevancias.\n",
    "        Args:\n",
    "            filename (str): Lista de relevancias ground-truth (ordenada según la función de\n",
    "               ranking a evaluar, NO en orden natural de mayor a menor).\n",
    "        \"\"\"\n",
    "        # Cálculo para la ordenación de entrada:\n",
    "        local_sum = 0\n",
    "        for i in range(len(rels)):\n",
    "            rel = rels[i]\n",
    "            local_sum += (2**rel - 1) / (math.log((i+1) + 1, 2))\n",
    "        # Repetimos cálculo con las relevancias ya ordenadas de mayor a menor\n",
    "        # (para calcular el Z de la ecuación(4))\n",
    "        sorted_rels = sorted(rels, reverse=True)\n",
    "        sorted_sum = 0\n",
    "        for i in range(len(sorted_rels)):\n",
    "            rel = sorted_rels[i]\n",
    "            sorted_sum += (2**rel - 1) / (math.log((i+1) + 1, 2))\n",
    "        # Dividimos la suma original por Z, para devolver el NDCG normalizado (cuyo\n",
    "        # valor será siempre <= 1.0, con 1.0 correspondiente a una ordenación de relevancias\n",
    "        # perfectamente coherente con la ordenación proporcionada inicialmente por el\n",
    "        # algoritmo de ranking).\n",
    "        if (sorted_sum == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            return local_sum/sorted_sum\n",
    "\n",
    "    def read_ranking_file_and_compute_ndcg(self, ranked_result_filename):\n",
    "        \"\"\" Lee un fichero de rankings (realizado por cualquiera de los métodos de ranking\n",
    "            desarrollados) y calcula el ndcq para cada consulta (en variable de instancia\n",
    "            self.query_ndcg, de tipo mapeo consulta->float, con consulta=str). También deja\n",
    "            en la variable de instancia self.query_docs un mapeo consulta->[Document], donde\n",
    "            a cada consulta (en formato cadena) se asigna la lista de documentos pertinente.\n",
    "            Dichas variables calculadas serán empleadas después por el método write_ndcg_result\n",
    "            para escribir el resultado de la evaluación en un correspondiente archivo de texto.\n",
    "        Args:\n",
    "            ranked_result_filename (str): Fichero de entrada.\n",
    "        \"\"\"\n",
    "        self.query_ndcg = {}\n",
    "        self.query_docs = {}\n",
    "        cur_q = \"\"\n",
    "        cur_rels = []\n",
    "        with open(ranked_result_filename, 'r') as f:\n",
    "            for line in f:\n",
    "                clean_l = line.strip().split(\":\")\n",
    "                l_type = clean_l[0].strip()\n",
    "                l_content = \":\".join(clean_l[1:]).strip()\n",
    "                if l_type == 'query':\n",
    "                    # Comienzo siguiente query -> calculamos NDCG de la que acabamos\n",
    "                    # de terminar:\n",
    "                    if len(cur_rels) > 0:\n",
    "                        self.query_ndcg[cur_q] = self.calc_ndcg(cur_rels)\n",
    "                    # Nueva consulta:\n",
    "                    cur_q = l_content\n",
    "                    cur_rels = []\n",
    "                    self.query_docs[cur_q] = []\n",
    "                elif l_type == 'url':\n",
    "                    # Nueva URL -> leer y añadir relevancia de la misma a lista cur_rels:\n",
    "                    doc = Document(l_content)\n",
    "                    self.query_docs[cur_q].append(doc)\n",
    "                    if (cur_q in self.rel_scores) and \\\n",
    "                       (doc.url in self.rel_scores[cur_q]):\n",
    "                        cur_rels.append(self.rel_scores[cur_q][doc.url])\n",
    "                    else:\n",
    "                        print(\"WARNING: No se encuentra el url %s para la consulta %s\"%(doc.url, cur_q))\n",
    "                elif l_type == 'title':\n",
    "                    doc.title = l_content\n",
    "                # ignore debug line for now\n",
    "\n",
    "        # Última consulta:\n",
    "        if len(cur_rels) > 0:\n",
    "            self.query_ndcg[cur_q] = self.calc_ndcg(cur_rels)\n",
    "            # cur_q = l_content\n",
    "            # cur_rels = []\n",
    "\n",
    "    def get_avg_ndcg(self):\n",
    "        \"\"\" Calcula el NDCG medio para todo el dataset, calculado simplemente como\n",
    "            la media del NDCG calculado para cada una de las consultas del mismo.\n",
    "        \"\"\"\n",
    "        sum_ndcg = 0\n",
    "        for i in self.query_ndcg:\n",
    "            sum_ndcg += self.query_ndcg[i]\n",
    "        return sum_ndcg / len(self.query_ndcg)\n",
    "\n",
    "    def write_ndcg_result(self, ndcg_result_filename):\n",
    "        \"\"\" Escribe los resultados de la métric NDCG (para cada consulta) en un\n",
    "            archivo de texto de salida.\n",
    "        Args:\n",
    "            ndcg_result_filename (str): Fichero de salida.\n",
    "        \"\"\"\n",
    "        with open(ndcg_result_filename, 'w') as f:\n",
    "            for query in self.query_ndcg:\n",
    "                f.write(\"query: \" + query + \"\\n\")\n",
    "                ndcg_score = self.query_ndcg[query]\n",
    "                f.write(\"ndcg: \" + str(ndcg_score) + \"\\n\")\n",
    "\n",
    "                for doc in self.query_docs[query]:\n",
    "                    f.write(\"  url: \" + doc.url + \"\\n\")\n",
    "                    f.write(\"    rating: \" + str(self.rel_scores[query][doc.url]) + \"\\n\")\n",
    "                    f.write(\"    title: \" + doc.title + \"\\n\")\n",
    "                    f.write(\"    debug:\" + \"\\n\")\n",
    "        print(f\"¡Escritura de fichero {ndcg_result_filename} con resultados NDCG de salida realizada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de todos los modelos\n",
    "\n",
    "\n",
    "Finalmente, usamos aquí las clases `Rank` y `NDCG` para evaluar los rankings realizados tanto sobre el conjunto de _training_ (_signal_) como el de validación (_dev_) con los cuatro métodos desarrollados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos unos parámetros base para el método del coseno con cierto sentido:\n",
    "params_cosine = {\n",
    "    \"url_weight\" : 0.5,\n",
    "    \"title_weight\": 0.2,\n",
    "    \"body_hits_weight\" : 0.1,\n",
    "    \"header_weight\" : 0.1,\n",
    "    \"anchor_weight\" : 0.1,\n",
    "    \"smoothing_body_length\" : 500\n",
    "}\n",
    "\n",
    "# Añadimos ahora unos parámetros adicionales relativamente adecuados para el BM25F:\n",
    "params_bm25f = {\n",
    "    **params_cosine,\n",
    "    \"b_url\" : 0.5,\n",
    "    \"b_title\" : 0.5,\n",
    "    \"b_header\" : 0.5,\n",
    "    \"b_body_hits\" : 0.5,\n",
    "    \"b_anchor\" : 0.5,\n",
    "    \"k1\": 1.0,\n",
    "    \"pagerank_lambda\" : 1.0,\n",
    "    \"pagerank_lambda_prime\" : 1.0\n",
    "}\n",
    "\n",
    "# Y finalmente añadimos un parámetro adicional necesario para el método de ventana:\n",
    "params_window = {\n",
    "    **params_bm25f,\n",
    "    \"B\": 2.0\n",
    "}\n",
    "\n",
    "#\n",
    "for dataset_filename in [\"pa3.signal.train\", \"pa3.signal.dev\"]:\n",
    "    query_dict = load_train_data(os.path.join(data_dir, dataset_filename))\n",
    "    for method, params in zip([\"baseline\", \"cosine\", \"bm25f\", \"window\"], [None, params_cosine, params_bm25f, params_window]):\n",
    "        # Realizamos el scoring con el dataset y el método actual, salvando los resultados\n",
    "        # en un fichero de texto correspondiente:\n",
    "        query_rankings = Rank.score(query_dict, method, theIDF, params)\n",
    "        train_or_dev = dataset_filename.split(\".\")[-1]\n",
    "        ranked_result_filename = os.path.join(\"output\", f\"ranked_{train_or_dev}_{method}.txt\")\n",
    "        Rank.write_ranking_to_file(query_rankings, ranked_result_filename)\n",
    "\n",
    "        # Creamos instancia de clase NDCG para realizar la métrica, y la usamos para\n",
    "        # * Cargar en ella las relevancias ground-truth.\n",
    "        # * Computar la métrica NDCG usando dichas relevancias y el ranking realizado previamente.\n",
    "        # * Salvar los resultados NDCG en un fichero de texto correspondiente.\n",
    "        # * Calcular e imprimir el NDCG medio (global) para el dataset y el método actual.\n",
    "        ndcg = NDCG()\n",
    "        ndcg.get_rel_scores_from_file(os.path.join(data_dir, dataset_filename.replace(\"signal\",\"rel\")))\n",
    "        ndcg.read_ranking_file_and_compute_ndcg(ranked_result_filename)\n",
    "        ndcg_result_filename = ranked_result_filename.replace(\"ranked\", \"ndcg\")\n",
    "        ndcg_result_file = os.path.join(\"output\", ndcg_result_filename)\n",
    "        ndcg.write_ndcg_result(ndcg_result_filename)\n",
    "        avg_ndcg = ndcg.get_avg_ndcg()\n",
    "        print(f\"NDCG global obtenido para el método {method} sobre el archivo de dataset {dataset_filename}: {avg_ndcg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una inspección rápida de los resultados nos muestra que parecen tener bastante sentido, si bien dejaremos su discusión detallada para el último apartado del _notebook_, \"[Discusión](#discusion)\".\n",
    "\n",
    "# Aprendizaje automático de pesos\n",
    "\n",
    "Hasta ahora, los pesos para las diferentes funciones se han especificado manualmente. Sin embargo, en los sistemas reales, a medida que se añaden más y más posibles señales útiles para la clasificación (_features_ tanto textuales como no textuales), dicha especificación manual puede volverse un desafío, al menos para tener una cierta confianza en que dicha especificación se hace de una forma óptima (o al menos cercana a una configuración óptima).\n",
    "\n",
    "En esta última sección del _notebook_ emplearemos una sencilla técnica de _machine learning_, en concreto la estimación puntual usando regresión lineal, para permitir que los algoritmos de ordenación \"aprendan\" automáticamente los pesos necesarios para sus respectivas funciones de _ranking_.\n",
    "\n",
    "## Entrenamiento de un modelo de regresión lineal\n",
    "\n",
    "Cualquera de los métodos de _ranking_ que hemos desarrollado acaban asociando a cada consulta $q_i$ dada un conjunto de documentos $d_j, \\hspace{.2em} \\forall j=1\\dots 10$, a cada uno de los cuales asocia a su vez un vector $x_{ij}$ de características (_features_) asociadas al correspondiente par consulta-documento. Se producirá también un valor real de _scoring_ correspondiente, al cual denominaremos $y_{i,j}$ (etiqueta asociada al vector $x_{i,j}$).\n",
    "\n",
    "En la aproximación denominada \"puntual\" (_pointwise_), la más sencilla en la que podremos aplicar _machine learning_ a la estimación de los pesos, se desecha completamente la estructura grupal de los datos (agrupados de 10 en 10 por las consultas), viéndose simplemente nuestros datos de entrenamiento como una lista plana de pares $\\{(x_{i}, y_{i})\\}$. El problema de ajuste de pesos en nuestro problema de _ranking_ equivale entonces a aprender una adecuada función $f(x)$ tal que, aplicada a cada vector $(x_{i})$, consiga un valor de _scoring_ que se ajuste lo más posible a $y_{i}$, esto es, $f(x_{i})≈y_{i} , \\forall i=1\\dots m$, con $m=|\\{documents\\}|$.\n",
    "\n",
    "Una vez establecido el problema de aprendizaje planteado (en nuestro caso, un problema de regresión), lo resolveremos usando una aproximación de las más sencillas propuestas para ello, la **regresión lineal**. Esto significa que parametrizaremos $f$ como una simple función lineal que asignará un _scoring_ a cada vector $x$  correspondiente a una consulta-documento como sigue:\n",
    "\n",
    "$$\n",
    "f(x) = wx+b\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Aquí el vector de pesos ${w}$ el término de _bias_ $b$ son exactamente los parámetros que necesitarán ser aprendidos, de forma que se minimice la función de pérdida definida así:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m (f(x_{i})-y_{i})^2\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "Es lo que se conoce en la literatura como **aproximación por mínimos cuadrados ordinaria** (_ordinary least squares_).\n",
    "\n",
    "## Diseño de los vectores de características\n",
    "\n",
    "Para ilustrar la técnica de aprendizaje de pesos, usaremos como base la clase `CosineSimilarityScore`, que permite extraer un valor individual de similaridad para cada campo `url`, `title`, `header`, `body` y `anchor` del documento. Así, podremos representar cada par consulta($q_i$)-documento($q_j$) como un vector de cinco dimensiones $x_{ij}=(s_u,s_t,s_h,s_b,s_a)$, donde cada componente $s_f$ se corresponde con el _scoring tf-idf_ correspondiente al campo $f$ del documento (esto es, $s_f = q_i \\cdot d_{i,f}$, denotando el operador $\\cdot$ el producto escalar entre los correspondientes vectores de consulta $q_i$ y el vector de documento $d_i$ para el campo específico $f$.\n",
    "\n",
    "**Nota importante:** En ningún caso se debe confundir estos vectores $x_{ij}$ con los vectores de consulta y/o documento. Éstos últimos tienen tantas dimensiones como tamaño tiene el vocabulario, y por tanto, un componente numérico diferente por cada término del mismo. Son vectores, por tanto, que se mueven en una dimensión muy alta, si bien son también tremendamente dispersos (_sparse_), esto es, tienen gran cantidad de ceros. No es, por tanto y obviamente, sobre ellos donde se realiza el aprendizaje, sino sobre estos vectores $x_{ij} \\in R^5$, mucho más compactos, en los que cada componente no se corresponde con un término ni de la consulta ni del documento, sino precisamente ya con un valor de _scoring_ calculado para cada par, y correspondiente a cada uno de los cinco campos diferentes $f \\in \\{url,title,header,body,anchor\\}$.\n",
    "\n",
    "Usaremos el esquema de pesado _ddd.qqq = lnb.btn_ (según la notación SMART), tal y como se implementó en la clase base a utilizar `CosineSimilarityScore`. Recordar que esto significa:\n",
    "\n",
    "1. Para el documento (_lnb_):\n",
    " - Frecuencia logarítmica de los términos del documento.\n",
    " - No usar IDF en los términos del documento.\n",
    " - Normalizar el vector de documento (si bien usando en este caso la pequeña modificación de cálculo de la norma por longitud de documento que se implementó en la función `normalize_doc_vec` de la clase `CosineSimilarityScorer`).\n",
    "\n",
    "2. Para la consulta (_btn_):\n",
    " - Esquema de conteo booleano en los términos de la consulta.\n",
    " - Usar IDF en los términos de la consulta.\n",
    " - No normalizar el vector de consulta.\n",
    "\n",
    "## Clase _PointwiseLR_\n",
    "\n",
    "Definimos la clase _PointwiseLR_ simplemente para agrupar en ella las tres funciones principales para realizar el aprendizaje de la máquina de regresión lineal sobre la predicción \"punto a punto\". Esto es, considerando cada par consulta-documento como un punto (=vector) independiente, sin tener en cuenta la estructura de documentos resultado agrupados por consulta.\n",
    "\n",
    "Las funciones principales son:\n",
    "\n",
    "* `get_feature_vectors_and_relevances`: para obtener los vectores de entrenamiento en formato numpy (tanto los vectores de entrada como la variable de salida a regresionar).\n",
    "* `train_model`: para realizar el entrenamiento en sí, usando `sklearn`.\n",
    "* `predict_with_model`: para usar el modelo entrenado para predecir nuevos valores de relevancia a partir de nuevas entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseLR:\n",
    "\n",
    "    # def __init__(self):\n",
    "    #    self.model = None\n",
    "\n",
    "    def get_feature_vectors_and_relevances(signal_file, idf, relevance_file):\n",
    "        \"\"\"\n",
    "        Crear el conjunto de todos los vectores de características correspondientes\n",
    "        al fichero de entrenamiento (fichero de señal) y del diccionario IDF, con su\n",
    "        correspondiente vector de relevancias (ground-truth) obtenidas a partir del\n",
    "        correspondiente archivo de relevancias.\n",
    "\n",
    "        Args:\n",
    "            signal_file: Ruta al archivo de señal.\n",
    "            relevance_file: Ruta al archivo de relevancias.\n",
    "            idf: Objeto de la clase Idf (ya inicializado con el IDF de todos los términos de la colección)\n",
    "\n",
    "        Returns:\n",
    "            feature_vecs: Array numpy de dimension (N, 5), siendo N el número total de pares (consulta, documento)\n",
    "                          en el archivo de señal.\n",
    "            relevance_vec: Array numpy de dimensión (N,), siendo N el número total de pares (consulta, documento)\n",
    "                           en el archivo de relevancias.\n",
    "\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        doc_weight_scheme = {\"tf\": 'l', \"df\": 'n', \"norm\": \"default\"}\n",
    "        query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None}\n",
    "        query_dict = load_train_data(signal_file)\n",
    "        params_cosine = {\n",
    "            \"url_weight\" : 0.0,\n",
    "            \"title_weight\": 0.0,\n",
    "            \"body_hits_weight\" : 0.0,\n",
    "            \"header_weight\" : 0.0,\n",
    "            \"anchor_weight\" : 0.0,\n",
    "            \"smoothing_body_length\" : 500\n",
    "        }\n",
    "        cs = CosineSimilarityScorer(idf, query_dict, params_cosine, query_weight_scheme, doc_weight_scheme)\n",
    "        ndcg = NDCG()\n",
    "        ndcg.get_rel_scores_from_file(relevance_file)\n",
    "\n",
    "        feature_vecs = []\n",
    "        relevance_vecs = []\n",
    "        for i, q in enumerate(query_dict.keys()):\n",
    "            ### BEGIN YOUR CODE (FIXME)\n",
    "            # Bucle que recorre todos los urls para cada consulta, obteniendo el score correspondiente:\n",
    "            for j, d_url in enumerate(query_dict[q]):\n",
    "                d = query_dict[q][d_url]\n",
    "                q_vec = cs.get_query_vector(q)\n",
    "                d_vec = cs.get_doc_vector(q, d)\n",
    "                feature_vec = np.array([cs.get_sim_score(q,d,\"url\"),\n",
    "                                        cs.get_sim_score(q,d,\"title\"),\n",
    "                                        cs.get_sim_score(q,d,\"headers\"),\n",
    "                                        cs.get_sim_score(q,d,\"anchors\"),\n",
    "                                        cs.get_sim_score(q,d,\"body_hits\")])\n",
    "                feature_vecs.append(feature_vec)\n",
    "                relevance_vecs.append(ndcg.rel_scores[str(q)][d_url])\n",
    "\n",
    "        feature_vecs = np.array(feature_vecs)\n",
    "        relevance_vecs = np.array(relevance_vecs)\n",
    "\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return feature_vecs, relevance_vecs\n",
    "\n",
    "    def train_model(x, y):\n",
    "        \"\"\" Entrena el modelo de regresión lineal usando la clase LinearRegression del paquete sklearn.\n",
    "        Args:\n",
    "                x: Array numpy de dimensión (N, 5) con los vectores x_ij (aplanados, para todas los\n",
    "                   pares consulta-documento (q_i,d_j). Será la variable independiente para la regresión lineal.\n",
    "\n",
    "                y: Array numpy de dimensión (N,) con los valores de relevancia para cada uno de los vectores x_ij\n",
    "                   anteriores. Será la variable dependiente para la regresión lineal.\n",
    "\n",
    "        Returns:\n",
    "                El modelo entrenado (usable para predecir con el método siguiente, predict_with_model)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        model = LinearRegression()\n",
    "        # Entrenamos el modelo con los datos de entrenamiento:\n",
    "        model.fit(x, y)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return model\n",
    "\n",
    "    def predict_with_model(model, x):\n",
    "        \"\"\" Predice _scorings_ finales (netos) para una lista de vectores de entrada utilizando el modelo entrenado.\n",
    "        Args:\n",
    "                x: Array numpy de dimensión (N, 5) con los vectores con las características de entrada de cada par\n",
    "                   consulta-documento en el que predecir el _scoring_ neto.\n",
    "\n",
    "        Returns:\n",
    "                y_pred: Array numpy de dimensión (N,) con las relevancias predichas para cada vector correspondiente a\n",
    "                        un par consulta-documento, basándose en el modelo de regresión lineal entrenado.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        predictions = model.predict(x)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda siguiente se prueba toda la funcionalidad de la clase anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtención de vectores y relevancias de entrenamiento a partir de los archivos de señal y de relevancia:\n",
    "train_feature_vecs, train_relevances = PointwiseLR.get_feature_vectors_and_relevances(\"pa3-data/pa3.signal.train\", theIDF, \"pa3-data/pa3.rel.train\")\n",
    "assert train_feature_vecs.shape == (7026, 5), f\"¡Error en creación de vectores de training! {train_feature_vecs.shape} != (7026, 5)\"\n",
    "\n",
    "# Entrenamiento:\n",
    "model = PointwiseLR.train_model(train_feature_vecs, train_relevances)\n",
    "\n",
    "# Predicción:\n",
    "dev_feature_vecs, dev_relevances = PointwiseLR.get_feature_vectors_and_relevances(\"pa3-data/pa3.signal.dev\", theIDF, \"pa3-data/pa3.rel.dev\")\n",
    "assert dev_feature_vecs.shape == (1187, 5), f\"¡Error en creación de vectores de test! {train_feature_vecs.shape} != (1187, 5)\"\n",
    "\n",
    "dev_predicts = PointwiseLR.predict_with_model(model, dev_feature_vecs)\n",
    "assert dev_predicts.shape == (1187,), f\"¡Error en obtención de predicciones! {train_feature_vecs.shape} != (1187,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo aprendido\n",
    "\n",
    "Finalmente, en esta sección, y usando las predicciones del modelo entrenado, computamos su MSE (error mínimo cuadrático), actualizamos los correspondientes parámetros del método de ranking basado en la similaridad del coseno usando dichos parámetros, y volvemos a realizar el ranking tanto sobre el conjunto de aprendizaje como el de test. También obtenemos las respectivas puntuaciones NDCG para ambos, y las comparamos con las del modelo inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Imprimimos los coeficientes del modelo obtenido, y su error cuadrático medio (MSE):\n",
    "print(\"Parámetros del modelo: \")\n",
    "print(f\"  Coeficientes['url','title','headers','anchors','body_hits']:  {model.coef_}\")\n",
    "print(f\"  Término independiente (descartable, no afecta al ranking): {model.intercept_}\")\n",
    "print (\"\\nMean Squared Error:\", mean_squared_error(dev_relevances, dev_predicts))\n",
    "\n",
    "# Normalizamos pesos para que sumen 1.0:\n",
    "weights = model.coef_ / np.sum(model.coef_)\n",
    "\n",
    "# Inicializamos una nueva instancia del CosineSimilarityScorer con los parámetros aprendidos:\n",
    "params_cosine_trained = {\n",
    "    \"url_weight\" : weights[0],\n",
    "    \"title_weight\": weights[1],\n",
    "    \"header_weight\" : weights[2],\n",
    "    \"anchor_weight\" : weights[3],\n",
    "    \"body_hits_weight\" : weights[4],\n",
    "    \"smoothing_body_length\" : 500\n",
    "}\n",
    "print(f\"\\nParámetros aprendidos para CosineSimilarityScorer: \\n  {params_cosine_trained}\\n\")\n",
    "\n",
    "# Repetimos el ranking con dicho modelo, en los datasets de training y de test, salvando\n",
    "# los resultados en los respectivos ficheros ranked_{train|dev}_cosine_Learned_LR.txt,\n",
    "# y mostrando las respectivas métricas NDCG obtenidas:\n",
    "\n",
    "for dataset_filename in [\"pa3.signal.train\", \"pa3.signal.dev\"]:\n",
    "    # Lectura dataset:\n",
    "    query_dict = load_train_data(os.path.join(data_dir, dataset_filename))\n",
    "    train_or_dev = dataset_filename.split(\".\")[-1]\n",
    "    # Ejecutamos función de ranking (método coseno con parámetros entrenados):\n",
    "    query_rankings = Rank.score(query_dict, \"cosine\", theIDF, params_cosine_trained)\n",
    "    # Salvamos resultados:\n",
    "    ranked_result_filename = os.path.join(\"output\", f\"ranked_{train_or_dev}_cosine_Learned_LR.txt\")\n",
    "    Rank.write_ranking_to_file(query_rankings, ranked_result_filename)\n",
    "    # Medimos NDCG resultante:\n",
    "    ndcg = NDCG()\n",
    "    ndcg.get_rel_scores_from_file(os.path.join(data_dir, dataset_filename.replace(\"signal\",\"rel\")))\n",
    "    ndcg.read_ranking_file_and_compute_ndcg(ranked_result_filename)\n",
    "    ndcg_result_filename = ranked_result_filename.replace(\"ranked\", \"ndcg\")\n",
    "    ndcg_result_file = os.path.join(\"output\", ndcg_result_filename)\n",
    "    ndcg.write_ndcg_result(ndcg_result_filename)\n",
    "    avg_ndcg = ndcg.get_avg_ndcg()\n",
    "    print(f\"NDCG global obtenido para el método cosine con parámetros aprendidos mediante LR sobre el archivo de dataset {dataset_filename}: {avg_ndcg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos NDCGs salen algo mejores que los del modelo del coseno sin entrenar:\n",
    "* Mejora NDCG _training_: 0.8161916184849138 $→$ 0.8261348918944399\n",
    "* Mejora NDCG _test_: 0.8016941532045303 $→$ 0.8070937216021422\n",
    "\n",
    "\n",
    "<a id=\"discusion\"></a>\n",
    "\n",
    "# Discusión\n",
    "\n",
    "En todo caso, es obvio que la aproximación de aprendizaje por regresión lineal es un tanto ingenua por varias razones, lo que hace que no se mejoren demasiado significativamente los resultados de base obtenidos por el método del coseno (en los que se empleó un peso homogéneo para cada uno de los cinco campos). Además, el método del coseno, incluso mejorado con los pesos aprendidos, es aún inferior al BM25F (el que a priori mostró los mejores resultados):\n",
    "\n",
    "* NDCG con BM25F (_train_): 0.8433854652383972\n",
    "* NDCG con BM25F (_test_): 0.8295600444655024\n",
    "\n",
    "Es interesante también comentar que, en nuestro caso, el cómputo del _boosting_ por el factor de ventana no mejoró dicho método (lo que no es tan extraño, ya que, si se observan el tipo de la mayoría de consultas realizadas, no se basan tanto en la búsqueda de frases más o menos largas, sino más bien en la utilización de unos pocos términos clave, lo que de alguna manera limita bastante más la utilidad de la técnica):\n",
    "\n",
    "* NDCG con ventana (_train_): 0.8394620230330289\n",
    "* NDCG con ventana (_test_): 0.8236233212551707\n",
    "\n",
    "Algunas conclusiones adicionales del estudio:\n",
    "\n",
    "1. Tiene cierta lógica que el método BM25F sea el que mejor se comporta. Es al fin y al cabo uno de los más utilizados, y además en nuestro caso es el más completo porque utiliza un tipo de normalización de longitud especializada por campos, añade también la información adicional de la _feature_ no textual _pagerank_, etc., cosas que en nuestra implementación no hemos añadido obviamente al resto de funciones de _ranking_.\n",
    "\n",
    "2. Una mejora significativa que podría añadirse al algoritmo de aprendizaje sería implementar un algoritmo que **sí** tuviera en cuenta la estructura del conjunto de aprendizaje. Esto es, tener en cuenta que lo que nos interesa es que se nos haga un buen _ranking_ de los resultados **para una consulta dada**, y no tanto que, para todos los pares consulta-documento, se ajuste una misma función de _ranking_, tal y como hemos implementado con nuestro regresor lineal. Se trataría, pues, de implementar un algoritmo de aprendizaje _pairwise_ (en contraposición al esquema _pointwise_ que nosotros hemos utilizado). En dicho enfoque, cada ejemplo consiste en realidad en un par de documentos resultado para una consulta dada, y su etiqueta de ground-truth, si el primero tiene mayor relevancia que el segundo. En ese contexto, cada consulta que posea N posibles resultados genera $N\\times(N-1)/2$ pares de ejemplo. Sin embargo, esto exige aplicar algoritmos de aprendizaje algo más avanzados que un regresor lineal, lo que quedaría ya un poco fuera del alcance de los objetivos básicos de este tema. En todo caso, he aquí un enlace a un paquete de python con el que se podría abordar el problema, y donde se introduce un poco este tema del aprendizaje de una función de ranking: [https://xgboost.readthedocs.io/en/stable/tutorials/learning_to_rank.html](https://xgboost.readthedocs.io/en/stable/tutorials/learning_to_rank.html)\n",
    "\n",
    "3. Obviamente, todos los métodos con parámetros y funciones de apoyo intercambiables, dependen directamente de una buena elección tanto de los primeros como de las segundas. Esto es particularmente cierto en el caso del método BM25F, al que, al añadirle además el posible factor de _boost_ del tamaño de la ventana, acaba teniendo un espacio de posibles configuraciones de dimensiones bastante considerables, que no han sido exploradas en este notebook.\n",
    "\n",
    "\n",
    "# Ejercicios\n",
    "\n",
    "En particular, la última de las conclusiones expuestas el apartado anterior nos motiva a proponer el siguiente **ejercicio**:\n",
    "\n",
    "Tomando como base la clase _WindowScorer_ (la más completa de las desarrolladas, al combinar BM25F con el uso de la _feature_ no textual adicional del tamaño de la ventana más pequeña que contiene a la consulta), intentar mejorar sus resultados. Para ello:\n",
    "\n",
    "* Se puede realizar una búsqueda manual (intentando ajustar los diferentes parámetros mediante un simple razonamiento de la importancia que creemos que debe tener cada campo, y/o eligiendo los parámetros adicionales con valores típicos que puedan tener sentido). Recordar que el conjunto completo de parámetros que se pueden configurar para la clase _WindowScorer_ es el siguiente:\n",
    "\n",
    "```\n",
    "    params_window = {\n",
    "        \"url_weight\" : 0.5,\n",
    "        \"title_weight\": 0.2,\n",
    "        \"body_hits_weight\" : 0.1,\n",
    "        \"header_weight\" : 0.1,\n",
    "        \"anchor_weight\" : 0.1,\n",
    "        \"smoothing_body_length\" : 500\n",
    "        \"b_url\" : 0.5,\n",
    "        \"b_title\" : 0.5,\n",
    "        \"b_header\" : 0.5,\n",
    "        \"b_body_hits\" : 0.5,\n",
    "        \"b_anchor\" : 0.5,\n",
    "        \"k1\": 1.0,\n",
    "        \"pagerank_lambda\" : 1.0,\n",
    "        \"pagerank_lambda_prime\" : 1.0\n",
    "        \"B\": 2.0\n",
    "    }\n",
    "\n",
    "```\n",
    "\n",
    "* Como alternativa, se puede hacer una búsqueda automatizada (aleatoria, o bien relativamente estructurada) del conjunto de parámetros ajustables, aprovechando la facilidad que nos ofrece el código desarrollado para realizar un bucle que pruebe sistemáticamente muchas variaciones de los mismos (siempre manteniendo un número de combinaciones no tan elevado como para hacerlo computacionalmente viable). Esta tipo de aproximación, por cierto, denominada _\"Random|Grid Parameter Search\"_ es ampliamente utilizada en muchos ámbitos del _machine learning_ en general, y también en el contexto de la Recuperación de Información que nos ocupa en particular.\n",
    "* También puede realizarse una clase nueva, basada en la _WindowScorer_, donde se redefinan ciertos métodos para cambiar alguna de las funciones auxiliares (p.e., implementando algún esquema SMART diferente, cambiando la función para el _boosting_ adaptativo por tamaño de ventana mínima, o cualquier otra cosa que creáis que pueda tener sentido).\n",
    "* Puede resultar también muy ilustrativo crear una nueva clase _RandomScorer_, extremadamente sencilla (sólo hay que basarla en la clase raíz adecuada, y redefinir en ella la función de scoring neto para que, simplemente, devuelva un número aleatorio), y evaluar también su rendimiento vía NDGC tanto en el conjunto de training como en el de test. Con ello, obtendremos unos valores de base que nos pueden orientar sobre el valor mínimo de NDGC que puede obtenerse con un método _dummy_, que en este caso simplemente ordenará aleatoriamente los resultados obtenidos para cada búsqueda, poniendo así mejor en contexto los méritos relativos de unos métodos respecto a otros.\n",
    "\n",
    "Una vez obtenida nuestra solución óptima, es por supuesto interesante reflexionar sobre el conjunto concreto de parámetros / funciones con las que se ha obtenido dicho óptimo, intentando argumentar sobre los valores finales obtenidos, la importancia relativa de los parámetros más influyentes, y su posible razón, en el contexto del _dataset_ que nos ocupa.\n",
    "\n",
    "Orientativamente, no obstante, el profesorado de la asignatura _\"Information Retrieval\"_ de la Universidad de Stanford que creó este dataset para la misma, informó (sin publicar su solución particular) que, combinando varias de las opciones propuestas, logró obtener NDCG sólo marginalmente superiores al ~0.829 obtenido por el BM25F para el conjunto de test (_dev_) configurado con los parámetros por defecto indicados más arriba. Por último, téngase en cuenta también, por supuesto, que **en ningún caso se debe optimizar directamente sobre el conjunto de test**, sino que lo correcto es optimizar los parámetros sobre el NDCG obtenido sobre el training, para posteriormente simplemente informar sobre en NDCG sobre el test al configurar el método con los parámetros óptimos así obtenidos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
