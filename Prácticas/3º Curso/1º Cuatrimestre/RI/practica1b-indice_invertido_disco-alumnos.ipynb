{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen\n",
    "\n",
    "En esta práctica, aplicaremos los conocimientos adquiridos en las clases de teoría y prácticas anteriores para crear un índice invertido considerando restricciones impuestas por el hardware subyacente a la hora de construirlo. En concreto, las tareas a realizar en esta práctica son:\n",
    "\n",
    "1. **Construir un índice usando el algoritmo *naïve* y ser capaz de guardarlo/cargarlo de disco**. Usaremos el mismo corpus de documentos que en la práctica anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requisitos para la construcción del índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade aquí los imports que necesites\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Límite de memoria disponible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que no rebasas los límites de uso de memoria pre-establecidos para realizar la construcción del índice de la manera adecuada, debes ejecutar este notebook lanzando `jupyter-notebook` desde un terminal del *shell*, en el que previamente hayas ejecutado la siguiente orden:\n",
    "\n",
    "```bash\n",
    "$ ulimit -v $(echo $(( 1024 * 1024 )))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual memory              (kbytes, -v) unlimited\n"
     ]
    }
   ],
   "source": [
    "!ulimit -a | grep \"virtual memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si has establecido correctamente los límites de uso de memoria del proceso `python3` que está ejecutando este *notebook*, el resultado de ejecutar la orden anterior debería ser:\n",
    "```\n",
    "virtual memory              (kbytes, -v) 1048576\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus de documentos. Directorios de entrada y salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajaremos en esta práctica con el mismo corpus de documentos que en la anterior. Ejecuta la siguiente celda para obtenerlo en caso de que no lo tengas ya descargado en el mismo directorio del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL del corpus de documentos\n",
    "data_url = 'http://ditec.um.es/~rtitos/docencia/ri/practica1-datos.zip'\n",
    "\n",
    "local_filename = os.path.basename(urlparse(data_url).path)\n",
    "urllib.request.urlretrieve(data_url, local_filename)\n",
    "zip_ref = zipfile.ZipFile(local_filename, 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se definen las rutas relativas a los directorios que utilizaremos en el notebook tanto para generar los ficheros resultantes del indizado, como los directorios en los que se ubican los ficheros de entrada (corpus, tests, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de entrada y salida (generados)\n",
    "corpus_dir = 'data/corpus'\n",
    "toy_dir = 'data/toy'\n",
    "out_dir_naive='output/naive/index'\n",
    "out_dir_naive_toy='output/naive/toy_index'\n",
    "out_dir_bsbi='output/bsbi/index'\n",
    "out_dir_bsbi_toy='output/bsbi/toy_index'\n",
    "\n",
    "# Cambia esto a \"True\" para indexar el corpus íntegro en memoria (requiere más de 1GiB de memoria)\n",
    "build_naive_index_full = True\n",
    "\n",
    "terms_filename = 'terms.dict'\n",
    "docs_filename = 'docs.dict'\n",
    "postings_index_filename = 'postings.index'\n",
    "postings_metadata_filename = 'postings_metadata.dict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuta la siguiente celda para asegurarte de que todo está en su sitio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found required directory at 'data/corpus'.\n",
      "Found required directory at 'data/toy'.\n",
      "Output directory 'output/naive/index' already exists.\n",
      "Output directory 'output/naive/toy_index' already exists.\n",
      "Output directory 'output/bsbi/index' already exists.\n",
      "Output directory 'output/bsbi/toy_index' already exists.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Check if all directories exist\n",
    "def check_directories(*directories):\n",
    "    for path in directories:\n",
    "        directory = Path(path)\n",
    "        if not directory.is_dir():\n",
    "            raise FileNotFoundError(f\"Cannot find required directory at path '{path}'.\")\n",
    "        else:\n",
    "            print(f\"Found required directory at '{path}'.\")\n",
    "\n",
    "# Create output directories\n",
    "def create_directories(*directories):\n",
    "    for path in directories:\n",
    "        try:\n",
    "            os.makedirs(path, exist_ok=False)\n",
    "            print(f\"Output directory '{path}' has been created.\")\n",
    "        except FileExistsError:\n",
    "            print(f\"Output directory '{path}' already exists.\")\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create directory at path '{path}'. Error: {e}\")\n",
    "\n",
    "check_directories(corpus_dir, toy_dir)\n",
    "create_directories(out_dir_naive, out_dir_naive_toy, out_dir_bsbi, out_dir_bsbi_toy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El índice se almacenará en el directorio indicado por la variable `out_dir_naive`. También utilizaremos un directorio, dado por el valor de `out_dir_naive_toy`, donde se generará un índice de prueba a partir de datos de juguete. Por su parte, `tmp_dir` indica la ruta al directorio en el que se guardarán algunos archivos temporales para los índices de juguete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La clase auxiliar *IdMap*\n",
    "\n",
    "Para hacer más eficiente la construcción de índices, representaremos los términos del vocabulario como *termIDs* (en lugar de cadenas), donde cada *termID* es un número de serie único. Podemos construir el mapeo de términos a termIDs sobre la marcha mientras procesamos la colección. Del mismo modo, también representamos los documentos como *docIDs* (en lugar de cadenas).\n",
    "\n",
    "Con el fin de simplificar la correspondencia entre cadenas e ids numéricos, debes programar una clase auxiliar llamada `IdMap`. Esta clase se encargará de asignar biyectivamente término a termID y doc a docID.\n",
    "\n",
    "Para esto, la clase deberá contener dos atributos:\n",
    "* `str_to_id`: Un diccionario que mapeará cada cadena a su id numéricos, permitiendo un acceso eficiente al id numérico a partir de la cadena.\n",
    "* `id_to_str`: Una lista para asociar cada id numérico (posición en la lista) a su correspondiente cadena de caracteres, permitiendo un acceso y almacenamiento eficientes de ids numéricos a cadenas.\n",
    "\n",
    "\n",
    "A la vista de estos requisitos, programa las funciones `_get_str` y `_get_id` en el siguiente código. La única interfaz a esta clase es proporcionada por `__getitem__` que obtiene el mapeo correcto dependiendo del tipo de clave.\n",
    "\n",
    "**Documentación recomendada**: *Si lo necesitas, consulta [la documentación sobre métodos especiales (o \"mágicos\")](https://docs.python.org/3.7/reference/datamodel.html#special-method-names) de Python, tal como como `__getitem__` ). También puede resultarte útil este [breve tutorial](https://www.omkarpathak.in/2018/04/11/python-getitem-and-setitem/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        ### Begin your code\n",
    "        # Asegurarse de que ambas estructuras tienen la misma longitud\n",
    "        assert len(self.str_to_id) == len(self.id_to_str), \\\n",
    "            \"Inconsistencia: str_to_id y id_to_str tienen longitudes diferentes\"\n",
    "        return len(self.id_to_str)\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "        # Verificar que el índice está en el rango válido\n",
    "        if i < 0 or i >= len(self.id_to_str):\n",
    "            raise IndexError(f\"ID {i} fuera de rango. Rango válido: [0, {len(self.id_to_str)-1}]\")\n",
    "        return self.id_to_str[i]\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        # idx is ID mapped to this string (or next unassigned ID if key is missing)\n",
    "        ### Begin your code\n",
    "        # Si la cadena ya existe en el diccionario, devuelve su ID\n",
    "        if s in self.str_to_id:\n",
    "            idx = self.str_to_id[s]\n",
    "        else:\n",
    "            # Si no existe, asigna un nuevo ID (el siguiente disponible)\n",
    "            idx = len(self.id_to_str)\n",
    "            # Añade el mapeo en ambas estructuras\n",
    "            self.str_to_id[s] = idx\n",
    "            self.id_to_str.append(s)\n",
    "        ### End your code\n",
    "        assert len(self.str_to_id) == len(self.id_to_str)  \n",
    "        return idx\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegúrese de que supera los siguientes tests sencillos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdMap = IdMap()\n",
    "assert testIdMap['a'] == 0, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['bcd'] == 1, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['a'] == 0, \"Unable to retrieve the id of an existing string\"\n",
    "assert testIdMap[1] == 'bcd', \"Unable to retrive the string corresponding to a\\\n",
    "                                given id\"\n",
    "try:\n",
    "    testIdMap[2]\n",
    "except IndexError as e:\n",
    "    assert True, \"Doesn't throw an IndexError for out of range numeric ids\"\n",
    "assert len(testIdMap) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de un índice invertido en memoria: Algoritmo *naïve*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a construir un índice invertido siguiendo un algoritmo *naïve* como el que se describe al comienzo del tema 2 de teoría, el cual no tiene en cuenta los requisitos de espacio en memoria del algoritmo ni las características del hardware del computador en el que se ejecuta. Según este algoritmo, el índice al completo, incluyendo tanto su diccionario de términos como las *postings lists* de cada término del vocabulario, se mantienen en memoria mientras se va construyendo el índice.\n",
    "\n",
    "**IMPORTANTE: Llevar a cabo la construcción de esta forma resultará únicamente posible en el caso de que no estemos usando `ulimit` para imponer limitaciones el uso de memoria por parte del proceso encargado de ejecutar este *notebook* de Jupyter  (ver indicaciones al comienzo del cuaderno).**\n",
    "\n",
    "Recordemos que, para construir un índice, los pasos generales son:\n",
    "1. Escanear la colección reuniendo todos los pares término-documento.\n",
    "2. Ordenar los pares con el término como clave primaria y el docID como clave secundaria.\n",
    "3. Generar una lista con los docID de cada término\n",
    "\n",
    "En nuestro caso, ya que no nos preocupará el tamaño de las estructuras de datos que mantengamos en memoria, realizaremos la construcción de una manera ligeramente distinta: en lugar de generar una lista de tuplas (termId,docId) durante el escaneo del corpus, utilizaremos un diccionario Python para mantener la correspondencia entre cada término (identificado por su *termId*) y la lista de documentos (*docIds*) en que aparece (*postings list*). El uso de estas estructuras de datos en memoria nos simplifica la tarea por varias razones:\n",
    "- No es necesario ordenar por *termId* (orden primario de tuplas (termid, docid)), sino que los términos del diccionario están automáticamente ordenados por *termId* gracias a que, desde Python 3.7, los diccionarios son ordenados (mantienen el orden de inserción). Puesto que los términos se insertan en orden creciente de *termId*, a la hora de escribir el índice en disco podremos simplemente recorrer el diccionario a la hora de escribir las *postings lists* sin que sea necesario realizar ninguna ordenación previa.\n",
    "- No es necesario ordenar por *docId* (orden secundario) sino que para cada término construiremos su *postings list* manteniendo el orden de los *docIds* en todo momento, al tiempo que evitamos duplicados. Esto es sencillo puesto que los documentos se escanean en orden creciente de *docId* (se generará un nuevo *docId* al considerar cada nuevo documento del corpus). Para cada término encontrado en el corpus, simplemente se comprueba si ya existe en el diccionario; en tal caso, basta con comparar el *docId* actual con el del último *posting* de la lista asociada el término, para decidir si resulta necesario añadirlo o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveIndex:\n",
    "    \"\"\"A class that implements a naive inverted index built entirely in memory\n",
    "    \n",
    "    Attributes\n",
    "    ---------\n",
    "    term_id_map(IdMap): For mapping terms to termIDs\n",
    "    doc_id_map(IdMap): For mapping relative paths of documents (eg \n",
    "        0/3dradiology.stanford.edu_) to docIDs\n",
    "    data_dir(str): Path to data\n",
    "    out_dir(str): Path to directory where index files will be saved to/loaded from\n",
    "    postings: Dictionary mapping: termID->postings lists [docid1,docid2...]\n",
    "    postings_metadata: Dictionary mapping: termID->(start_position_in_index_file, \n",
    "                                                    number_of_postings_in_list,\n",
    "                                                    length_in_bytes_of_postings_list)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, out_dir):\n",
    "        self.term_id_map = IdMap()\n",
    "        self.doc_id_map = IdMap()\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = out_dir\n",
    "        self.postings = {}\n",
    "        self.postings_metadata = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Parsear* los documentos del *corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programa los métodos `parse_all_subdirectories` y `parse_directory` de la clase `NaiveIndex`, de la forma que se indica:\n",
    "\n",
    "- `parse_directory`: Escanea los documentos existentes en un directorio y los añade a un diccionario de *postings* que mantiene la asociación entre cada *termID* y su *postings list* (lista de *docIds*). Recibe como argumentos la ruta al directorio a escanear y el diccionario de *postings* sobre el que debe operar.\n",
    "\n",
    "- `parse_all_subdirectories`: Debe hacer uso de la función anterior, para escanear todos los subdirectorios ('0', '1', etc.) en los que se dividen los ficheros del corpus. Como resultado de ejecutar esta función, el atributo `postings` de la clase `NaiveIndex` contendrá un diccionario con tantos elementos como términos distintos encontrados en el corpus, y para cada uno de ellos los *docId* en que aparece. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACLARACIÓN**: *En las siguientes celdas, el hacer que `NaiveIndex` herede de `NaiveIndex` no es más que una forma sencilla de añadir un nuevo método a una clase ya existente. Aunque esto puede resultar confuso, en realidad se utiliza simplemente para dividir en varias celdas una definición de clase dentro de un cuaderno de Jupyter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveIndex(NaiveIndex):\n",
    "    def parse_directory(self, dir_path, postings):\n",
    "        \"\"\"Parse all documents at the given path, inserts\n",
    "        to the dictionary of postings lists given as argument.\n",
    "        Should use self.term_id_map and self.doc_id_map to get termIDs and docIDs.\n",
    "        \"\"\"\n",
    "        # read the lines of each doc, extract its terms and insert the docid in the list of postlist of that term\n",
    "        ### Begin your code\n",
    "        # Obtener lista de archivos en el directorio y ordenarlos\n",
    "        filenames = sorted(os.listdir(dir_path))\n",
    "        \n",
    "        # Procesar cada archivo\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dir_path, filename)\n",
    "            \n",
    "            # Verificar que es un archivo (no un directorio)\n",
    "            if os.path.isfile(filepath):\n",
    "                # Obtener o asignar docID para este documento\n",
    "                # Usar ruta relativa desde data_dir para consistencia\n",
    "                relative_path = os.path.relpath(filepath, self.data_dir)\n",
    "                doc_id = self.doc_id_map[relative_path]\n",
    "                \n",
    "                # Leer el contenido del archivo\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Extraer términos únicos del documento\n",
    "                terms = set(content.split())\n",
    "                \n",
    "                # Para cada término único en el documento\n",
    "                for term in terms:\n",
    "                    # Obtener o asignar termID\n",
    "                    term_id = self.term_id_map[term]\n",
    "                    \n",
    "                    # Si el término no está en postings, inicializar su lista\n",
    "                    if term_id not in postings:\n",
    "                        postings[term_id] = []\n",
    "                    \n",
    "                    # Añadir docID a la posting list del término\n",
    "                    # Solo si no está ya (evitar duplicados, aunque con set no debería pasar)\n",
    "                    if not postings[term_id] or postings[term_id][-1] != doc_id:\n",
    "                        postings[term_id].append(doc_id)\n",
    "        ### End your code\n",
    "\n",
    "    def parse_all_subdirectories(self):\n",
    "        \"\"\"Parse all documents and generate dictionary of postings lists \n",
    "        Should use self.term_id_map and self.doc_id_map to get termIDs and docIDs.\n",
    "        \"\"\"\n",
    "        assert not self.postings\n",
    "        # Remember to sort the list of directories to assure they are read in the right order\n",
    "        ### Begin your code\n",
    "        # Obtener lista de subdirectorios y ordenarlos\n",
    "        subdirs = sorted(os.listdir(self.data_dir))\n",
    "        \n",
    "        # Procesar cada subdirectorio en orden\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(self.data_dir, subdir)\n",
    "            \n",
    "            # Verificar que es un directorio\n",
    "            if os.path.isdir(subdir_path):\n",
    "                print(f\"Procesando subdirectorio: {subdir}\")\n",
    "                # Parsear todos los documentos del subdirectorio\n",
    "                self.parse_directory(subdir_path, self.postings)\n",
    "        \n",
    "        print(f\"Indexación completada: {len(self.term_id_map)} términos, {len(self.doc_id_map)} documentos\")\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza los ficheros de juguete en la ruta indicada por `toy_dir` para probar tu código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/toy/0/fine.txt\n",
      "data/toy/0/hello.txt\n",
      "data/toy/1/byebye.txt\n",
      "data/toy/1/bye.txt\n",
      "data/toy/2/fine.txt\n",
      "data/toy/2/hello.txt\n"
     ]
    }
   ],
   "source": [
    "!find $toy_dir -type f | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprueba si la función funciona como se espera con los datos de juguete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm fine , thank you\n",
      "\n",
      "hi hi\n",
      "how are you ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(Path(toy_dir)/'0/fine.txt', 'r') as file:\n",
    "    print(file.read())\n",
    "with open(Path(toy_dir)/'0/hello.txt', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda comprueba si la función `parse_all_subdirectories` es capaz de escanear correctamente los ficheros de juguete situados en `toy_dir`, y a continuación, muestra el objeto índice creado con `obj.__dict__`, de forma que se pueda ver el valor del atributo `td_pairs`, entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras escanear los documentos y construir el diccionario sobre los datos de juguete, deberíamos tener un atributo `postings_lists` con el siguiente contenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando subdirectorio: 0\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 2\n",
      "Indexación completada: 14 términos, 6 documentos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'term_id_map': <__main__.IdMap at 0x7215f8271600>,\n",
       " 'doc_id_map': <__main__.IdMap at 0x7215f8270fd0>,\n",
       " 'data_dir': 'data/toy',\n",
       " 'output_dir': 'output/naive/toy_index',\n",
       " 'postings': {0: [0, 4],\n",
       "  1: [0, 1, 2, 4, 5],\n",
       "  2: [0, 4],\n",
       "  3: [0, 2, 4],\n",
       "  4: [0, 4],\n",
       "  5: [1, 5],\n",
       "  6: [1, 5],\n",
       "  7: [1, 2, 5],\n",
       "  8: [1, 5],\n",
       "  9: [2],\n",
       "  10: [2],\n",
       "  11: [2, 3],\n",
       "  12: [2],\n",
       "  13: [2]},\n",
       " 'postings_metadata': {}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_index = NaiveIndex(toy_dir, out_dir_naive_toy)\n",
    "toy_index.parse_all_subdirectories()\n",
    "toy_index.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 4],\n",
       " 1: [0, 1, 2, 4, 5],\n",
       " 2: [0, 4],\n",
       " 3: [0, 2, 4],\n",
       " 4: [0, 4],\n",
       " 5: [1, 5],\n",
       " 6: [1, 5],\n",
       " 7: [1, 2, 5],\n",
       " 8: [1, 5],\n",
       " 9: [2],\n",
       " 10: [2],\n",
       " 11: [2, 3],\n",
       " 12: [2],\n",
       " 13: [2]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_index.postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: \n",
    "\n",
    "```\n",
    "{0: [0, 4],\n",
    " 1: [0, 4],\n",
    " 2: [0, 2, 4],\n",
    " 3: [0, 4],\n",
    " 4: [0, 1, 2, 4, 5],\n",
    " 5: [1, 5],\n",
    " 6: [1, 5],\n",
    " 7: [1, 5],\n",
    " 8: [1, 2, 5],\n",
    " 9: [2],\n",
    " 10: [2],\n",
    " 11: [2],\n",
    " 12: [2, 3],\n",
    " 13: [2]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribe algunas pruebas para asegurarte de que efectivamente el método `parse_all_subdirectories` funciona como se espera en los datos de juguete (`toy_dir`). Por ejemplo, deberías asegurarte de que una palabra dada recibe el mismo id cada vez que aparece)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term->TermIDs\n",
      "('i'm', 0)\n",
      "('you', 1)\n",
      "('fine', 2)\n",
      "(',', 3)\n",
      "('thank', 4)\n",
      "('hi', 5)\n",
      "('how', 6)\n",
      "('?', 7)\n",
      "('are', 8)\n",
      "('to', 9)\n",
      "('good', 10)\n",
      "('bye', 11)\n",
      "('later', 12)\n",
      "('see', 13)\n",
      "\n",
      "Verificación de consistencia:\n",
      "'you' -> ID primera consulta: 1, ID segunda consulta: 1\n",
      "✓ El término 'you' recibe consistentemente el ID 1\n",
      "\n",
      "Verificación bidireccional:\n",
      "ID 0 -> 'i'm' -> ID 0\n",
      "ID 1 -> 'you' -> ID 1\n",
      "ID 2 -> 'fine' -> ID 2\n",
      "✓ Mapeo bidireccional correcto\n",
      "\n",
      "Doc->DocIDs\n",
      "('0/fine.txt', 0)\n",
      "('0/hello.txt', 1)\n",
      "('1/bye.txt', 2)\n",
      "('1/byebye.txt', 3)\n",
      "('2/fine.txt', 4)\n",
      "('2/hello.txt', 5)\n",
      "\n",
      "Verificación de consistencia:\n",
      "'0/fine.txt' -> ID primera consulta: 0, ID segunda consulta: 0\n",
      "✓ El documento recibe consistentemente el ID 0\n",
      "\n",
      "Verificación de orden de procesamiento:\n",
      "Total de documentos procesados: 6\n",
      "  DocID 0: 0/fine.txt\n",
      "  DocID 1: 0/hello.txt\n",
      "  DocID 2: 1/bye.txt\n",
      "  DocID 3: 1/byebye.txt\n",
      "  DocID 4: 2/fine.txt\n",
      "  DocID 5: 2/hello.txt\n",
      "✓ Documentos procesados en orden\n",
      "\n",
      "Verificación de posting lists:\n",
      "Posting list para 'you' (termID=4): [0, 4]\n",
      "  Aparece en 2 documentos\n",
      "✓ Posting list ordenada correctamente\n"
     ]
    }
   ],
   "source": [
    "print(\"Term->TermIDs\")\n",
    "### Begin your code\n",
    "# Mostrar todos los términos y sus IDs\n",
    "for term in toy_index.term_id_map.id_to_str:\n",
    "    term_id = toy_index.term_id_map[term]\n",
    "    print(f\"('{term}', {term_id})\")\n",
    "\n",
    "# Verificar que un término recibe siempre el mismo ID\n",
    "print(\"\\nVerificación de consistencia:\")\n",
    "test_term = \"you\"\n",
    "id1 = toy_index.term_id_map[test_term]\n",
    "id2 = toy_index.term_id_map[test_term]\n",
    "print(f\"'{test_term}' -> ID primera consulta: {id1}, ID segunda consulta: {id2}\")\n",
    "assert id1 == id2, f\"El término '{test_term}' no recibe el mismo ID\"\n",
    "print(f\"✓ El término '{test_term}' recibe consistentemente el ID {id1}\")\n",
    "\n",
    "# Verificar que IDs diferentes corresponden a términos diferentes\n",
    "print(\"\\nVerificación bidireccional:\")\n",
    "for i in range(min(3, len(toy_index.term_id_map))):\n",
    "    term = toy_index.term_id_map[i]\n",
    "    recovered_id = toy_index.term_id_map[term]\n",
    "    print(f\"ID {i} -> '{term}' -> ID {recovered_id}\")\n",
    "    assert i == recovered_id, f\"Mapeo inconsistente para ID {i}\"\n",
    "print(f\"✓ Mapeo bidireccional correcto\")\n",
    "\n",
    "### End your code\n",
    "print(\"\\nDoc->DocIDs\")\n",
    "### Begin your code\n",
    "# Mostrar todos los documentos y sus IDs\n",
    "for doc_path in toy_index.doc_id_map.id_to_str:\n",
    "    doc_id = toy_index.doc_id_map[doc_path]\n",
    "    print(f\"('{doc_path}', {doc_id})\")\n",
    "\n",
    "# Verificar que un documento recibe siempre el mismo ID\n",
    "print(\"\\nVerificación de consistencia:\")\n",
    "test_doc = toy_index.doc_id_map[0]  # Obtener el primer documento\n",
    "id1 = toy_index.doc_id_map[test_doc]\n",
    "id2 = toy_index.doc_id_map[test_doc]\n",
    "print(f\"'{test_doc}' -> ID primera consulta: {id1}, ID segunda consulta: {id2}\")\n",
    "assert id1 == id2, f\"El documento '{test_doc}' no recibe el mismo ID\"\n",
    "print(f\"✓ El documento recibe consistentemente el ID {id1}\")\n",
    "\n",
    "# Verificar el orden de procesamiento (deben estar ordenados)\n",
    "print(\"\\nVerificación de orden de procesamiento:\")\n",
    "print(f\"Total de documentos procesados: {len(toy_index.doc_id_map)}\")\n",
    "for i in range(len(toy_index.doc_id_map)):\n",
    "    doc = toy_index.doc_id_map[i]\n",
    "    print(f\"  DocID {i}: {doc}\")\n",
    "print(f\"✓ Documentos procesados en orden\")\n",
    "\n",
    "# Verificar algunas postings lists\n",
    "print(\"\\nVerificación de posting lists:\")\n",
    "if 4 in toy_index.postings:  # termID para \"you\"\n",
    "    postings_you = toy_index.postings[4]\n",
    "    print(f\"Posting list para 'you' (termID=4): {postings_you}\")\n",
    "    print(f\"  Aparece en {len(postings_you)} documentos\")\n",
    "    assert all(postings_you[i] <= postings_you[i+1] for i in range(len(postings_you)-1)), \\\n",
    "        \"La posting list no está ordenada\"\n",
    "    print(f\"✓ Posting list ordenada correctamente\")\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: \n",
    "```\n",
    "Term->TermIDs\n",
    "(\"i'm\", 0)\n",
    "('fine', 1)\n",
    "(',', 2)\n",
    "('thank', 3)\n",
    "('you', 4)\n",
    "('hi', 5)\n",
    "('how', 6)\n",
    "('are', 7)\n",
    "('?', 8)\n",
    "('bye', 9)\n",
    "('good', 10)\n",
    "('to', 11)\n",
    "('see', 12)\n",
    "('later', 13)\n",
    "Doc->DocIDs\n",
    "('data/toy/0/fine.txt', 0)\n",
    "('data/toy/0/hello.txt', 1)\n",
    "('data/toy/1/byebye.txt', 2)\n",
    "('data/toy/1/bye.txt', 3)\n",
    "('data/toy/2/fine.txt', 4)\n",
    "('data/toy/2/hello.txt', 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación de información sobre el índice en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, añade a la clase `NaiveIndex` los siguientes métodos:\n",
    "\n",
    "- `get_term_postings_from_mem`: Debe devolver la lista de *postings* (*docIds*) de un determinado término dado como argumento.\n",
    "\n",
    "- `docids_to_docnames`: A partir de una lista de *postings* dada como argumento, debe devolver una lista en la que el elemento i-ésimo será la ruta al fichero correspondiente añ *docId* i-esimo en la lista dada.\n",
    "\n",
    "- `retrieve`: Debe devolver los nombres de los documentos en los que aparece un término dado como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NaiveIndex(NaiveIndex):\n",
    "\n",
    "    def get_term_postings_from_mem(self, term):\n",
    "        postings = []\n",
    "        assert self.postings\n",
    "        ### Begin your code        \n",
    "        # Obtener el termID del término\n",
    "        if term in self.term_id_map.str_to_id:\n",
    "            term_id = self.term_id_map[term]\n",
    "            # Si el término existe en el índice, obtener su posting list\n",
    "            if term_id in self.postings:\n",
    "                postings = self.postings[term_id]\n",
    "        ### End your code        \n",
    "        return postings\n",
    "\n",
    "    def docids_to_docnames(self, postings):\n",
    "        doc_names = []\n",
    "        ### Begin your code        \n",
    "        # Convertir cada docID a su nombre de documento correspondiente\n",
    "        for doc_id in postings:\n",
    "            doc_name = self.doc_id_map[doc_id]\n",
    "            doc_names.append(doc_name)\n",
    "        ### End your code        \n",
    "        return doc_names\n",
    "\n",
    "    def retrieve(self, term):\n",
    "        postings = self.get_term_postings_from_mem(term)\n",
    "        return self.docids_to_docnames(self.get_term_postings_from_mem(term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprueba que tus métodos funcionan. Recuerda que, tras añadir nuevos métodos a una clase, debes volver a construir un nuevo objeto de dicha clase para que contenga los nuevos métodos añadidos por celdas como la anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando subdirectorio: 0\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 2\n",
      "Indexación completada: 14 términos, 6 documentos\n"
     ]
    }
   ],
   "source": [
    "toy_index = NaiveIndex(toy_dir, out_dir_naive_toy)\n",
    "toy_index.parse_all_subdirectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_index.get_term_postings_from_mem(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: \n",
    "```\n",
    "[1, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0/fine.txt', '0/hello.txt', '1/bye.txt', '2/fine.txt', '2/hello.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_index.retrieve(\"you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado:\n",
    "\n",
    "```\n",
    "['data/toy/0/fine.txt',\n",
    " 'data/toy/0/hello.txt',\n",
    " 'data/toy/1/bye.txt',\n",
    " 'data/toy/2/fine.txt',\n",
    " 'data/toy/2/hello.txt']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar el índice invertido en disco\n",
    "\n",
    "Una vez construido el índice, queremos guardarlo en disco para poder utilizarlo posteriormente con el fin de recuperar información del mismo sin necesidad de volver a construirlo. Esto también nos permitirá liberar la memoria ocupada por las *postings lists*, que hasta este punto se mantienen en memoria (atributo `postings`). Los pasos a seguir son los siguientes:\n",
    "\n",
    "1. Guardar en un fichero el índice, es decir, las *postings list* de todos los términos del vocabulario, en orden creciente por *termId*.\n",
    "\n",
    "1. Guardar en un fichero los *metadatos* necesarios para localizar los *postings* de un término en el fichero de índice anterior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escribir los `postings` a un fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes programar los métodos `save_term_postings` y `save_postings` de la clase `NaiveIndex`, como se indica a continuación:\n",
    "\n",
    "- `save_term_postings`: A partir de un fichero `f` previamente abierto con `with open(...) as f`, debe obtener el *offset* actual y a continuación escribir la lista de *postings* dada como argumento. Para escribir  las *postings lists* (listas de *docIDs*) en el disco, utilizaremos el paquete `struct` y su método `pack`; el módulo `struct` contiene funciones estáticas de codificación y decodificación, que permiten transformar una lista de enteros a un array de bytes, y viceversa. [Ver documentación aquí](https://docs.python.org/3/library/struct.html). En particular, se puede obtener una lista de enteros así: `struct.pack(f'{len(term_postings)}i', *term_postings)`\n",
    "\n",
    "- `save_postings`: Debe crear un fichero en el directorio de salida del índice `output_dir`, con el nombre indicado por la variable `postings_index_filename`, en el que se guardará la *postings list* de cada término del vocabulario, en orden creciente por *termId*. Debe hacer uso de la función `save_term_postings` anterior. Conforme se van guardando las *posting lists* en el fichero, será necesario construir un nuevo diccionario (atributo `postings_metadata`) con la correspondencia entre cada término (*termId*) y los metadatos para localizar sus *postings* en el fichero del índice. Para cada término, los metadatos necesarios para recuperar posteriormente los postings son: el desplazamiento (*offset*) dentro del fichero de índice donde comienza, la longitud de la lista de *postings* y el tamaño en bytes de dicha lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construir el atributo `postings_metadata` de la clase `NaiveIndex`\n",
    "Se trata de un diccionario que relaciona los termIDs con una tripleta de metadatos que es útil para leer y escribir las *postings lists* en el archivo de índice a/desde el disco:\n",
    "\n",
    "* `start_position_in_index_file`:posición (en bytes) de la *posting list* en el fichero índice.\n",
    "\n",
    "* `number_of_postings_in_list`: número de entradas (docIDs) de lista.\n",
    "\n",
    "* `length_in_bytes_of_postings_list`: longitud en bytes de la *postings list* codificada.\n",
    "\n",
    "Se supone que este mapeo se mantiene en memoria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "class NaiveIndex(NaiveIndex):\n",
    "    def save_term_postings(self, file, term_postings):\n",
    "        at_offset = -1  # offset where the postings are going to be stored (-1 is a non valid value)\n",
    "        num_bytes = 0\n",
    "        assert term_postings, \"save_term_postings expets a non-empty postings list\"\n",
    "        ### Begin your code        \n",
    "        # Obtener la posición actual en el archivo (offset)\n",
    "        at_offset = file.tell()\n",
    "        \n",
    "        # Empaquetar la lista de postings como bytes\n",
    "        # Formato: '{N}i' donde N es el número de enteros de 4 bytes\n",
    "        packed_data = struct.pack(f'{len(term_postings)}i', *term_postings)\n",
    "        \n",
    "        # Escribir los bytes en el archivo\n",
    "        file.write(packed_data)\n",
    "        \n",
    "        # Calcular el número de bytes escritos\n",
    "        num_bytes = len(packed_data)\n",
    "        ### End your code        \n",
    "        assert at_offset >= 0 and num_bytes > 0, \"save_term_postings not functional\"\n",
    "        return at_offset, num_bytes\n",
    "        \n",
    "    def save_postings(self):\n",
    "        \"\"\"\n",
    "        Writes the inverted index in self.postings_list to a file\n",
    "        \"\"\"\n",
    "        filename = postings_index_filename\n",
    "        assert not self.postings_metadata, \"Postings metadata must be empty before index is saved to file\"\n",
    "        ### Begin your code      \n",
    "        # Crear la ruta completa al archivo de postings\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        # Abrir el archivo en modo binario de escritura\n",
    "        with open(filepath, 'wb') as f:\n",
    "            # Recorrer todos los términos en orden (los diccionarios mantienen orden de inserción desde Python 3.7)\n",
    "            for term_id, postings_list in self.postings.items():\n",
    "                # Guardar la posting list y obtener offset y tamaño\n",
    "                offset, num_bytes = self.save_term_postings(f, postings_list)\n",
    "                \n",
    "                # Guardar los metadatos: (offset, longitud_lista, tamaño_en_bytes)\n",
    "                self.postings_metadata[term_id] = (offset, len(postings_list), num_bytes)\n",
    "        ### End your code   \n",
    "        assert self.postings_metadata, \"Postings metadata not built after index saved to file\"\n",
    "\n",
    "    def release_postings_from_memory(self):\n",
    "        \"\"\"\n",
    "        Releases the postings from memory, keeping the metadata to locate postings in file\n",
    "        \"\"\"\n",
    "        self.postings = {}\n",
    "        # NO vaciar postings_metadata - se necesita para acceder al índice en disco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, prueba tu código y muestra `postings_metadata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando subdirectorio: 0\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 2\n",
      "Indexación completada: 14 términos, 6 documentos\n"
     ]
    }
   ],
   "source": [
    "toy_index = NaiveIndex(toy_dir, out_dir_naive_toy)\n",
    "toy_index.parse_all_subdirectories()\n",
    "toy_index.save_postings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0, 2, 8),\n",
       " 1: (8, 5, 20),\n",
       " 2: (28, 2, 8),\n",
       " 3: (36, 3, 12),\n",
       " 4: (48, 2, 8),\n",
       " 5: (56, 2, 8),\n",
       " 6: (64, 2, 8),\n",
       " 7: (72, 3, 12),\n",
       " 8: (84, 2, 8),\n",
       " 9: (92, 1, 4),\n",
       " 10: (96, 1, 4),\n",
       " 11: (100, 2, 8),\n",
       " 12: (108, 1, 4),\n",
       " 13: (112, 1, 4)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_index.postings_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado:\n",
    "```\n",
    "{0: (0, 2, 8),\n",
    " 1: (8, 2, 8),\n",
    " 2: (16, 3, 12),\n",
    " 3: (28, 2, 8),\n",
    " 4: (36, 5, 20),\n",
    " 5: (56, 2, 8),\n",
    " 6: (64, 2, 8),\n",
    " 7: (72, 2, 8),\n",
    " 8: (80, 3, 12),\n",
    " 9: (92, 1, 4),\n",
    " 10: (96, 1, 4),\n",
    " 11: (100, 1, 4),\n",
    " 12: (104, 2, 8),\n",
    " 13: (112, 1, 4)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tras guardar el índice en disco, volcamos con `hexdump` el contenido del fichero en el que hemos escrito los *postings*, deberíamos observar los *docIds* de cada *postings list* codificados, uno tras otro, codificados como un entero de 4 bytes (en *little endian*). NOTA: La primera columna mostrada por `hexdump` corresponde al offset del primer byte del fichero que aparece en esa línea (similar a como ocurría en el programa `okteta` que viste en la asignatura de \"Fundamentos de Computadores\"); el resto de columnas son el contenido del fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs.dict  postings.index  postings_metadata.dict  terms.dict\n",
      "00000000  00 00 00 00 04 00 00 00  00 00 00 00 01 00 00 00  |................|\n",
      "00000010  02 00 00 00 04 00 00 00  05 00 00 00 00 00 00 00  |................|\n",
      "00000020  04 00 00 00 00 00 00 00  02 00 00 00 04 00 00 00  |................|\n",
      "00000030  00 00 00 00 04 00 00 00  01 00 00 00 05 00 00 00  |................|\n",
      "00000040  01 00 00 00 05 00 00 00  01 00 00 00 02 00 00 00  |................|\n",
      "00000050  05 00 00 00 01 00 00 00  05 00 00 00 02 00 00 00  |................|\n",
      "00000060  02 00 00 00 02 00 00 00  03 00 00 00 02 00 00 00  |................|\n",
      "00000070  02 00 00 00                                       |....|\n",
      "00000074\n",
      "00000000  00 00 00 00 04 00 00 00  00 00 00 00 01 00 00 00  |................|\n",
      "00000010  02 00 00 00 04 00 00 00  05 00 00 00 00 00 00 00  |................|\n",
      "00000020  04 00 00 00 00 00 00 00  02 00 00 00 04 00 00 00  |................|\n",
      "00000030  00 00 00 00 04 00 00 00  01 00 00 00 05 00 00 00  |................|\n",
      "00000040  01 00 00 00 05 00 00 00  01 00 00 00 02 00 00 00  |................|\n",
      "00000050  05 00 00 00 01 00 00 00  05 00 00 00 02 00 00 00  |................|\n",
      "00000060  02 00 00 00 02 00 00 00  03 00 00 00 02 00 00 00  |................|\n",
      "00000070  02 00 00 00                                       |....|\n",
      "00000074\n"
     ]
    }
   ],
   "source": [
    "!ls $out_dir_naive_toy\n",
    "!hexdump -C $out_dir_naive_toy/$postings_index_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: \n",
    "\n",
    "```\n",
    "00000000  00 00 00 00 04 00 00 00  00 00 00 00 04 00 00 00  |................|\n",
    "00000010  00 00 00 00 02 00 00 00  04 00 00 00 00 00 00 00  |................|\n",
    "00000020  04 00 00 00 00 00 00 00  01 00 00 00 02 00 00 00  |................|\n",
    "00000030  04 00 00 00 05 00 00 00  01 00 00 00 05 00 00 00  |................|\n",
    "00000040  01 00 00 00 05 00 00 00  01 00 00 00 05 00 00 00  |................|\n",
    "00000050  01 00 00 00 02 00 00 00  05 00 00 00 02 00 00 00  |................|\n",
    "00000060  02 00 00 00 02 00 00 00  02 00 00 00 03 00 00 00  |................|\n",
    "00000070  02 00 00 00                                       |....|\n",
    "00000074\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escribir los metadatos que referencian al fichero de *postings* (`postings_metadata`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes programar el método `save_postings_metadata` de la clase `NaiveIndex`, que guarde en un fichero el contenido del diccionario `postings_metadata`. El nombre del fichero vendrá dado por la variable `postings_metadata_filename` y se creará igualmente en el directorio de salida; para cada término del vocabulario, se deberá escribir en orden creciente por *termId*, junto con el propio *termId*, la tripleta de metadatos anteriomente descrita *(offset, num_postings, num_bytes)*. NOTA: **No se debe asumir** que el diccionario `postings_metadata` está ordenado por *termId* (aunque en esta versión *naïve* en realidad sí lo está)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NaiveIndex(NaiveIndex):\n",
    "\n",
    "    def save_postings_metadata(self):\n",
    "        \"\"\"\n",
    "        Escribe un diccionario de offsets en un archivo binario.\n",
    "        \"\"\"\n",
    "        filename = postings_metadata_filename\n",
    "        if not self.postings_metadata:\n",
    "            print(\"'save_postings_metadata' called, but no metadata found\")\n",
    "            return\n",
    "        ### Begin your code\n",
    "        # The values key, offset, length and size_in_bytes will be written as a 4-byte-integer (struct.pack('i',key)\n",
    "        \n",
    "        # Crear la ruta completa al archivo de metadatos\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        # Abrir el archivo en modo binario de escritura\n",
    "        with open(filepath, 'wb') as f:\n",
    "            # Ordenar por termID (clave del diccionario) para asegurar orden creciente\n",
    "            sorted_term_ids = sorted(self.postings_metadata.keys())\n",
    "            \n",
    "            # Para cada termID en orden\n",
    "            for term_id in sorted_term_ids:\n",
    "                # Obtener la tripleta de metadatos (offset, num_postings, num_bytes)\n",
    "                offset, num_postings, num_bytes = self.postings_metadata[term_id]\n",
    "                \n",
    "                # Empaquetar y escribir: termID, offset, num_postings, num_bytes\n",
    "                # Todos como enteros de 4 bytes ('i' = signed int de 4 bytes)\n",
    "                packed_data = struct.pack('iiii', term_id, offset, num_postings, num_bytes)\n",
    "                f.write(packed_data)\n",
    "            \n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando subdirectorio: 0\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 2\n",
      "Indexación completada: 14 términos, 6 documentos\n"
     ]
    }
   ],
   "source": [
    "toy_index = NaiveIndex(toy_dir, out_dir_naive_toy)\n",
    "toy_index.parse_all_subdirectories()\n",
    "toy_index.save_postings()\n",
    "toy_index.save_postings_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0, 2, 8),\n",
       " 1: (8, 5, 20),\n",
       " 2: (28, 2, 8),\n",
       " 3: (36, 3, 12),\n",
       " 4: (48, 2, 8),\n",
       " 5: (56, 2, 8),\n",
       " 6: (64, 2, 8),\n",
       " 7: (72, 3, 12),\n",
       " 8: (84, 2, 8),\n",
       " 9: (92, 1, 4),\n",
       " 10: (96, 1, 4),\n",
       " 11: (100, 2, 8),\n",
       " 12: (108, 1, 4),\n",
       " 13: (112, 1, 4)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_index.postings_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs.dict  postings.index  postings_metadata.dict  terms.dict\n",
      "00000000  00 00 00 00 00 00 00 00  02 00 00 00 08 00 00 00  |................|\n",
      "00000010  01 00 00 00 08 00 00 00  05 00 00 00 14 00 00 00  |................|\n",
      "00000020  02 00 00 00 1c 00 00 00  02 00 00 00 08 00 00 00  |................|\n",
      "00000030  03 00 00 00 24 00 00 00  03 00 00 00 0c 00 00 00  |....$...........|\n",
      "00000040  04 00 00 00 30 00 00 00  02 00 00 00 08 00 00 00  |....0...........|\n",
      "00000050  05 00 00 00 38 00 00 00  02 00 00 00 08 00 00 00  |....8...........|\n",
      "00000060  06 00 00 00 40 00 00 00  02 00 00 00 08 00 00 00  |....@...........|\n",
      "00000070  07 00 00 00 48 00 00 00  03 00 00 00 0c 00 00 00  |....H...........|\n",
      "00000080  08 00 00 00 54 00 00 00  02 00 00 00 08 00 00 00  |....T...........|\n",
      "00000090  09 00 00 00 5c 00 00 00  01 00 00 00 04 00 00 00  |....\\...........|\n",
      "000000a0  0a 00 00 00 60 00 00 00  01 00 00 00 04 00 00 00  |....`...........|\n",
      "000000b0  0b 00 00 00 64 00 00 00  02 00 00 00 08 00 00 00  |....d...........|\n",
      "000000c0  0c 00 00 00 6c 00 00 00  01 00 00 00 04 00 00 00  |....l...........|\n",
      "000000d0  0d 00 00 00 70 00 00 00  01 00 00 00 04 00 00 00  |....p...........|\n",
      "000000e0\n",
      "00000000  00 00 00 00 00 00 00 00  02 00 00 00 08 00 00 00  |................|\n",
      "00000010  01 00 00 00 08 00 00 00  05 00 00 00 14 00 00 00  |................|\n",
      "00000020  02 00 00 00 1c 00 00 00  02 00 00 00 08 00 00 00  |................|\n",
      "00000030  03 00 00 00 24 00 00 00  03 00 00 00 0c 00 00 00  |....$...........|\n",
      "00000040  04 00 00 00 30 00 00 00  02 00 00 00 08 00 00 00  |....0...........|\n",
      "00000050  05 00 00 00 38 00 00 00  02 00 00 00 08 00 00 00  |....8...........|\n",
      "00000060  06 00 00 00 40 00 00 00  02 00 00 00 08 00 00 00  |....@...........|\n",
      "00000070  07 00 00 00 48 00 00 00  03 00 00 00 0c 00 00 00  |....H...........|\n",
      "00000080  08 00 00 00 54 00 00 00  02 00 00 00 08 00 00 00  |....T...........|\n",
      "00000090  09 00 00 00 5c 00 00 00  01 00 00 00 04 00 00 00  |....\\...........|\n",
      "000000a0  0a 00 00 00 60 00 00 00  01 00 00 00 04 00 00 00  |....`...........|\n",
      "000000b0  0b 00 00 00 64 00 00 00  02 00 00 00 08 00 00 00  |....d...........|\n",
      "000000c0  0c 00 00 00 6c 00 00 00  01 00 00 00 04 00 00 00  |....l...........|\n",
      "000000d0  0d 00 00 00 70 00 00 00  01 00 00 00 04 00 00 00  |....p...........|\n",
      "000000e0\n"
     ]
    }
   ],
   "source": [
    "!ls $out_dir_naive_toy\n",
    "!hexdump -C $out_dir_naive_toy/$postings_metadata_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado:\n",
    "```\n",
    "00000000  00 00 00 00 00 00 00 00  02 00 00 00 08 00 00 00  |................|\n",
    "00000010  01 00 00 00 08 00 00 00  02 00 00 00 08 00 00 00  |................|\n",
    "00000020  02 00 00 00 10 00 00 00  03 00 00 00 0c 00 00 00  |................|\n",
    "00000030  03 00 00 00 1c 00 00 00  02 00 00 00 08 00 00 00  |................|\n",
    "00000040  04 00 00 00 24 00 00 00  05 00 00 00 14 00 00 00  |....$...........|\n",
    "00000050  05 00 00 00 38 00 00 00  02 00 00 00 08 00 00 00  |....8...........|\n",
    "00000060  06 00 00 00 40 00 00 00  02 00 00 00 08 00 00 00  |....@...........|\n",
    "00000070  07 00 00 00 48 00 00 00  02 00 00 00 08 00 00 00  |....H...........|\n",
    "00000080  08 00 00 00 50 00 00 00  03 00 00 00 0c 00 00 00  |....P...........|\n",
    "00000090  09 00 00 00 5c 00 00 00  01 00 00 00 04 00 00 00  |....\\...........|\n",
    "000000a0  0a 00 00 00 60 00 00 00  01 00 00 00 04 00 00 00  |....`...........|\n",
    "000000b0  0b 00 00 00 64 00 00 00  01 00 00 00 04 00 00 00  |....d...........|\n",
    "000000c0  0c 00 00 00 68 00 00 00  02 00 00 00 08 00 00 00  |....h...........|\n",
    "000000d0  0d 00 00 00 70 00 00 00  01 00 00 00 04 00 00 00  |....p...........|\n",
    "000000e0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el índice al completo en disco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, estamos en disposición de crear un método `save` que se encargue de guardar el índice construido al completo en disco, en cuatro sencillos pasos:\n",
    "\n",
    "1. Guardar el mapeo entre términos y *termId* usando el método `save_terms_dict`.\n",
    "\n",
    "1. Guardar el mapeo entre rutas a documentos y *docIds* usando el método `save_docs_dict`.\n",
    "\n",
    "1. Guardar el fichero de índice con los *postings*.\n",
    "\n",
    "1. Guardar los metadatos para localizar los *postings* en el fichero de índice.\n",
    "\n",
    "1. Liberar la memoria ocupada por los *postings* y sus metadatos, una vez que ambos han sido escritos a disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveIndex(NaiveIndex):\n",
    "    def save_terms_dict(self):\n",
    "        with open(os.path.join(self.output_dir, terms_filename), 'wb') as f:\n",
    "            pkl.dump(self.term_id_map, f)\n",
    "    def save_docs_dict(self):\n",
    "        with open(os.path.join(self.output_dir, docs_filename), 'wb') as f:\n",
    "            pkl.dump(self.doc_id_map, f)\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Dumps doc_id_map and term_id_map into output directory\"\"\"\n",
    "        ### Begin your code        \n",
    "        # 1. Guardar el mapeo término -> termID\n",
    "        self.save_terms_dict()\n",
    "        \n",
    "        # 2. Guardar el mapeo documento -> docID\n",
    "        self.save_docs_dict()\n",
    "        \n",
    "        # 3. Guardar el fichero de índice con los postings\n",
    "        self.save_postings()\n",
    "        \n",
    "        # 4. Guardar los metadatos para localizar los postings\n",
    "        self.save_postings_metadata()\n",
    "        \n",
    "        # 5. Liberar la memoria ocupada por postings y metadatos\n",
    "        self.release_postings_from_memory()\n",
    "        \n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando subdirectorio: 0\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 2\n",
      "Indexación completada: 14 términos, 6 documentos\n"
     ]
    }
   ],
   "source": [
    "toy_index = NaiveIndex(toy_dir, out_dir_naive_toy)\n",
    "toy_index.parse_all_subdirectories()\n",
    "toy_index.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los metadatos del índice en disco para poder recuperar información\n",
    "\n",
    "A partir de este momento, el fichero de *postings* se mantiene en disco, y sólo es necesario mantener en memoria los metadatos necesarios para poder acceder a las partes del fichero de *postings* donde se guardan los *docIds* en los que aparecen los términos de la consulta. Así pues, para poder realizar recuperación de información sobre el índice en disco, debes programar los siguientes métodos de `NaiveIndex`:\n",
    "\n",
    "- `load_next_metadata_entry`: Lee una entrada del fichero de metadatos, y devuelve una cuádrupla con el *termId* y los metadatos para leer la *posting list* del fichero de índice. La cuádrupla estará  formada por *(termId, offset, número de *postings*, tamaño en bytes de la lista)*.\n",
    "\n",
    "- `load_postings_metadata`: Carga el fichero de metadatos de disco, haciendo uso de la función `load_next_metadata_entry` y construye el atributo `postings_metadata` de la clase `NaiveIndex`.\n",
    "\n",
    "- `load`: Método que utilizará el código cliente que haga uso de `NaiveIndex` para cargar un índice y poder utilizarlo para realizar consultas. Cargará en memoria tanto el diccionario (mapeo de términos con *termIds* y documentos con *docIds*) como los metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveIndex(NaiveIndex):\n",
    "    def load_terms_dict(self):\n",
    "        ## Load term<->termID mapping\n",
    "        with open(os.path.join(self.output_dir, terms_filename), 'rb') as f:\n",
    "            self.term_id_map = pkl.load(f)\n",
    "    def load_docs_dict(self):\n",
    "        ## Load doc<->docID mapping\n",
    "        with open(os.path.join(self.output_dir, docs_filename), 'rb') as f:\n",
    "            self.doc_id_map = pkl.load(f)\n",
    "\n",
    "    def load_next_metadata_entry(self, f):\n",
    "        key_data = None\n",
    "        key_data = f.read(4)\n",
    "        if key_data:\n",
    "            key = struct.unpack('i', key_data)[0]\n",
    "            offset, length, size_in_bytes = struct.unpack('iii', f.read(12))\n",
    "            return key, offset, length, size_in_bytes\n",
    "        return None,None,None,None\n",
    "        \n",
    "    def load_postings_metadata(self):\n",
    "        \"\"\" Load postings metadata from file \"\"\"\n",
    "        filename = postings_metadata_filename\n",
    "        assert not self.postings_metadata\n",
    "        ### Begin your code\n",
    "        # Crear la ruta completa al archivo de metadatos\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        # Abrir el archivo en modo binario de lectura\n",
    "        with open(filepath, 'rb') as f:\n",
    "            # Leer todas las entradas hasta el final del archivo\n",
    "            while True:\n",
    "                # Leer una entrada usando load_next_metadata_entry\n",
    "                term_id, offset, length, size_in_bytes = self.load_next_metadata_entry(f)\n",
    "                \n",
    "                # Si no hay más entradas (key es None), salir del bucle\n",
    "                if term_id is None:\n",
    "                    break\n",
    "                \n",
    "                # Guardar los metadatos: termID -> (offset, length, size_in_bytes)\n",
    "                self.postings_metadata[term_id] = (offset, length, size_in_bytes)\n",
    "        \n",
    "        ### End your code\n",
    "           \n",
    "    def load(self):\n",
    "        \"\"\"Loads index from output directory\"\"\"\n",
    "        assert not self.postings_metadata\n",
    "        ### Begin your code        \n",
    "        # 1. Cargar el mapeo término <-> termID\n",
    "        self.load_terms_dict()\n",
    "        \n",
    "        # 2. Cargar el mapeo documento <-> docID\n",
    "        self.load_docs_dict()\n",
    "        \n",
    "        # 3. Cargar los metadatos para acceder a los postings en disco\n",
    "        self.load_postings_metadata()\n",
    "        \n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación de información a partir del índice en disco\n",
    "\n",
    "Debes programar los siguientes métodos de la clase `NaiveIndex`, necesarios para leer de disco el diccionario y poder realizar recuperación de información a partir del índice invertido:\n",
    "\n",
    "- `load_posting_at_file_offset`: Debes desplazarte al `offset` del fichero `f` que se pasa como parámetro y a partir de ahí leer `length` enteros de 4 bytes. Usa `struct.unpack` para extraer la lista de los enteros leída.\n",
    "\n",
    "- `get_term_postings_from_file`: Debe asumir que los `postings_metadata` están ubicados en memoria, y a partir del mapeo de términos a *termId*, debe acceder al desplazamiento adecuado del fichero del índice y leer la *posting list* del término, haciendo uso de la función `load_next_posting` anterior.\n",
    "\n",
    "- `retrieve`: Es la función central de recuperación de información mediante el índice. Debe comprobar si los *postings* están ubicados en memoria (atributo `postings`) o en disco, en cuyo caso usará `get_term_postings_from_file` para leer únicamente de disco los *postings* del término consultado. En caso de que el índice esté íntegramente cargado en memoria, evitará acceder a disco. Adicionalmente, recibirá un argumento `sanity_check`, en función del cual el método comprobará que `get_term_postings_from_file` devuelve lo mismo que `get_term_postings_from_mem`, siempre que tanto `postings` como `postings_metadata` estén adecuadamente construidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveIndex(NaiveIndex):\n",
    "\n",
    "    def load_posting_at_file_offset(self, f, offset, length):\n",
    "        assert length > 0\n",
    "        postings_list = None\n",
    "        ### Begin your code\n",
    "        # Desplazarse al offset especificado en el archivo\n",
    "        f.seek(offset)\n",
    "        \n",
    "        # Leer 'length' enteros de 4 bytes (length * 4 bytes en total)\n",
    "        num_bytes = length * 4\n",
    "        data = f.read(num_bytes)\n",
    "        \n",
    "        # Desempaquetar los datos como una lista de enteros\n",
    "        # Formato: '{length}i' donde length es el número de enteros\n",
    "        postings_list = list(struct.unpack(f'{length}i', data))\n",
    "        \n",
    "        ### End your code\n",
    "        return postings_list\n",
    "\n",
    "    def get_term_postings_from_file(self, term):\n",
    "        disk_postings = []\n",
    "        assert self.postings_metadata, \"Must read dictionary from disk before retrieval!\"\n",
    "        ### Begin your code   \n",
    "        # Verificar si el término existe en el mapeo de términos\n",
    "        if term not in self.term_id_map.str_to_id:\n",
    "            # El término no existe en el vocabulario, devolver lista vacía\n",
    "            return disk_postings\n",
    "        \n",
    "        # Obtener el termID del término\n",
    "        term_id = self.term_id_map[term]\n",
    "        \n",
    "        # Verificar si el término tiene metadatos (existe en el índice)\n",
    "        if term_id not in self.postings_metadata:\n",
    "            # El término no tiene postings, devolver lista vacía\n",
    "            return disk_postings\n",
    "        \n",
    "        # Obtener los metadatos del término\n",
    "        offset, length, size_in_bytes = self.postings_metadata[term_id]\n",
    "        \n",
    "        # Abrir el archivo de índice y leer la posting list\n",
    "        filepath = os.path.join(self.output_dir, postings_index_filename)\n",
    "        with open(filepath, 'rb') as f:\n",
    "            disk_postings = self.load_posting_at_file_offset(f, offset, length)\n",
    "        \n",
    "        ### End your code\n",
    "        return disk_postings\n",
    "\n",
    "    def retrieve(self, term, sanity_check=False):\n",
    "        postings = []\n",
    "        sanity_postings = None\n",
    "        where = None  # should be \"memory\" if the postings are in memory or \"disk\" if the postings are in disk\n",
    "        ### Begin your code        \n",
    "        # Comprobar si los postings están en memoria\n",
    "        if self.postings:\n",
    "            # Los postings están en memoria\n",
    "            where = \"memory\"\n",
    "            postings = self.get_term_postings_from_mem(term)\n",
    "            \n",
    "            # Si sanity_check está activado y hay metadatos, comparar con disco\n",
    "            if sanity_check and self.postings_metadata:\n",
    "                sanity_postings = self.get_term_postings_from_file(term)\n",
    "                assert postings == sanity_postings, \\\n",
    "                    f\"Sanity check failed: postings from memory {postings} != postings from disk {sanity_postings}\"\n",
    "                print(f\"✓ Sanity check passed for term '{term}': memory and disk postings match\")\n",
    "        \n",
    "        elif self.postings_metadata:\n",
    "            # Los postings NO están en memoria, pero hay metadatos (índice en disco)\n",
    "            where = \"disk\"\n",
    "            postings = self.get_term_postings_from_file(term)\n",
    "        \n",
    "        else:\n",
    "            # No hay ni postings en memoria ni metadatos\n",
    "            raise RuntimeError(\"Index not loaded: no postings in memory and no metadata available\")\n",
    "        \n",
    "        ### End your code\n",
    "        doc_names = [self.doc_id_map[i] for i in postings]\n",
    "        return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando subdirectorio: 0\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 2\n",
      "Indexación completada: 14 términos, 6 documentos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0/fine.txt', '0/hello.txt', '1/bye.txt', '2/fine.txt', '2/hello.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_index = NaiveIndex(toy_dir, out_dir_naive)\n",
    "toy_index.parse_all_subdirectories()\n",
    "toy_index.save()\n",
    "toy_index.retrieve(\"you\", sanity_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/toy/0/fine.txt\n",
      "data/toy/0/hello.txt\n",
      "data/toy/1/bye.txt\n",
      "data/toy/2/fine.txt\n",
      "data/toy/2/hello.txt\n"
     ]
    }
   ],
   "source": [
    "!grep -rlw \"you\" $toy_dir | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexando el corpus de documentos íntegro de forma *naïve* provoca `MemoryError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hayas realizado un conjunto suficiente de pruebas sobre el conjunto de datos de juguete, es posible indexar el corpus al completo. Sin embargo, si estás aplicando los límites en el uso de memoria indicados, la celda siguiente mostrará un fallo `MemoryError` durante la ejecución, ya que generar el diccionario de términos con sus postings lists requiere más memoria de la disponible. \n",
    "\n",
    "> Si deseas completar el indexado usando el algoritmo *naive*, debes establecerlo en la variable `build_naive_index_full` al comienzo de este *notebook* y luego ejecutarlo con `jupyter-notebook` desde otro terminal del shell en el que **no hayas impuesto mediante `ulimit` ningún límite en el uso de la memoria**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando subdirectorio: 0\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 1\n",
      "Procesando subdirectorio: 2\n",
      "Procesando subdirectorio: 2\n",
      "Procesando subdirectorio: 3\n",
      "Procesando subdirectorio: 3\n",
      "Procesando subdirectorio: 4\n",
      "Procesando subdirectorio: 4\n",
      "Procesando subdirectorio: 5\n",
      "Procesando subdirectorio: 5\n",
      "Procesando subdirectorio: 6\n",
      "Procesando subdirectorio: 6\n",
      "Procesando subdirectorio: 7\n",
      "Procesando subdirectorio: 7\n",
      "Procesando subdirectorio: 8\n",
      "Procesando subdirectorio: 8\n",
      "Procesando subdirectorio: 9\n",
      "Procesando subdirectorio: 9\n",
      "Indexación completada: 347071 términos, 98998 documentos\n",
      "Indexación completada: 347071 términos, 98998 documentos\n"
     ]
    }
   ],
   "source": [
    "if build_naive_index_full:\n",
    "    naive_index_full = NaiveIndex(corpus_dir, out_dir_naive)\n",
    "    naive_index_full.parse_all_subdirectories()\n",
    "    naive_index_full.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72996\n",
      "drwxrwxr-x 2 pyros05 pyros05     4096 oct 19 18:33 .\n",
      "drwxrwxr-x 4 pyros05 pyros05     4096 oct 19 18:25 ..\n",
      "-rw-rw-r-- 1 pyros05 pyros05  6804541 oct 19 18:39 docs.dict\n",
      "-rw-rw-r-- 1 pyros05 pyros05 55286272 oct 19 18:39 postings.index\n",
      "-rw-rw-r-- 1 pyros05 pyros05  5553136 oct 19 18:39 postings_metadata.dict\n",
      "-rw-rw-r-- 1 pyros05 pyros05  7088518 oct 19 18:39 terms.dict\n"
     ]
    }
   ],
   "source": [
    "!ls -al $out_dir_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probemos a usar el índice para recuperar todos los documentos que contienen el término \"porcupine\" (puercoespín)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_naive_index_full:\n",
    "    naive_index_full.retrieve(\"porcupine\", sanity_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/corpus/5/searchworks.stanford.edu_view_5614345\n",
      "data/corpus/6/wnt.stanford.edu_\n",
      "data/corpus/7/www.scs.stanford.edu_07wi-cs244b_notes_\n",
      "data/corpus/9/www.stanford.edu_group_dahlia_genetics_cultivars_valley_porcupine_valley_porcupine.htm\n",
      "data/corpus/9/www.stanford.edu_group_nusselab_cgi-bin_wnt_inhibitors\n",
      "data/corpus/9/www.stanford.edu_group_nusselab_cgi-bin_wnt_porcupine\n",
      "data/corpus/9/www.stanford.edu_group_nusselab_cgi-bin_wnt_smallmolecules\n",
      "data/corpus/9/www.stanford.edu_group_Urchin_language.htm\n"
     ]
    }
   ],
   "source": [
    "!grep -rlw \"porcupine\" $corpus_dir | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar índice desde disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_naive_index_full:\n",
    "    index_in_disk = NaiveIndex(corpus_dir, out_dir_naive)\n",
    "    index_in_disk.load()\n",
    "    index_in_disk.retrieve(\"porcupine\")\n",
    "    index_in_disk.retrieve(\"porcupine\", sanity_check=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
