{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IouBgb62Xu8D"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Dm5rJAt9O2r"
   },
   "source": [
    "En esta segunda práctica diseñaremos las primeras etapas para llegar a construir **funciones de ranking para ordenar los resultados de un motor de búsqueda**. Utilizaremos un _dataset_ de entrenamiento que incluye numerosas consultas, cada una con un conjunto limitado de resultados (habitualmente unos 10 por consulta). Este dataset se basa en un corpus de ~100K documentos y ~340K términos, extraídos de un motor de búsqueda comercial aplicado a la web de la Universidad de Stanford, por lo que es bastante realista. El corpus ha sido preprocesado para optimizar las necesidades de almacenamiento y cómputo del notebook. Todas estas características lo hacen idóneo para las prácticas de la asignatura.\n",
    "\n",
    "Para cada par consulta(_q_)-documento(_d_), se proporcionan varias características (_features_) que serán de utilidad para realizar este _ranking_. Se proporciona también un conjunto de entrenamiento con pares consulta-documentos en los que a cada documento se le ha asignado, manualmente, un valor de relevancia (etiquetado previamente), que será utilizado fundamentalmente para medir a posteriori la calidad de las funciones de ranking implementadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-uC7sYt9O2v"
   },
   "source": [
    "## Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQ4s4hhZ9O2x"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "import math\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX9vlymJXu8G"
   },
   "outputs": [],
   "source": [
    "!mkdir -p output # Creación de subdirectorio auxiliar para salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAiiYOII9O2y"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWKIVy3l9O2y"
   },
   "source": [
    "Los datos para esta práctica están disponibles como archivo .zip en este [enlace](http://web.stanford.edu/class/cs276/pa/pa3-data.zip). El _dataset_ está dividido en dos conjuntos separados:\n",
    "1. **Conjunto de entrenamiento** formado por 731 consultas (`pa3.(signal|rel).train`)\n",
    "2. **Conjunto de test** formado por 124 consultas (`pa3.(signal|rel).dev`)\n",
    "\n",
    "La idea será que, a la vez que ajustemos y maximicemos el rendimiento en el conjunto de entrenamiento, verifiquemos el rendimiento de los parámetros ajustados en el conjunto de desarrollo para asegurarnos de que no estamos sobreajustando el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGuUUZ289O2z",
    "outputId": "2a2a68ce-02e0-40df-f5d0-2d76ae313feb"
   },
   "outputs": [],
   "source": [
    "# Descarga del dataset:\n",
    "data_dir = 'pa3-data'\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/{}.zip'.format(data_dir)\n",
    "urllib.request.urlretrieve(data_url, '{}.zip'.format(data_dir))\n",
    "\n",
    "# Descomprimimos el archivo .zip:\n",
    "with zipfile.ZipFile('{}.zip'.format(data_dir), 'r') as zip_fh:\n",
    "    zip_fh.extractall()\n",
    "print('Datos descargados y descomprimidos en {}...\\n'.format(data_dir))\n",
    "\n",
    "# Imprimimos la estructura del directorio:\n",
    "print('Estructura del directorio:')\n",
    "print(data_dir + os.path.sep)\n",
    "for sub_dir in os.listdir(data_dir):\n",
    "    if not sub_dir.startswith('.'):\n",
    "        print('  - ' + sub_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na86I9oR_6_8"
   },
   "source": [
    "## Descripción de los archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYZRf0osABOs"
   },
   "source": [
    "### Archivos de \"señal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrrAwJ7u9O2z"
   },
   "source": [
    "- **pa3.signal.(_train_|_test_)**: Contienen el conjunto de consultas correspondiente (_train_|_test_) junto con los documentos devueltos para cada consulta individual por un determinado motor de búsqueda. La lista de documentos se encuentra convenientemente mezclada y no está en el mismo orden que el devuelto por el motor de búsqueda empleado originalmente. Cada consulta tiene siempre 10 o menos documentos de respuesta. Por ejemplo, el formato de un par consulta-documento $(q,d)$ es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hphP485Z9O2z",
    "outputId": "03b07431-38b6-4fd0-a289-a44cfe7ee88f"
   },
   "outputs": [],
   "source": [
    "# Mostramos un conjunto de líneas ilustrativo del archivo de training (la primera\n",
    "# consulta (\"query:\"), con sus primeros dos documentos (\"url:\") completos (líneas 0 a 29),\n",
    "# y otro par consulta-documentos adicional más adelante en el fichero (líneas 233\n",
    "# en adelante):\n",
    "filename = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines[0:29]:\n",
    "        print(l, end=\"\")\n",
    "    print(\"    ...\\n\")\n",
    "    for l in lines[233:256]:\n",
    "        print(l, end=\"\")\n",
    "    print(\"    ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8zvHeCHBekC"
   },
   "source": [
    "Atendiendo a la salida anterior se ejemplifica perfectamente la estructura de estos archivos de señal. El patrón se repite para cada URL, hasta que todas las URL (siempre 10 o menos) de la correspondiente consulta estén completas. A continuación este patrón general se repite para cada consulta posterior. Hay solo un campo `title`, otro `pagerank`, y otro `body_length` para cada URL, pero puede haber múltiples campos `header`, `body_hits` y `anchor_text` (junto con los correspondientes `stanford_anchor_count`). He aquí el significado de los mencionados campos:\n",
    "\n",
    "* El campo `title` es el título de la página.\n",
    "\n",
    "* El campo `pagerank` es un número entero de 0 a 9 que indica una calidad independiente de la consulta de la página (cuanto mayor sea dicho valor, mejor será la calidad de la página).\n",
    "\n",
    "* El campo `body_length` indica cuántos términos están presentes en el cuerpo del documento.\n",
    "\n",
    "* Cada campo `header` indica un subtítulo contenido en la página.\n",
    "\n",
    "* Cada campo `body_hits` especifica, para cada término de la consulta $q$, la _posting list_ posicional para ese término en el documento (posiciones de la palabra en el mismo, siempre ordenadas en orden creciente).\n",
    "\n",
    "* Cada campo `anchor_text`, siempre seguido inmediatamente por un subcampo `stanford_anchor_count` correspondiente, indica el texto de un enlace desde cualquier página del dominio de páginas web en las que está basado el dataset ([https://www.stanford.edu/](https://www.stanford.edu/)) al documento actual, junto con el número total de enlaces con dicho texto en el dataset. Por ejemplo, si el texto de anclaje (`anchor_text`) es _\"Stanford math department\"_ y el recuento (`stanford_anchor_count`) es 9, eso significa que hay nueve enlaces a la página actual (desde otras páginas en https://www.stanford.edu/) donde el texto de anclaje es exactamente _\"Stanford math department\"_. Así, en el ejemplo anterior del segundo documento de la segunda consulta, podemos ver que el anclaje _\"Cardinal nights\"_ aparece en 208 páginas del dominio https://www.stanford.edu/ apuntando al documento https://alcohol.stanford.edu/cardinal-nights en cuestión.\n",
    "\n",
    "El campo `pagerank` será empleado más adelante como ejemplo de _feature_ no textual, mientras que el resto de campos se corresponden con distintas _zonas_ del documento, que actuarán por tanto como _features_ textuales diferenciadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6E8pO2SXu8I"
   },
   "source": [
    "### Archivos de relevancias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-0o6MoT9O20"
   },
   "source": [
    "* **pa3.rel.(train|dev)**: Estos archivos contienen una lista de juicios de relevancia (etiquetados manualmente) para cada par consulta-documento $(q,d)$ en los respectivos archivos de señal (_train|test_). El valor de relevancia es siempre un entero entre -1 y 3, con un dato mayor significando que el documento tiene más relevancia. −1 significaría que el documento ha sido simplemente ignorado. De nuevo, el patrón se repite para cada consulta, hasta que se alcanza el final del archivo.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pn18Fbqu9O20",
    "outputId": "55800626-04c5-48dd-ceba-c1d9370f4924"
   },
   "outputs": [],
   "source": [
    "# Mostramos las primeras filas del archivo:\n",
    "filename = os.path.join(data_dir, \"pa3.rel.train\")\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines[0:25]:\n",
    "        print(l.strip())\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM233rYX9O20"
   },
   "source": [
    "Finalmente, las funciones de _ranking_ requieren también ciertos estadísticos a nivel de colección de documento (tales como la frecuencia inversa de documento, o _idf_), que no están contenidos en los anteriores archivos. Para eso proporcionamos los archivos **docs.dict**, **terms.dict** y **BSBI.dict**, con los que pueden calcularse los correspondientes valores de _idf_ necesarios. En este caso, se trata simplemente de archivos binarios (`pickle` de python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzkT9RNMY_Mz",
    "outputId": "4d4318c6-e0c3-4f73-e22b-0d8f2569f863"
   },
   "outputs": [],
   "source": [
    "! file pa3-data/*.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnyrxvj9Xu8I"
   },
   "source": [
    "## _Parsing_ de los archivos del _dataset_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcwvID98Xu8I"
   },
   "source": [
    "En primer lugar, definimos las clases `Query` y `Document` y la función `load_train_data`, necesarias para hacer el _parsing_ de los datos textuales contenidos en los ficheros de entrada:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri10Y90iZjGM"
   },
   "source": [
    "### Clase _Query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSSGCuMmXu8J"
   },
   "outputs": [],
   "source": [
    "class Query:\n",
    "    \"\"\"Clase utilizada para almacenar una consulta.\"\"\"\n",
    "    def __init__(self, query):\n",
    "        self.query_words = query.split(\" \")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for w in self.query_words:\n",
    "            yield w\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Query):\n",
    "            return False\n",
    "        return self.query_words == other.query_words\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" \".join(self.query_words)\n",
    "\n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83D_-GlvZrDd"
   },
   "source": [
    "### Clase _Document_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65I0G1HFXu8J"
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"Clase utilizada para almacenar información útil para un documento.\"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url        # Cadena\n",
    "        self.title = None     # Cadena\n",
    "        self.headers = None   # [Lista de cadenas]\n",
    "        self.body_hits = None # Diccionario: Término->[Lista de posiciones]\n",
    "        self.body_length = 0  # Entero\n",
    "        self.pagerank = 0     # Entero\n",
    "        self.anchors = None   # Diccionario: Cadena->[Conteo total de ocurrencias (anchor_counts)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for u in self.url:\n",
    "            yield u\n",
    "\n",
    "    def __str__(self):\n",
    "        result = [];\n",
    "        NEW_LINE = \"\\n\"\n",
    "        result.append(\"url: \"+ self.url + NEW_LINE);\n",
    "        if (self.title is not None): result.append(\"title: \" + self.title + NEW_LINE);\n",
    "        if (self.headers is not None): result.append(\"headers: \" + str(self.headers) + NEW_LINE);\n",
    "        if (self.body_hits is not None): result.append(\"body_hits: \" + str(self.body_hits) + NEW_LINE);\n",
    "        if (self.body_length != 0): result.append(\"body_length: \" + str(self.body_length) + NEW_LINE);\n",
    "        if (self.pagerank != 0): result.append(\"pagerank: \" + str(self.pagerank) + NEW_LINE);\n",
    "        if (self.anchors is not None): result.append(\"anchors: \" + str(self.anchors) + NEW_LINE);\n",
    "        return \" \".join(result)\n",
    "\n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoF9vbf0bonp"
   },
   "source": [
    "### Función _load_train_data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNgosNnFbusz"
   },
   "source": [
    "La siguiente función realiza el _parsing_ de toda la información textual relevante contenida en un archivo de _training | test_ dado, ya en una estructura python (`query_dict`) fácilmente accesible programáticamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1W0eE3UXu8J"
   },
   "outputs": [],
   "source": [
    "def load_train_data(feature_file_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        feature_file_name: Camino al fichero con las features de entrada.\n",
    "\n",
    "    Returns:\n",
    "       query_dict: Diccionario de tipo \"Consulta -> (URL -> Documento)\"\". Por ejemplo:\n",
    "        {computer science master: {'http://cs.stanford.edu/people/eroberts/mscsed/Admissions-MSInCSEducation.html':\n",
    "          {title: ms in computer science education stanford computer science\n",
    "           headers: [\"master's degree in computer science education\"]\n",
    "           body_hits: {'computer': [15], 'science': [16]}\n",
    "           body_length: 741\n",
    "           anchors: {'computer science': 2},\n",
    "          'http://scpd.stanford.edu/online-engineering-courses.jsp': title: online engineering courses stanford university\n",
    "           headers: ['computer science and information technology']\n",
    "           body_hits: {'science': [136], 'master': [188], 'computer': [223]}\n",
    "           body_length: 687,\n",
    "           }\n",
    "         ...,\n",
    "         }\n",
    "    \"\"\"\n",
    "    line = None\n",
    "    url = None\n",
    "    anchor_text = None\n",
    "    query = None\n",
    "    query_dict = {}\n",
    "    try:\n",
    "        with open(feature_file_name, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                token_index = line.index(\":\")\n",
    "                key = line[:token_index].strip()\n",
    "                value = line[token_index + 1:].strip()\n",
    "                if key == \"query\":\n",
    "                    # Nueva consulta:\n",
    "                    query = Query(value)\n",
    "                    query_dict[query] = {}\n",
    "                elif key == \"url\":\n",
    "                    # Nuevo documento encontrado para la actual consulta:\n",
    "                    url = value;\n",
    "                    query_dict[query][url] = Document(url);\n",
    "                elif key == \"title\":\n",
    "                    query_dict[query][url].title = str(value);\n",
    "                elif key == \"header\":\n",
    "                    if query_dict[query][url].headers is None:\n",
    "                        query_dict[query][url].headers = []\n",
    "                    query_dict[query][url].headers.append(value)\n",
    "                elif key == \"body_hits\":\n",
    "                    if query_dict[query][url].body_hits is None:\n",
    "                        query_dict[query][url].body_hits = {}\n",
    "                    temp = value.split(\" \",maxsplit=1);\n",
    "                    term = temp[0].strip();\n",
    "                    if term not in query_dict[query][url].body_hits:\n",
    "                        positions_int = []\n",
    "                        query_dict[query][url].body_hits[term] = positions_int\n",
    "                    else:\n",
    "                        positions_int = query_dict[query][url].body_hits[term]\n",
    "                    positions = temp[1].strip().split(\" \")\n",
    "                    for position in positions:\n",
    "                        positions_int.append(int(position))\n",
    "                elif key == \"body_length\":\n",
    "                    query_dict[query][url].body_length = int(value);\n",
    "                elif key == \"pagerank\":\n",
    "                    query_dict[query][url].pagerank = int(value);\n",
    "                elif key == \"anchor_text\":\n",
    "                    anchor_text = value\n",
    "                    if query_dict[query][url].anchors is None:\n",
    "                        query_dict[query][url].anchors = {}\n",
    "                elif key == \"stanford_anchor_count\":\n",
    "                    query_dict[query][url].anchors[anchor_text] = int(value)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fichero {feature_file_name} no encontrado!\")\n",
    "\n",
    "    return query_dict\n",
    "\n",
    "# Cargamos archivo completo de training:\n",
    "file_name = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "query_dict = load_train_data(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV6V5TPOXu8J"
   },
   "source": [
    "Probamos ahora el mapeo generado (`query_dict`) para unas cuantas entradas del fichero `\"pa3.signal.train\"`. Obsérvese que éste es de tipo diccionario python _Consulta->(URL->Documento)_, y por tanto posibles usos válidos de este mapeo serían, por ejemplo, los siguientes:\n",
    "\n",
    "* Acceso a todos los documentos resultado de una consulta:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`query_dict[Query(\"stanford aoerc pool hours\")]`\n",
    "\n",
    "* Acceso a un documento:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`query_dict[Query(\"stanford aoerc pool hours\")]['http://events.stanford.edu/2014/February/18/']`\n",
    "\n",
    "* Acceso a un determinado campo de un documento:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`query_dict[Query(\"stanford aoerc pool hours\")]['http://events.stanford.edu/2014/February/18/'].body_hits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkM0LKRq9O22",
    "outputId": "8411414b-11b3-42b6-ba68-67285d087e6a"
   },
   "outputs": [],
   "source": [
    "# Acceder a todos los documentos (dados por sus URLs) de una consulta:\n",
    "query_text = \"stanford aoerc pool hours\"\n",
    "query = query_dict[Query(query_text)]\n",
    "print(f'Documentos (URLs) para la consulta \"{query_text}\":')\n",
    "for url in query.keys():\n",
    "    print(f\"  {url}\")\n",
    "\n",
    "# Acceder a un documento completo, dado por su URL, dentro de la consulta:\n",
    "query_text = \"stanford aoerc pool hours\"\n",
    "url_text = \"http://events.stanford.edu/2014/February/18/\"\n",
    "print(f'\\nDocumento \"{url_text}\" correspondiente a la consulta \"{query_text}\":')\n",
    "print(query_dict[Query(query_text)][url_text])\n",
    "\n",
    "# Acceder sólo a un campo específico dentro de un documento:\n",
    "print(f'Campo \"body_hits\" para documento \"{url_text} correspondiente a la consulta \"{query_text}:\"')\n",
    "print(query_dict[Query(query_text)][url_text].body_hits)\n",
    "\n",
    "# Acceder a otros campos de otro documento, correspondiente a otra consulta diferente:\n",
    "query_text = \"alumni association benefits\"\n",
    "url_text = \"http://alumni.stanford.edu/get/page/membership/benefits/creditcard\"\n",
    "sample_doc = query_dict[Query(query_text)][url_text]\n",
    "print(f\"\\nVarios campos individuales del par consulta-documento {query_text}-{url_text}:\\b\")\n",
    "print(\"  url:\", sample_doc.url)\n",
    "print(\"  headers:\", sample_doc.headers)\n",
    "print(\"  body_hits:\",sample_doc.body_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1Zk0N9FXu8K"
   },
   "source": [
    "## Construcción del diccionario IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIWdmjzN9O22"
   },
   "source": [
    "Construiremos ahora la clase que nos permitirá computar los IDF de los términos a partir de los correspondientes archivos <b>.dict</b> que usaremos también como entrada (similares a los utilizados en el _notebook_ de la práctica 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utKNTJOWaX0F"
   },
   "source": [
    "### Clase _IdMap_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6VYzQqMaWsR"
   },
   "source": [
    "Comenzamos creando la clase IdMap, para mapear cadenas (_tokens_) a sus correspondientes identificadores numéricos (_tokenIDs_) y viceversa. Valdrá tanto para documentos (identificados por su URL) como para términos (cadenas con un simple token):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrNnTHZIXu8K"
   },
   "outputs": [],
   "source": [
    "class IdMap:\n",
    "    \"\"\"Clase auxiliar para almacenar mapeos entre strings e identificadores numéricos de tokens.\"\"\"\n",
    "# Copia aquí el código de tu clase IdMap   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLf_NOKfaQz_"
   },
   "source": [
    "### Lectura de diccionarios (_docs_, _terms_, _postings\\_dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5fGSAAwaMZ1"
   },
   "source": [
    "Cargamos ahora los archivos `docs.dict`, `terms.dict` y `BSBI.dict` creados en la práctica 1, e imprimimos sus respectivas longitudes y los primeros registros para recordar sus respectivas estructuras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMa_aqLj9O23",
    "outputId": "e7c242a8-5f5a-41f0-d8bf-98bc169c758e"
   },
   "outputs": [],
   "source": [
    "# Leemos los tres archivos e imprimimos sus longitudes:\n",
    "with open(\"pa3-data/terms.dict\", 'rb') as f:\n",
    "    terms = pkl.load(f)\n",
    "with open(\"pa3-data/docs.dict\", 'rb') as f:\n",
    "    docs = pkl.load(f)\n",
    "with open('pa3-data/BSBI.dict', 'rb') as f:\n",
    "    postings_dict, termsID = pkl.load(f)\n",
    "print(f\"Leídos {len(docs)} documentos y {len(terms)} términos\")\n",
    "\n",
    "# Chequeamos la consistencia de longitudes:\n",
    "assert len(terms) == len(termsID) == len(postings_dict), \\\n",
    "       \"Inconsistencia en longitudes de terms, termsID y/o postings_dict\"\n",
    "\n",
    "# Imprimimos los primeros diez valores de los idMaps docs y terms, y también de postings_dict y termsId:\n",
    "print(\"----------- terms: -----------\")\n",
    "print(\"10 primeros términos:         \", terms.id_to_str[:10])\n",
    "print(\"10 primeros mappings term->id:\", list(zip(list(terms.str_to_id.keys()),terms.str_to_id.values()))[:10])\n",
    "print(\"\\n--------- docs: ------------\")\n",
    "print(\"10 primeros documentos:       \", docs.id_to_str[:10])\n",
    "print(\"10 primeros mappings doc->id: \", list(zip(list(docs.str_to_id.keys()),docs.str_to_id.values()))[:10])\n",
    "print(\"\\n------ postings_dict: ------\")\n",
    "print(\"10 primeros postings_dict:    \", list(zip(list(postings_dict.keys()),postings_dict.values()))[:10])\n",
    "print(\"10 primeros termsID:          \", termsID[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2Eqmc1Mexm9"
   },
   "source": [
    "### Clase _Idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QheGaxs-l7I8"
   },
   "source": [
    "A continuación, la clase `Idf` que, a partir de los diccionarios anteriores, calcula la frecuencia (absoluta) de cada término, y el correspondiente valor IDF pesado logarítmicamente, como es habitual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5i2iM7JG9O23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Idf:\n",
    "    \"\"\"Construye un diccionario para poder devolver el IDF de un término (tanto si el término consultado está\n",
    "       como si no está en el diccionario construido).\n",
    "       Recuérdese de la práctica 1 que el diccionario \"postings_dict\" mapea cada termID a una tupla\n",
    "       (posicion_de_comienzo_en_fichero_indice, numero_de_postings_en_la_lista, longitud_en_bytes_de_la_lista).\n",
    "       Dado que queremos protegernos del caso posible de consulta de un término que no aparezca en la colección,\n",
    "       aplicaremos el suavizado de Laplace típico (suma de +1 una unidad tanto en el numerador como en el\n",
    "       denominador de la proporción, evitando así la posible división por cero).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Construcción del diccionario IDF\"\"\"\n",
    "        try:\n",
    "            with open(\"pa3-data/docs.dict\", 'rb') as f:\n",
    "                docs = pkl.load(f)\n",
    "            self.total_doc_num = len(docs)\n",
    "            print(\"Número total de documentos de la colección:\", self.total_doc_num)\n",
    "\n",
    "            with open(\"pa3-data/terms.dict\", 'rb') as f:\n",
    "                terms = pkl.load(f)\n",
    "            self.total_term_num = len(terms)\n",
    "            print(\"Número total de términos:\", self.total_term_num)\n",
    "\n",
    "            with open('pa3-data/BSBI.dict', 'rb') as f:\n",
    "                postings_dict, termsID = pkl.load(f)\n",
    "\n",
    "            self.idf = {}\n",
    "            self.raw = {}\n",
    "            ### BEGIN YOUR CODE (FIXME)\n",
    " \n",
    "            ### END YOUR CODE (FIXME)\n",
    "        except FileNotFoundError:\n",
    "            print(\"¡Ficheros de diccionario de documentos / términos / índice no encontrados!\")\n",
    "\n",
    "    def get_raw(self, term):\n",
    "        \"\"\"Devuelve el conteo crudo de ocurrencias de documentos conteniendo un término,\n",
    "           dado, tanto si está como si no en el diccionario.\n",
    "        Args:\n",
    "            term(str) : Término del que se va a devolver su conteo crudo.\n",
    "        Return(float):\n",
    "            Idf del término.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "       \n",
    "        ### END YOUR CODE (FIXME)\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        \"\"\"Devuelve el IDF de un término, tanto si está como si no en el diccionario.\n",
    "        Args:\n",
    "            term(str) : Término del que se va a devolver su IDF.\n",
    "        Return(float):\n",
    "            Idf del término.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "       \n",
    "            \n",
    "        ### END YOUR CODE (FIXME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUHpupNQXu8K"
   },
   "source": [
    "Creamos ahora una instancia de esta clase `Idf`, y probamos a consultar en ella la IDF de unos cuantos términos, tanto existentes como inexistentes. Obsérvese que, a mayor frecuencia (términos más comunes en el corpus) de un término, menor peso IDF, y viceversa (términos menos comunes tienen un mayor peso IDF). Del mismo modo, un término inexistente tendrá un IDF muy cercano a 5.0 (ya que la cantidad total de documentos es aproximadamente 100K=$10^5$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5Rx57FnXu8K",
    "outputId": "1243ce8d-6f28-418f-dfda-7bf6ccb1a347",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theIDF = Idf()\n",
    "print()\n",
    "print(f'IDF(\"the\") =                  {theIDF.get_idf(\"the\"):5.3f}  (raw count={theIDF.get_raw(\"the\")})')\n",
    "print(f'IDF(\"and\") =                  {theIDF.get_idf(\"and\"):5.3f}  (raw count={theIDF.get_raw(\"and\")})')\n",
    "print(f'IDF(\"stanford\") =             {theIDF.get_idf(\"stanford\"):5.3f}  (raw count={theIDF.get_raw(\"stanford\")})')\n",
    "print(f'IDF(\"university\") =           {theIDF.get_idf(\"university\"):5.3f}  (raw count={theIDF.get_raw(\"university\")})')\n",
    "print(f'IDF(\"quantitative\") =         {theIDF.get_idf(\"quantitative\"):5.3f}  (raw count={theIDF.get_raw(\"quantitative\")})')\n",
    "print(f'IDF(\"supercalifragilistic\") = {theIDF.get_idf(\"supercalifragilistic\"):5.3f}  (raw count={theIDF.get_raw(\"supercalifragilistic\")})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNyiwZARXu8K"
   },
   "source": [
    "Lo siguiente son unos cuantos tests adicionales, que aseguran la corrección de la implementación de la clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CorTGDy29O23",
    "outputId": "4fc04aca-7ff0-48b4-d002-648aeaec99d2"
   },
   "outputs": [],
   "source": [
    "assert len(theIDF.idf) == 347071, 'Longitud incorrecta de diccionario idf.'\n",
    "assert theIDF.get_idf(\"bilibalabulu\") > 4.9, \\\n",
    "       \"Término no localizado no manejado correctamente\"\n",
    "assert theIDF.get_idf(\"data\") < theIDF.get_idf(\"radiology\"), \\\n",
    "       \"El idf de los términos más raros debe ser mayor que el de términos más comunes.\"\n",
    "assert theIDF.get_idf(\"to\") < theIDF.get_idf(\"design\"), \\\n",
    "       \"El idf de los términos más raros debe ser mayor que el de términos más comunes.\"\n",
    "print(\"Tests de clase Idf() superados.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
