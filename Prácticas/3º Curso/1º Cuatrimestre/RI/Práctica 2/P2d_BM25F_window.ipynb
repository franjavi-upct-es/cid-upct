{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDMGUZM79O27"
   },
   "source": [
    "# Función de _ranking_ 2: BM25F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpVGsCr0CB1R"
   },
   "source": [
    "Usaremos aquí la frecuencia de términos normalizada por campos ($ftf$, de _Field dependent normalized Term Frequency_). Así, para un término $t$ dado, y un campo $f \\in \\{url, header, body, title, anchor\\}$ en el documento $d$, usaremos:\n",
    "\n",
    "\\begin{equation}\n",
    "ftf_{d,f,t} = \\frac{tf_{d,f,t}}{1 + B_f((\\text{len}_{d,f} / \\text{avlen}_f) - 1)}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $tf_{d,f,t}$ es la frecuencia cruda de $t$ en el campo $f$ del documento $d$, $len_{d,f}$ es la longitud del campo $f$ en $d$, y $avlen_f$ es la longitud media del mismo campo en toda la colección.\n",
    "\n",
    "Por supuesto, las correspondientes variables $avlen_{body}$, $avlen_{url}$, $avlen_{title}$, $avlen_{header}$ y $avlen_{anchor}$ se deberán computar usando de nuevo el conjunto de _training_. Los valores de $B_f$ serán parámetros adicionales dependientes de cada uno de los campos $f$, y al igual que los $c_f$ de la sección anterior, deberán ser ajustados. Si $avlen_f$ fuese cero, entonces definiremos $ftf_{d,f,t} = 0$ (si bien esto no debería ser necesario en este _dataset_ en concreto).\n",
    "\n",
    "El peso global para el término $t$ en el documento $d$, usando ya todos los campos, sería:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{d,t} = \\sum_{f} W_f \\cdot ftf_{d,f,t}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Siendo, de nuevo, los distintos $W_f$ parámetros que determinan los pesos de importancia relativos dados a cada uno de los campos.\n",
    "\n",
    "Puesto que, además, tenemos también una característica adicional no textual (el <b>pagerank</b>), la incorporaremos también en nuestra función de _ranking_ usando el método descrito en las transparencias de teoría.\n",
    "\n",
    "En concreto, pues, y resumiendo, el _scoring_ global para el $d$ respecto a la consulta $q$ quedará definido como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{t \\in q} \\frac{w_{d,t}}{K_1 + w_{d,t}}idf_t + \\lambda V_{j}(f)\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "De nuevo aquí $K_1$ es un parámetro libre, y la función $V_{j}$ podría ser cualquier de las funciones logarítmicas, de saturación o sigmoide mencionadas en las transparencias de teoría, y que en este caso fijaremos simplemente como $V_{pagerank}(pr) = log(\\lambda'+pr)$.\n",
    "\n",
    "$\\lambda$ y $\\lambda'$ son los dos últimos parámetros libres adicionales para este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ves5WrKbIZg_"
   },
   "source": [
    "## Clase _BM25FScorer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kobpdseOIekj"
   },
   "source": [
    "Definimos aquí la clase `BM25FScorer`, basada en la `CosineSimilarityScorer` anterior, pero incorporando en este caso todas las modificaciones necesarias para implementar la nueva funcionalidad descrita en los párrafos anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohony5it9O28"
   },
   "outputs": [],
   "source": [
    "class BM25FScorer(CosineSimilarityScorer):\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None):\n",
    "        super().__init__(idf, query_dict, params, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme)\n",
    "\n",
    "        # Añadimos aquí los pesos ya específicos para BM25, y los nuevos parámetros libres...\n",
    "        self.b_url = params['b_url']\n",
    "        self.b_title = params['b_title']\n",
    "        self.b_header = params['b_header']\n",
    "        self.b_body_hits = params['b_body_hits']\n",
    "        self.b_anchor = params['b_anchor']\n",
    "        self.k1 = params['k1']\n",
    "        self.pagerank_lambda = params['pagerank_lambda']\n",
    "        self.pagerank_lambda_prime = params['pagerank_lambda_prime']\n",
    "\n",
    "        # ... y aqui tres estructuras de datos adicionales, necesarias para la implementación\n",
    "        # (relativas al cálculo de longitudes totales de cada documento, longitudes medias\n",
    "        # para cada campo, y scorings previos de cada documento por su pagerank):\n",
    "        self.lengths = {} # Document -> field -> length\n",
    "        self.avg_length = {} # field -> length average\n",
    "        self.pagerank_scores = {}\n",
    "\n",
    "        # Cálculo inicial de las longitudes medias por campo (ver definición de método\n",
    "        # calc_avg_length() justo a continuación):\n",
    "        self.calc_avg_length()\n",
    "\n",
    "    def calc_avg_length(self, debug=False):\n",
    "        \"\"\" Computa las longitudes medias de cada campo en la colección.\n",
    "            En el proceso rellena también el diccionario self.lengths\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        self.avg_length = {\"title\": avg_len_title, \"headers\": avg_len_headers,\n",
    "                           \"anchors\": avg_len_anchors, \"url\": avg_len_url,\n",
    "                           \"body_hits\": avg_len_body_hits}\n",
    "\n",
    "    \n",
    "    def normalize_doc_vec(self, q, d, doc_vec, debug=False):\n",
    "        \"\"\" Normalizar las frecuencias crudas de los diferentes campos en el documento\n",
    "            d usando la ecuación (1) especificada más arriba.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            doc_vec (dict) : El vector de documento a normalizar.\n",
    "        Return:\n",
    "            doc_vec (dict) : El vector de documento normalizado.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "\n",
    "\n",
    "        ### END YOUR CODE (FIXME)\n",
    "\n",
    "        return doc_vec\n",
    "\n",
    "    def get_net_vector(self, q, d):\n",
    "        \"\"\" Obtener el vector neto global para el par (q,d), usando la ecuación (2) anterior.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "        Return:\n",
    "            doc_vec (dict) : El vector de documento normalizado (ya sólo uno, incluyendo todos los términos incluídos en todos los campos).\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "\n",
    "    \n",
    "        ### END YOUR CODE (FIXME)\n",
    "\n",
    "    def get_net_score(self, q, d):\n",
    "        \"\"\" Obtener la puntuación global BM25F para el par (q,d), usando la ecuación (3) anterior.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "        Return:\n",
    "            doc_vec (dict) : El scoring neto global, incluyendo ya también la puntuación por pagerank.\n",
    "        \"\"\"\n",
    "        q_vec = self.get_query_vector(q)\n",
    "        n_vec = self.get_net_vector(q, d)\n",
    "        score = 0\n",
    "        for term in n_vec.keys():\n",
    "            if term in q_vec.keys():\n",
    "                score += (n_vec[term]/(self.k1+n_vec[term])) * self.idf.get_idf(term)\n",
    "        score += self.pagerank_lambda * math.log(self.pagerank_lambda_prime + d.pagerank)\n",
    "        # print(\"PAGERANK:\", d.pagerank)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFqNinMtgd99"
   },
   "source": [
    "Probamos la nueva clase _BM25FScorer_, primero con unos ciertos parámetros iniciales en los que hemos fijado $b_f=0 \\quad \\forall f$, así como $k_1=\\lambda = \\lambda'=0$, simplemente para depurar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okUhImHagb-r",
    "outputId": "cb9ad2d4-c2f0-45af-deb2-3f2cb5b8a771"
   },
   "outputs": [],
   "source": [
    "# Imprimimos la consulta y el documento de ejemplo:\n",
    "q = Query(\"stanford aoerc pool hours\")\n",
    "d = query_dict[q]['http://events.stanford.edu/2014/February/18/']\n",
    "print(\"CONSULTA:\", q,\"\\n\")\n",
    "print(\"DOCUMENTO:\", d)\n",
    "\n",
    "# Usamos consulta booleana, sin normalizar, e incluyendo en ella el IDF...\n",
    "query_weight_scheme = {\"tf\": 'b', \"df\": 't', \"norm\": None}\n",
    "# ... y con conteo de frecuencias crudas iniciales para el documento, normalizados por\n",
    "# zonas de acuerdo a la ecuación (1):\n",
    "doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": \"default\"}\n",
    "\n",
    "# Creamos el scorer BM25F con los anteriores parámetros, e inicialmente con los respectivos b_f\n",
    "# inicializados a 0.0 para comprobar la corrección de los cálculos en los vectores separados\n",
    "# por campos:\n",
    "params_bm25f = {\n",
    "    \"url_weight\" : 0.1,\n",
    "    \"title_weight\": 0.15,\n",
    "    \"body_hits_weight\" : 0.2,\n",
    "    \"header_weight\" : 0.25,\n",
    "    \"anchor_weight\" : 0.30,\n",
    "    \"b_url\" : 0.0,\n",
    "    \"b_title\" : 0.0,\n",
    "    \"b_header\" : 0.0,\n",
    "    \"b_body_hits\" : 0.0,\n",
    "    \"b_anchor\" : 0.0,\n",
    "    \"k1\": 0.0,\n",
    "    \"pagerank_lambda\" : 0.0,\n",
    "    \"pagerank_lambda_prime\" : 0.0,\n",
    "}\n",
    "bm25f_scorer = BM25FScorer(theIDF, query_dict, params_bm25f, query_weight_scheme, doc_weight_scheme)\n",
    "\n",
    "print('\\nVector de consulta:', bm25f_scorer.get_query_vector(q))\n",
    "print('\\nVector de documento:', bm25f_scorer.get_doc_vector(q, d))\n",
    "print('\\nVector neto:', bm25f_scorer.get_net_vector(q, d))\n",
    "print('\\nScoring neto:', bm25f_scorer.get_net_score(q, d))\n",
    "\n",
    "assert bm25f_scorer.get_net_score(q, d)  == sum([theIDF.get_idf(term) if term in q else 0 for term, val in bm25f_scorer.get_net_vector(q, d).items()]), \\\n",
    "       \"Fallo en chequeo de la clase BM25FScorer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxUCHJMmkXDC"
   },
   "source": [
    "Si observamos la información impresa por la celda anterior, y prestamos atención al vector de documento impreso (separado por campos), comprobamos que los vectores resultantes coinciden con los de conteo originales (como corresponde a los valores $b_f=0 \\quad \\forall f$ usados para depurar en primera instancia).\n",
    "\n",
    "Obsérvese también que el vector neto combina ya todos los términos en un sólo vector, según los pesos indicados en los parámetros usados (p.e., para el término _\"events\"_, que aparece con un valor de 1.0 tanto en el campo **url** como en el campo **title**, el valor es de 0.25, como corresponde a la suma $0.1*1.0+0.15*1.0 = 0.25$, con $w_u=0.1$ y $w_t=0.15$. El término _\"aoerc\"_, por su parte, aparece con un valor 1.4, correspondiente en este caso a la suma $0.2*7.0 = 1.4$, al aparecer únicamente en el campo **body_hits**, con $w_b=0.2$ y un conteo de apariciones de exactamente 7.0 en dicho campo.\n",
    "\n",
    "Finalmente, puede comprobarse también que el _scoring_ neto obtenido coincide en este caso con la simple suma de los IDF de todos los términos incluidos en la consulta, como debe ser el caso al aplicar la ecuación (3) con $k_1=\\lambda = \\lambda'=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1Ynm4FIrs9K"
   },
   "source": [
    "Un ejercicio interesante es cambiar ahora los parámetros libres con otros valores con más sentido, para observar sus respectivas influencias en los resultados. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnZRqlSQuG8k",
    "outputId": "6a77380a-5bbf-4b86-f1fa-957d14bc5f0d"
   },
   "outputs": [],
   "source": [
    "params_bm25f = {\n",
    "    \"url_weight\" : 0.1,\n",
    "    \"title_weight\": 0.1,\n",
    "    \"body_hits_weight\" : 0.1,\n",
    "    \"header_weight\" : 0.1,\n",
    "    \"anchor_weight\" : 0.1,\n",
    "    \"b_url\" : 0.5,\n",
    "    \"b_title\" : 0.5,\n",
    "    \"b_header\" : 0.5,\n",
    "    \"b_body_hits\" : 0.5,\n",
    "    \"b_anchor\" : 0.5,\n",
    "    \"k1\": 1.0,\n",
    "    \"pagerank_lambda\" : 1.0,\n",
    "    \"pagerank_lambda_prime\" : 2.0,\n",
    "}\n",
    "bm25f_scorer = BM25FScorer(theIDF, query_dict, params_bm25f, query_weight_scheme, doc_weight_scheme)\n",
    "\n",
    "print('\\nVector de consulta:', bm25f_scorer.get_query_vector(q))\n",
    "print('\\nVector de documento:', bm25f_scorer.get_doc_vector(q, d))\n",
    "print('\\nVector neto:', bm25f_scorer.get_net_vector(q, d))\n",
    "print('\\nScoring neto:', bm25f_scorer.get_net_score(q, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida sería la siguiente:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vector de consulta: Counter({'aoerc': 4.995630807762446, 'pool': 2.446627545736658, 'hours': 1.2882309766291968, 'stanford': 0.14313251558629017})\n",
    "\n",
    "Vector de documento: {'url': Counter({'events': 1.1888553231609935, 'stanford': 1.1888553231609935, 'edu': 1.1888553231609935, '2014': 1.1888553231609935, 'february': 1.1888553231609935, '18': 1.1888553231609935}), 'title': Counter({'events': 0.9730647184709617, 'at': 0.9730647184709617, 'stanford': 0.9730647184709617, 'tuesday': 0.9730647184709617, 'february': 0.9730647184709617, '18': 0.9730647184709617, '2014': 0.9730647184709617}), 'headers': Counter({'stanford': 4.865323592354809, 'university': 0.9730647184709617, 'event': 0.9730647184709617, 'calendar': 0.9730647184709617, 'teaching': 0.9730647184709617, 'sex': 0.9730647184709617, 'at': 0.9730647184709617, 'rodin': 0.9730647184709617, 'the': 0.9730647184709617, 'complete': 0.9730647184709617, 'collection': 0.9730647184709617, 'rec': 0.9730647184709617, 'trx': 0.9730647184709617, 'suspension': 0.9730647184709617, 'training': 0.9730647184709617, 'memorial': 0.9730647184709617, 'church': 0.9730647184709617, 'open': 0.9730647184709617, 'visiting': 0.9730647184709617, 'hours': 0.9730647184709617, 'alternative': 0.9730647184709617, 'transportation': 0.9730647184709617, 'counseling': 0.9730647184709617, 'tm': 0.9730647184709617, '3': 0.9730647184709617, 'hour': 0.9730647184709617, 'univ': 0.9730647184709617, 'shc': 0.9730647184709617, 'employees': 0.9730647184709617, 'retirees': 0.9730647184709617, 'family': 0.9730647184709617, 'members': 0.9730647184709617}), 'anchors': Counter(), 'body_hits': Counter({'stanford': 14.09836268453252, 'aoerc': 9.868853879172764, 'pool': 1.409836268453252})}\n",
    "\n",
    "Vector neto: {'events': 0.21619200416319553, 'stanford': 2.1125606318519283, 'edu': 0.11888553231609936, '2014': 0.21619200416319553, 'february': 0.21619200416319553, '18': 0.21619200416319553, 'at': 0.19461294369419235, 'tuesday': 0.09730647184709618, 'university': 0.09730647184709618, 'event': 0.09730647184709618, 'calendar': 0.09730647184709618, 'teaching': 0.09730647184709618, 'sex': 0.09730647184709618, 'rodin': 0.09730647184709618, 'the': 0.09730647184709618, 'complete': 0.09730647184709618, 'collection': 0.09730647184709618, 'rec': 0.09730647184709618, 'trx': 0.09730647184709618, 'suspension': 0.09730647184709618, 'training': 0.09730647184709618, 'memorial': 0.09730647184709618, 'church': 0.09730647184709618, 'open': 0.09730647184709618, 'visiting': 0.09730647184709618, 'hours': 0.09730647184709618, 'alternative': 0.09730647184709618, 'transportation': 0.09730647184709618, 'counseling': 0.09730647184709618, 'tm': 0.09730647184709618, '3': 0.09730647184709618, 'hour': 0.09730647184709618, 'univ': 0.09730647184709618, 'shc': 0.09730647184709618, 'employees': 0.09730647184709618, 'retirees': 0.09730647184709618, 'family': 0.09730647184709618, 'members': 0.09730647184709618, 'aoerc': 0.9868853879172765, 'pool': 0.1409836268453252}\n",
    "\n",
    "Scoring neto: 4.093638107806798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXsizwpZtt5D"
   },
   "source": [
    "Es interesante reevaluar varias veces la celda anterior jugando con pequeños cambios aislados en los diferentes parámetros libres, e interpretar de esta forma su efecto inmediato tanto en los vectores de documento separados por campos como en el vector neto resultado, y el correspondiente scoring neto final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_83SObY9O28"
   },
   "source": [
    "# Función de ranking 3: ventana más pequeña"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Pt8fJcW9O28"
   },
   "source": [
    "Finalmente, añadiremos la influencia de los tamaños de ventana al algoritmo BM25F. Para una consulta deda $q$, definimos la _ventana más pequeña_ $w_{q,d}$ como la secuencia más corta de tokens en el documento $d$ tal que todos los términos en la consulta $q$ están presentes en dicha secuencia. Una ventana sólo puede especificarse para un campo en particular, y para el caso concreto del campo _anchor\\_text_, se exige que todos los términos en $q$ estén presentes en un enlace particular (esto es, si un término ocurre en el texto de un enlace, y otro en el de otro enlace diferente al mismo documento), no se considerará una misma ventana. Si, por otro lado, $d$ no contiene alguno de los términos de la consulta y, por tanto, no se puede encontrar dicha ventana, entonces definimos $w_{q,d} = \\infty$.\n",
    "\n",
    "Intuitivamente, cuanto más pequeña sea la ventana $w_{q,d}$, más relevante debería ser el documento $d$ para la consulta $q$. Por lo tanto, multiplicaremos el _scoring_ BM25F del documento por un factor de _boost_ basado en $w_{q,d}$, de forma que:\n",
    "\n",
    "* Si $w_{q,d} = \\infty$, entonces el factor de _boost_ es 1.0.\n",
    "* Si $w_{q,d} = |Q|$, siendo $Q$ el número de términos únicos en $q$, entonces multiplicaremos el _scoring_ original por un factor predeterminado máximo $B$, estrictamente mayor que uno.\n",
    "* Para valores de $w_{q,d}$ entre la longitud de la consulta e infinito, el factor de _boost_ deberá moverse entre B (valor máximo) y 1.0 (valor mínimo), decrementándose progresivamente conforme crece el tamaño de $w_{q,d}$.\n",
    "\n",
    "Para esto último, podría aquí usarse un decrecimiento de tipo exponencial, o bien del tipo $\\frac{1}{x}$. La siguiente gráfica ilustra una posible implementación de este último tipo de decrecimiento, para 4 valores diferentes de B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "xn27upzqXu8S",
    "outputId": "2d7afe47-48d6-4694-9940-edd6e70dd7f4"
   },
   "outputs": [],
   "source": [
    "# Ilustramos cuatro funciones de decrecimiento basado en 1/x para los valores\n",
    "# máximos de B = {4.0, 2.0, 1.75, 1.25}:\n",
    "len_q_list = 10.0\n",
    "max_win_len = 50\n",
    "len_min = np.arange(len_q_list,max_win_len+1)\n",
    "for B in [4.0, 2.0, 1.75, 1.25]:\n",
    "    factor_win = 1.0+(B-1.0)*len_q_list/len_min\n",
    "    plt.plot(len_min,factor_win,'+-',label=str(B));\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3HS9zzKyZi"
   },
   "source": [
    "##  Clase _WindowScorer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMC9H_YRK3A6"
   },
   "source": [
    "He aquí la definición de una clase _SmallestWindowScorer_ para implementar la técnica anterior. Se basa en la clase anterior _BM25Scorer_, ampliándola fundamentalmente con el método `get_boost_score`, que realiza el trabajo principal apoyándose a su vez en el método `min_sublist_with_all`. Éste último es el que en última instancia realiza la búsqueda efectiva de la ventana de texto más pequeña que contiene a todos los términos de la consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK1elX969O29"
   },
   "outputs": [],
   "source": [
    "class SmallestWindowScorer(BM25FScorer):\n",
    "    def __init__(self, idf, query_dict, params, query_weight_scheme=None, doc_weight_scheme=None):\n",
    "        super().__init__(idf, query_dict, params, query_weight_scheme=query_weight_scheme, doc_weight_scheme=doc_weight_scheme) #modified\n",
    "        # Añadimos el parámetro \"B\" (máximo boosting alcanzable):\n",
    "        self.B = params[\"B\"]\n",
    "\n",
    "    \n",
    "    def min_sublist_with_all(self, A, B):\n",
    "        \"\"\" Calcula el tamaño de la mínima sublista de B que contiene a toda la\n",
    "            lista de términos de A.\n",
    "        Args:\n",
    "            A (lista de términos) : Lista de términos en la consulta.\n",
    "            B (lista de términos) : Lista de términos en la que realizar la\n",
    "                                    búsqueda de la sublista más pequeña.\n",
    "        Return:\n",
    "            min_length (dict) : La longitud de la sublista más pequeña encontrada\n",
    "                                (float('inf') si no existe tal sublista).\n",
    "        \"\"\"\n",
    "        # BEGIN YOUR CODE\n",
    "        \n",
    "\n",
    "        ### END YOUR CODE\n",
    "        # Devolvemos la longitud de la mínima sublista encontrada:\n",
    "        return min_length\n",
    "    \n",
    "\n",
    "    def get_boost_score(self, q, d, debug=False):\n",
    "        \"\"\" Calcula el factor de boost basado en la técnica 'smallest window'.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            debug (bool) : Flag para imprimir posible información de depuración.\n",
    "        Return:\n",
    "            factor_win (float) : Factor de boost, entre 1 y B.\n",
    "        \"\"\"\n",
    "\n",
    "        # Lista de términos de la consulta de entrada:\n",
    "        q_list = str(q).split()\n",
    "\n",
    "        # Extraemos todas las listas de términos del documento, separadas\n",
    "        # y procesadas debidamente según campos:\n",
    "        all_lists = []\n",
    "        try: # Campo url:\n",
    "            d_url_list, _ = self.parse_url(d.url)\n",
    "        except:\n",
    "            d_url_list = []\n",
    "        try: # Campo title:\n",
    "            d_title_list = d.title.split()\n",
    "        except:\n",
    "            d_title_list = []\n",
    "        try: # Campo headers:\n",
    "            d_headers_lists = [h.split() for h in d.headers]\n",
    "        except:\n",
    "            d_headers_lists = []\n",
    "        try: # Campo anchors:\n",
    "            d_anchors_lists = [a.split() for a in d.anchors]\n",
    "        except:\n",
    "            d_anchors_lists = []\n",
    "        try: # Campo body:\n",
    "            # Construimos lista ficticia de términos a partir de body_hits,\n",
    "            # rellenando con \"-\" donde no se conoce el término:\n",
    "            max_pos = -1\n",
    "            for k in d.body_hits.keys():\n",
    "                max_pos_k = max(d.body_hits[k])\n",
    "                if max_pos_k > max_pos:\n",
    "                    max_pos = max_pos_k\n",
    "            d_body_hits_list = (max_pos+1)*[\"-\"]\n",
    "            for k in d.body_hits.keys():\n",
    "                for pos in d.body_hits[k]:\n",
    "                    d_body_hits_list[pos] = k\n",
    "        except:\n",
    "            d_body_hits_list = []\n",
    "\n",
    "        # Lista de todas las listas de términos a procesar para este documento:\n",
    "        all_lists += [d_url_list] + [d_title_list] + d_headers_lists + d_anchors_lists + [d_body_hits_list]\n",
    "        if debug:\n",
    "            print(f\"\\nq_list: {q_list}\")\n",
    "            print(\"\\nall_lists:\")\n",
    "            for l in all_lists:\n",
    "                print(f\" {l}\")\n",
    "\n",
    "        # Cómputo de la mínima ventana para todas las listas de todos los campos\n",
    "        # (tamaño definitivo de la mínima ventana para este documento):\n",
    "        if debug:\n",
    "            print(f\"\\n Ternas (q_list, lista, min_dist):\")\n",
    "        len_min = float(\"inf\")\n",
    "        for i,lt in enumerate(all_lists):\n",
    "            min_dist = self.min_sublist_with_all(q_list,lt)\n",
    "            if debug:\n",
    "               print(f\"{q_list}     {lt}     {min_dist}\")\n",
    "            if min_dist < len_min:\n",
    "                len_min = min_dist\n",
    "        factor_win = 1.0+(self.B-1)*len(q_list)/len_min if len_min != float('inf') else 1.0\n",
    "        return factor_win\n",
    "\n",
    "\n",
    "    def get_net_score(self, q, d):\n",
    "        \"\"\" Obtener el scoring neto para un par consulta - documento utilizando\n",
    "            un factor de boosting computado usando la similaridad por la técnica\n",
    "            del mínimo tamaño de ventana.\n",
    "        Args:\n",
    "            d (Document) : El documento.\n",
    "            q (Query) : La consulta.\n",
    "\n",
    "        Return:\n",
    "            El scoring crudo multiplicado por el factor de boost.\n",
    "        \"\"\"\n",
    "        boost = self.get_boost_score(q, d)\n",
    "        raw_score = super().get_net_score(q, d)\n",
    "        return boost * raw_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-UXO3U5r3i8"
   },
   "source": [
    "Probamos en la celda siguiente la clase anterior. Definimos unos parámetros por defecto para la clase (incluyendo un $B$ máximo de 2.0), elegimos una consulta _q_ y un documento _d_ de prueba, y calculamos un factor de _boost_ en modo `debug=True`, para comprobar la corrección de los cómputos intermedios para calcularlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aimjMixfr1sy",
    "outputId": "bdff9c06-4e4a-4742-f8b0-27bc1aa3de5e"
   },
   "outputs": [],
   "source": [
    "# Parámetros para la creación de la clase:\n",
    "params_window = {\n",
    "    \"B\": 2.0,\n",
    "    \"url_weight\" : 1.0,\n",
    "    \"title_weight\": 0.1,\n",
    "    \"body_hits_weight\" : 0.25,\n",
    "    \"header_weight\" : 0.5,\n",
    "    \"anchor_weight\" : 0.3,\n",
    "    \"b_url\" : 0.0,\n",
    "    \"b_title\" : 0.0,\n",
    "    \"b_header\" : 0.0,\n",
    "    \"b_body_hits\" : 0.0,\n",
    "    \"b_anchor\" : 0.0,\n",
    "    \"k1\": 2.0,\n",
    "    \"pagerank_lambda\" : 0.1,\n",
    "    \"pagerank_lambda_prime\" : 1.0,\n",
    "}\n",
    "\n",
    "# Consulta y documento de prueba:\n",
    "q = Query(\"stanford parking\")\n",
    "d = query_dict[q][\"https://transportation.stanford.edu/\"]\n",
    "\n",
    "# Definición de instancia de la clase:\n",
    "smallest_window_scorer = SmallestWindowScorer(theIDF, query_dict, params_window)\n",
    "\n",
    "# Prueba de funcionamiento interno del método get_boost_score(...):\n",
    "smallest_window_scorer.get_boost_score(q, d, debug=True)\n",
    "\n",
    "# Prueba del método get_net_score(...) que calcula el scoring neto:\n",
    "print(f\"\\nScoring neto tras usar el factor de boost: {smallest_window_scorer.get_net_score(q, d):5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrDAZMc59O29"
   },
   "source": [
    "# Ranking del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYtFPDmw540V"
   },
   "source": [
    "## Clase _Rank_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot7f6TYW9O29"
   },
   "source": [
    "Definimos una sencilla clase conteniendo sólo métodos de clase, que agrupa varias funciones de utilidad en la construcción de _rankings_ de documentos resultado de la búsqueda para una determinada consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pxirYsET9O29",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Rank:\n",
    "    # Sólo métodos de clase:\n",
    "    def score(query_dict, score_type, idf, params):\n",
    "        \"\"\" Llamar a esta función para puntuar (y ordenar según esta puntuación)\n",
    "            todos los documentos correspondientes a una consulta, en un conjunto\n",
    "            completo (dado en forma de mapping consultas -> {documentos}).\n",
    "        Args:\n",
    "            query_dict (dict) :  Mapeo Query->url->Document.\n",
    "            score_type (str) : Tipo de scorer a usar (\"baseline\", \"cosine\", \"bm25f\", \"window\").\n",
    "            idf (dict) : Diccionario IDF.\n",
    "            params(dict) : Parámetros para el scorer usado.\n",
    "        Return\n",
    "            query_rankings (dict) : Un mapeo Query->Document->(r,s) (r=ranking=entero, comenzando en 1; s=score=float).\n",
    "        \"\"\"\n",
    "        # Seleccionar subclase concreta de AbstractScorer para crear el tipo de instancia\n",
    "        # concreta de scorer a utilizar:\n",
    "        if score_type == \"baseline\": scorer = BaselineScorer(idf)\n",
    "        elif score_type == \"cosine\": scorer = CosineSimilarityScorer(idf, query_dict, params)\n",
    "        elif score_type == \"bm25f\": scorer = BM25FScorer(idf, query_dict, params)\n",
    "        elif score_type == \"window\": scorer = SmallestWindowScorer(idf, query_dict, params)\n",
    "        else: print('Tipo erróneo de scorer (debe ser \"baseline\", \"cosine\", \"bm25f\" o \"window\")!')\n",
    "\n",
    "        # Diccionario donde se almacenará el mapping consultas->rankings devuelto:\n",
    "        query_rankings = {} # query -> rank\n",
    "        # Bucle que recorre todas las consultas en el diccionario de entrada:\n",
    "        for i, query in enumerate(query_dict.keys()):\n",
    "            q = query\n",
    "            \n",
    "            ### BEGIN YOUR CODE (FIXME)\n",
    "            # Bucle que recorre todos los urls para cada consulta, obteniendo el score correspondiente:\n",
    "           \n",
    "            # Ordenamos los documentos por sus scorings...\n",
    "            \n",
    "            # ... y asignamos el mapeo Document->(ranking,score) resultante de todos los documentos\n",
    "            # a la consulta correspondiente en el mapeo de salida Query->Document->ranking:\n",
    "            \n",
    "            ### END YOUR CODE (FIXME)\n",
    "\n",
    "        return query_rankings\n",
    "\n",
    "    def write_ranking_to_file(query_rankings, ranked_result_filename):\n",
    "        \"\"\" Función que exporta los rankings obtenidos sobre un dataset de\n",
    "           consultas-documentos a un fichero de texto.\n",
    "        Args:\n",
    "            query_rankings (dict) : Un mapeo Query->Document->ranking (ranking=entero, comenzando en 1).\n",
    "            ranked_result_filename (str): Ruta al archivo de salida.\n",
    "        \"\"\"\n",
    "        with open(ranked_result_filename, \"w\") as f:\n",
    "            for query, docs in query_rankings.items():\n",
    "                f.write(\"query: \"+ query.__str__() + \"\\n\")\n",
    "                for doc, rank in docs.items():\n",
    "                    output_info = \"  url: \" + doc.url + \"\\n\" + \\\n",
    "                                  \"    title: \" + doc.title + \"\\n\" + \\\n",
    "                                  \"    rank:  \" + str(rank[0]) + \"\\n\" + \\\n",
    "                                  \"    score: \" + str(rank[1]) + \"\\n\"\n",
    "                    f.write(output_info)\n",
    "        print(f\"¡Escritura de archivo {ranked_result_filename} realizada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMnzsLUxt5j6"
   },
   "source": [
    "## Generación de archivos de rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgKwyTVCt0qf"
   },
   "source": [
    "Usando la clase `Rank` anterior, realizamos las ordenaciones de todas las consultas contenidas en `query_dict` (provenientes del archivo de _training_ inicial). Realizamos cuatro _rankings_ usando las cuatro técnicas desarrolladas (_\"baseline\"_, _\"cosine\"_, _\"bm25f\"_, _\"window\"_), guardando cada una de ellas en el correspondiente archivo `output/ranked_train_{tecnica}.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9-xkU-Jtxxu",
    "outputId": "305ec9e3-a5e7-4a0e-8a93-3e596cb5f8b1"
   },
   "outputs": [],
   "source": [
    "for method, params in zip([\"baseline\", \"cosine\", \"bm25f\", \"window\"], [None, params_cosine, params_bm25f, params_window]):\n",
    "    query_dict = load_train_data(os.path.join(data_dir, \"pa3.signal.train\"))\n",
    "    query_rankings = Rank.score(query_dict, method, theIDF, params)\n",
    "    Rank.write_ranking_to_file(query_rankings, os.path.join(\"output\", \"ranked_train_\"+method+\".txt\"))\n",
    "    print(f\"Rankings realizados para {len(query_rankings)} consultas, (usando el {method} scorer)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hpfqu9AkQgg"
   },
   "source": [
    "A título de ejemplo de los resultados obtenidos, a continuación mostramos los _rankings_ realizados por los cuatro métodos para los diez documentos obtenidos para la primera consulta del _dataset_ de _training_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGE4qRsI2yJ6",
    "outputId": "04b68174-198e-421f-f1f4-2ce73dc10526"
   },
   "outputs": [],
   "source": [
    "!echo RANKING 1ª CONSULTA, BASELINE:\n",
    "!head -41 output/ranked_train_baseline.txt\n",
    "!echo\n",
    "\n",
    "!echo RANKING 1ª CONSULTA, COSINE:\n",
    "!head -41 output/ranked_train_cosine.txt\n",
    "!echo\n",
    "\n",
    "!echo RANKING 1ª CONSULTA, BM25F:\n",
    "!head -41 output/ranked_train_bm25f.txt\n",
    "!echo\n",
    "\n",
    "!echo RANKING 1ª CONSULTA, WINDOW:\n",
    "!head -41 output/ranked_train_window.txt\n",
    "!echo\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
