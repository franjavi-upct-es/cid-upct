{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5npwAXYBXu8L"
   },
   "source": [
    "# Vectores de conteo de términos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sat8Hl159O21"
   },
   "source": [
    "Cada par consulta-documento $(q,d)$ de los archivos de entrada proporciona información relativa a los diferentes términos de cada consulta en cinco campos diferentes de cada documento, a saber: <b>url</b>, <b>title</b>, <b>headers</b>, <b>body</b> y <b>anchors</b> (el campo <b>pagerank</b> adicional no se utilizará hasta más adelante).\n",
    "\n",
    "Las funciones de _ranking_ construirán inicialmente los correspondientes cinco vectores de conteo de términos crudos ($rs$, de _raw score_) para cada uno de estos pares $(q,d)$, precisamente a partir de las coincidencias (_hits_) en estos cinco diferentes campos. Estos vectores $rs$ simplemente contarán cuantas veces ocurre cada término de búsqueda en un determinado campo. Para el campo <b>anchor</b>, se asumirá la simplificación de que hay un gran documento que contiene todos los enlaces, con el texto del enlace multiplicado por el campo <b>stanford_anchor_count</b>. Seguiremos un enfoque análogo también para el campo <b>header</b>.\n",
    "    \n",
    "Así, para el par $(q,d)$ de ejemplo donde la consulta era $q=[\\text{stanford aoerc pool hours}]^T$ y el documento $d=$\"http://events.stanford.edu/2014/February/18/\" (mostrado en el ejemplo de una celda anterior de este mismo notebook), el vector ${rs}_{b}$ correspondiente al campo <b>body</b> será $[{10 \\ 7 \\ 1 \\ 0}]^T$, dado que había 10 apariciones del término \"stanford\" en dicho campo, 7 para el término \"aoerc\", 1 para el término \"pool\" y ninguna para el término \"hours\". Análogamente, el vector ${rs}_{a}$ para el campo <b>anchor</b> será simplemente $[\\text{0 0 0 0}]^T$, dado que no hay ningún enlace (<b>anchor</b>) para este documento. Finalmente el vector ${rs}_{t}$ para el campo <b>title</b> será $[\\text{1 0 0 0}]^T$, $[\\text{1 0 0 0}]^T$ también para el vector ${rs}_{u}$ del campo <b>url</b> (estos dos fácilmente deducibles desde los correspondientes campos `title` y `url`, que son únicos en cada documento), y ${rs}_{h}$ $[\\text{5 0 0 1}]^T$ para el campo <b>header</b> (este último acumulado para las distintas cabeceras --esto es, \"subtítulos\"-- presentes en el documento). Todo esto se puede corroborar fácilmente observando el contenido de dicho documento en el archivo de señal original:\n",
    "    \n",
    "```   \n",
    "query: stanford aoerc pool hours\n",
    " url: http://events.stanford.edu/2014/February/18/\n",
    "  title: events at stanford tuesday february 18 2014\n",
    "  header: stanford university event calendar\n",
    "  header: teaching sex at stanford\n",
    "  header: rodin the complete stanford collection\n",
    "  header: stanford rec trx suspension training\n",
    "  header: memorial church open visiting hours\n",
    "  header: alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members\n",
    "  body_hits: stanford 239 271 318 457 615 642 663 960 966 971\n",
    "  body_hits: aoerc 349 401 432 530 549 578 596\n",
    "  body_hits: pool 521\n",
    "  body_length: 981\n",
    "  pagerank: 1    \n",
    "```\n",
    "    \n",
    "Un ejemplo adicional, usando la misma consulta, pero un documento de respuesta diferente, para ilustrar los vectores correspondientes a los campos <b>anchor</b>, no presentes en el documento anterior:\n",
    "```\n",
    "  url: https://cardinalrec.stanford.edu/facilities/aoerc/\n",
    "    ...\n",
    "    anchor_text: gyms aoerc\n",
    "      stanford_anchor_count: 3\n",
    "    anchor_text: aoerc\n",
    "      stanford_anchor_count: 13\n",
    "    anchor_text: http cardinalrec stanford edu facilities aoerc\n",
    "      stanford_anchor_count: 4\n",
    "    anchor_text: arrillaga outdoor education and recreation center aoerc link is external\n",
    "      stanford_anchor_count: 1\n",
    "    anchor_text: the arrillaga outdoor education and research center aoerc\n",
    "      stanford_anchor_count: 2\n",
    "    anchor_text: aoerc will shutdown for maintenance\n",
    "      stanford_anchor_count: 2\n",
    "```\n",
    "\n",
    "Aquí, el vector para  <b>anchor</b> será $[\\text{4 25 0 0}]^T$, dado que sólo hay un total de 4 enlaces (campo <b>stanford_anchor_count</b>) para el término \"stanford\", pero 25 (=3+13+4+1+2+2) para el término “aoerc”.\n",
    "\n",
    "(Nótese que, en lo referente al campo  <b>url</b> es necesario _\"tokenizar\"_ previamente sólo los caracteres alfanuméricos. Nótese también que, al calcular los conteos de términos crudos, todo se hace convirtiendo previamente los _tokens_ a minúsculas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KctGIAoX9O24"
   },
   "source": [
    "# Clase base para todos los _scorers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpiKjGh4dVxU"
   },
   "source": [
    "## Clase _AbstractScorer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ouo50wcPXu8L"
   },
   "source": [
    "Construimos ahora una clase base abstracta, de la que habrá que ir reimplementando métodos en las clases hijas, conforme vayamos implementando los diferentes métodos de _scoring_. En todo caso, esta clase AbstractScorer recogerá una funcionalidad básica común a todos. Lo fundamental serán los distintos métodos `parse_`_field_, que transforman la información textual disponible en el archivo de señal para cada campo en la correspondiente información numérica. Aprovechamos también aquí para añadir los posibles esquemas de _weighting_ SMART vistos en teoría (si bien en esta práctica no programaremos todas las posibilidades disponibles, sino sólo alguna de las más comúnmente utilizadas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Número total de documentos de la colección: 98998\n",
      "Número total de términos: 347071\n",
      "\n",
      "Datos cargados: 731 consultas\n"
     ]
    }
   ],
   "source": [
    "# Imports necesarios\n",
    "import sys\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "import math\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Definiciones de clases del notebook anterior (P2a)\n",
    "class Query:\n",
    "    \"\"\"Clase utilizada para almacenar una consulta.\"\"\"\n",
    "    def __init__(self, query):\n",
    "        self.query_words = query.split(\" \")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for w in self.query_words:\n",
    "            yield w\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Query):\n",
    "            return False\n",
    "        return self.query_words == other.query_words\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" \".join(self.query_words)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "class Document:\n",
    "    \"\"\"Clase utilizada para almacenar información útil para un documento.\"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url        # Cadena\n",
    "        self.title = None     # Cadena\n",
    "        self.headers = None   # [Lista de cadenas]\n",
    "        self.body_hits = None # Diccionario: Término->[Lista de posiciones]\n",
    "        self.body_length = 0  # Entero\n",
    "        self.pagerank = 0     # Entero\n",
    "        self.anchors = None   # Diccionario: Cadena->[Conteo total de ocurrencias (anchor_counts)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for u in self.url:\n",
    "            yield u\n",
    "\n",
    "    def __str__(self):\n",
    "        result = [];\n",
    "        NEW_LINE = \"\\n\"\n",
    "        result.append(\"url: \"+ self.url + NEW_LINE);\n",
    "        if (self.title is not None): result.append(\"title: \" + self.title + NEW_LINE);\n",
    "        if (self.headers is not None): result.append(\"headers: \" + str(self.headers) + NEW_LINE);\n",
    "        if (self.body_hits is not None): result.append(\"body_hits: \" + str(self.body_hits) + NEW_LINE);\n",
    "        if (self.body_length != 0): result.append(\"body_length: \" + str(self.body_length) + NEW_LINE);\n",
    "        if (self.pagerank != 0): result.append(\"pagerank: \" + str(self.pagerank) + NEW_LINE);\n",
    "        if (self.anchors is not None): result.append(\"anchors: \" + str(self.anchors) + NEW_LINE);\n",
    "        return \" \".join(result)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "class Idf:\n",
    "    \"\"\"Construye un diccionario para poder devolver el IDF de un término.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Construcción del diccionario IDF\"\"\"\n",
    "        try:\n",
    "            with open(\"pa3-data/docs.dict\", 'rb') as f:\n",
    "                docs = pkl.load(f)\n",
    "            self.total_doc_num = len(docs)\n",
    "            print(\"Número total de documentos de la colección:\", self.total_doc_num)\n",
    "\n",
    "            with open(\"pa3-data/terms.dict\", 'rb') as f:\n",
    "                terms = pkl.load(f)\n",
    "            self.total_term_num = len(terms)\n",
    "            print(\"Número total de términos:\", self.total_term_num)\n",
    "\n",
    "            with open('pa3-data/BSBI.dict', 'rb') as f:\n",
    "                postings_dict, termsID = pkl.load(f)\n",
    "\n",
    "            self.idf = {}\n",
    "            self.raw = {}\n",
    "            for term_id, term_str in enumerate(terms.id_to_str):\n",
    "                if term_id in postings_dict:\n",
    "                    num_docs_with_term = postings_dict[term_id][1]\n",
    "                    self.raw[term_str] = num_docs_with_term\n",
    "                    self.idf[term_str] = math.log((self.total_doc_num + 1) / (num_docs_with_term + 1))\n",
    "        except FileNotFoundError:\n",
    "            print(\"¡Ficheros de diccionario de documentos / términos / índice no encontrados!\")\n",
    "\n",
    "    def get_raw(self, term):\n",
    "        return self.raw.get(term, 0)\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        if term in self.idf:\n",
    "            return self.idf[term]\n",
    "        else:\n",
    "            return math.log(self.total_doc_num + 1)\n",
    "\n",
    "def load_train_data(feature_file_name):\n",
    "    \"\"\"Carga los datos de entrenamiento.\"\"\"\n",
    "    line = None\n",
    "    url = None\n",
    "    anchor_text = None\n",
    "    query = None\n",
    "    query_dict = {}\n",
    "    try:\n",
    "        with open(feature_file_name, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                token_index = line.index(\":\")\n",
    "                key = line[:token_index].strip()\n",
    "                value = line[token_index + 1:].strip()\n",
    "                if key == \"query\":\n",
    "                    query = Query(value)\n",
    "                    query_dict[query] = {}\n",
    "                elif key == \"url\":\n",
    "                    url = value;\n",
    "                    query_dict[query][url] = Document(url);\n",
    "                elif key == \"title\":\n",
    "                    query_dict[query][url].title = str(value);\n",
    "                elif key == \"header\":\n",
    "                    if query_dict[query][url].headers is None:\n",
    "                        query_dict[query][url].headers = []\n",
    "                    query_dict[query][url].headers.append(value)\n",
    "                elif key == \"body_hits\":\n",
    "                    if query_dict[query][url].body_hits is None:\n",
    "                        query_dict[query][url].body_hits = {}\n",
    "                    temp = value.split(\" \",maxsplit=1);\n",
    "                    term = temp[0].strip();\n",
    "                    if term not in query_dict[query][url].body_hits:\n",
    "                        positions_int = []\n",
    "                        query_dict[query][url].body_hits[term] = positions_int\n",
    "                    else:\n",
    "                        positions_int = query_dict[query][url].body_hits[term]\n",
    "                    positions = temp[1].strip().split(\" \")\n",
    "                    for position in positions:\n",
    "                        positions_int.append(int(position))\n",
    "                elif key == \"body_length\":\n",
    "                    query_dict[query][url].body_length = int(value);\n",
    "                elif key == \"pagerank\":\n",
    "                    query_dict[query][url].pagerank = int(value);\n",
    "                elif key == \"anchor_text\":\n",
    "                    anchor_text = value\n",
    "                    if query_dict[query][url].anchors is None:\n",
    "                        query_dict[query][url].anchors = {}\n",
    "                elif key == \"stanford_anchor_count\":\n",
    "                    query_dict[query][url].anchors[anchor_text] = int(value)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fichero {feature_file_name} no encontrado!\")\n",
    "    return query_dict\n",
    "\n",
    "class IdMap:\n",
    "    \"\"\"Clase auxiliar para almacenar mapeos entre strings e identificadores numéricos de tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor de la clase IdMap.\"\"\"\n",
    "        self.str_to_id = {}  # Diccionario: string -> id numérico\n",
    "        self.id_to_str = []  # Lista: índice = id, valor = string\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Devuelve el número de elementos en el mapeo.\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "    \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Devuelve el string correspondiente al id numérico i.\n",
    "        \n",
    "        Args:\n",
    "            i (int): Identificador numérico.\n",
    "            \n",
    "        Returns:\n",
    "            str: String correspondiente al id.\n",
    "        \"\"\"\n",
    "        return self.id_to_str[i]\n",
    "    \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Devuelve el id numérico correspondiente al string s.\n",
    "        Si el string no existe, lo añade y devuelve su nuevo id.\n",
    "        \n",
    "        Args:\n",
    "            s (str): String del que obtener su id.\n",
    "            \n",
    "        Returns:\n",
    "            int: Identificador numérico del string.\n",
    "        \"\"\"\n",
    "        if s not in self.str_to_id:\n",
    "            # Si no existe, lo añadimos\n",
    "            new_id = len(self.id_to_str)\n",
    "            self.str_to_id[s] = new_id\n",
    "            self.id_to_str.append(s)\n",
    "            return new_id\n",
    "        return self.str_to_id[s]\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"Permite acceso mediante corchetes: idmap[key]\n",
    "        Si key es int, devuelve el string.\n",
    "        Si key es str, devuelve el id.\n",
    "        \n",
    "        Args:\n",
    "            key: Puede ser int (devuelve string) o str (devuelve id).\n",
    "            \n",
    "        Returns:\n",
    "            El string o id correspondiente.\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):\n",
    "            return self._get_str(key)\n",
    "        elif isinstance(key, str):\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError(\"La clave debe ser int o str\")\n",
    "    \n",
    "    def __contains__(self, key):\n",
    "        \"\"\"Permite usar 'in' para verificar existencia.\n",
    "        \n",
    "        Args:\n",
    "            key: Puede ser int o str.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True si existe, False en caso contrario.\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):\n",
    "            return 0 <= key < len(self.id_to_str)\n",
    "        elif isinstance(key, str):\n",
    "            return key in self.str_to_id\n",
    "        return False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Representación en string del objeto.\"\"\"\n",
    "        return f\"IdMap(size={len(self)})\"\n",
    "\n",
    "# Cargar datos necesarios\n",
    "print(\"Cargando datos...\")\n",
    "data_dir = 'pa3-data'\n",
    "theIDF = Idf()\n",
    "print()\n",
    "file_name = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "query_dict = load_train_data(file_name)\n",
    "print(f\"Datos cargados: {len(query_dict)} consultas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0ARI8fuJ9O25"
   },
   "outputs": [],
   "source": [
    "class AbstractScorer:\n",
    "    \"\"\" Una clase básica abstracta para un scorer.\n",
    "        Implementa una funcionalidad básica de construcción de vectores de consulta y de documento.\n",
    "        Tendrá que ser extendida adecuadamente por cada scorer específico.\n",
    "    \"\"\"\n",
    "    def __init__(self, idf, query_weight_scheme=None, doc_weight_scheme=None):\n",
    "        self.idf = idf\n",
    "        self.TFTYPES = [\"url\", \"title\", \"body_hits\", \"header\", \"anchor\"]\n",
    "        # Esquemas por defecto:\n",
    "        self.default_query_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": None} # Esquema natural, no, none\n",
    "        self.default_doc_weight_scheme = {\"tf\": 'n', \"df\": 'n', \"norm\": None}   # Esquema natural, no, none\n",
    "        self.query_weight_scheme = query_weight_scheme if query_weight_scheme is not None \\\n",
    "                                   else self.default_query_weight_scheme\n",
    "        self.doc_weight_scheme = doc_weight_scheme if doc_weight_scheme is not None \\\n",
    "                                 else self.default_doc_weight_scheme\n",
    "\n",
    "    def parse_url(self, url):\n",
    "        \"\"\"Parsea la URL del documento, devolviendo un Counter de los tokens encontrados en la URL.\n",
    "        Args:\n",
    "            url: el url del que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            Lista de tokens del URL (una vez limpios), y Counter resultado.\n",
    "        \"\"\"\n",
    "        if url:\n",
    "            url_token_in_term = url.replace(\"http:\",\".\").replace('/','.').replace('?','.') \\\n",
    "                                   .replace('=','.').replace(\"%20\",\".\").replace(\"...\",\".\").replace(\"..\",\".\")\\\n",
    "                                   .replace('-','.').lower();\n",
    "            url_token = url_token_in_term.strip(\".\").split('.')\n",
    "            return url_token, Counter(url_token)\n",
    "        else:\n",
    "            return [], Counter([])\n",
    "\n",
    "    def parse_title(self, title):\n",
    "        \"\"\"Parsea el campo title del documento, devolviendo un Counter de los tokens encontrados en el mismo.\n",
    "        Args:\n",
    "            title: el title del que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        if title:\n",
    "            return Counter(title.split(\" \"))\n",
    "        else:\n",
    "            return Counter([])\n",
    "\n",
    "    def parse_headers(self, headers):\n",
    "        \"\"\"Parsea los campos headers del documento, devolviendo un Counter de los tokens encontrados en los mismos.\n",
    "        Args:\n",
    "            headers: la lista de headers sobre los que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        headers_token = []\n",
    "        # BEGIN YOUR CODE\n",
    "        if headers is not None:\n",
    "            for header in headers:\n",
    "                # Dividir cada header en tokens y añadirlos a la lista\n",
    "                header_tokens = header.split(\" \")\n",
    "                headers_token.extend(header_tokens)\n",
    "        # END YOUR CODE\n",
    "        return Counter(headers_token)\n",
    "\n",
    "    def parse_anchors(self, anchors):\n",
    "        \"\"\"Parsea los campos anchors del documento, devolviendo un Counter de los tokens encontrados en los mismos.\n",
    "        Args:\n",
    "            anchors: la lista de anchors sobre los que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        anchor_count_map = Counter({})\n",
    "        if anchors is not None:\n",
    "            for anchor in anchors:\n",
    "                count = anchors[anchor]\n",
    "                anchor_tokens = anchor.split(\" \")\n",
    "                for anchor_token in anchor_tokens:\n",
    "                    if(anchor_token in anchor_count_map.keys()):\n",
    "                        anchor_count_map[anchor_token] += count\n",
    "                    else:\n",
    "                        anchor_count_map[anchor_token] = count\n",
    "        return anchor_count_map\n",
    "\n",
    "    def parse_body_hits(self, body_hits):\n",
    "        \"\"\"Parsea los campos body_hits del documento, devolviendo un Counter de los tokens encontrados en los mismos.\n",
    "        Args:\n",
    "            body_hits: la lista de anchors sobre los que se va a hacer el parsing.\n",
    "        Returns:\n",
    "            El Counter resultado.\n",
    "        \"\"\"\n",
    "        body_hits_count_map = Counter({})\n",
    "        #BEGIN YOUR CODE\n",
    "        if body_hits is not None:\n",
    "            # body_hits es un diccionario: término -> lista de posiciones\n",
    "            # El conteo es simplemente la longitud de la lista de posiciones\n",
    "            for term, positions in body_hits.items():\n",
    "                body_hits_count_map[term] = len(positions)\n",
    "        #END YOUR CODE\n",
    "        return body_hits_count_map\n",
    "\n",
    "    def get_query_vector(self, q, query_weight_scheme = None):\n",
    "        \"\"\" Obtiene un vector numérico para la consulta q.\n",
    "        Args:\n",
    "            q (Query): Query(\"Una consulta determinada\")\n",
    "        Returns:\n",
    "            query_vec (dict): El vector resultado.\n",
    "        \"\"\"\n",
    "        # En subclases de esta AbstractScorer, podrían tenerse en cuenta todas las\n",
    "        # posibilidades SMART, usando diferentes esquemas de frecuencia del término (tf),\n",
    "        # frecuencia de documento (idf) y normalización. En todo caso, nótese que en\n",
    "        # general no se suele necesitar normalización para la consulta en ningún caso, ya que\n",
    "        # dicha normalización no variaría con respecto a todos los documentos resultados de una\n",
    "        # misma consulta, lo que resultaría en un simple factor de escalado común que no\n",
    "        # afectaría al posterior ranking de los mismos.\n",
    "        #\n",
    "        # if query_weight_scheme is None:\n",
    "        #     query_weight_scheme = self.query_weight_scheme\n",
    "\n",
    "        query_vec = {}\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        # En nuestro caso base, usaremos simplemente el contador básico de términos, sin\n",
    "        # normalización ni uso de idf:\n",
    "        query_vec = Counter(q.query_words)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return query_vec\n",
    "\n",
    "    def get_doc_vector(self, q, d, doc_weight_scheme=None):\n",
    "        \"\"\" Obtiene un vector numérico para el documento d.\n",
    "        Args:\n",
    "        q (Query) : Query(\"Una consulta\")\n",
    "        d (Document) : Query(\"Una consulta\")[\"Un URL\"]\n",
    "        Returns:\n",
    "        doc_vec (dict) : Un diccionario de conteo de la frecuencia de términos, con un subdiccionario para\n",
    "                         cada tipo de campo (tipo_de_campo -> (término -> conteo))\n",
    "                    Ejemplo: \"{'url':   {'stanford': 1, 'aoerc': 0, 'pool': 0, 'hours': 0},\n",
    "                               'title': {'stanford': 1, 'aoerc': 0, 'pool': 0, 'hours': 0},\n",
    "                               ...\n",
    "                               }\"\n",
    "        \"\"\"\n",
    "        # De nuevo, podrían considerarse todas las posibilidades SMART en las subclases\n",
    "        # de esta AbstractScorer, si bien en esta clase base nos contentaremos con un simple\n",
    "        # conteo crudo de los términos en los distintos campos:\n",
    "        #\n",
    "        # if doc_weight_scheme is None:\n",
    "        #    doc_weight_scheme = self.doc_weight_scheme\n",
    "\n",
    "        doc_vec = {}\n",
    "        ### BEGIN YOUR CODE (FIXME)\n",
    "        # Sólo para depurar:\n",
    "        # print(f\"URL:        {d.url}          ->   {self.parse_url(d.url)}\")\n",
    "        # print(f\"TITLE:      {d.title}        ->   {self.parse_title(d.title)}\")\n",
    "        # print(f\"HEADERS:    {d.headers}      ->   {self.parse_headers(d.headers)}\")\n",
    "        # print(f\"ANCHORS:    {d.anchors}      ->   {self.parse_anchors(d.anchors)}\")\n",
    "        # print(f\"BODY_HITS:  {d.body_hits}    ->   {self.parse_body_hits(d.body_hits)}\")\n",
    "        #\n",
    "        # Simple conteo crudo de los términos por campos:\n",
    "        _, url_counter = self.parse_url(d.url)\n",
    "        doc_vec['url'] = url_counter\n",
    "        doc_vec['title'] = self.parse_title(d.title)\n",
    "        doc_vec['headers'] = self.parse_headers(d.headers)\n",
    "        doc_vec['anchors'] = self.parse_anchors(d.anchors)\n",
    "        doc_vec['body_hits'] = self.parse_body_hits(d.body_hits)\n",
    "        ### END YOUR CODE (FIXME)\n",
    "        return doc_vec\n",
    "\n",
    "    # Métodos no implementados en la clase base; en su caso, serán reimplementados en cada scorer concreto:\n",
    "\n",
    "    def normalize_doc_vec(self, q, d, doc_vec):\n",
    "        \"\"\" Normalizar el vector de documento.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "            doc_vec (dict) : El vector de documento\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_sim_score(self, q, d):\n",
    "        \"\"\" Devuelve la puntuación para una consulta q y documento d dados.\n",
    "        Args:\n",
    "            q (Query): la consulta.\n",
    "            d (Document) : el documento.\n",
    "        Returns:\n",
    "            La puntuación para el par (q,d).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_net_score(self, q, d):\n",
    "        \"\"\" Calcular el scoring neto entre la consulta y el documento.\n",
    "        Args:\n",
    "            q (Query) : La consulta.\n",
    "            d (Document) : El documento.\n",
    "        Return:\n",
    "            score (float) : La puntuación resultado.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUaAmqMFXu8O"
   },
   "source": [
    "Probamos la clase abstracta con una consulta y un documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOEnHyJR9O24",
    "outputId": "4d9ab66e-6755-4185-dad3-7367300f6ac4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query q:  stanford aoerc pool hours\n",
      "\n",
      "Document d:  url: http://events.stanford.edu/2014/February/18/\n",
      " title: events at stanford tuesday february 18 2014\n",
      " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
      " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
      " body_length: 981\n",
      " pagerank: 1\n",
      "\n",
      "Vector consulta:\n",
      "  Counter({'stanford': 1, 'aoerc': 1, 'pool': 1, 'hours': 1})\n",
      "\n",
      "Vector documento:\n",
      "  url        -> Counter({'events': 1, 'stanford': 1, 'edu': 1, '2014': 1, 'february': 1, '18': 1})\n",
      "  title      -> Counter({'events': 1, 'at': 1, 'stanford': 1, 'tuesday': 1, 'february': 1, '18': 1, '2014': 1})\n",
      "  headers    -> Counter({'stanford': 5, 'university': 1, 'event': 1, 'calendar': 1, 'teaching': 1, 'sex': 1, 'at': 1, 'rodin': 1, 'the': 1, 'complete': 1, 'collection': 1, 'rec': 1, 'trx': 1, 'suspension': 1, 'training': 1, 'memorial': 1, 'church': 1, 'open': 1, 'visiting': 1, 'hours': 1, 'alternative': 1, 'transportation': 1, 'counseling': 1, 'tm': 1, '3': 1, 'hour': 1, 'univ': 1, 'shc': 1, 'employees': 1, 'retirees': 1, 'family': 1, 'members': 1})\n",
      "  anchors    -> Counter()\n",
      "  body_hits  -> Counter({'stanford': 10, 'aoerc': 7, 'pool': 1})\n"
     ]
    }
   ],
   "source": [
    "q = Query(\"stanford aoerc pool hours\")\n",
    "d = query_dict[q]['http://events.stanford.edu/2014/February/18/']\n",
    "print(\"Query q: \", q)\n",
    "print()\n",
    "print(\"Document d: \", d)\n",
    "\n",
    "a_scorer = AbstractScorer(theIDF)\n",
    "query_vec = a_scorer.get_query_vector(q)\n",
    "print(f\"Vector consulta:\")\n",
    "print(f\"  {query_vec}\")\n",
    "print()\n",
    "doc_vec = a_scorer.get_doc_vector(q, d)\n",
    "print(f\"Vector documento:\")\n",
    "for k, v in doc_vec.items():\n",
    "    print(f\"  {k:10s} -> {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida tendría que ser la siguiente:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Query q:  stanford aoerc pool hours\n",
    "\n",
    "Document d:  url: http://events.stanford.edu/2014/February/18/\n",
    " title: events at stanford tuesday february 18 2014\n",
    " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
    " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
    " body_length: 981\n",
    " pagerank: 1\n",
    "\n",
    "Vector consulta:\n",
    "  Counter({'stanford': 1, 'aoerc': 1, 'pool': 1, 'hours': 1})\n",
    "\n",
    "Vector documento:\n",
    "  url        -> Counter({'events': 1, 'stanford': 1, 'edu': 1, '2014': 1, 'february': 1, '18': 1})\n",
    "  title      -> Counter({'events': 1, 'at': 1, 'stanford': 1, 'tuesday': 1, 'february': 1, '18': 1, '2014': 1})\n",
    "  headers    -> Counter({'stanford': 5, 'university': 1, 'event': 1, 'calendar': 1, 'teaching': 1, 'sex': 1, 'at': 1, 'rodin': 1, 'the': 1, 'complete': 1, 'collection': 1, 'rec': 1, 'trx': 1, 'suspension': 1, 'training': 1, 'memorial': 1, 'church': 1, 'open': 1, 'visiting': 1, 'hours': 1, 'alternative': 1, 'transportation': 1, 'counseling': 1, 'tm': 1, '3': 1, 'hour': 1, 'univ': 1, 'shc': 1, 'employees': 1, 'retirees': 1, 'family': 1, 'members': 1})\n",
    "  anchors    -> Counter()\n",
    "  body_hits  -> Counter({'stanford': 10, 'aoerc': 7, 'pool': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDKy2-qXXu8P"
   },
   "source": [
    "## Clase _BaselineScorer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92wK25zfXu8P"
   },
   "source": [
    "Definimos aquí un simple _\"scorer baseline\"_, que nos servirá para probar la funcionalidad de la clase abstracta base. La clase `BaselineScorer` heredará directamente de la clase `AbstractScorer`, reimplementando solamente el método `get_sim_score`, que simplemente acumulará, para aquellos términos en la consulta, los contadores absolutos de dichos términos (TFs) en el vector de documento correspondiente, utilizando en cada caso el campo `url`, `title`, `headers`, `anchors`, o `body_hits` que se le indique en cada momento mediante el método `set_field_type(...)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7qHe3qn09O26"
   },
   "outputs": [],
   "source": [
    "class BaselineScorer(AbstractScorer):\n",
    "    def __init__(self, idf):\n",
    "        super().__init__(idf)\n",
    "        self.field_type = \"url\"  # Campo por defecto de inicialización de la clase.\n",
    "\n",
    "    def set_field_type(self, field_type):\n",
    "        self.field_type = field_type\n",
    "\n",
    "    def get_sim_score(self, q, d):\n",
    "        score = 0\n",
    "        # BEGIN YOUR CODE\n",
    "        # Simplemente acumularemos los TFs de cada término de la query para el documento dado\n",
    "        \n",
    "        # Obtener el vector de consulta\n",
    "        query_vec = self.get_query_vector(q)\n",
    "\n",
    "        # Obtener el vector del documento (con todos sus campos)\n",
    "        doc_vec = self.get_doc_vector(q, d)\n",
    "\n",
    "        # Obtener el Counter del campo específico seleccionado\n",
    "        field_vec = doc_vec[self.field_type]\n",
    "\n",
    "        # Para cada término en la consulta, sumar su TF en el campo del documento\n",
    "        for term in query_vec:\n",
    "            if term in field_vec:\n",
    "                score += field_vec[term]\n",
    "        # END YOUR CODE\n",
    "        return score\n",
    "\n",
    "    # En este caso el scoring neto coincide con el devuelto por get_sim_score\n",
    "    # (no se combinan los distintos campos, simplemente se tiene en cuenta el\n",
    "    #  campo fijado previamente con set_field_type):\n",
    "    def get_net_score(self, q, d):\n",
    "        return self.get_sim_score(q, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw43K2JRXu8P"
   },
   "source": [
    "Probamos el _baseline scorer_ con una consulta y un par de documentos de ejemplo, para todos los campos posibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVlIJ1Xk9O26",
    "outputId": "7eec5ee6-d4c8-4fc5-be89-960b9d813a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "\n",
      "Consulta:  stanford aoerc pool hours\n",
      "Vector de consulta:\n",
      "Counter({'stanford': 1, 'aoerc': 1, 'pool': 1, 'hours': 1})\n",
      "\n",
      "---------\n",
      "\n",
      "Documento:\n",
      " url: http://events.stanford.edu/2014/February/18/\n",
      " title: events at stanford tuesday february 18 2014\n",
      " headers: ['stanford university event calendar', 'teaching sex at stanford', 'rodin the complete stanford collection', 'stanford rec trx suspension training', 'memorial church open visiting hours', 'alternative transportation counseling tm 3 hour stanford univ shc employees retirees family members']\n",
      " body_hits: {'stanford': [239, 271, 318, 457, 615, 642, 663, 960, 966, 971], 'aoerc': [349, 401, 432, 530, 549, 578, 596], 'pool': [521]}\n",
      " body_length: 981\n",
      " pagerank: 1\n",
      "\n",
      "Vectores de documento:\n",
      "  url vector (computed similarity=1):\n",
      "  Counter({'events': 1, 'stanford': 1, 'edu': 1, '2014': 1, 'february': 1, '18': 1})\n",
      "\n",
      "  title vector (computed similarity=1):\n",
      "  Counter({'events': 1, 'at': 1, 'stanford': 1, 'tuesday': 1, 'february': 1, '18': 1, '2014': 1})\n",
      "\n",
      "  headers vector (computed similarity=6):\n",
      "  Counter({'stanford': 5, 'university': 1, 'event': 1, 'calendar': 1, 'teaching': 1, 'sex': 1, 'at': 1, 'rodin': 1, 'the': 1, 'complete': 1, 'collection': 1, 'rec': 1, 'trx': 1, 'suspension': 1, 'training': 1, 'memorial': 1, 'church': 1, 'open': 1, 'visiting': 1, 'hours': 1, 'alternative': 1, 'transportation': 1, 'counseling': 1, 'tm': 1, '3': 1, 'hour': 1, 'univ': 1, 'shc': 1, 'employees': 1, 'retirees': 1, 'family': 1, 'members': 1})\n",
      "\n",
      "  anchors vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "  body_hits vector (computed similarity=18):\n",
      "  Counter({'stanford': 10, 'aoerc': 7, 'pool': 1})\n",
      "\n",
      "---------\n",
      "\n",
      "Documento:\n",
      " url: https://cardinalrec.stanford.edu/facilities/aoerc/\n",
      " title: \n",
      " pagerank: 4\n",
      " anchors: {'gyms aoerc': 3, 'aoerc': 13, 'http cardinalrec stanford edu facilities aoerc': 4, 'arrillaga outdoor education and recreation center aoerc link is external': 1, 'the arrillaga outdoor education and research center aoerc': 2, 'aoerc will shutdown for maintenance': 2}\n",
      "\n",
      "Vectores de documento:\n",
      "  url vector (computed similarity=2):\n",
      "  Counter({'https:': 1, 'cardinalrec': 1, 'stanford': 1, 'edu': 1, 'facilities': 1, 'aoerc': 1})\n",
      "\n",
      "  title vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "  headers vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "  anchors vector (computed similarity=29):\n",
      "  Counter({'aoerc': 25, 'http': 4, 'cardinalrec': 4, 'stanford': 4, 'edu': 4, 'facilities': 4, 'gyms': 3, 'arrillaga': 3, 'outdoor': 3, 'education': 3, 'and': 3, 'center': 3, 'the': 2, 'research': 2, 'will': 2, 'shutdown': 2, 'for': 2, 'maintenance': 2, 'recreation': 1, 'link': 1, 'is': 1, 'external': 1})\n",
      "\n",
      "  body_hits vector (computed similarity=0):\n",
      "  Counter()\n",
      "\n",
      "---------\n",
      "\n",
      "Tests de clase BaselineScorer() superados.\n"
     ]
    }
   ],
   "source": [
    "baseline_scorer = BaselineScorer(theIDF)\n",
    "\n",
    "q = Query(\"stanford aoerc pool hours\")\n",
    "print(\"---------\\n\")\n",
    "print(\"Consulta: \", q)\n",
    "print(f\"Vector de consulta:\\n{baseline_scorer.get_query_vector(q)}\")\n",
    "print(\"\\n---------\\n\")\n",
    "\n",
    "d1 = query_dict[q]['http://events.stanford.edu/2014/February/18/']          # Ejemplo que tiene \"body_hits\".\n",
    "d2 = query_dict[q]['https://cardinalrec.stanford.edu/facilities/aoerc/']    # Ejemplo que tiene \"anchors\".\n",
    "\n",
    "for i,d in enumerate([d1,d2]):\n",
    "    doc_vectors = baseline_scorer.get_doc_vector(q,d)\n",
    "    print(\"Documento:\\n\", d)\n",
    "    print(f\"Vectores de documento:\")\n",
    "    similarities = {}\n",
    "    for k in doc_vectors.keys(): # Para cada campo:\n",
    "        baseline_scorer.set_field_type(k)\n",
    "        similarity = baseline_scorer.get_sim_score(q,d)\n",
    "        print(f\"  {k} vector (computed similarity={similarity}):\\n  {doc_vectors[k]}\\n\")\n",
    "        similarities[k] = similarity\n",
    "    # print(similarities)\n",
    "    if i==0:\n",
    "        assert similarities == {'url': 1, 'title': 1, 'headers': 6, 'anchors': 0, 'body_hits': 18}, \\\n",
    "          \"Scorer de similaridad baseline utilizando pesos por defecto no obtiene resultado esperado para d1\"\n",
    "    elif i==1:\n",
    "        assert similarities == {'url': 2, 'title': 0, 'headers': 0, 'anchors': 29, 'body_hits': 0}, \\\n",
    "          \"Scorer de similaridad baseline utilizando pesos por defecto no obtiene resultado esperado para d2\"\n",
    "    print(\"---------\\n\")\n",
    "\n",
    "print(\"Tests de clase BaselineScorer() superados.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
