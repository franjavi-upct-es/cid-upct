{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web crawling\n",
    "\n",
    "El web crawling consiste en extraer los enlaces de una página web e ir recorriéndolos para descubrir el contenido de la web. Los enlaces pueden ser a otros sitios web o pertenecer a un mismo sitio web. En este ejercicio, vamos a implementar un crawler para extraer el contenido de la web https://books.toscrape.com/. Para ello, deberás completar los métodos de la clase WebCrawler.\n",
    "\n",
    "Para trabajar con las URLs que encontremos en las páginas, usaremos la función \"urljoin(url_base, url_nueva)\" que lo que hace es ver si url_nueva es absoluta o relativa. Si es absoluta, devuelve esa misma URL, pero si es relativa, la concatena a la url_base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, cargaremos los paquetes necesarios\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import os\n",
    "import re\n",
    "import pip._vendor.requests as requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebCrawler:\n",
    "    def __init__(self, base_url, output_dir=\"output\"):\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = output_dir\n",
    "        # usamos un diccionario porque queremos poder comprobar rápidamente si una URL ya se ha descargado y a la vez\n",
    "        # poder recorrer el diccionario en el orden en el que hemos insertado los elementos (el diccionario de Python\n",
    "        # mantiene el orden de inserción). Insertaremos las URLs como la clave, y pondremos '' como valor asociado.\n",
    "        self.urls = {base_url: \"\"}\n",
    "        \n",
    "        # Crear el directorio de salida si no existe\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.mkdir(self.output_dir)\n",
    "    \n",
    "    def sanitize_text(self, text):\n",
    "        \"\"\"Limpia el texto para eliminar caracteres no permitidos en nombres de archivo y convertir las mayúsculas en minúsculas\n",
    "        Args:\n",
    "            text : Texto que se quiere limpiar.\n",
    "            \n",
    "        Return:\n",
    "            clean_text : Texto limpio.\n",
    "        \"\"\"\n",
    "        \n",
    "        clean_text = re.sub(r'\\s+', ' ', text)  # sustituye cualquier secuencia de espacios y tabuladores por un espacio solamente\n",
    "        clean_text = re.sub(r'[\\\\/*?:\"<>|]', \"-\", clean_text)  # sustituye los caracteres \\/*?:\"<>| por un guión -\n",
    "        clean_text = clean_text.lower()\n",
    "\n",
    "        return clean_text\n",
    "\n",
    "    def download_page(self, url):\n",
    "        \"\"\"Descarga el contenido HTML de una página\n",
    "        Args:\n",
    "            url : url de la web.\n",
    "            \n",
    "        Return:\n",
    "            req.text : texto html de la página.\n",
    "        \"\"\"\n",
    "        ## BEGIN YOUR CODE\n",
    "        \n",
    "        ## END YOUR CODE\n",
    "        return req.text\n",
    "\n",
    "    def save_content(self, url, content):\n",
    "        \"\"\"Guarda el contenido de texto de la página en un archivo. ¡OJO! Solo el contenido de texto, no las etiquetas del HTML\n",
    "        Args:\n",
    "            url : url de la web.\n",
    "            content: texto extraído del objeto BeautifulSoup con el contenido de la web.\n",
    "        \"\"\"\n",
    "        ## BEGIN YOUR CODE\n",
    "        \n",
    "        ## END YOUR CODE\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"Realiza el proceso de crawling en el sitio web\"\"\"\n",
    "\n",
    "        i = 0\n",
    "        while i < len(self.urls.keys()):\n",
    "\n",
    "        ## BEGIN YOUR CODE\n",
    "            \n",
    "        ## END YOUR CODE\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def extract_links(self, soup, current_url):\n",
    "        \"\"\"Extrae enlaces de una página y los agrega a la lista de URLs (self.urls) si son nuevos\n",
    "        Args:\n",
    "            soup : objeto BeautifulSoup con el contenido de la web.\n",
    "            current_url: url de la que se está extrayendo la información.\n",
    "        \"\"\"\n",
    "        ## BEGIN YOUR CODE\n",
    "        \n",
    "        ## END YOUR CODE\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Método principal para iniciar el proceso de crawling\"\"\"\n",
    "        print(\"Iniciando crawling...\")\n",
    "        self.crawl()\n",
    "        print(\"Crawling completado.\")\n",
    "        print(f\"URLs encontradas: {len(self.urls.keys())}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuta ahora el scraper para recorrer la web y descargar su contenido. OJO, puede tardar unos minutos, así que puedes ver el directorio \"output\" y comprobar si van descargándose las webs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = WebCrawler(\"https://books.toscrape.com/\")\n",
    "crawler.run()\n",
    "print(crawler.urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salida:\n",
    "```\n",
    "Iniciando crawling...\n",
    "Crawling completado.\n",
    "URLs encontradas: 1195\n",
    "{'https://books.toscrape.com/': '', 'https://books.toscrape.com/index.html': '', 'https://books.toscrape.com/catalogue/category/books_1/index.html': '' ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El total de URLs encontradas debe ser de 1195."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cerrando el círculo: Obtención del índice invertido del sitio web books.toscrape\n",
    "\n",
    "Ahora que sabemos cómo se realiza el proceso de obtención de la información de documentos web, vamos a construir su índice invertido para poder buscar información. Vemos así cómo hemos conseguido implementar todos los procesos de un sistema de recuperación de información mediante cada una de las prácticas.\n",
    "\n",
    "A continuación, se deberán leer los documentos para extraer el vocabulario y calcular la frecuencia de término según se realizó en la primera práctica de la asignatura. **Esto nos permitirá comprobar que hemos leido correctamente las páginas web**. Para ello, copia las celdas correspondientes y modifícalas según sea necesario. Al final muestra el número de términos que contiene la colección, el tamaño del vocabulario y los 10 términos más frecuentes con su conteo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN YOUR CODE\n",
    "        \n",
    "## END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Número de términos en la colección: 444877\n",
    "\n",
    "444877\n",
    "\n",
    "Tamaño del vocabulario : 35462\n",
    "\n",
    "Los 10 términos más frecuentes son:\n",
    "\n",
    "  to: 18380\n",
    "\n",
    "  the: 18220\n",
    "\n",
    "  in: 15913\n",
    "\n",
    "  and: 12193\n",
    "\n",
    "  stock: 11284\n",
    "\n",
    "  of: 10940\n",
    "\n",
    "  a: 9668\n",
    "\n",
    "  add: 9560\n",
    "\n",
    "  basket: 9304\n",
    "  \n",
    "  ...: 6065\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez extraído el vocabulario y la frecuencia de término, construye ahora el índice invertido de los documentos copiando de la práctica 1 el código correspondiente y modificando lo que sea necesario. Vamos a usar el índice sencillo que hicimos al comienzo que simplemente es un diccionario de términos la posting list de documentos donde aparece cada término. Puedes usar todas las celdas que necesites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN YOUR CODE\n",
    "        \n",
    "## END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_inverted_index = dict(sorted(inverted_index.items()))  # suponemos que el índice invertido se llama inverted_index\n",
    "\n",
    "print(sorted_inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salida (¡OJO! es tan larga que se puede quedar colgado VSCode si la metemos entera en una celda de markdown):\n",
    "\n",
    "```\n",
    "{'!': [286], '!!': [492], '#01-#05': [1186], '#1': [1, 11, 21, 82, 85, 86, 97, 294, 297, 317, 319, 322, 323, 370, 373, 382, 385, 386, 520, 55\n",
    "...\n",
    " '\\ufeffintroduction': [1024], '\\ufeffwritten': [1024]}\n",
    "\n",
    " ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos ahora la lista de postings para el término \"dagger\" y obtenemos los nombres de documento correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_list = inverted_index['dagger']  \n",
    "posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docnames(filenames, docids):\n",
    "   \n",
    "    return [filenames[i] for i in list(docids)]\n",
    "    \n",
    "    \n",
    "retrieve_docnames(file_paths,posting_list)   # suponemos que file_paths contiene una lista con las rutas a los documentos ordenados según docid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "['output\\\\https---books.toscrape.com-catalogue-category-books-fantasy_19-page-2.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-category-books-romance_8-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-category-books-romance_8-page-1.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-category-books_1-page-33.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-category-books_1-page-35.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-changing-the-game-play-by-play-2_317-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-dark-lover-black-dagger-brotherhood-1_319-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-grey-fifty-shades-4_592-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-harry-potter-and-the-chamber-of-secrets-harry-potter-2_325-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-harry-potter-and-the-half-blood-prince-harry-potter-6_326-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-harry-potter-and-the-order-of-the-phoenix-harry-potter-5_327-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-i-had-a-nice-time-and-other-lies-how-to-find-love-sht-like-that_814-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-keep-me-posted_594-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-meternity_478-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-page-33.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-page-35.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-paper-and-fire-the-great-library-2_339-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-soldier-talon-3_222-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-the-beast-black-dagger-brotherhood-14_342-index.html',\n",
    "\n",
    " 'output\\\\https---books.toscrape.com-catalogue-the-rose-the-dagger-the-wrath-and-the-dawn-2_278-index.html',\n",
    " \n",
    " 'output\\\\https---books.toscrape.com-catalogue-will-you-wont-you-want-me_644-index.html']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio extra de solución más abierta:\n",
    "\n",
    "Haz ahora web scrapping para obtener un diccionario que contenga la siguiente información de cada libro que pertenezca a una lista de categorías: título, precio, disponibilidad, URL de la imagen, rating y URL del producto. Luego, almacena la información en un archivo CSV por categoría cuyo nombre tendrá el formato `categoria_numerolibros_books.csv` (p. ej. fiction_10_books.csv) y cuya primera línea contendrá los nombres de las columnas.\n",
    "\n",
    "Ayudas:\n",
    "\n",
    "- Ten en cuenta la estructura de las urls de los libros de una categoría. Por ejemplo, los de la categoría \"fiction_10\" tienen la siguiente URL: https://books.toscrape.com/catalogue/category/books/fiction_10/index.html\n",
    "- Mira cómo se recorren las páginas dentro de una categoría: https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html\n",
    "https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html\n",
    "...\n",
    "https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-7.html -> provoca 404 Not Found\n",
    "para eso puedes comprobar el atributo \"status_code\" de la respuesta de request.get(URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN YOUR CODE\n",
    "        \n",
    "## END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
