{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1vSaOcRfN2ZcZgYV_UlcpfVkkQL5QCk1G",
     "timestamp": 1743498803116
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "d1d0da351d6a494abd0e0d6f31159f1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64bbe03021fb422592e0b311daa34111",
       "IPY_MODEL_d48584a7b50c4f48b10dcc9cf60198f2",
       "IPY_MODEL_2ceab2ef4a924c8ea3074f086ee82f1a"
      ],
      "layout": "IPY_MODEL_a1f20cd298cf49b3b8d4cb4043f50bde"
     }
    },
    "64bbe03021fb422592e0b311daa34111": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bc810205fa746d6b9f3646a8394e0e0",
      "placeholder": "​",
      "style": "IPY_MODEL_9bed457de5b84e6b891c065db592d0f0",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "d48584a7b50c4f48b10dcc9cf60198f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c50a2e2957b4ddaadd1703459d972ce",
      "max": 1156999,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ab9997ed66448649d72ffdc4a40039d",
      "value": 1156999
     }
    },
    "2ceab2ef4a924c8ea3074f086ee82f1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d237a6c26fe74036b3e766d61fb12a6b",
      "placeholder": "​",
      "style": "IPY_MODEL_b0bbcea98f3748b2bbf19e0687477a22",
      "value": " 1.16M/1.16M [00:00&lt;00:00, 13.6MB/s]"
     }
    },
    "a1f20cd298cf49b3b8d4cb4043f50bde": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bc810205fa746d6b9f3646a8394e0e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bed457de5b84e6b891c065db592d0f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c50a2e2957b4ddaadd1703459d972ce": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ab9997ed66448649d72ffdc4a40039d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d237a6c26fe74036b3e766d61fb12a6b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0bbcea98f3748b2bbf19e0687477a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b49f2f32a45d4bf9aa7a3a4d337d2476": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_413fe2b07dab42a986f3e2fda595e494",
       "IPY_MODEL_bab26af02c864045bff3a6df51e54c4c",
       "IPY_MODEL_244baf939d3343b2bf216fa78cda6316"
      ],
      "layout": "IPY_MODEL_d583e87296764c91abbab59a3743b258"
     }
    },
    "413fe2b07dab42a986f3e2fda595e494": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05091c06524649868370f27126a4295b",
      "placeholder": "​",
      "style": "IPY_MODEL_b21866fec3664223a8a0b467afbcc0e0",
      "value": "tokenizer.model: 100%"
     }
    },
    "bab26af02c864045bff3a6df51e54c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54f841803a9a4801a122d9818d788d68",
      "max": 4689074,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0d839fec2424618b3f440ab5c0419d5",
      "value": 4689074
     }
    },
    "244baf939d3343b2bf216fa78cda6316": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2f66f70bd994f9c87eaf53143b5bcd2",
      "placeholder": "​",
      "style": "IPY_MODEL_02ca0fa3049e43cfadfdda51643b42a8",
      "value": " 4.69M/4.69M [00:00&lt;00:00, 38.5MB/s]"
     }
    },
    "d583e87296764c91abbab59a3743b258": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05091c06524649868370f27126a4295b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b21866fec3664223a8a0b467afbcc0e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54f841803a9a4801a122d9818d788d68": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0d839fec2424618b3f440ab5c0419d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d2f66f70bd994f9c87eaf53143b5bcd2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02ca0fa3049e43cfadfdda51643b42a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dff5b2a46d104d62a1d546125c3bae65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2267e81241e24b42a806a0c865abfaee",
       "IPY_MODEL_f70bbb0e9d4b4be98952bc600fa1d270",
       "IPY_MODEL_677be9edbdbd40aea63e444f03903c5b"
      ],
      "layout": "IPY_MODEL_b1ac750860774e04a8464bbe57eb88cb"
     }
    },
    "2267e81241e24b42a806a0c865abfaee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f6955abe4a548b4b5072d93d8b9427f",
      "placeholder": "​",
      "style": "IPY_MODEL_da701420da584b36aa81f09a1c7ad0ce",
      "value": "tokenizer.json: 100%"
     }
    },
    "f70bbb0e9d4b4be98952bc600fa1d270": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a5c66a612594e489d4f87d402db3dc1",
      "max": 33384568,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_75b592a6b27e4872889f3b15d803a0bc",
      "value": 33384568
     }
    },
    "677be9edbdbd40aea63e444f03903c5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39e06aa6c3b44a2b97561da9d573ddd6",
      "placeholder": "​",
      "style": "IPY_MODEL_0f976216b2be45699633d4f810060f58",
      "value": " 33.4M/33.4M [00:00&lt;00:00, 160MB/s]"
     }
    },
    "b1ac750860774e04a8464bbe57eb88cb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f6955abe4a548b4b5072d93d8b9427f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da701420da584b36aa81f09a1c7ad0ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a5c66a612594e489d4f87d402db3dc1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75b592a6b27e4872889f3b15d803a0bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "39e06aa6c3b44a2b97561da9d573ddd6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f976216b2be45699633d4f810060f58": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "891da8327e7144f5be99c2064e42a9c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81d4bf2a8cb24e9991c1e2ed05b4ff2a",
       "IPY_MODEL_6fe83e05fb984b2aa34981631a0fae64",
       "IPY_MODEL_c43c4ce26df5487b8b29d8efb0da2b7e"
      ],
      "layout": "IPY_MODEL_2e2216b2423d4b6599d2d392203bd192"
     }
    },
    "81d4bf2a8cb24e9991c1e2ed05b4ff2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea7ee438abd042de859baf753c2d79b1",
      "placeholder": "​",
      "style": "IPY_MODEL_d2c2baa244884918800e8950686b9307",
      "value": "added_tokens.json: 100%"
     }
    },
    "6fe83e05fb984b2aa34981631a0fae64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a351b642cf7d4a9998b0814bd2eda275",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c5b5534337042c69bf97f4cda96bebd",
      "value": 35
     }
    },
    "c43c4ce26df5487b8b29d8efb0da2b7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_773cf9bca16a462b88f9daf44e910bd7",
      "placeholder": "​",
      "style": "IPY_MODEL_babf90ef1f93402486b8a41875d09d59",
      "value": " 35.0/35.0 [00:00&lt;00:00, 3.13kB/s]"
     }
    },
    "2e2216b2423d4b6599d2d392203bd192": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea7ee438abd042de859baf753c2d79b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2c2baa244884918800e8950686b9307": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a351b642cf7d4a9998b0814bd2eda275": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c5b5534337042c69bf97f4cda96bebd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "773cf9bca16a462b88f9daf44e910bd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "babf90ef1f93402486b8a41875d09d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "038887d5b52c4794b3b27e2b61afd5a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e9505a04b804a6882074ef9800edd61",
       "IPY_MODEL_8c0a83ed7f274c9ab55abdff8fc995b4",
       "IPY_MODEL_1d4c85e0cbfa41c6805b77c25c0f7a16"
      ],
      "layout": "IPY_MODEL_16b1d07eb1ee46f5acf720a32cad43be"
     }
    },
    "2e9505a04b804a6882074ef9800edd61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3772e77022e44bc881d907e38cfe0c4d",
      "placeholder": "​",
      "style": "IPY_MODEL_8171f6a23ecf4be69d08b85cbdcefb25",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "8c0a83ed7f274c9ab55abdff8fc995b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa20cfd2e8fb4923b9af5d7f3755df48",
      "max": 662,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_efa6eab66b2a4ba2966c180eea06cfbc",
      "value": 662
     }
    },
    "1d4c85e0cbfa41c6805b77c25c0f7a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2801fd67e9ee495183ea905ed0742751",
      "placeholder": "​",
      "style": "IPY_MODEL_8e34da31bf2b41049293a9d9cfbcf797",
      "value": " 662/662 [00:00&lt;00:00, 54.1kB/s]"
     }
    },
    "16b1d07eb1ee46f5acf720a32cad43be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3772e77022e44bc881d907e38cfe0c4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8171f6a23ecf4be69d08b85cbdcefb25": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa20cfd2e8fb4923b9af5d7f3755df48": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efa6eab66b2a4ba2966c180eea06cfbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2801fd67e9ee495183ea905ed0742751": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e34da31bf2b41049293a9d9cfbcf797": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b2bc6b726fe420986bca85c79ca3031": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d82eb97ed8dd432ea34b6ed4cf429a2e",
       "IPY_MODEL_f03e99d732704819b6ae43d675049cb1",
       "IPY_MODEL_91fa5c0d018049e3a4f370b70b4cc3a7"
      ],
      "layout": "IPY_MODEL_1967a47545784e2f820e213971990a01"
     }
    },
    "d82eb97ed8dd432ea34b6ed4cf429a2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6db7e35e8b2a471e81a1009a8c75e2cf",
      "placeholder": "​",
      "style": "IPY_MODEL_4a35cde0e4494984a7dda170b59197cd",
      "value": "config.json: 100%"
     }
    },
    "f03e99d732704819b6ae43d675049cb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0281b6e6d30445e1ad55cd83f5861b7b",
      "max": 899,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83112f47e10e4c1d91700599f8cf30fb",
      "value": 899
     }
    },
    "91fa5c0d018049e3a4f370b70b4cc3a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5c4ef73811c46c09bc33ad923321fa6",
      "placeholder": "​",
      "style": "IPY_MODEL_e928194a1f1542f4a18dc6503383a31e",
      "value": " 899/899 [00:00&lt;00:00, 102kB/s]"
     }
    },
    "1967a47545784e2f820e213971990a01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6db7e35e8b2a471e81a1009a8c75e2cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a35cde0e4494984a7dda170b59197cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0281b6e6d30445e1ad55cd83f5861b7b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83112f47e10e4c1d91700599f8cf30fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5c4ef73811c46c09bc33ad923321fa6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e928194a1f1542f4a18dc6503383a31e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f4fbd36948646f28815bb19ec8df820": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e3bd74c757e649a1b56f78a75337f47f",
       "IPY_MODEL_00f74bcbb1cf4a8a8e4a4b35f88c5732",
       "IPY_MODEL_f52fb8b8041249eab0da3370ba531e6f"
      ],
      "layout": "IPY_MODEL_ca98899f761a41bca0cbbdcacb8b364e"
     }
    },
    "e3bd74c757e649a1b56f78a75337f47f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_904130bf18cc45098a8a9bef21d86176",
      "placeholder": "​",
      "style": "IPY_MODEL_52536763b40d4928a901a0a7a4b3e43b",
      "value": "model.safetensors: 100%"
     }
    },
    "00f74bcbb1cf4a8a8e4a4b35f88c5732": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fe45f24590049c6bddccf1890d1e963",
      "max": 1999811208,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_94ecc152f0694147b14f9d9e6becaf21",
      "value": 1999811208
     }
    },
    "f52fb8b8041249eab0da3370ba531e6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba15e89ca9844fe9a373f587b6f6fcc1",
      "placeholder": "​",
      "style": "IPY_MODEL_c272963e790d4a80ac87d28420b44a62",
      "value": " 2.00G/2.00G [00:10&lt;00:00, 191MB/s]"
     }
    },
    "ca98899f761a41bca0cbbdcacb8b364e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "904130bf18cc45098a8a9bef21d86176": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52536763b40d4928a901a0a7a4b3e43b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fe45f24590049c6bddccf1890d1e963": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94ecc152f0694147b14f9d9e6becaf21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba15e89ca9844fe9a373f587b6f6fcc1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c272963e790d4a80ac87d28420b44a62": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db896d63325d46cd85b8fcdf027457fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd06915332304dd1a68ff20f989623bc",
       "IPY_MODEL_3f28fa372f7646f9ac6532590115a8ef",
       "IPY_MODEL_04e5fa9f8e5d403292b20f33cc0d4c5e"
      ],
      "layout": "IPY_MODEL_a2f6169d72ce474b93b0069f3bc8cbd2"
     }
    },
    "bd06915332304dd1a68ff20f989623bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddc6da9f520d43e3b8211a6a091d509e",
      "placeholder": "​",
      "style": "IPY_MODEL_7a6d90dc03154705b8d0ad0ca740b28f",
      "value": "generation_config.json: 100%"
     }
    },
    "3f28fa372f7646f9ac6532590115a8ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7327c1708594c1d8640e23646a6d74b",
      "max": 215,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ae723d11dcb4c64ac1f287e22c5536a",
      "value": 215
     }
    },
    "04e5fa9f8e5d403292b20f33cc0d4c5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f94b605fd57e46ffa4a29cd75996161b",
      "placeholder": "​",
      "style": "IPY_MODEL_6654ad223cee46b684ac0c07b62cebed",
      "value": " 215/215 [00:00&lt;00:00, 20.4kB/s]"
     }
    },
    "a2f6169d72ce474b93b0069f3bc8cbd2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddc6da9f520d43e3b8211a6a091d509e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a6d90dc03154705b8d0ad0ca740b28f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7327c1708594c1d8640e23646a6d74b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ae723d11dcb4c64ac1f287e22c5536a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f94b605fd57e46ffa4a29cd75996161b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6654ad223cee46b684ac0c07b62cebed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sesión 8 - Demostración de la capacidad In-Context Learning de los LLMs para resumen de textos\n",
    "\n",
    "En este boletín veremos el uso de una técnica llamada **In-Context Learning** mediante la cual los LLMs pueden realizar tareas sin necesidad de ser específicamente entrenados para ellas.\n",
    "\n",
    "En los últimos años, el enfoque predominante para resolver cualquier tarea de PLN se ha basado en dos etapas:\n",
    "- **Pre-entrenamiento**: En esta etapa se realiza un entrenamiento auto-supervisado del modelo a partir de enormes cantidades de texto no etiquetado. El pre-entrenamiento no está enfocado a ninguna tarea en particular.\n",
    "\n",
    "- **Ajuste (Fine-Tuning)**: En esta etapa se realiza un reentrenamiento del modelo pre-entrenado para resolver una tarea concreta utilizando un conjunto de datos, usualmente etiquetados, creado para tal fin.\n",
    "\n",
    "La creación de conjuntos de datos de entrenamiento para el ajuste fino presenta problemas importantes: no siempre se dispone de suficientes datos anotados y recopilarlos suele llevar mucho tiempo. A medida que aumenta la demanda de aplicaciones de PLN la necesidad de abordar nuevos dominios y tareas se vuelve cada vez más urgente, agravando aún más los problemas que suponen la escasez de conjuntos de datos y la creación de nuevos conjuntos.\n",
    "\n",
    "Ante este problema, el **In-Context Learning** representa una alternativa: en lugar de realizar un fine-tuning con nuevos datos, los LLMs pueden adaptarse a nuevas tareas simplemente a partir del contexto proporcionado en el prompt. De esya forma, podemos utilizar un mismo modelo para diferentes tareas sin necesidad de realizar un ajuste fino para cada una de ellas.\n",
    "\n",
    "Hay **tres formas** diferentes de realizar el **In-Context Learning**:\n",
    "\n",
    "- **Zero Shot Learning**: En el prompt se le indican al modelo las instrucciones para realizar una tarea específica, sin incluir ningún ejemplo de cómo realizarla.\n",
    "- **Few Shot Learning**: En el prompt se le indican al modelo las instrucciones para resolver una tarea en concreto y, además, se incluyen algunos ejemplos para que el modelo 'entienda' el patrón antes de resolverla.\n",
    "- **Chain of Thought**: En este caso, además de  instrucciones y ejemplos, en el prompt se incluye el proceso de razonamiento que el modelo debe seguir para resolver la tarea.\n",
    "\n",
    "En esta sesión:\n",
    "1. Instalamos las librerias necesarias.\n",
    "2. Cargaremos el modelo gemma-3-1b-it, lo optimizaremos (cuantizándolo a 4 bits) y lo usaremos sin más ajustes para resolver diferentes tareas de resumen de textos.\n",
    "3. Visualizaremos la generación progresiva de respuestas con un LLM en tiempo real. Para ello: (1) estructuraremos los mensajes de entrada aplicando una plantilla; y (2) generaremos un relato.\n",
    "4. Resumiremos textos utilizando las tres posibilidades de in-context-learning: (1) zero-shot, (2) few-shot y (3) chain-of-thought.\n",
    "\n"
   ],
   "metadata": {
    "id": "EoR7F0fEXXyX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primero, instalaremos las liberías necesarias."
   ],
   "metadata": {
    "id": "KXvRsIfDWk43"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalamos las librerías necesarias\n",
    "!pip install transformers huggingface_hub\n",
    "!pip install -U bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsX-y0mDWnSy",
    "outputId": "cfa56ed6-4b31-4322-adef-f42c2b39ac04",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743499807161,
     "user_tz": -120,
     "elapsed": 98923,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.0/76.0 MB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m79.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m64.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m47.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m70.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.45.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apartado 0 - Generación Progresiva de Respuestas con un LLM en Tiempo Real\n"
   ],
   "metadata": {
    "id": "Xm_3b8lCVtcl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Antes de explorar las diferentes técnicas de In-Context Learning **vamos a ver cómo se generan las respuestas de un LLM de manera progresiva, mostrando tal respuesta a medida que se va generando**.\n",
    "\n",
    "Los LLMs pueden generar texto a partir de un prompt. En algunas aplicaciones, tal texto no puede visualizarse hasta que se ha generado por completo. Sin embargo, en otras aplicaciones, como los chatbots, es más apropiado mostrar la respuesta a medida que se genera, proporcionando una experiencia de usuario más dinámica y fluida.\n"
   ],
   "metadata": {
    "id": "sUdcDVcWWNaf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para la realización de esta práctica vamos a usar un modelo que requiere una licencia de uso. Por ello, es necesario **iniciar una sesion en Hugging Face, usando un token**. **El token que se ha proporcionado es válido hasta el jueves 3 de abril. Para ejecutar el cuaderno después de ese día tendréis que generar un nuevo token.**\n",
    "\n",
    "Para solicitar una licencia de uso y crear un token hay que acceder a la página del modelo en Hugging Face. **El proceso es bastante sencillo y el uso de este modelo es gratuito.** Podéis encontrar una descripción detallada en: https://huggingface.co/docs/hub/security-tokens\n",
    "\n"
   ],
   "metadata": {
    "id": "COyTx7wMXiAv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Importamos las librerias necesarias\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Este token solo va a funcionar hoy, jueves 3 de abril, para poder ejecutar el cuaderno después de ese día teneis que incluir vuestro propio token.\n",
    "login(token=\"\")"
   ],
   "metadata": {
    "id": "PNSEshjzXX6C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta práctica vamos a usar el modelo **gemma-3-1b-it**, que es una variante de la serie Gemma 3, desarrollada por Google.\n",
    "\n",
    "**gemma-3-1b-it** tiene aproximadamente 1.000 millones (`1b`) de paramétros y ha sido entrenado para seguir instrucciones específicas, lo que lo hace ideal para tareas que requieren comprensión de comandos y generan respuestas basadas en ellos (it).\n",
    "\n",
    "Ahora, cargamos el modelo:"
   ],
   "metadata": {
    "id": "O8d2v-JZYv0D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Importamos las librerias necesarias\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "# Definimos el modelo que vamos a usar\n",
    "model_path = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Configuración la cuantización de 4-bit\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit = True)\n",
    "\n",
    "# Cargamos el tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Cargamos el modelo\n",
    "# Estamos cuantizando el modelo a 4-bits para que ocupe menos espacio en GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ").eval()"
   ],
   "metadata": {
    "id": "uImHmszqYvdl",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382,
     "referenced_widgets": [
      "d1d0da351d6a494abd0e0d6f31159f1a",
      "64bbe03021fb422592e0b311daa34111",
      "d48584a7b50c4f48b10dcc9cf60198f2",
      "2ceab2ef4a924c8ea3074f086ee82f1a",
      "a1f20cd298cf49b3b8d4cb4043f50bde",
      "7bc810205fa746d6b9f3646a8394e0e0",
      "9bed457de5b84e6b891c065db592d0f0",
      "9c50a2e2957b4ddaadd1703459d972ce",
      "7ab9997ed66448649d72ffdc4a40039d",
      "d237a6c26fe74036b3e766d61fb12a6b",
      "b0bbcea98f3748b2bbf19e0687477a22",
      "b49f2f32a45d4bf9aa7a3a4d337d2476",
      "413fe2b07dab42a986f3e2fda595e494",
      "bab26af02c864045bff3a6df51e54c4c",
      "244baf939d3343b2bf216fa78cda6316",
      "d583e87296764c91abbab59a3743b258",
      "05091c06524649868370f27126a4295b",
      "b21866fec3664223a8a0b467afbcc0e0",
      "54f841803a9a4801a122d9818d788d68",
      "a0d839fec2424618b3f440ab5c0419d5",
      "d2f66f70bd994f9c87eaf53143b5bcd2",
      "02ca0fa3049e43cfadfdda51643b42a8",
      "dff5b2a46d104d62a1d546125c3bae65",
      "2267e81241e24b42a806a0c865abfaee",
      "f70bbb0e9d4b4be98952bc600fa1d270",
      "677be9edbdbd40aea63e444f03903c5b",
      "b1ac750860774e04a8464bbe57eb88cb",
      "1f6955abe4a548b4b5072d93d8b9427f",
      "da701420da584b36aa81f09a1c7ad0ce",
      "4a5c66a612594e489d4f87d402db3dc1",
      "75b592a6b27e4872889f3b15d803a0bc",
      "39e06aa6c3b44a2b97561da9d573ddd6",
      "0f976216b2be45699633d4f810060f58",
      "891da8327e7144f5be99c2064e42a9c1",
      "81d4bf2a8cb24e9991c1e2ed05b4ff2a",
      "6fe83e05fb984b2aa34981631a0fae64",
      "c43c4ce26df5487b8b29d8efb0da2b7e",
      "2e2216b2423d4b6599d2d392203bd192",
      "ea7ee438abd042de859baf753c2d79b1",
      "d2c2baa244884918800e8950686b9307",
      "a351b642cf7d4a9998b0814bd2eda275",
      "3c5b5534337042c69bf97f4cda96bebd",
      "773cf9bca16a462b88f9daf44e910bd7",
      "babf90ef1f93402486b8a41875d09d59",
      "038887d5b52c4794b3b27e2b61afd5a5",
      "2e9505a04b804a6882074ef9800edd61",
      "8c0a83ed7f274c9ab55abdff8fc995b4",
      "1d4c85e0cbfa41c6805b77c25c0f7a16",
      "16b1d07eb1ee46f5acf720a32cad43be",
      "3772e77022e44bc881d907e38cfe0c4d",
      "8171f6a23ecf4be69d08b85cbdcefb25",
      "aa20cfd2e8fb4923b9af5d7f3755df48",
      "efa6eab66b2a4ba2966c180eea06cfbc",
      "2801fd67e9ee495183ea905ed0742751",
      "8e34da31bf2b41049293a9d9cfbcf797",
      "4b2bc6b726fe420986bca85c79ca3031",
      "d82eb97ed8dd432ea34b6ed4cf429a2e",
      "f03e99d732704819b6ae43d675049cb1",
      "91fa5c0d018049e3a4f370b70b4cc3a7",
      "1967a47545784e2f820e213971990a01",
      "6db7e35e8b2a471e81a1009a8c75e2cf",
      "4a35cde0e4494984a7dda170b59197cd",
      "0281b6e6d30445e1ad55cd83f5861b7b",
      "83112f47e10e4c1d91700599f8cf30fb",
      "b5c4ef73811c46c09bc33ad923321fa6",
      "e928194a1f1542f4a18dc6503383a31e",
      "4f4fbd36948646f28815bb19ec8df820",
      "e3bd74c757e649a1b56f78a75337f47f",
      "00f74bcbb1cf4a8a8e4a4b35f88c5732",
      "f52fb8b8041249eab0da3370ba531e6f",
      "ca98899f761a41bca0cbbdcacb8b364e",
      "904130bf18cc45098a8a9bef21d86176",
      "52536763b40d4928a901a0a7a4b3e43b",
      "9fe45f24590049c6bddccf1890d1e963",
      "94ecc152f0694147b14f9d9e6becaf21",
      "ba15e89ca9844fe9a373f587b6f6fcc1",
      "c272963e790d4a80ac87d28420b44a62",
      "db896d63325d46cd85b8fcdf027457fe",
      "bd06915332304dd1a68ff20f989623bc",
      "3f28fa372f7646f9ac6532590115a8ef",
      "04e5fa9f8e5d403292b20f33cc0d4c5e",
      "a2f6169d72ce474b93b0069f3bc8cbd2",
      "ddc6da9f520d43e3b8211a6a091d509e",
      "7a6d90dc03154705b8d0ad0ca740b28f",
      "b7327c1708594c1d8640e23646a6d74b",
      "1ae723d11dcb4c64ac1f287e22c5536a",
      "f94b605fd57e46ffa4a29cd75996161b",
      "6654ad223cee46b684ac0c07b62cebed"
     ]
    },
    "outputId": "28043688-7a44-4615-9a0a-f7d20034717e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743499890015,
     "user_tz": -120,
     "elapsed": 36927,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1d0da351d6a494abd0e0d6f31159f1a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b49f2f32a45d4bf9aa7a3a4d337d2476"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dff5b2a46d104d62a1d546125c3bae65"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "891da8327e7144f5be99c2064e42a9c1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "038887d5b52c4794b3b27e2b61afd5a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b2bc6b726fe420986bca85c79ca3031"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f4fbd36948646f28815bb19ec8df820"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db896d63325d46cd85b8fcdf027457fe"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para interactuar con el modelo Gemma, necesitamos estructurar los mensajes de entrada en un formato específico para que el modelo pueda comprender lo que le estamos pidiendo.\n",
    "\n",
    "En este caso, configuramos un mensaje que simula una conversación, donde el usuario pide al modelo que genere una historia en la que un perro y un gato se pelean por una albóndiga."
   ],
   "metadata": {
    "id": "iyVXlNwvbgyI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Estructuramos los mensajes de entrada en el formato requerido por Gemma\n",
    "messages = [\n",
    "  {\n",
    "     \"role\": \"user\",\n",
    "     \"content\": \"Escribe una historia en la que un perro y un gato se pelean \\\n",
    "                 por una albóndiga. Quiero que la historia sea muy graciosa.\",\n",
    "  },\n",
    "]\n",
    "\n",
    "# Aplicamos un template de chat al mensaje de entrada utilizando el tokenizador.\n",
    "# Esto formatea los mensajes según el formato esperado por el modelo,\n",
    "# añadiendo un prompt\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    ").to(model.device)"
   ],
   "metadata": {
    "id": "jpqZgGfRbicc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora, vamos a generar el cuento:"
   ],
   "metadata": {
    "id": "B0lF2RWPekbE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Importamos las librerias necesarias\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# Inicializamos un TextIteratorStreamer, que permite generar texto en tiempo real\n",
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt = True,  # Omite el prompt en la salida\n",
    "    skip_special_tokens = True # Omite tokens especiales\n",
    ")\n",
    "\n",
    "# Configuramos los parámetros para la generación\n",
    "generation_kwargs = dict(\n",
    "\n",
    "    **inputs,\n",
    "\n",
    "    # Limitamos el máximo de tokens que el modelo puede responder\n",
    "    max_new_tokens=1000,\n",
    "\n",
    "    # Usamos el streamer para manejar la salida generada\n",
    "    streamer = streamer,\n",
    "\n",
    "    # Habilita el muestreo, lo que permite ajustar la aleatoriedad en la generación\n",
    "    do_sample=True,\n",
    "\n",
    "    # Temperatura: controla el grado de aleatoriedad en la generación\n",
    "    temperature=0.7,\n",
    "\n",
    "    # top-p: umbral para seleccionar las palabras candidatas para la siguiente palabra\n",
    "    top_p = 0.9,\n",
    "\n",
    "    # top-k: número de palabras candidatas a ser la siguiente palabra generada\n",
    "    top_k = 50\n",
    ")\n",
    "\n",
    "# Configuramos un hilo para no bloquear el notebook\n",
    "thread = Thread(target = model.generate, kwargs = generation_kwargs)\n",
    "\n",
    "# Iniciamos el hilo\n",
    "thread.start()\n",
    "\n",
    "# Mostramos la respuesta del modelo\n",
    "print(\"Respuesta del modelo:\")\n",
    "for token in streamer:\n",
    "  print(token, end=\"\", flush = True)  # `flush=True` para ver la salida inmediatamente\n",
    "\n",
    "# Esperar a que termine la generación\n",
    "thread.join()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "UMaZQ6NJZT0D",
    "outputId": "da526727-7311-4bd1-f046-e40576b4f722",
    "collapsed": true,
    "executionInfo": {
     "status": "error",
     "timestamp": 1743500066844,
     "user_tz": -120,
     "elapsed": 937,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Respuesta del modelo:\n",
      "El sol caía sobre la pequeña ciudad de Hollow "
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-ff4cfe570a60>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;31m# Mostramos la respuesta del modelo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Respuesta del modelo:\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstreamer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m   \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflush\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# `flush=True` para ver la salida inmediatamente\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/streamers.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 226\u001B[0;31m         \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext_queue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_signal\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.11/queue.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m                 \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_qsize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnot_empty\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"'timeout' must be a non-negative number\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.11/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    325\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m    \u001B[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    326\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 327\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    328\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apartado 1 - Resumen de textos con LLMs mediante In-Context Learning"
   ],
   "metadata": {
    "id": "E5CYfAu1D-tE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a seguir usando el modelo gemma-3-1b-it."
   ],
   "metadata": {
    "id": "GV-un3LIEX0r"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a definir una función que realice el resumen de un par de textos. Esta función nos permite modificar tanto el prompt a utilizar como los parámetros para el muestro: temperatura, top-p y top-k.\n",
    "\n",
    "Esta función nos permitirá tener un codigo más limpio.\n"
   ],
   "metadata": {
    "id": "77hiHv8_D-_q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_response(base_prompt,\n",
    "                      texts,\n",
    "                      model = model,\n",
    "                      tokenizer = tokenizer,\n",
    "                      temp = 0.9,\n",
    "                      t_p = 0.95,\n",
    "                      t_k = 35):\n",
    "\n",
    "  responses = []\n",
    "\n",
    "  # Para cada texto a resumir\n",
    "  for text in texts:\n",
    "\n",
    "    # Estructuramos los mensajes de entrada en el formato requerido por Gemma\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": base_prompt + \" \" + text,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Aplicamos un template de chat al mensaje de entrada utilizando el tokenizador.\n",
    "    # Esto formatea los mensajes según el formato esperado por el modelo, añadiendo un prompt\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = True,\n",
    "        return_tensors =\"pt\",\n",
    "        return_dict = True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generamos la respuesta del modelo\n",
    "    outputs = model.generate(**inputs,\n",
    "                            max_new_tokens = 1000,\n",
    "                            temperature = temp,\n",
    "                            top_k = t_k,\n",
    "                            top_p = t_p,\n",
    "                            do_sample = True\n",
    "                            )\n",
    "\n",
    "    # Obtenemos la respuesta generada\n",
    "    # Realizo un poco de postprocesamiento para obtener solo el nuevo texto generado\n",
    "    response = \"\".join(tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"model\\n\")[-1].split(\"\\n\")[:-1])\n",
    "\n",
    "    # Mostramos la respuesta\n",
    "    print(\"Texto a resumir:\")\n",
    "    print(text)\n",
    "    print(\"Respuesta del modelo:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "    responses.append(response)\n",
    "\n",
    "  return responses"
   ],
   "metadata": {
    "id": "1TCZalOmECwg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apartado 1.1 - Zero-Shot Learning\n"
   ],
   "metadata": {
    "id": "5th6EXJTWqi8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comenzamos explorando **la estrategia más sencilla** de In-Context Learning: el **Zero-Shot Learning**.\n",
    "\n",
    "En esta estrategia, usamos un modelo para resolver una tarea sin proporcionarle ejemplos previos de cómo hacerlo: simplemente le damos una lista de instrucciones en el prompt, y el modelo intenta generar una respuesta basándose en el prompt y en el conocimiento adquirido durante su preentrenamiento."
   ],
   "metadata": {
    "id": "PH88PTmkWtZX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El flujo general consiste en los siguientes pasos:\n",
    "\n",
    "1.   Cargar el modelo y su tokenizador.\n",
    "2.   Tokenizar el texto y procesarlo con el modelo: convertimos el texto en una representación numérica que el modelo pueda interpretar.\n",
    "3.   Predicción: aplicamos la función softmax para calcular la probabilidad de pertenencia del texto a cada una de las etiquetas.\n",
    "4.   Evaluación: calculamos la probabilidad de que la premisa (texto) implique la hipótesis (cada una de las etiquetas), lo que nos permitirá clasificar correctamente el texto."
   ],
   "metadata": {
    "id": "LOEQa9IWf9bF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos el prompt y los textos a resumir:"
   ],
   "metadata": {
    "id": "wD5KcLxKcw8L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "Eres un modelo cuya tarea es resumir el texto recibido en una sola oración.\n",
    "\n",
    "Por favor, resume el siguiente texto:\"\"\"\n",
    "\n",
    "\n",
    "full_texts = [\n",
    "\"El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia \\\n",
    "artificial que permite a las máquinas entender, interpretar y generar texto de \\\n",
    "manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas \\\n",
    "áreas, como chatbots, traducción automática y análisis de sentimientos.\",\n",
    "\n",
    "\"Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos \\\n",
    "textuales para identificar patrones y relaciones entre palabras y frases. \\\n",
    "Esto permite que las máquinas realicen tareas como responder preguntas, \\\n",
    "generar texto o identificar la intención detrás de un mensaje.\",\n",
    "\n",
    "\"La tokenización es uno de los primeros pasos en el PLN, donde un texto se \\\n",
    "divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser \\\n",
    "palabras, subpalabras o incluso caracteres, y se utilizan para facilitar \\\n",
    "el procesamiento y la comprensión del lenguaje por parte de los modelos \\\n",
    "de aprendizaje automático.\"\n",
    "\n",
    "\"Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos \\\n",
    "textuales para identificar patrones y relaciones entre palabras y frases. \\\n",
    "Esto permite que las máquinas realicen tareas como responder preguntas, \\\n",
    "generar texto o identificar la intención detrás de un mensaje.\",\n",
    "\n",
    "\"La tokenización es uno de los primeros pasos en el PLN, donde un texto se \\\n",
    "divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser \\\n",
    "palabras, subpalabras o incluso caracteres, y se utilizan para facilitar \\\n",
    "el procesamiento y la comprensión del lenguaje por parte de los modelos \\\n",
    "de aprendizaje automático.\"\n",
    "]"
   ],
   "metadata": {
    "id": "R1U2HaB2czph"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Procedemos a usar el modelo para resumir cada uno de los textos:"
   ],
   "metadata": {
    "id": "x8QdjBP9_z-c"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "resumenes_zero = generate_response(zero_shot_prompt, full_texts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5dGIFg0_0No",
    "outputId": "6b2cd500-01c0-4d45-8d31-1ef11d5d8cf3",
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743500886534,
     "user_tz": -120,
     "elapsed": 13760,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que busca que las máquinas comprendan y respondan al lenguaje humano.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Los modelos de lenguaje en Procesamiento del Lenguaje Natural (PLN) se aprenden con grandes cantidades de texto para reconocer patrones y relaciones, lo que les permite a las máquinas comprender y responder a preguntas, generar texto, y entender el significado de mensajes.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "La tokenización es el paso inicial en el procesamiento del lenguaje natural, esencialmente dividiendo textos en unidades más pequeñas (tokens) para que los modelos de aprendizaje automático puedan entender y trabajar con el lenguaje.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "La tokenización es un paso fundamental en el Procesamiento del Lenguaje Natural (PLN), que consiste en dividir textos en unidades más pequeñas como palabras o subpalabras, esenciales para que los modelos de aprendizaje automático puedan procesar y entender el lenguaje.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apartado 1.2 - Few-Shot Learning\n"
   ],
   "metadata": {
    "id": "Fn3uT6IgfyMm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usaremos ahora **Few-Shot Learning**. En esta estrategia, proporcionamos al modelo algunos ejemplos para resolver la tarea.\n",
    "\n",
    "Estos ejemplos se incluyen en el prompt para guiar al modelo en la tarea específica. Gracias a estos ejemplos, el modelo tiene acceso a muestras que le permiten entender mejor el patrón de la tarea y generar una respuesta más precisa."
   ],
   "metadata": {
    "id": "vjmSvdXTkeHb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al igual que antes, el flujo general consiste en los siguientes pasos:\n",
    "\n",
    "1.   Cargar el modelo de clasificación y su tokenizador.\n",
    "2.   Tokenizar el texto y procesarlo con el modelo.\n",
    "3.   Predicción.\n",
    "4.   Evaluación."
   ],
   "metadata": {
    "id": "w7yGew0-h7QC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "Eres un modelo cuya tarea es resumir el texto recibido en una sola oración.\n",
    "\n",
    "Ejemplos:\n",
    "Texto: El análisis de sentimientos es una técnica del PLN que permite identificar las emociones o actitudes expresadas en un texto. Se utiliza ampliamente en redes sociales, encuestas y reseñas de productos para comprender las opiniones y el estado de ánimo de los usuarios. Los modelos de análisis de sentimientos pueden clasificar el texto como positivo, negativo o neutral.\n",
    "Resumen: El análisis de sentimientos identifica emociones en el texto, clasificado como positivo, negativo o neutral, y se usa en redes sociales y reseñas de productos.\n",
    "Texto: La traducción automática ha avanzado mucho gracias al uso de modelos neuronales que pueden aprender a traducir entre diferentes idiomas sin necesidad de reglas explícitas. Sin embargo, aún existen desafíos para traducir correctamente expresiones idiomáticas y textos con significados complejos.\n",
    "Resumen: La traducción automática ha mejorado con modelos neuronales, pero sigue siendo un reto traducir expresiones idiomáticas.\n",
    "\n",
    "No incluyas ninguno de los ejemplos vistos en el resumen del texto de entrada.\n",
    "Ahora, por favor, resume el siguiente texto:\"\"\"\n"
   ],
   "metadata": {
    "id": "iucnUx2xkEB3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Procedemos a usar el modelo para resumir cada uno de los textos:"
   ],
   "metadata": {
    "id": "e8-cS0XViDUh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "resumenes_few = generate_response(few_shot_prompt, full_texts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZGi_29RABGl",
    "outputId": "f99a3d45-3313-4f67-d7e1-88cc1c14b66d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743500936500,
     "user_tz": -120,
     "elapsed": 13750,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es el campo de la IA que permite a las máquinas comprender y responder al lenguaje humano, y que tiene aplicaciones en chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Resumen: La tokenización es el primer paso en el procesamiento del lenguaje natural, dividiendo textos en unidades más pequeñas (tokens) para que los modelos de aprendizaje automático puedan procesarlos y comprender su significado.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apartado 1.3 - Chain of Thought"
   ],
   "metadata": {
    "id": "VV4tKw4lkZvP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usamos ahora el enfoque **Chain of Thought**.\n",
    "\n",
    "En esta estrategia, **el prompt no solo incluye ejemplos**, sino **también el razonamiento detallado** que el modelo debe seguir para llegar a la respuesta correcta.\n",
    "\n",
    "Al estructurar el prompt de esta manera, guiamos al modelo en la resolución de la tarea, permitiéndole identificar patrones y mejorar la precisión de sus respuestas."
   ],
   "metadata": {
    "id": "IimnQ05wkicb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al igual que antes, el flujo general consiste en:\n",
    "\n",
    "1.   Cargar el modelo de clasificación y su tokenizador.\n",
    "2.   Tokenizar el texto y procesarlo con el modelo.\n",
    "3.   Predicción.\n",
    "4.   Evaluación."
   ],
   "metadata": {
    "id": "rtZq5UBAknPv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cot_prompt = \"\"\"\n",
    "Eres un modelo cuya tarea es resumir el texto recibido en una sola oración.\n",
    "Primero, analiza el texto detenidamente y proporciona un análisis del contenido, destacando los puntos clave que deben ser incluidos en el resumen. Luego, resume el texto en una sola oración, manteniendo la coherencia y abarcando los puntos más importantes.\n",
    "Finalmente, da la respuesta en el siguiente formato:\n",
    "\n",
    "Resumen: [resumen del texto de entrada]\n",
    "\n",
    "Ejemplos:\n",
    "Texto: El análisis de sentimientos es una técnica del PLN que permite identificar las emociones o actitudes expresadas en un texto. Se utiliza ampliamente en redes sociales, encuestas y reseñas de productos para comprender las opiniones y el estado de ánimo de los usuarios. Los modelos de análisis de sentimientos pueden clasificar el texto como positivo, negativo o neutral.\n",
    "Resumen: El análisis de sentimientos identifica emociones en el texto, clasificado como positivo, negativo o neutral, y se usa en redes sociales y reseñas de productos.\n",
    "Texto: La traducción automática ha avanzado mucho gracias al uso de modelos neuronales que pueden aprender a traducir entre diferentes idiomas sin necesidad de reglas explícitas. Sin embargo, aún existen desafíos para traducir correctamente expresiones idiomáticas y textos con significados complejos.\n",
    "Resumen: La traducción automática ha mejorado con modelos neuronales, pero sigue siendo un reto traducir expresiones idiomáticas.\n",
    "\n",
    "Ahora, por favor, resume el siguiente texto:\"\"\"\n"
   ],
   "metadata": {
    "id": "UdB7p9_Dk6GS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Procedemos a usar el modelo para resumir cada uno de los textos:"
   ],
   "metadata": {
    "id": "4uTb3T4Vk7uf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "resumenes_cot = generate_response(cot_prompt, full_texts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHiuBdpbAEQ_",
    "outputId": "42d77acb-0dae-48ad-be35-3316e0d46380",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743500982847,
     "user_tz": -120,
     "elapsed": 18795,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "Resumen: El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender y procesar lenguaje humano, y tiene aplicaciones diversas como chatbots, traducción automática y análisis de sentimiento.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Resumen: Los modelos de lenguaje en Procesamiento del Lenguaje Natural (PLN) se aprenden con grandes cantidades de datos textuales para reconocer patrones y relaciones entre palabras, permitiendo que las máquinas comprendan, respondan y procesen texto.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "Resumen: La tokenización es un proceso fundamental en el Procesamiento del Lenguaje Natural (PLN) que consiste en dividir textos en unidades más pequeñas (tokens), como palabras o subpalabras, para facilitar el análisis y la comprensión de la información.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a ver los resumenes obtenidos con cada una de las técnicas:"
   ],
   "metadata": {
    "id": "zjcyTEYqDaFR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for idx in range(len(resumenes_zero)):\n",
    "  print(resumenes_zero[idx])\n",
    "  print(resumenes_few[idx])\n",
    "  print(resumenes_cot[idx])\n",
    "  print()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymYIqwP2CbwP",
    "outputId": "50d39740-674d-47e0-8570-0b4d8481ab5f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743334728356,
     "user_tz": -120,
     "elapsed": 11,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El Procesamiento del Lenguaje Natural (PLN) es el campo de la inteligencia artificial que permite a las máquinas comprender y responder al lenguaje humano.\n",
      "El Procesamiento del Lenguaje Natural (PLN) es la rama de la IA que permite a las máquinas comprender y procesar el lenguaje humano, abarcando áreas como chatbots, traducción automática y análisis de sentimientos.\n",
      "Resumen: El Procesamiento del Lenguaje Natural (PLN) es el campo de la IA que permite a las máquinas comprender y generar lenguaje humano, abarcando aplicaciones como chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Los modelos de lenguaje en Procesamiento del Lenguaje Natural (PLN) se entrenan con grandes cantidades de texto para aprender patrones y relaciones en el lenguaje, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.\n",
      "Los modelos de lenguaje en PLN aprenden de grandes cantidades de texto para entender patrones y relaciones, lo que les permite realizar tareas como responder preguntas, generar texto y comprender la intención de un mensaje.\n",
      "Resumen: Los modelos de lenguaje en Procesamiento de Lenguaje Natural (PLN) se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones, lo que les permite realizar tareas como responder preguntas, generar texto y comprender la intención de un mensaje.\n",
      "\n",
      "La tokenización es un paso fundamental en el Procesamiento del Lenguaje Natural (PLN) que consiste en dividir textos en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "El texto indica que la tokenización es un paso fundamental en el Procesamiento del Lenguaje Natural (PLN), donde se divide texto en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión.\n",
      "Resumen: La tokenización es un paso fundamental en el Procesamiento del Lenguaje Natural (PLN), que consiste en dividir textos en unidades más pequeñas (tokens) como palabras, subpalabras o caracteres, para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apartado 2 - La aleatoriedad en las respuestas"
   ],
   "metadata": {
    "id": "MG9jhHlGsU5p"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Los LLMs **no son deterministas**, es decir, no siempre producen la misma respuesta para una misma entrada.\n",
    "\n",
    "Esto se debe a que la **generación de texto incorpora un cierto grado de aleatoriedad** que permite que las respuestas sean más variadas y naturales, en lugar de rígidas y repetitivas.\n",
    "\n",
    "Para **controlar este comportamiento**, existen una serie de parámetros que le indican al modelo como realizar la decodificación (generación de la siguiente palabra):\n",
    "```\n",
    "  # Generamos la respuesta del modelo\n",
    "  outputs = model.generate(inputs.input_ids,\n",
    "                           max_length=50,\n",
    "                           temperature=0.9,\n",
    "                           top_k=35,\n",
    "                           top_p=0.95,\n",
    "                           do_sample=True\n",
    "                           )\n",
    "```\n",
    "\n",
    "- **Temperatura**: que controla lo aleatorio que es el texto generado dando más peso (t>1) o menos peso (t<1) a las palabras menos probables.\n",
    "- **Top-p**: que elige las palabras más probables hasta que su probabilidad acumulada supere un umbral (p).\n",
    "- **Top-k**: que limita la cantidad de palabras candidatas que el modelo puede elegir en cada paso a las k más ptobables."
   ],
   "metadata": {
    "id": "-0fAW-EasuQ-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a definir una función que realice el resumen de un par de textos. Esta función nos permite varias los valores de temperatura, top-p y top-k.\n",
    "\n",
    "Esta función nos permitirá tener un codigo más limpio.\n"
   ],
   "metadata": {
    "id": "Jom7Gz7eukkf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Definimos el prompt\n",
    "few_shot_prompt = \"\"\"\n",
    "Eres un modelo cuya tarea es resumir el texto recibido en una sola oración.\n",
    "\n",
    "Ejemplos:\n",
    "Texto: El análisis de sentimientos es una técnica del PLN que permite identificar las emociones o actitudes expresadas en un texto. Se utiliza ampliamente en redes sociales, encuestas y reseñas de productos para comprender las opiniones y el estado de ánimo de los usuarios. Los modelos de análisis de sentimientos pueden clasificar el texto como positivo, negativo o neutral.\n",
    "Resumen: El análisis de sentimientos identifica emociones en el texto, clasificado como positivo, negativo o neutral, y se usa en redes sociales y reseñas de productos.\n",
    "Texto: La traducción automática ha avanzado mucho gracias al uso de modelos neuronales que pueden aprender a traducir entre diferentes idiomas sin necesidad de reglas explícitas. Sin embargo, aún existen desafíos para traducir correctamente expresiones idiomáticas y textos con significados complejos.\n",
    "Resumen: La traducción automática ha mejorado con modelos neuronales, pero sigue siendo un reto traducir expresiones idiomáticas.\n",
    "\n",
    "No incluyas ninguno de los ejemplos vistos en el resumen del texto de entrada.\n",
    "Ahora, por favor, resume el siguiente texto:\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "WTnzvDB3uRjs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a ver como se comporta el modelo si variamos estos parámetros:"
   ],
   "metadata": {
    "id": "SSDcJD5wumAN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generación determinista (con la mínima aleatoriedad posible)"
   ],
   "metadata": {
    "id": "VWLtbdmUvey5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta configuración, el modelo siempre elegirá la palabra más probable en cada paso (greedy decoding), sin ningún tipo de variación.\n",
    "\n",
    "Estableciendo `top_k=0` y `top_p=0`, estamos deshabilitando estas opciones para el muestreo.\n",
    "\n",
    "Al fijar `temperature=0.01`, el modelo elimina al máximo la aleatoriedad (prácticamente greedy decoding) y elegirá la palabra más probable en cada momento. Como consecuencia, el modelo generará la misma respuesta o una respuesta muy parecida cada vez que se ejecute sobre una misma entrada. Es importante tener en cuenta que no es posible eliminar la aleatoriedad al completo.\n",
    "\n",
    "Como resultado, las respuestas son consistentes y predecibles, pero también pueden volverse monótonas y poco creativas.\n",
    "\n",
    "Este tipo de configuración es útil en tareas donde la precisión es más importante que la diversidad, como en resúmenes técnicos o extracción de entidades."
   ],
   "metadata": {
    "id": "7CjSBBjIyio1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generate_response(few_shot_prompt, full_texts, temp = 0.01, t_p = 0, t_k = 0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PD0JJouvnPy",
    "outputId": "9538e77b-effe-42bb-8cd3-69d10b0d297b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743501761743,
     "user_tz": -120,
     "elapsed": 17136,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas comprender y generar texto, y se utiliza en chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "La tokenización es un proceso fundamental en el Procesamiento del Lenguaje Natural (PLN) que divide textos en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas comprender y generar texto, y se utiliza en chatbots, traducción automática y análisis de sentimientos.',\n",
       " 'Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.',\n",
       " 'La tokenización es un proceso fundamental en el Procesamiento del Lenguaje Natural (PLN) que divide textos en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generate_response(few_shot_prompt, full_texts, temp = 0.01, t_p = 0, t_k = 0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzzcMRHj1RkO",
    "outputId": "b0811cdc-af4e-4f13-d564-8d68803cb7b8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743501810566,
     "user_tz": -120,
     "elapsed": 16874,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas comprender y generar texto, y se utiliza en chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "La tokenización es un proceso fundamental en el Procesamiento del Lenguaje Natural (PLN) que divide textos en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas comprender y generar texto, y se utiliza en chatbots, traducción automática y análisis de sentimientos.',\n",
       " 'Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.',\n",
       " 'La tokenización es un proceso fundamental en el Procesamiento del Lenguaje Natural (PLN) que divide textos en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.']"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos observar que con la configuración escogida (`temperature = 0.01`, `top_p = 0`, `top_k = 0`), las diferentes llamadas a la función generate_response producen los mismos resúmenes.\n",
    "\n",
    "Esto es justo lo esperado, ya que esta configuración reduce al mínimo la aleatoreidad del modelo, que responde así de la forma más determinista posible, eligiendo las palabras más probables en cada paso (greedy decoding) y generando respuestas idénticas o muy similares en cada ejecución.\n",
    "\n",
    "Sin embargo, aún reduciendo al mínimo la aleatoriedad, es posible que algunas respuestas varíen ligeramente debido a factores internos del proceso de generación."
   ],
   "metadata": {
    "id": "IPBtKmxj4n7I"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generación con aletoriedad controlada (equilibrio entre coherencia y creatividad)"
   ],
   "metadata": {
    "id": "wC6LCW3uxpZX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta configuración, la temperatura moderada y el muestreo restringido hacen que el modelo tenga cierta variabilidad en sus respuestas sin perder coherencia.\n",
    "\n",
    "Para ello, limitamos la selección de palabras a un conjunto de opciones más probables mediante `top_k=50` y `top_p=0.9`. Por otro lado, fijamos  `temperature=0.7`, con lo que se introduce un grado controlado de aleatoriedad que evita respuestas excesivamente rígidas sin hacerlas impredecibles o incoherentes.\n",
    "\n",
    "Como resultado, el texto generado mantiene un equilibrio entre diversidad y precisión.\n",
    "\n",
    "Esta configuración es ideal para tareas como la generación de texto creativo o el uso en chatbots."
   ],
   "metadata": {
    "id": "uozn7mEQysGI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generate_response(few_shot_prompt, full_texts, temp = 0.7, t_p = 0.9, t_k = 50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26vd-5-Bxwx8",
    "outputId": "100e9bd3-9074-4a8e-a31a-c6fec951e7a6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743501940862,
     "user_tz": -120,
     "elapsed": 17371,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es una rama de la IA que permite a las máquinas comprender y procesar el lenguaje humano, abarcando áreas como chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones, lo que permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "La tokenización es un paso fundamental en el Procesamiento del Lenguaje Natural (PLN), donde el texto se divide en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['El Procesamiento del Lenguaje Natural (PLN) es una rama de la IA que permite a las máquinas comprender y procesar el lenguaje humano, abarcando áreas como chatbots, traducción automática y análisis de sentimientos.',\n",
       " 'Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones, lo que permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.',\n",
       " 'La tokenización es un paso fundamental en el Procesamiento del Lenguaje Natural (PLN), donde el texto se divide en unidades más pequeñas (tokens) para facilitar el análisis y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generate_response(few_shot_prompt, full_texts, temp = 0.7, t_p = 0.9, t_k = 50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfPZKQJX1S11",
    "outputId": "16f7d404-d100-4898-d2e9-dfe3e0579af0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743501976014,
     "user_tz": -120,
     "elapsed": 17494,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es el campo de la IA que permite a las máquinas entender y procesar el lenguaje humano, y se utiliza en diversas aplicaciones como chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que permite la generación de texto, la respuesta a preguntas y la identificación de la intención.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['El Procesamiento del Lenguaje Natural (PLN) es el campo de la IA que permite a las máquinas entender y procesar el lenguaje humano, y se utiliza en diversas aplicaciones como chatbots, traducción automática y análisis de sentimientos.',\n",
       " 'Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que permite la generación de texto, la respuesta a preguntas y la identificación de la intención.',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Con la configuración escogida (`temperature = 0.7`, `top_p = 0.9`, `top_k = 50`), las diferentes llamadas a gnerate_resumen producen resultados  similares, pero no idénticos.\n",
    "\n",
    "Con esta configuración el modelo mantiene un grado de aleatoriedad que produce respuestas diferentes en cada ejecución (diversidad), pero siempre dentro de un rango coherente (calidad).\n",
    "\n",
    "Es importante recordar que, aunque la variabilidad es mayor que en configuraciones más deterministas, el modelo sigue generando respuestas dentro de un conjunto de opciones probables, por lo que las diferencias no suelen ser muy grandes."
   ],
   "metadata": {
    "id": "qgKIvFvv5ur6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generación creativa (mayor aleatoriedad)"
   ],
   "metadata": {
    "id": "7xqwCbqlxxEf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta configuración, la temperatura alta y el muestreo sin restricciones hacen que el modelo explore opciones más inusuales en la generación de texto.\n",
    "\n",
    "Al establecer `temperature=1.2`, se introduce un alto grado de aleatoriedad, lo que fomenta respuestas más diversas y menos predecibles. Además, con `top_p=1.0` y `top_k=0`, no se restringe la selección de palabras, permitiendo que el modelo elija entre un rango amplio de opciones posibles.\n",
    "\n",
    "Como resultado, el texto generado puede ser más creativo e inesperado, aunque también aumenta el riesgo de incoherencias.\n",
    "\n",
    "Esta configuración es útil en tareas donde se busca diversidad, como la generación de textos literarios o el brainstorming a partir de una idea."
   ],
   "metadata": {
    "id": "hWEgYwO7zISf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generate_response(few_shot_prompt, temperature = 1.2, top_p = 1.0, top_k = 0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0lPCZkMyCUa",
    "outputId": "8bb9abee-371e-4a0d-f350-7d0a0240fcb6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743334806410,
     "user_tz": -120,
     "elapsed": 11535,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas comprender, interpretar y generar texto similar al humano, y se aplica en áreas como chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas comprender, interpretar y generar texto similar al humano, y se aplica en áreas como chatbots, traducción automática y análisis de sentimientos.',\n",
       " 'Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones entre palabras y frases, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generate_response(few_shot_prompt, temperature = 1.2, top_p = 1.0, top_k = 0)"
   ],
   "metadata": {
    "id": "RxsigJUB1Tg5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "68c7e661-0648-4b9d-9ea3-ad0ff7468c2d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1743334820767,
     "user_tz": -120,
     "elapsed": 11647,
     "user": {
      "displayName": "juan pastor",
      "userId": "09767725131573871922"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Texto a resumir:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender, interpretar y generar texto de manera similar a como lo hacen los seres humanos. Este campo se aplica en muchas áreas, como chatbots, traducción automática y análisis de sentimientos.\n",
      "Respuesta del modelo:\n",
      "El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender y procesar el lenguaje humano, y se aplica en diversas áreas como chatbots, traducción automática y análisis de sentimientos.\n",
      "\n",
      "Texto a resumir:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para identificar patrones y relaciones entre palabras y frases. Esto permite que las máquinas realicen tareas como responder preguntas, generar texto o identificar la intención detrás de un mensaje.\n",
      "Respuesta del modelo:\n",
      "Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.\n",
      "\n",
      "Texto a resumir:\n",
      "La tokenización es uno de los primeros pasos en el PLN, donde un texto se divide en partes más pequeñas llamadas tokens. Estos tokens pueden ser palabras, subpalabras o incluso caracteres, y se utilizan para facilitar el procesamiento y la comprensión del lenguaje por parte de los modelos de aprendizaje automático.\n",
      "Respuesta del modelo:\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['El Procesamiento del Lenguaje Natural (PLN) es un campo de la inteligencia artificial que permite a las máquinas entender y procesar el lenguaje humano, y se aplica en diversas áreas como chatbots, traducción automática y análisis de sentimientos.',\n",
       " 'Los modelos de lenguaje en PLN se entrenan con grandes cantidades de datos textuales para aprender patrones y relaciones, lo que les permite realizar tareas como responder preguntas, generar texto y entender la intención de un mensaje.',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Con la configuración escogida (`temperature = 1.2`, `top_p = 1.0`, `top_k = 0`), las diferentes llamadas a generate_resumen producen resultados notablemente diferentes.\n",
    "\n",
    "Con esta configuración, el modelo tiene un alto grado de aleatoriedad en la generación de texto que incrementa la creatividad de las respuestas, pero también puede generar respuestas incoherentes e inesperadas.\n",
    "\n",
    "En resumen, debido al alto grado de aleatoriedad en la configuración, los resultados tienden a ser muy diferentes entre sí en cada ejecución, lo que puede ser útil cuando se busca diversidad, pero también puede resultar en respuestas menos coherentes."
   ],
   "metadata": {
    "id": "TqUW2I_h6DC1"
   }
  }
 ]
}
