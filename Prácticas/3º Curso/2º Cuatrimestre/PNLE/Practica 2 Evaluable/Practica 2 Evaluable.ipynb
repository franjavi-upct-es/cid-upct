{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aed90a95",
      "metadata": {
        "id": "aed90a95"
      },
      "source": [
        "# Instalación de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d4265e55",
      "metadata": {
        "id": "d4265e55",
        "outputId": "713039b9-0c74-451b-b96c-aba664793027",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-26 21:31:52--  https://raw.githubusercontent.com/franjavi-upct-es/cid-upct/refs/heads/main/Pr%C3%A1cticas/3%C2%BA%20Curso/2%C2%BA%20Cuatrimestre/PNLE/Practica%202%20Evaluable/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4937 (4.8K) [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]   4.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-26 21:31:53 (74.1 MB/s) - ‘requirements.txt’ saved [4937/4937]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/franjavi-upct-es/cid-upct/refs/heads/main/Pr%C3%A1cticas/3%C2%BA%20Curso/2%C2%BA%20Cuatrimestre/PNLE/Practica%202%20Evaluable/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ntOn9Cz3txE-"
      },
      "id": "ntOn9Cz3txE-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4b60c868",
      "metadata": {
        "id": "4b60c868"
      },
      "source": [
        "# 1. Importaciones y configuración global\n",
        "\n",
        "**Descripción**\n",
        "\n",
        "Se importan las librerías necesarias para:\n",
        "- Operaciones con archivos, JSON y expresiones regulares.\n",
        "- Acceso a la API de Reddit (`praw`).\n",
        "- Procesamiento numérico (`numpy`).\n",
        "- Manejo de fechas.\n",
        "- Modelos y utilidades de `scikit-learn`, `fasttext`, `Hugging Face` y `SentenceTransformers`.\n",
        "- Stopwords y stemmer con `nltk`.\n",
        "Además, se definen subreddits, número de hilos/comentarios a extraer, y se crea la carpeta `data` para guardar los JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8c8d43",
      "metadata": {
        "id": "cf8c8d43",
        "outputId": "8e124a92-97da-427c-f5f0-a514e6a1032a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-7ea812452401>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# scikit-learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rec\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"char\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/char/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefchararray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__all__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefchararray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/defchararray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare_chararrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from numpy.strings import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mmultiply\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstrings_multiply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/strings/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__all__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/strings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from numpy._core.umath import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0misalpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0misdigit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import praw\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import fasttext\n",
        "import warnings\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# SBERT y similitud\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# NLTK para limpieza léxica\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Configuración básica\n",
        "SUBREDDITS = ['technology', 'programming', 'machinelearning',\n",
        "              'datascience', 'computerscience', 'gadgets']\n",
        "THREADS_PER_SUB = 20\n",
        "COMMENTS_PER_THREAD = 50\n",
        "JSON_DIR = 'data'\n",
        "os.makedirs(JSON_DIR, exist_ok=True)\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
        "STEMMER = SnowballStemmer('spanish')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "reddit = praw.Reddit(\n",
        "    client_id='ShOBXaW1U-PMc1hhr88znw',\n",
        "    client_secret='o12KLkUR18D5wZqnPxG5lp8jQFszgg',\n",
        "    user_agent='pln-practica-2025',\n",
        "    check_for_async=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc3d608",
      "metadata": {
        "id": "6cc3d608"
      },
      "source": [
        "# 2. Funciones de preprocesamiento de texto\n",
        "## 2.1 Conversión de timestamps\n",
        "\n",
        "Convierte segundos UNIX a formato legible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6299aefd",
      "metadata": {
        "id": "6299aefd"
      },
      "outputs": [],
      "source": [
        "def convertir_fecha(utc_timestamp):\n",
        "    \"\"\"\n",
        "    Convierte un timestamp UNIX a 'YYYY-MM-DD HH:MM:SS'.\n",
        "    \"\"\"\n",
        "    return datetime.fromtimestamp(utc_timestamp).strftime('%Y-%m-%d %H:%M:%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a286bf08",
      "metadata": {
        "id": "a286bf08"
      },
      "source": [
        "## 2.2 Limpieza léxica\n",
        "- Elimina URLs, meciones y enlaces Markdown.\n",
        "- Tokeniza, filtra stopwords y tokens muy cortos.\n",
        "- Aplica stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482a825a",
      "metadata": {
        "id": "482a825a"
      },
      "outputs": [],
      "source": [
        "LEXICAL_PATTERN = re.compile(r\"http\\S+|www\\.\\S+|\\[.*?\\]\\(.*?\\)|@[A-Za-z0-9_]+\")\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "    - Elimina URLs, menciones y markdown.\n",
        "    - Tokeniza en palabras, pasa a minúsculas.\n",
        "    - Filtra stopwords y tokens < 3 caracteres.\n",
        "    - Aplica stemming.\n",
        "    \"\"\"\n",
        "    texto_limpio = LEXICAL_PATTERN.sub('', texto)\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", texto_limpio.lower())\n",
        "    procesados = [\n",
        "        STEMMER.stem(tok)\n",
        "        for tok in tokens\n",
        "        if tok not in STOPWORDS and len(tok) > 2\n",
        "    ]\n",
        "    return \" \".join(procesados)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f2b58c",
      "metadata": {
        "id": "e0f2b58c"
      },
      "source": [
        "# 3. Extracción y guardado del corpus\n",
        "Recorre cada subreddit, extrae los hilos \"hot\" y hasta 50 comentarios por hilo, los preprocesa y guarda un JSON por subreddit en `data/corpus_<sr>.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6f5d05",
      "metadata": {
        "id": "ae6f5d05"
      },
      "outputs": [],
      "source": [
        "def extraer_y_guardar_corpus():\n",
        "    corpus = {}\n",
        "    for sr in SUBREDDITS:\n",
        "        hilos = []\n",
        "        for post in reddit.subreddit(sr).hot(limit=THREADS_PER_SUB):\n",
        "            hilo = {\n",
        "                'title': post.title,\n",
        "                'flair': post.link_flair_text,\n",
        "                'author': str(post.author),\n",
        "                'date': convertir_fecha(post.created_utc),\n",
        "                'score': post.score,\n",
        "                'description': limpiar_texto(post.selftext),\n",
        "                'comments': []\n",
        "            }\n",
        "            post.comments.replace_more(limit=0)\n",
        "            for c in post.comments.list()[:COMMENTS_PER_THREAD]:\n",
        "                hilo['comments'].append({\n",
        "                    'user': str(c.author),\n",
        "                    'comment': limpiar_texto(c.body),\n",
        "                    'score': c.score,\n",
        "                    'date': convertir_fecha(c.created_utc)\n",
        "                })\n",
        "            hilos.append(hilo)\n",
        "        ruta = os.path.join(JSON_DIR, f'corpus_{sr}.json')\n",
        "        with open(ruta, 'w', encoding='utf-8') as f:\n",
        "            json.dump(hilos, f, ensure_ascii=False, indent=4)\n",
        "        corpus[sr] = hilos\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e8cabb",
      "metadata": {
        "id": "d4e8cabb"
      },
      "source": [
        "# 4. Prepraración de datos y clasificación\n",
        "## 4.1 Separación por hilo\n",
        "Evita fugas de información: 14 hilos para entrenamiento y 6 para validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4c860f",
      "metadata": {
        "id": "1c4c860f"
      },
      "outputs": [],
      "source": [
        "def split_by_thread(corpus):\n",
        "    X_train, y_train, X_val, y_val = [], [], [], []\n",
        "    for sr, threads in corpus.items():\n",
        "        train_threads, val_threads = threads[:14], threads[14:]\n",
        "        for th in train_threads:\n",
        "            for c in th['comments']:\n",
        "                X_train.append(c['comment']); y_train.append(sr)\n",
        "        for th in val_threads:\n",
        "            for c in th['comments']:\n",
        "                X_val.append(c['comment']); y_val.append(sr)\n",
        "    return X_train, y_train, X_val, y_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75b7a97",
      "metadata": {
        "id": "e75b7a97"
      },
      "source": [
        "## 4.2 Modelos baseline\n",
        "1. **TF-IDF + RandomForest**\n",
        "2. **FastText embeddings + SVM lineal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9d6cd6",
      "metadata": {
        "id": "0e9d6cd6"
      },
      "outputs": [],
      "source": [
        "def train_baselines(X_train, y_train, X_val, y_val):\n",
        "    # TF-IDF + Random Forest\n",
        "    vec = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "    X_tr_vec = vec.fit_transform(X_train)\n",
        "    X_val_vec = vec.transform(X_val)\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "    rf.fit(X_tr_vec, y_train)\n",
        "    preds_rf = rf.predict(X_val_vec)\n",
        "    print(\"--- TF-IDF + Random Forest ---\")\n",
        "    print(classification_report(y_val, preds_rf))\n",
        "    print(confusion_matrix(y_val, preds_rf))\n",
        "\n",
        "    # fastText oficial + SVM\n",
        "    with tempfile.NamedTemporaryFile('w+', delete=False, encoding='utf-8') as tmp:\n",
        "        for doc in X_train:\n",
        "            tmp.write(doc + '\\n')\n",
        "        tmp_path = tmp.name\n",
        "    ft_model = fasttext.train_unsupervised(tmp_path, model='skipgram', dim=100, ws=5, minCount=2)\n",
        "\n",
        "    def embed_docs(docs):\n",
        "        return np.vstack([ft_model.get_sentence_vector(doc) for doc in docs])\n",
        "\n",
        "    X_tr_ft = embed_docs(X_train)\n",
        "    X_val_ft = embed_docs(X_val)\n",
        "    svm = SVC(kernel='linear', probability=True)\n",
        "    svm.fit(X_tr_ft, y_train)\n",
        "    preds_svm = svm.predict(X_val_ft)\n",
        "    print(\"--- fastText oficial + SVM ---\")\n",
        "    print(classification_report(y_val, preds_svm))\n",
        "    print(confusion_matrix(y_val, preds_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "116548db",
      "metadata": {
        "id": "116548db"
      },
      "source": [
        "## 4.3 Fine-tuning con Transformers\n",
        "\n",
        "Utiliza BERT multilingüe para clasificación. Carga el modelo pre-entrenado si existe, si no lo entrena y guarda en `finetune/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79edf883",
      "metadata": {
        "id": "79edf883"
      },
      "outputs": [],
      "source": [
        "def train_transformer(X_train, y_train, X_val, y_val):\n",
        "    from pathlib import Path\n",
        "    finetune_dir = 'finetune'\n",
        "    if Path(finetune_dir).exists():\n",
        "        print(\"Cargando modelo ya entrenado…\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(finetune_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(finetune_dir)\n",
        "    else:\n",
        "        print(\"Entrenando modelo desde cero…\")\n",
        "        datos = {'text': X_train + X_val, 'label': y_train + y_val}\n",
        "        ds = Dataset.from_dict(datos).class_encode_column('label')\n",
        "        train_ds, val_ds = ds.train_test_split(\n",
        "            test_size=len(X_val)/len(ds)\n",
        "        ).values()\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "        def tokenize_fn(batch):\n",
        "            return tokenizer(\n",
        "                batch['text'], padding='max_length',\n",
        "                truncation=True, max_length=128\n",
        "            )\n",
        "        train_ds = train_ds.map(tokenize_fn, batched=True) \\\n",
        "                         .rename_column('label','labels')\n",
        "        val_ds   = val_ds.map(tokenize_fn, batched=True)   \\\n",
        "                         .rename_column('label','labels')\n",
        "\n",
        "        collator = DataCollatorWithPadding(tokenizer)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            'bert-base-multilingual-uncased',\n",
        "            num_labels=len(set(y_train))\n",
        "        )\n",
        "        args = TrainingArguments(\n",
        "            output_dir=finetune_dir, num_train_epochs=3,\n",
        "            per_device_train_batch_size=16, per_gpu_eval_batch_size=16,\n",
        "            eval_strategy='epoch', save_strategy='epoch',\n",
        "            load_best_model_at_end=True\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=model, args=args,\n",
        "            train_dataset=train_ds, eval_dataset=val_ds,\n",
        "            tokenizer=tokenizer, data_collator=collator\n",
        "        )\n",
        "        trainer.train()\n",
        "        model.save_pretrained(finetune_dir)\n",
        "        tokenizer.save_pretrained(finetune_dir)\n",
        "\n",
        "    # Evaluación\n",
        "    datos = {'text': X_val, 'label': y_val}\n",
        "    val_ds = Dataset.from_dict(datos).class_encode_column('label')\n",
        "    val_ds = val_ds.map(\n",
        "        lambda b: tokenizer(\n",
        "            b['text'], padding='max_length',\n",
        "            truncation=True, max_length=128\n",
        "        ), batched=True\n",
        "    ).rename_column('label','labels')\n",
        "    res = Trainer(model=model).predict(val_ds)\n",
        "    preds = np.argmax(res.predictions, axis=1)\n",
        "    print('--- BERT Fine-Tuning ---')\n",
        "    print(classification_report(val_ds['labels'], preds))\n",
        "    print(confusion_matrix(val_ds['labels'], preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c399c8",
      "metadata": {
        "id": "f6c399c8"
      },
      "source": [
        "# 5. Similitud de hilos\n",
        "## 5.1 FastText\n",
        "Embedding promedio de comentarios por cada hilo y similitud coseno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab87e0ea",
      "metadata": {
        "id": "ab87e0ea"
      },
      "outputs": [],
      "source": [
        "def buscar_hilos_similares_fasttext(corpus, top_k=5):\n",
        "    # Prepara un archivo temporal con cada hilo como línea\n",
        "    with tempfile.NamedTemporaryFile('w+', delete=False, encoding='utf-8') as tmp:\n",
        "        for threads in corpus.values():\n",
        "            for hilo in threads:\n",
        "                comments_text = ' '.join(c['comment'] for c in hilo['comments'])\n",
        "                tmp.write(comments_text + '\\n')\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "    ft_model = fasttext.train_unsupervised(tmp_path, model='skipgram', dim=100, ws=5, minCount=2)\n",
        "    ids, vectors = [], []\n",
        "    for sr, threads in corpus.items():\n",
        "        for idx, hilo in enumerate(threads):\n",
        "            ids.append((sr, idx))\n",
        "            text = ' '.join(c['comment'] for c in hilo['comments'])\n",
        "            vectors.append(ft_model.get_sentence_vector(text))\n",
        "    sims = cosine_similarity(vectors)\n",
        "    similares = {\n",
        "        ids[i]: [\n",
        "            (ids[j], float(sims[i][j]))\n",
        "            for j in np.argsort(sims[i])[-top_k-1:-1][::-1]\n",
        "        ]\n",
        "        for i in range(len(ids))\n",
        "    }\n",
        "    return similares"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d2a00f",
      "metadata": {
        "id": "20d2a00f"
      },
      "source": [
        "## 5.2 SBERT\n",
        "Embeddings de título + comentarios con SentenceTransformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7898b9c7",
      "metadata": {
        "id": "7898b9c7"
      },
      "outputs": [],
      "source": [
        "def buscar_hilos_similares_sbert(corpus, model_name='all-MiniLM-L6-v2', top_k=5):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    ids, texts = [], []\n",
        "    for sr, threads in corpus.items():\n",
        "        for idx, hilo in enumerate(threads):\n",
        "            ids.append((sr, idx))\n",
        "            combined = hilo['title'] + ' ' + ' '.join(\n",
        "                c['comment'] for c in hilo['comments']\n",
        "            )\n",
        "            texts.append(combined)\n",
        "    embs = model.encode(texts)\n",
        "    sims = cosine_similarity(embs)\n",
        "    similares = {\n",
        "        ids[i]: [\n",
        "            (ids[j], float(sims[i][j]))\n",
        "            for j in np.argsort(sims[i])[-top_k-1:-1][::-1]\n",
        "        ]\n",
        "        for i in range(len(ids))\n",
        "    }\n",
        "    return similares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5093bc",
      "metadata": {
        "id": "ea5093bc"
      },
      "source": [
        "# 6. Análisis de sentimiento y resumen automático\n",
        "## 6.1 Sentimiento y emoción\n",
        "Pipelines de Hugging Face para sentimiento (`finiteautomata/beto-sentiment-analysis`) y emoción (`pysentimiento/robertuito-emotion-analysis`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af0c609",
      "metadata": {
        "id": "5af0c609"
      },
      "outputs": [],
      "source": [
        "def analisis_sentimiento(corpus):\n",
        "    sent_pipe = pipeline(\n",
        "        'sentiment-analysis',\n",
        "        model='finiteautomata/beto-sentiment-analysis',\n",
        "        truncation=True, max_length=128\n",
        "    )\n",
        "    emo_pipe = pipeline(\n",
        "        'text-classification',\n",
        "        model='pysentimiento/robertuito-emotion-analysis',\n",
        "        return_all_scores=True,\n",
        "        truncation=True, max_length=128\n",
        "    )\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            for c in hilo['comments']:\n",
        "                text = c['comment'][:512]\n",
        "                s = sent_pipe(text)[0]\n",
        "                e = emo_pipe(text)[0]\n",
        "                c['sentiment'] = s['label']\n",
        "                c['sentiment_score'] = s['score']\n",
        "                c['emotion'] = {item['label']: item['score'] for item in e}\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e7659f",
      "metadata": {
        "id": "80e7659f"
      },
      "source": [
        "## 6.2 Resumen preentrenado y zero-shot\n",
        "- **mT5 multilingual XLSum**\n",
        "- **Flan-T5 small**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e60c2a3",
      "metadata": {
        "id": "0e60c2a3"
      },
      "outputs": [],
      "source": [
        "def resumen_preentrenado(corpus, model_name='csebuetnlp/mT5_multilingual_XLSum'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            inp = hilo['title'] + \": \" + hilo['description']\n",
        "            tokens = tokenizer(inp, return_tensors='pt',\n",
        "                               truncation=True, max_length=512)\n",
        "            out = model.generate(**tokens, max_length=100, num_beams=4)\n",
        "            hilo['summary_pretrained'] = tokenizer.decode(\n",
        "                out[0], skip_special_tokens=True\n",
        "            )\n",
        "    return corpus\n",
        "\n",
        "def resumen_zero_shot(corpus, model_name='google/flan-t5-small'):\n",
        "    zsl_pipe = pipeline('text2text-generation', model=model_name)\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            prompt = f\"Resume: {hilo['title']}. {hilo['description']}\"\n",
        "            gen = zsl_pipe(prompt, max_length=100)[0]\n",
        "            hilo['summary_zero_shot'] = gen['generated_text']\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8590f4fe",
      "metadata": {
        "id": "8590f4fe"
      },
      "source": [
        "# 7. Detección de contenido inapropiado\n",
        "## 7.1 Zero-shot + Chain-of-thought\n",
        "Utiliza BART-MNLI para clasificación y Flan-T5 para explicar razonamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c8f22f",
      "metadata": {
        "id": "12c8f22f"
      },
      "outputs": [],
      "source": [
        "def deteccion_inapropiado(corpus):\n",
        "    zsl_cls = pipeline('zero-shot-classification',\n",
        "                       model='facebook/bart-large-mnli')\n",
        "    cot_pipe = pipeline('text2text-generation',\n",
        "                        model='google/flan-t5-small')\n",
        "    labels = ['apropiado', 'inapropiado']\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            for c in hilo['comments']:\n",
        "                res = zsl_cls(c['comment'], labels)\n",
        "                c['zs_label'] = res['labels'][0]\n",
        "                c['zs_score'] = res['scores'][0]\n",
        "                prompt = (\n",
        "                    f\"Evalúa si este comentario contiene lenguaje inapropiado. \"\n",
        "                    f\"Primero explica tu razonamiento y luego clasifica. \"\n",
        "                    f\"Comentario: {c['comment']}\"\n",
        "                )\n",
        "                c['cot_output'] = cot_pipe(prompt, max_new_tokens=50)[0]['generated_text']\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6e8674",
      "metadata": {
        "id": "ca6e8674"
      },
      "source": [
        "## 7.2 Few-Shot para r/OpinionesPolemicas\n",
        "Inyecta ejemplos manuales en el prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494b67ff",
      "metadata": {
        "id": "494b67ff"
      },
      "outputs": [],
      "source": [
        "FEW_SHOT_EXAMPLES = [\n",
        "    ('Este comentario es ofensivo y soez','inapropiado'),\n",
        "    ('¡Me encanta esta publicación!','apropiado'),\n",
        "    ('Qué horror, no soporto esto.','apropiado'),\n",
        "]\n",
        "\n",
        "def deteccion_inapropiado_fsl(corpus):\n",
        "    zsl_pipe = pipeline('zero-shot-classification',\n",
        "                       model='facebook/bart-large-mnli')\n",
        "    prompt_fsl = 'Clasifica como apropiado o inapropiado:\\n' + \"\".join([\n",
        "        f'Ejemplo: {ex[0]} -> {ex[1]}.\\n' for ex in FEW_SHOT_EXAMPLES\n",
        "    ])\n",
        "    results = {}\n",
        "    for sr, threads in corpus.items():\n",
        "        if sr != 'OpinionesPolemicas':\n",
        "            continue\n",
        "        for idx, hilo in enumerate(threads[:10]):\n",
        "            for c in hilo['comments']:\n",
        "                text = f\"{prompt_fsl}Comentario: {c['comment']} ->\"\n",
        "                r = zsl_pipe(text, candidate_labels=['apropiado','inapropiado'])\n",
        "                results[(sr, idx, c['date'])] = {\n",
        "                    'label_zsl': r['labels'][0],\n",
        "                    'score': r['scores'][0]\n",
        "                }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5ad74e",
      "metadata": {
        "id": "9e5ad74e"
      },
      "source": [
        "# 8. Ejecución del código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065e18ae",
      "metadata": {
        "id": "065e18ae"
      },
      "outputs": [],
      "source": [
        "# 1) Extracción\n",
        "corpus = extraer_y_guardar_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e43a3c",
      "metadata": {
        "id": "e6e43a3c"
      },
      "outputs": [],
      "source": [
        "# 2) Clasificación\n",
        "X_train, y_train, X_val, y_val = split_by_thread(corpus)\n",
        "train_baselines(X_train, y_train, X_val, y_val)\n",
        "train_transformer(X_train, y_train, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec4a9bdf",
      "metadata": {
        "id": "ec4a9bdf"
      },
      "outputs": [],
      "source": [
        "# 3) Similitud\n",
        "sims_ft = buscar_hilos_similares_fasttext(corpus)\n",
        "sims_sbert = buscar_hilos_similares_sbert(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51b74d0",
      "metadata": {
        "id": "d51b74d0"
      },
      "outputs": [],
      "source": [
        "# 4) Sentimiento y emoción\n",
        "corpus = analisis_sentimiento(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ad0320",
      "metadata": {
        "id": "65ad0320"
      },
      "outputs": [],
      "source": [
        "# 5) Resúmenes\n",
        "corpus = resumen_preentrenado(corpus)\n",
        "corpus = resumen_zero_shot(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635dd385",
      "metadata": {
        "id": "635dd385"
      },
      "outputs": [],
      "source": [
        "# 6) Detección inapropiado\n",
        "corpus = deteccion_inapropiado(corpus)\n",
        "inap_fsl = deteccion_inapropiado_fsl(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd8b0be",
      "metadata": {
        "id": "8bd8b0be"
      },
      "outputs": [],
      "source": [
        "# Guardar análisis extra\n",
        "with open(os.path.join(JSON_DIR,'analysis_extras.json'),\n",
        "          'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        'sims_ft': sims_ft,\n",
        "        'sims_sbert': sims_sbert,\n",
        "        'inap_fsl': inap_fsl\n",
        "    }, f, ensure_ascii=False, indent=4)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}