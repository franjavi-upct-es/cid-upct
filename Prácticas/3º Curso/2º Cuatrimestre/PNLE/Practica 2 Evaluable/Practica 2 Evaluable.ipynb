{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed90a95",
   "metadata": {},
   "source": [
    "# Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4265e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw numpy scikit-learn gensim transformers datasets sentence-transformers nltk torch sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60c868",
   "metadata": {},
   "source": [
    "# 1. Importaciones y configuración global\n",
    "\n",
    "**Descripción**\n",
    "\n",
    "Se importan las librerías necesarias para:\n",
    "- Operaciones con archivos, JSON y expresiones regulares.\n",
    "- Acceso a la API de Reddit (`praw`).\n",
    "- Procesamiento numérico (`numpy`).\n",
    "- Manejo de fechas.\n",
    "- Modelos y utilidades de `scikit-learn`, `gensim`, `Hugging Face` y `SentenceTransformers`.\n",
    "- Stopwords y stemmer con `nltk`.\n",
    "Además, se definen subreddits, número de hilos/comentarios a extraer, y se crea la carpeta `data` para guardar los JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import praw\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# FastText embeddings\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# SBERT y similitud\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLTK para limpieza léxica\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Configuración básica\n",
    "SUBREDDITS = ['technology', 'programming', 'machinelearning',\n",
    "              'datascience', 'computerscience', 'gadgets']\n",
    "THREADS_PER_SUB = 20\n",
    "COMMENTS_PER_THREAD = 50\n",
    "JSON_DIR = 'data'\n",
    "os.makedirs(JSON_DIR, exist_ok=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
    "STEMMER = SnowballStemmer('spanish')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "reddit = praw.Reddit(\n",
    "    client_id='ShOBXaW1U-PMc1hhr88znw',\n",
    "    client_secret='o12KLkUR18D5wZqnPxG5lp8jQFszgg',\n",
    "    user_agent='pln-practica-2025'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3d608",
   "metadata": {},
   "source": [
    "# 2. Funciones de preprocesamiento de texto\n",
    "## 2.1 Conversión de timestamps\n",
    "\n",
    "Convierte segundos UNIX a formato legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_fecha(utc_timestamp):\n",
    "    \"\"\"\n",
    "    Convierte un timestamp UNIX a 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    return datetime.fromtimestamp(utc_timestamp).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286bf08",
   "metadata": {},
   "source": [
    "## 2.2 Limpieza léxica\n",
    "- Elimina URLs, meciones y enlaces Markdown.\n",
    "- Tokeniza, filtra stopwords y tokens muy cortos.\n",
    "- Aplica stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXICAL_PATTERN = re.compile(r\"http\\S+|www\\.\\S+|\\[.*?\\]\\(.*?\\)|@[A-Za-z0-9_]+\")\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    - Elimina URLs, menciones y markdown.\n",
    "    - Tokeniza en palabras, pasa a minúsculas.\n",
    "    - Filtra stopwords y tokens < 3 caracteres.\n",
    "    - Aplica stemming.\n",
    "    \"\"\"\n",
    "    texto_limpio = LEXICAL_PATTERN.sub('', texto)\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", texto_limpio.lower())\n",
    "    procesados = [\n",
    "        STEMMER.stem(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in STOPWORDS and len(tok) > 2\n",
    "    ]\n",
    "    return \" \".join(procesados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2b58c",
   "metadata": {},
   "source": [
    "# 3. Extracción y guardado del corpus\n",
    "Recorre cada subreddit, extrae los hilos \"hot\" y hasta 50 comentarios por hilo, los preprocesa y guarda un JSON por subreddit en `data/corpus_<sr>.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_y_guardar_corpus():\n",
    "    corpus = {}\n",
    "    for sr in SUBREDDITS:\n",
    "        hilos = []\n",
    "        for post in reddit.subreddit(sr).hot(limit=THREADS_PER_SUB):\n",
    "            hilo = {\n",
    "                'title': post.title,\n",
    "                'flair': post.link_flair_text,\n",
    "                'author': str(post.author),\n",
    "                'date': convertir_fecha(post.created_utc),\n",
    "                'score': post.score,\n",
    "                'description': limpiar_texto(post.selftext),\n",
    "                'comments': []\n",
    "            }\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for c in post.comments.list()[:COMMENTS_PER_THREAD]:\n",
    "                hilo['comments'].append({\n",
    "                    'user': str(c.author),\n",
    "                    'comment': limpiar_texto(c.body),\n",
    "                    'score': c.score,\n",
    "                    'date': convertir_fecha(c.created_utc)\n",
    "                })\n",
    "            hilos.append(hilo)\n",
    "        ruta = os.path.join(JSON_DIR, f'corpus_{sr}.json')\n",
    "        with open(ruta, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hilos, f, ensure_ascii=False, indent=4)\n",
    "        corpus[sr] = hilos\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8cabb",
   "metadata": {},
   "source": [
    "# 4. Prepraración de datos y clasificación\n",
    "## 4.1 Separación por hilo\n",
    "Evita fugas de información: 14 hilos para entrenamiento y 6 para validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_thread(corpus):\n",
    "    X_train, y_train, X_val, y_val = [], [], [], []\n",
    "    for sr, threads in corpus.items():\n",
    "        train_threads, val_threads = threads[:14], threads[14:]\n",
    "        for th in train_threads:\n",
    "            for c in th['comments']:\n",
    "                X_train.append(c['comment']); y_train.append(sr)\n",
    "        for th in val_threads:\n",
    "            for c in th['comments']:\n",
    "                X_val.append(c['comment']); y_val.append(sr)\n",
    "    return X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b7a97",
   "metadata": {},
   "source": [
    "## 4.2 Modelos baseline\n",
    "1. **TF-IDF + RandomForest**\n",
    "2. **FastText embeddings + SVM lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baselines(X_train, y_train, X_val, y_val):\n",
    "    # TF-IDF + Random Forest\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "    X_tr_vec = vec.fit_transform(X_train)\n",
    "    X_val_vec = vec.transform(X_val)\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "    rf.fit(X_tr_vec, y_train)\n",
    "    preds_rf = rf.predict(X_val_vec)\n",
    "    print(\"--- TF-IDF + Random Forest ---\")\n",
    "    print(classification_report(y_val, preds_rf))\n",
    "    print(confusion_matrix(y_val, preds_rf))\n",
    "\n",
    "    # FastText + SVM\n",
    "    ft_model = FastText(\n",
    "        sentences=[doc.split() for doc in X_train],\n",
    "        vector_size=100, window=5, min_count=2, workers=4\n",
    "    )\n",
    "    def embed_docs(docs):\n",
    "        embs = []\n",
    "        for doc in docs:\n",
    "            vecs = [ft_model.wv[w] for w in doc.split() if w in ft_model.wv]\n",
    "            embs.append(np.mean(vecs, axis=0) if vecs else np.zeros(100))\n",
    "        return np.vstack(embs)\n",
    "\n",
    "    X_tr_ft = embed_docs(X_train)\n",
    "    X_val_ft = embed_docs(X_val)\n",
    "    svm = SVC(kernel='linear', probability=True)\n",
    "    svm.fit(X_tr_ft, y_train)\n",
    "    preds_svm = svm.predict(X_val_ft)\n",
    "    print(\"--- FastText + SVM ---\")\n",
    "    print(classification_report(y_val, preds_svm))\n",
    "    print(confusion_matrix(y_val, preds_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116548db",
   "metadata": {},
   "source": [
    "## 4.3 Fine-tuning con Transformers\n",
    "\n",
    "Utiliza BERT multilingüe para clasificación. Carga el modelo pre-entrenado si existe, si no lo entrena y guarda en `finetune/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79edf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(X_train, y_train, X_val, y_val):\n",
    "    from pathlib import Path\n",
    "    finetune_dir = 'finetune'\n",
    "    if Path(finetune_dir).exists():\n",
    "        print(\"Cargando modelo ya entrenado…\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(finetune_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(finetune_dir)\n",
    "    else:\n",
    "        print(\"Entrenando modelo desde cero…\")\n",
    "        datos = {'text': X_train + X_val, 'label': y_train + y_val}\n",
    "        ds = Dataset.from_dict(datos).class_encode_column('label')\n",
    "        train_ds, val_ds = ds.train_test_split(\n",
    "            test_size=len(X_val)/len(ds)\n",
    "        ).values()\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "        def tokenize_fn(batch):\n",
    "            return tokenizer(\n",
    "                batch['text'], padding='max_length',\n",
    "                truncation=True, max_length=128\n",
    "            )\n",
    "        train_ds = train_ds.map(tokenize_fn, batched=True) \\\n",
    "                         .rename_column('label','labels')\n",
    "        val_ds   = val_ds.map(tokenize_fn, batched=True)   \\\n",
    "                         .rename_column('label','labels')\n",
    "\n",
    "        collator = DataCollatorWithPadding(tokenizer)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            'bert-base-multilingual-uncased',\n",
    "            num_labels=len(set(y_train))\n",
    "        )\n",
    "        args = TrainingArguments(\n",
    "            output_dir=finetune_dir, num_train_epochs=3,\n",
    "            per_device_train_batch_size=16, per_gpu_eval_batch_size=16,\n",
    "            eval_strategy='epoch', save_strategy='epoch',\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model, args=args,\n",
    "            train_dataset=train_ds, eval_dataset=val_ds,\n",
    "            tokenizer=tokenizer, data_collator=collator\n",
    "        )\n",
    "        trainer.train()\n",
    "        model.save_pretrained(finetune_dir)\n",
    "        tokenizer.save_pretrained(finetune_dir)\n",
    "\n",
    "    # Evaluación\n",
    "    datos = {'text': X_val, 'label': y_val}\n",
    "    val_ds = Dataset.from_dict(datos).class_encode_column('label')\n",
    "    val_ds = val_ds.map(\n",
    "        lambda b: tokenizer(\n",
    "            b['text'], padding='max_length',\n",
    "            truncation=True, max_length=128\n",
    "        ), batched=True\n",
    "    ).rename_column('label','labels')\n",
    "    res = Trainer(model=model).predict(val_ds)\n",
    "    preds = np.argmax(res.predictions, axis=1)\n",
    "    print('--- BERT Fine-Tuning ---')\n",
    "    print(classification_report(val_ds['labels'], preds))\n",
    "    print(confusion_matrix(val_ds['labels'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c399c8",
   "metadata": {},
   "source": [
    "# 5. Similitud de hilos\n",
    "## 5.1 FastText\n",
    "Embedding promedio de comentarios por cada hilo y similitud coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_hilos_similares_fasttext(corpus, top_k=5):\n",
    "    ids, vectors = [], []\n",
    "    ft = FastText(\n",
    "        sentences=[c['comment'].split()\n",
    "                   for threads in corpus.values()\n",
    "                   for c in threads['comments']],\n",
    "        vector_size=100, window=5, min_count=2, workers=4\n",
    "    )\n",
    "    for sr, threads in corpus.items():\n",
    "        for idx, hilo in enumerate(threads):\n",
    "            ids.append((sr, idx))\n",
    "            doc_vecs = [\n",
    "                ft.wv[w]\n",
    "                for c in hilo['comments']\n",
    "                for w in c['comment'].split()\n",
    "                if w in ft.wv\n",
    "            ]\n",
    "            vectors.append(np.mean(doc_vecs, axis=0) if doc_vecs else np.zeros(100))\n",
    "    sims = cosine_similarity(vectors)\n",
    "    similares = {\n",
    "        ids[i]: [\n",
    "            (ids[j], float(sims[i][j]))\n",
    "            for j in np.argsort(sims[i])[-top_k-1:-1][::-1]\n",
    "        ]\n",
    "        for i in range(len(ids))\n",
    "    }\n",
    "    return similares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2a00f",
   "metadata": {},
   "source": [
    "## 5.2 SBERT\n",
    "Embeddings de título + comentarios con SentenceTransformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_hilos_similares_sbert(corpus, model_name='all-MiniLM-L6-v2', top_k=5):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    ids, texts = [], []\n",
    "    for sr, threads in corpus.items():\n",
    "        for idx, hilo in enumerate(threads):\n",
    "            ids.append((sr, idx))\n",
    "            combined = hilo['title'] + ' ' + ' '.join(\n",
    "                c['comment'] for c in hilo['comments']\n",
    "            )\n",
    "            texts.append(combined)\n",
    "    embs = model.encode(texts)\n",
    "    sims = cosine_similarity(embs)\n",
    "    similares = {\n",
    "        ids[i]: [\n",
    "            (ids[j], float(sims[i][j]))\n",
    "            for j in np.argsort(sims[i])[-top_k-1:-1][::-1]\n",
    "        ]\n",
    "        for i in range(len(ids))\n",
    "    }\n",
    "    return similares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5093bc",
   "metadata": {},
   "source": [
    "# 6. Análisis de sentimiento y resumen automático\n",
    "## 6.1 Sentimiento y emoción\n",
    "Pipelines de Hugging Face para sentimiento (`finiteautomata/beto-sentiment-analysis`) y emoción (`pysentimiento/robertuito-emotion-analysis`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_sentimiento(corpus):\n",
    "    sent_pipe = pipeline(\n",
    "        'sentiment-analysis',\n",
    "        model='finiteautomata/beto-sentiment-analysis',\n",
    "        truncation=True, max_length=128\n",
    "    )\n",
    "    emo_pipe = pipeline(\n",
    "        'text-classification',\n",
    "        model='pysentimiento/robertuito-emotion-analysis',\n",
    "        return_all_scores=True,\n",
    "        truncation=True, max_length=128\n",
    "    )\n",
    "    for threads in corpus.values():\n",
    "        for hilo in threads:\n",
    "            for c in hilo['comments']:\n",
    "                text = c['comment'][:512]\n",
    "                s = sent_pipe(text)[0]\n",
    "                e = emo_pipe(text)[0]\n",
    "                c['sentiment'] = s['label']\n",
    "                c['sentiment_score'] = s['score']\n",
    "                c['emotion'] = {item['label']: item['score'] for item in e}\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e7659f",
   "metadata": {},
   "source": [
    "## 6.2 Resumen preentrenado y zero-shot\n",
    "- **mT5 multilingual XLSum**\n",
    "- **Flan-T5 small**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumen_preentrenado(corpus, model_name='csebuetnlp/mT5_multilingual_XLSum'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    for threads in corpus.values():\n",
    "        for hilo in threads:\n",
    "            inp = hilo['title'] + \": \" + hilo['description']\n",
    "            tokens = tokenizer(inp, return_tensors='pt',\n",
    "                               truncation=True, max_length=512)\n",
    "            out = model.generate(**tokens, max_length=100, num_beams=4)\n",
    "            hilo['summary_pretrained'] = tokenizer.decode(\n",
    "                out[0], skip_special_tokens=True\n",
    "            )\n",
    "    return corpus\n",
    "\n",
    "def resumen_zero_shot(corpus, model_name='google/flan-t5-small'):\n",
    "    zsl_pipe = pipeline('text2text-generation', model=model_name)\n",
    "    for threads in corpus.values():\n",
    "        for hilo in threads:\n",
    "            prompt = f\"Resume: {hilo['title']}. {hilo['description']}\"\n",
    "            gen = zsl_pipe(prompt, max_length=100)[0]\n",
    "            hilo['summary_zero_shot'] = gen['generated_text']\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590f4fe",
   "metadata": {},
   "source": [
    "# 7. Detección de contenido inapropiado\n",
    "## 7.1 Zero-shot + Chain-of-thought\n",
    "Utiliza BART-MNLI para clasificación y Flan-T5 para explicar razonamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deteccion_inapropiado(corpus):\n",
    "    zsl_cls = pipeline('zero-shot-classification',\n",
    "                       model='facebook/bart-large-mnli')\n",
    "    cot_pipe = pipeline('text2text-generation',\n",
    "                        model='google/flan-t5-small')\n",
    "    labels = ['apropiado', 'inapropiado']\n",
    "    for threads in corpus.values():\n",
    "        for hilo in threads:\n",
    "            for c in hilo['comments']:\n",
    "                res = zsl_cls(c['comment'], labels)\n",
    "                c['zs_label'] = res['labels'][0]\n",
    "                c['zs_score'] = res['scores'][0]\n",
    "                prompt = (\n",
    "                    f\"Evalúa si este comentario contiene lenguaje inapropiado. \"\n",
    "                    f\"Primero explica tu razonamiento y luego clasifica. \"\n",
    "                    f\"Comentario: {c['comment']}\"\n",
    "                )\n",
    "                c['cot_output'] = cot_pipe(prompt, max_new_tokens=50)[0]['generated_text']\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e8674",
   "metadata": {},
   "source": [
    "## 7.2 Few-Shot para r/OpinionesPolemicas\n",
    "Inyecta ejemplos manuales en el prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_EXAMPLES = [\n",
    "    ('Este comentario es ofensivo y soez','inapropiado'),\n",
    "    ('¡Me encanta esta publicación!','apropiado'),\n",
    "    ('Qué horror, no soporto esto.','apropiado'),\n",
    "]\n",
    "\n",
    "def deteccion_inapropiado_fsl(corpus):\n",
    "    zsl_pipe = pipeline('zero-shot-classification',\n",
    "                       model='facebook/bart-large-mnli')\n",
    "    prompt_fsl = 'Clasifica como apropiado o inapropiado:\\n' + \"\".join([\n",
    "        f'Ejemplo: {ex[0]} -> {ex[1]}.\\n' for ex in FEW_SHOT_EXAMPLES\n",
    "    ])\n",
    "    results = {}\n",
    "    for sr, threads in corpus.items():\n",
    "        if sr != 'OpinionesPolemicas':\n",
    "            continue\n",
    "        for idx, hilo in enumerate(threads[:10]):\n",
    "            for c in hilo['comments']:\n",
    "                text = f\"{prompt_fsl}Comentario: {c['comment']} ->\"\n",
    "                r = zsl_pipe(text, candidate_labels=['apropiado','inapropiado'])\n",
    "                results[(sr, idx, c['date'])] = {\n",
    "                    'label_zsl': r['labels'][0],\n",
    "                    'score': r['scores'][0]\n",
    "                }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ad74e",
   "metadata": {},
   "source": [
    "# 8. Ejecución del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Extracción\n",
    "corpus = extraer_y_guardar_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e43a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Clasificación\n",
    "X_train, y_train, X_val, y_val = split_by_thread(corpus)\n",
    "train_baselines(X_train, y_train, X_val, y_val)\n",
    "train_transformer(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Similitud\n",
    "sims_ft = buscar_hilos_similares_fasttext(corpus)\n",
    "sims_sbert = buscar_hilos_similares_sbert(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Sentimiento y emoción\n",
    "corpus = analisis_sentimiento(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Resúmenes\n",
    "corpus = resumen_preentrenado(corpus)\n",
    "corpus = resumen_zero_shot(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635dd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Detección inapropiado\n",
    "corpus = deteccion_inapropiado(corpus)\n",
    "inap_fsl = deteccion_inapropiado_fsl(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar análisis extra\n",
    "with open(os.path.join(JSON_DIR,'analysis_extras.json'),\n",
    "          'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'sims_ft': sims_ft,\n",
    "        'sims_sbert': sims_sbert,\n",
    "        'inap_fsl': inap_fsl\n",
    "    }, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
