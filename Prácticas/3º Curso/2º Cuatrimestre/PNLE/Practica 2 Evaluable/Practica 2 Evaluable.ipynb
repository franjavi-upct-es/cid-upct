{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aed90a95",
      "metadata": {
        "id": "aed90a95"
      },
      "source": [
        "# Instalación de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d4265e55",
      "metadata": {
        "id": "d4265e55",
        "outputId": "6e1aeb1b-0180-475e-f07c-a3db6b51f0bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-26 15:51:10--  https://raw.githubusercontent.com/franjavi-upct-es/cid-upct/refs/heads/main/Pr%C3%A1cticas/3%C2%BA%20Curso/2%C2%BA%20Cuatrimestre/PNLE/Practica%202%20Evaluable/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5804 (5.7K) [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]   5.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-26 15:51:11 (43.6 MB/s) - ‘requirements.txt’ saved [5804/5804]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/franjavi-upct-es/cid-upct/refs/heads/main/Pr%C3%A1cticas/3%C2%BA%20Curso/2%C2%BA%20Cuatrimestre/PNLE/Practica%202%20Evaluable/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pip setuptools wheel pip-tools\n",
        "!pip-sync requirements.txt"
      ],
      "metadata": {
        "id": "ntOn9Cz3txE-"
      },
      "id": "ntOn9Cz3txE-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-keras hf_xet transformers[torch]"
      ],
      "metadata": {
        "id": "_fdjSHM3wv43",
        "outputId": "7473021b-3a33-4949-d3a9-b7c103d38180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_fdjSHM3wv43",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b60c868",
      "metadata": {
        "id": "4b60c868"
      },
      "source": [
        "# 1. Importaciones y configuración global\n",
        "\n",
        "**Descripción**\n",
        "\n",
        "Se importan las librerías necesarias para:\n",
        "- Operaciones con archivos, JSON y expresiones regulares.\n",
        "- Acceso a la API de Reddit (`praw`).\n",
        "- Procesamiento numérico (`numpy`).\n",
        "- Manejo de fechas.\n",
        "- Modelos y utilidades de `scikit-learn`, `gensim`, `Hugging Face` y `SentenceTransformers`.\n",
        "- Stopwords y stemmer con `nltk`.\n",
        "Además, se definen subreddits, número de hilos/comentarios a extraer, y se crea la carpeta `data` para guardar los JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8c8d43",
      "metadata": {
        "id": "cf8c8d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import praw\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# FastText embeddings\n",
        "from gensim.models import FastText\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# SBERT y similitud\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# NLTK para limpieza léxica\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Configuración básica\n",
        "SUBREDDITS = ['technology', 'programming', 'machinelearning',\n",
        "              'datascience', 'computerscience', 'gadgets']\n",
        "THREADS_PER_SUB = 20\n",
        "COMMENTS_PER_THREAD = 50\n",
        "JSON_DIR = 'data'\n",
        "os.makedirs(JSON_DIR, exist_ok=True)\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
        "STEMMER = SnowballStemmer('spanish')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "reddit = praw.Reddit(\n",
        "    client_id='ShOBXaW1U-PMc1hhr88znw',\n",
        "    client_secret='o12KLkUR18D5wZqnPxG5lp8jQFszgg',\n",
        "    user_agent='pln-practica-2025',\n",
        "    check_for_async=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc3d608",
      "metadata": {
        "id": "6cc3d608"
      },
      "source": [
        "# 2. Funciones de preprocesamiento de texto\n",
        "## 2.1 Conversión de timestamps\n",
        "\n",
        "Convierte segundos UNIX a formato legible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6299aefd",
      "metadata": {
        "id": "6299aefd"
      },
      "outputs": [],
      "source": [
        "def convertir_fecha(utc_timestamp):\n",
        "    \"\"\"\n",
        "    Convierte un timestamp UNIX a 'YYYY-MM-DD HH:MM:SS'.\n",
        "    \"\"\"\n",
        "    return datetime.fromtimestamp(utc_timestamp).strftime('%Y-%m-%d %H:%M:%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a286bf08",
      "metadata": {
        "id": "a286bf08"
      },
      "source": [
        "## 2.2 Limpieza léxica\n",
        "- Elimina URLs, meciones y enlaces Markdown.\n",
        "- Tokeniza, filtra stopwords y tokens muy cortos.\n",
        "- Aplica stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482a825a",
      "metadata": {
        "id": "482a825a"
      },
      "outputs": [],
      "source": [
        "LEXICAL_PATTERN = re.compile(r\"http\\S+|www\\.\\S+|\\[.*?\\]\\(.*?\\)|@[A-Za-z0-9_]+\")\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "    - Elimina URLs, menciones y markdown.\n",
        "    - Tokeniza en palabras, pasa a minúsculas.\n",
        "    - Filtra stopwords y tokens < 3 caracteres.\n",
        "    - Aplica stemming.\n",
        "    \"\"\"\n",
        "    texto_limpio = LEXICAL_PATTERN.sub('', texto)\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", texto_limpio.lower())\n",
        "    procesados = [\n",
        "        STEMMER.stem(tok)\n",
        "        for tok in tokens\n",
        "        if tok not in STOPWORDS and len(tok) > 2\n",
        "    ]\n",
        "    return \" \".join(procesados)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f2b58c",
      "metadata": {
        "id": "e0f2b58c"
      },
      "source": [
        "# 3. Extracción y guardado del corpus\n",
        "Recorre cada subreddit, extrae los hilos \"hot\" y hasta 50 comentarios por hilo, los preprocesa y guarda un JSON por subreddit en `data/corpus_<sr>.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6f5d05",
      "metadata": {
        "id": "ae6f5d05"
      },
      "outputs": [],
      "source": [
        "def extraer_y_guardar_corpus():\n",
        "    corpus = {}\n",
        "    for sr in SUBREDDITS:\n",
        "        hilos = []\n",
        "        for post in reddit.subreddit(sr).hot(limit=THREADS_PER_SUB):\n",
        "            hilo = {\n",
        "                'title': post.title,\n",
        "                'flair': post.link_flair_text,\n",
        "                'author': str(post.author),\n",
        "                'date': convertir_fecha(post.created_utc),\n",
        "                'score': post.score,\n",
        "                'description': limpiar_texto(post.selftext),\n",
        "                'comments': []\n",
        "            }\n",
        "            post.comments.replace_more(limit=0)\n",
        "            for c in post.comments.list()[:COMMENTS_PER_THREAD]:\n",
        "                hilo['comments'].append({\n",
        "                    'user': str(c.author),\n",
        "                    'comment': limpiar_texto(c.body),\n",
        "                    'score': c.score,\n",
        "                    'date': convertir_fecha(c.created_utc)\n",
        "                })\n",
        "            hilos.append(hilo)\n",
        "        ruta = os.path.join(JSON_DIR, f'corpus_{sr}.json')\n",
        "        with open(ruta, 'w', encoding='utf-8') as f:\n",
        "            json.dump(hilos, f, ensure_ascii=False, indent=4)\n",
        "        corpus[sr] = hilos\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e8cabb",
      "metadata": {
        "id": "d4e8cabb"
      },
      "source": [
        "# 4. Prepraración de datos y clasificación\n",
        "## 4.1 Separación por hilo\n",
        "Evita fugas de información: 14 hilos para entrenamiento y 6 para validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4c860f",
      "metadata": {
        "id": "1c4c860f"
      },
      "outputs": [],
      "source": [
        "def split_by_thread(corpus):\n",
        "    X_train, y_train, X_val, y_val = [], [], [], []\n",
        "    for sr, threads in corpus.items():\n",
        "        train_threads, val_threads = threads[:14], threads[14:]\n",
        "        for th in train_threads:\n",
        "            for c in th['comments']:\n",
        "                X_train.append(c['comment']); y_train.append(sr)\n",
        "        for th in val_threads:\n",
        "            for c in th['comments']:\n",
        "                X_val.append(c['comment']); y_val.append(sr)\n",
        "    return X_train, y_train, X_val, y_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75b7a97",
      "metadata": {
        "id": "e75b7a97"
      },
      "source": [
        "## 4.2 Modelos baseline\n",
        "1. **TF-IDF + RandomForest**\n",
        "2. **FastText embeddings + SVM lineal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9d6cd6",
      "metadata": {
        "id": "0e9d6cd6"
      },
      "outputs": [],
      "source": [
        "def train_baselines(X_train, y_train, X_val, y_val):\n",
        "    # TF-IDF + Random Forest\n",
        "    vec = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "    X_tr_vec = vec.fit_transform(X_train)\n",
        "    X_val_vec = vec.transform(X_val)\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "    rf.fit(X_tr_vec, y_train)\n",
        "    preds_rf = rf.predict(X_val_vec)\n",
        "    print(\"--- TF-IDF + Random Forest ---\")\n",
        "    print(classification_report(y_val, preds_rf))\n",
        "    print(confusion_matrix(y_val, preds_rf))\n",
        "\n",
        "    # FastText + SVM\n",
        "    ft_model = FastText(\n",
        "        sentences=[doc.split() for doc in X_train],\n",
        "        vector_size=100, window=5, min_count=2, workers=4\n",
        "    )\n",
        "    def embed_docs(docs):\n",
        "        embs = []\n",
        "        for doc in docs:\n",
        "            vecs = [ft_model.wv[w] for w in doc.split() if w in ft_model.wv]\n",
        "            embs.append(np.mean(vecs, axis=0) if vecs else np.zeros(100))\n",
        "        return np.vstack(embs)\n",
        "\n",
        "    X_tr_ft = embed_docs(X_train)\n",
        "    X_val_ft = embed_docs(X_val)\n",
        "    svm = SVC(kernel='linear', probability=True)\n",
        "    svm.fit(X_tr_ft, y_train)\n",
        "    preds_svm = svm.predict(X_val_ft)\n",
        "    print(\"--- FastText + SVM ---\")\n",
        "    print(classification_report(y_val, preds_svm))\n",
        "    print(confusion_matrix(y_val, preds_svm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "116548db",
      "metadata": {
        "id": "116548db"
      },
      "source": [
        "## 4.3 Fine-tuning con Transformers\n",
        "\n",
        "Utiliza BERT multilingüe para clasificación. Carga el modelo pre-entrenado si existe, si no lo entrena y guarda en `finetune/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79edf883",
      "metadata": {
        "id": "79edf883"
      },
      "outputs": [],
      "source": [
        "def train_transformer(X_train, y_train, X_val, y_val):\n",
        "    from pathlib import Path\n",
        "    finetune_dir = 'finetune'\n",
        "    if Path(finetune_dir).exists():\n",
        "        print(\"Cargando modelo ya entrenado…\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(finetune_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(finetune_dir)\n",
        "    else:\n",
        "        print(\"Entrenando modelo desde cero…\")\n",
        "        datos = {'text': X_train + X_val, 'label': y_train + y_val}\n",
        "        ds = Dataset.from_dict(datos).class_encode_column('label')\n",
        "        train_ds, val_ds = ds.train_test_split(\n",
        "            test_size=len(X_val)/len(ds)\n",
        "        ).values()\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "        def tokenize_fn(batch):\n",
        "            return tokenizer(\n",
        "                batch['text'], padding='max_length',\n",
        "                truncation=True, max_length=128\n",
        "            )\n",
        "        train_ds = train_ds.map(tokenize_fn, batched=True) \\\n",
        "                         .rename_column('label','labels')\n",
        "        val_ds   = val_ds.map(tokenize_fn, batched=True)   \\\n",
        "                         .rename_column('label','labels')\n",
        "\n",
        "        collator = DataCollatorWithPadding(tokenizer)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            'bert-base-multilingual-uncased',\n",
        "            num_labels=len(set(y_train))\n",
        "        )\n",
        "        args = TrainingArguments(\n",
        "            output_dir=finetune_dir, num_train_epochs=3,\n",
        "            per_device_train_batch_size=16, per_gpu_eval_batch_size=16,\n",
        "            eval_strategy='epoch', save_strategy='epoch',\n",
        "            load_best_model_at_end=True\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=model, args=args,\n",
        "            train_dataset=train_ds, eval_dataset=val_ds,\n",
        "            tokenizer=tokenizer, data_collator=collator\n",
        "        )\n",
        "        trainer.train()\n",
        "        model.save_pretrained(finetune_dir)\n",
        "        tokenizer.save_pretrained(finetune_dir)\n",
        "\n",
        "    # Evaluación\n",
        "    datos = {'text': X_val, 'label': y_val}\n",
        "    val_ds = Dataset.from_dict(datos).class_encode_column('label')\n",
        "    val_ds = val_ds.map(\n",
        "        lambda b: tokenizer(\n",
        "            b['text'], padding='max_length',\n",
        "            truncation=True, max_length=128\n",
        "        ), batched=True\n",
        "    ).rename_column('label','labels')\n",
        "    res = Trainer(model=model).predict(val_ds)\n",
        "    preds = np.argmax(res.predictions, axis=1)\n",
        "    print('--- BERT Fine-Tuning ---')\n",
        "    print(classification_report(val_ds['labels'], preds))\n",
        "    print(confusion_matrix(val_ds['labels'], preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c399c8",
      "metadata": {
        "id": "f6c399c8"
      },
      "source": [
        "# 5. Similitud de hilos\n",
        "## 5.1 FastText\n",
        "Embedding promedio de comentarios por cada hilo y similitud coseno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab87e0ea",
      "metadata": {
        "id": "ab87e0ea"
      },
      "outputs": [],
      "source": [
        "def buscar_hilos_similares_fasttext(corpus, top_k=5):\n",
        "    ids, vectors = [], []\n",
        "    ft = FastText(\n",
        "        sentences=[c['comment'].split()\n",
        "                   for threads in corpus.values()\n",
        "                   for c in threads['comments']],\n",
        "        vector_size=100, window=5, min_count=2, workers=4\n",
        "    )\n",
        "    for sr, threads in corpus.items():\n",
        "        for idx, hilo in enumerate(threads):\n",
        "            ids.append((sr, idx))\n",
        "            doc_vecs = [\n",
        "                ft.wv[w]\n",
        "                for c in hilo['comments']\n",
        "                for w in c['comment'].split()\n",
        "                if w in ft.wv\n",
        "            ]\n",
        "            vectors.append(np.mean(doc_vecs, axis=0) if doc_vecs else np.zeros(100))\n",
        "    sims = cosine_similarity(vectors)\n",
        "    similares = {\n",
        "        ids[i]: [\n",
        "            (ids[j], float(sims[i][j]))\n",
        "            for j in np.argsort(sims[i])[-top_k-1:-1][::-1]\n",
        "        ]\n",
        "        for i in range(len(ids))\n",
        "    }\n",
        "    return similares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d2a00f",
      "metadata": {
        "id": "20d2a00f"
      },
      "source": [
        "## 5.2 SBERT\n",
        "Embeddings de título + comentarios con SentenceTransformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7898b9c7",
      "metadata": {
        "id": "7898b9c7"
      },
      "outputs": [],
      "source": [
        "def buscar_hilos_similares_sbert(corpus, model_name='all-MiniLM-L6-v2', top_k=5):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    ids, texts = [], []\n",
        "    for sr, threads in corpus.items():\n",
        "        for idx, hilo in enumerate(threads):\n",
        "            ids.append((sr, idx))\n",
        "            combined = hilo['title'] + ' ' + ' '.join(\n",
        "                c['comment'] for c in hilo['comments']\n",
        "            )\n",
        "            texts.append(combined)\n",
        "    embs = model.encode(texts)\n",
        "    sims = cosine_similarity(embs)\n",
        "    similares = {\n",
        "        ids[i]: [\n",
        "            (ids[j], float(sims[i][j]))\n",
        "            for j in np.argsort(sims[i])[-top_k-1:-1][::-1]\n",
        "        ]\n",
        "        for i in range(len(ids))\n",
        "    }\n",
        "    return similares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5093bc",
      "metadata": {
        "id": "ea5093bc"
      },
      "source": [
        "# 6. Análisis de sentimiento y resumen automático\n",
        "## 6.1 Sentimiento y emoción\n",
        "Pipelines de Hugging Face para sentimiento (`finiteautomata/beto-sentiment-analysis`) y emoción (`pysentimiento/robertuito-emotion-analysis`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af0c609",
      "metadata": {
        "id": "5af0c609"
      },
      "outputs": [],
      "source": [
        "def analisis_sentimiento(corpus):\n",
        "    sent_pipe = pipeline(\n",
        "        'sentiment-analysis',\n",
        "        model='finiteautomata/beto-sentiment-analysis',\n",
        "        truncation=True, max_length=128\n",
        "    )\n",
        "    emo_pipe = pipeline(\n",
        "        'text-classification',\n",
        "        model='pysentimiento/robertuito-emotion-analysis',\n",
        "        return_all_scores=True,\n",
        "        truncation=True, max_length=128\n",
        "    )\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            for c in hilo['comments']:\n",
        "                text = c['comment'][:512]\n",
        "                s = sent_pipe(text)[0]\n",
        "                e = emo_pipe(text)[0]\n",
        "                c['sentiment'] = s['label']\n",
        "                c['sentiment_score'] = s['score']\n",
        "                c['emotion'] = {item['label']: item['score'] for item in e}\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e7659f",
      "metadata": {
        "id": "80e7659f"
      },
      "source": [
        "## 6.2 Resumen preentrenado y zero-shot\n",
        "- **mT5 multilingual XLSum**\n",
        "- **Flan-T5 small**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e60c2a3",
      "metadata": {
        "id": "0e60c2a3"
      },
      "outputs": [],
      "source": [
        "def resumen_preentrenado(corpus, model_name='csebuetnlp/mT5_multilingual_XLSum'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            inp = hilo['title'] + \": \" + hilo['description']\n",
        "            tokens = tokenizer(inp, return_tensors='pt',\n",
        "                               truncation=True, max_length=512)\n",
        "            out = model.generate(**tokens, max_length=100, num_beams=4)\n",
        "            hilo['summary_pretrained'] = tokenizer.decode(\n",
        "                out[0], skip_special_tokens=True\n",
        "            )\n",
        "    return corpus\n",
        "\n",
        "def resumen_zero_shot(corpus, model_name='google/flan-t5-small'):\n",
        "    zsl_pipe = pipeline('text2text-generation', model=model_name)\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            prompt = f\"Resume: {hilo['title']}. {hilo['description']}\"\n",
        "            gen = zsl_pipe(prompt, max_length=100)[0]\n",
        "            hilo['summary_zero_shot'] = gen['generated_text']\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8590f4fe",
      "metadata": {
        "id": "8590f4fe"
      },
      "source": [
        "# 7. Detección de contenido inapropiado\n",
        "## 7.1 Zero-shot + Chain-of-thought\n",
        "Utiliza BART-MNLI para clasificación y Flan-T5 para explicar razonamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c8f22f",
      "metadata": {
        "id": "12c8f22f"
      },
      "outputs": [],
      "source": [
        "def deteccion_inapropiado(corpus):\n",
        "    zsl_cls = pipeline('zero-shot-classification',\n",
        "                       model='facebook/bart-large-mnli')\n",
        "    cot_pipe = pipeline('text2text-generation',\n",
        "                        model='google/flan-t5-small')\n",
        "    labels = ['apropiado', 'inapropiado']\n",
        "    for threads in corpus.values():\n",
        "        for hilo in threads:\n",
        "            for c in hilo['comments']:\n",
        "                res = zsl_cls(c['comment'], labels)\n",
        "                c['zs_label'] = res['labels'][0]\n",
        "                c['zs_score'] = res['scores'][0]\n",
        "                prompt = (\n",
        "                    f\"Evalúa si este comentario contiene lenguaje inapropiado. \"\n",
        "                    f\"Primero explica tu razonamiento y luego clasifica. \"\n",
        "                    f\"Comentario: {c['comment']}\"\n",
        "                )\n",
        "                c['cot_output'] = cot_pipe(prompt, max_new_tokens=50)[0]['generated_text']\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6e8674",
      "metadata": {
        "id": "ca6e8674"
      },
      "source": [
        "## 7.2 Few-Shot para r/OpinionesPolemicas\n",
        "Inyecta ejemplos manuales en el prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494b67ff",
      "metadata": {
        "id": "494b67ff"
      },
      "outputs": [],
      "source": [
        "FEW_SHOT_EXAMPLES = [\n",
        "    ('Este comentario es ofensivo y soez','inapropiado'),\n",
        "    ('¡Me encanta esta publicación!','apropiado'),\n",
        "    ('Qué horror, no soporto esto.','apropiado'),\n",
        "]\n",
        "\n",
        "def deteccion_inapropiado_fsl(corpus):\n",
        "    zsl_pipe = pipeline('zero-shot-classification',\n",
        "                       model='facebook/bart-large-mnli')\n",
        "    prompt_fsl = 'Clasifica como apropiado o inapropiado:\\n' + \"\".join([\n",
        "        f'Ejemplo: {ex[0]} -> {ex[1]}.\\n' for ex in FEW_SHOT_EXAMPLES\n",
        "    ])\n",
        "    results = {}\n",
        "    for sr, threads in corpus.items():\n",
        "        if sr != 'OpinionesPolemicas':\n",
        "            continue\n",
        "        for idx, hilo in enumerate(threads[:10]):\n",
        "            for c in hilo['comments']:\n",
        "                text = f\"{prompt_fsl}Comentario: {c['comment']} ->\"\n",
        "                r = zsl_pipe(text, candidate_labels=['apropiado','inapropiado'])\n",
        "                results[(sr, idx, c['date'])] = {\n",
        "                    'label_zsl': r['labels'][0],\n",
        "                    'score': r['scores'][0]\n",
        "                }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5ad74e",
      "metadata": {
        "id": "9e5ad74e"
      },
      "source": [
        "# 8. Ejecución del código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065e18ae",
      "metadata": {
        "id": "065e18ae"
      },
      "outputs": [],
      "source": [
        "# 1) Extracción\n",
        "corpus = extraer_y_guardar_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e43a3c",
      "metadata": {
        "id": "e6e43a3c"
      },
      "outputs": [],
      "source": [
        "# 2) Clasificación\n",
        "X_train, y_train, X_val, y_val = split_by_thread(corpus)\n",
        "train_baselines(X_train, y_train, X_val, y_val)\n",
        "train_transformer(X_train, y_train, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec4a9bdf",
      "metadata": {
        "id": "ec4a9bdf"
      },
      "outputs": [],
      "source": [
        "# 3) Similitud\n",
        "sims_ft = buscar_hilos_similares_fasttext(corpus)\n",
        "sims_sbert = buscar_hilos_similares_sbert(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51b74d0",
      "metadata": {
        "id": "d51b74d0"
      },
      "outputs": [],
      "source": [
        "# 4) Sentimiento y emoción\n",
        "corpus = analisis_sentimiento(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ad0320",
      "metadata": {
        "id": "65ad0320"
      },
      "outputs": [],
      "source": [
        "# 5) Resúmenes\n",
        "corpus = resumen_preentrenado(corpus)\n",
        "corpus = resumen_zero_shot(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635dd385",
      "metadata": {
        "id": "635dd385"
      },
      "outputs": [],
      "source": [
        "# 6) Detección inapropiado\n",
        "corpus = deteccion_inapropiado(corpus)\n",
        "inap_fsl = deteccion_inapropiado_fsl(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd8b0be",
      "metadata": {
        "id": "8bd8b0be"
      },
      "outputs": [],
      "source": [
        "# Guardar análisis extra\n",
        "with open(os.path.join(JSON_DIR,'analysis_extras.json'),\n",
        "          'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        'sims_ft': sims_ft,\n",
        "        'sims_sbert': sims_sbert,\n",
        "        'inap_fsl': inap_fsl\n",
        "    }, f, ensure_ascii=False, indent=4)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}